{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komputer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "main_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(main_dir)\n",
    "\n",
    "import model_classes\n",
    "from model_classes import *\n",
    "from MH_Lori import *\n",
    "from MH_MoE import *\n",
    "from dataloader import *\n",
    "import dataloader\n",
    "from helper_functions import *\n",
    "import torch\n",
    "from transformers import PretrainedConfig\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy\n",
    "#import lightning.pytorch as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pandas as pd\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_small= PretrainedConfig(\n",
    "    num_experts_per_token=4,\n",
    "    hidden_size=256,\n",
    "    num_attention_heads = 4,\n",
    "    num_MH_MOE_heads = 4,\n",
    "    num_experts=8,\n",
    "    batch_size = 20,\n",
    "    seq_len = 512,\n",
    "    capacity_factor = 3,\n",
    "    device = device,\n",
    "    intermediate_size = 512,\n",
    "    forward_layer_class = VectorizedMoE,\n",
    "    vocab_size = 30522,\n",
    "    n_layers = 12,\n",
    "    no_lori_segments = 32,\n",
    "    py_lightning_loging = False,\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(),\n",
    "    lr = 0.001, #SET TO 0.0002\n",
    "    betas = (0.9, 0.98),\n",
    "    treat_mh_lori_as_regular_lori = True,\n",
    "    load_balancing_coefficient=0.01,\n",
    "    proportions = [0, 0.8, 0.1, 0.1] # null, train, validation, test\n",
    ")\n",
    "\n",
    "config = config_small\n",
    "\n",
    "#training hiperparams\n",
    "save_every_n_baches = 500 #how often do you wish to save the model\n",
    "epochs = 10\n",
    "\n",
    "#path to folders where you want to save model checkpoints and val and train logs\n",
    "model_saving_path = 'D:/Projekt_NLP/Saved_stuff/saved_models'\n",
    "log_saving_path = 'D:/Projekt_NLP/Saved_stuff/logs/vectorized_moe'\n",
    "\n",
    "model_name='Normal_moe_model' #name of the model in saving logs\n",
    "saving_filename = 'Vectorised_MOE_175M-{epoch}-{step}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 513])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of usage:\n",
    "l = give_dataloaders(batch_size = config.batch_size, seq_len = config.seq_len + 1, proportions = config.proportions)\n",
    "train_dataloader = l[\"train_dataloader\"]\n",
    "val_dataloader = l[\"val_dataloader\"]\n",
    "test_dataloader = l[\"test_dataloader\"]\n",
    "sample = next(iter(train_dataloader))\n",
    "# print(sample)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Model Size: 167.79 MB, total number of parameters: 175,938,792\n",
      "Estimated Model Size: 108.06 MB, total number of parameters: 113,307,648\n",
      "Estimated Model Size: 8.00 MB, total number of parameters: 8,388,608\n",
      "Total GPU memory: 12.8843776 GB\n",
      "Reserved GPU memory: 0.182452224 GB\n",
      "Allocated GPU memory: 0.17644288 GB\n",
      "Free GPU memory: 0.006009344 GB\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(config_small).to(config_small.device)\n",
    "estimate_model_size(model)\n",
    "estimate_model_size(model.layers)\n",
    "estimate_model_size(model.layers[0].forward_layer)\n",
    "get_gpu_memory()\n",
    "print(isinstance(model, LightningModule))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute this to make shure all parameters are registerd properly\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter name: {name}, Parameter shape: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | loss_fn    | CrossEntropyLoss | 0     \n",
      "1 | embedding  | Embedding        | 7.8 M \n",
      "2 | layers     | ModuleList       | 28.3 M\n",
      "3 | final_proj | Linear           | 7.8 M \n",
      "------------------------------------------------\n",
      "44.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "44.0 M    Total params\n",
      "175.939   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komputer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]   VALIDATION: Batch 0, loss 10.483537673950195\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  2.11it/s]   VALIDATION: Batch 1, loss 10.502656936645508\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komputer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 0/? [00:00<?, ?it/s]    TRRAINING: Batch 0, loss 10.508197784423828\n",
      "Epoch 0: |          | 1/? [00:01<00:00,  0.99it/s, v_num=30]   TRRAINING: Batch 1, loss 10.082255363464355\n",
      "Epoch 0: |          | 2/? [00:01<00:00,  1.04it/s, v_num=30]   TRRAINING: Batch 2, loss 9.035494804382324\n",
      "Epoch 0: |          | 3/? [00:02<00:00,  1.06it/s, v_num=30]   TRRAINING: Batch 3, loss 9.248161315917969\n",
      "Epoch 0: |          | 4/? [00:03<00:00,  1.08it/s, v_num=30]   TRRAINING: Batch 4, loss 8.656417846679688\n",
      "Epoch 0: |          | 5/? [00:04<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 5, loss 8.846236228942871\n",
      "Epoch 0: |          | 6/? [00:05<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 6, loss 8.788739204406738\n",
      "Epoch 0: |          | 7/? [00:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 7, loss 8.476215362548828\n",
      "Epoch 0: |          | 8/? [00:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 8, loss 8.15264892578125\n",
      "Epoch 0: |          | 9/? [00:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 9, loss 7.96518611907959\n",
      "Epoch 0: |          | 10/? [00:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 10, loss 7.981219291687012\n",
      "Epoch 0: |          | 11/? [00:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 11, loss 7.664726257324219\n",
      "Epoch 0: |          | 12/? [00:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 12, loss 8.756779670715332\n",
      "Epoch 0: |          | 13/? [00:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 13, loss 7.772881984710693\n",
      "Epoch 0: |          | 14/? [00:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 14, loss 7.79608154296875\n",
      "Epoch 0: |          | 15/? [00:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 15, loss 7.762467861175537\n",
      "Epoch 0: |          | 16/? [00:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 16, loss 7.254389762878418\n",
      "Epoch 0: |          | 17/? [00:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 17, loss 7.849587917327881\n",
      "Epoch 0: |          | 18/? [00:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 18, loss 7.53448486328125\n",
      "Epoch 0: |          | 19/? [00:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 19, loss 7.1606340408325195\n",
      "Epoch 0: |          | 20/? [00:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 20, loss 7.450486660003662\n",
      "Epoch 0: |          | 21/? [00:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 21, loss 7.465909004211426\n",
      "Epoch 0: |          | 22/? [00:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 22, loss 7.37868595123291\n",
      "Epoch 0: |          | 23/? [00:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 23, loss 6.817311763763428\n",
      "Epoch 0: |          | 24/? [00:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 24, loss 7.2645440101623535\n",
      "Epoch 0: |          | 25/? [00:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 25, loss 7.276658535003662\n",
      "Epoch 0: |          | 26/? [00:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 26, loss 7.192057132720947\n",
      "Epoch 0: |          | 27/? [00:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 27, loss 7.167754173278809\n",
      "Epoch 0: |          | 28/? [00:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 28, loss 7.7360944747924805\n",
      "Epoch 0: |          | 29/? [00:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 29, loss 7.251797676086426\n",
      "Epoch 0: |          | 30/? [00:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 30, loss 7.109464168548584\n",
      "Epoch 0: |          | 31/? [00:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 31, loss 7.604310035705566\n",
      "Epoch 0: |          | 32/? [00:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 32, loss 7.105931758880615\n",
      "Epoch 0: |          | 33/? [00:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 33, loss 7.124337196350098\n",
      "Epoch 0: |          | 34/? [00:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 34, loss 7.034632205963135\n",
      "Epoch 0: |          | 35/? [00:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 35, loss 6.157717704772949\n",
      "Epoch 0: |          | 36/? [00:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 36, loss 7.41326379776001\n",
      "Epoch 0: |          | 37/? [00:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 37, loss 7.211265563964844\n",
      "Epoch 0: |          | 38/? [00:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 38, loss 7.4043073654174805\n",
      "Epoch 0: |          | 39/? [00:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 39, loss 7.177266597747803\n",
      "Epoch 0: |          | 40/? [00:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 40, loss 6.732626914978027\n",
      "Epoch 0: |          | 41/? [00:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 41, loss 6.950361728668213\n",
      "Epoch 0: |          | 42/? [00:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 42, loss 7.008089542388916\n",
      "Epoch 0: |          | 43/? [00:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 43, loss 6.728946685791016\n",
      "Epoch 0: |          | 44/? [00:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 44, loss 6.471684455871582\n",
      "Epoch 0: |          | 45/? [00:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 45, loss 6.713799953460693\n",
      "Epoch 0: |          | 46/? [00:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 46, loss 7.525759220123291\n",
      "Epoch 0: |          | 47/? [00:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 47, loss 7.027349948883057\n",
      "Epoch 0: |          | 48/? [00:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 48, loss 7.090670108795166\n",
      "Epoch 0: |          | 49/? [00:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 49, loss 6.760639190673828\n",
      "Epoch 0: |          | 50/? [00:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 50, loss 6.711191654205322\n",
      "Epoch 0: |          | 51/? [00:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 51, loss 7.067437648773193\n",
      "Epoch 0: |          | 52/? [00:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 52, loss 6.9738640785217285\n",
      "Epoch 0: |          | 53/? [00:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 53, loss 6.525794982910156\n",
      "Epoch 0: |          | 54/? [00:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 54, loss 6.720350742340088\n",
      "Epoch 0: |          | 55/? [00:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 55, loss 6.800471305847168\n",
      "Epoch 0: |          | 56/? [00:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 56, loss 6.717338562011719\n",
      "Epoch 0: |          | 57/? [00:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 57, loss 6.802865028381348\n",
      "Epoch 0: |          | 58/? [00:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 58, loss 8.02166748046875\n",
      "Epoch 0: |          | 59/? [00:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 59, loss 7.177406311035156\n",
      "Epoch 0: |          | 60/? [00:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 60, loss 7.087682247161865\n",
      "Epoch 0: |          | 61/? [00:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 61, loss 7.018270969390869\n",
      "Epoch 0: |          | 62/? [00:51<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 62, loss 6.4839653968811035\n",
      "Epoch 0: |          | 63/? [00:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 63, loss 6.7876296043396\n",
      "Epoch 0: |          | 64/? [00:53<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 64, loss 6.7778000831604\n",
      "Epoch 0: |          | 65/? [00:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 65, loss 6.865878105163574\n",
      "Epoch 0: |          | 66/? [00:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 66, loss 6.686295986175537\n",
      "Epoch 0: |          | 67/? [00:55<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 67, loss 6.721011161804199\n",
      "Epoch 0: |          | 68/? [00:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 68, loss 7.030735969543457\n",
      "Epoch 0: |          | 69/? [00:57<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 69, loss 6.750662803649902\n",
      "Epoch 0: |          | 70/? [00:58<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 70, loss 6.501544952392578\n",
      "Epoch 0: |          | 71/? [00:59<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 71, loss 6.153887748718262\n",
      "Epoch 0: |          | 72/? [00:59<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 72, loss 6.7974853515625\n",
      "Epoch 0: |          | 73/? [01:00<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 73, loss 7.024542331695557\n",
      "Epoch 0: |          | 74/? [01:01<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 74, loss 6.636587619781494\n",
      "Epoch 0: |          | 75/? [01:02<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 75, loss 6.685127258300781\n",
      "Epoch 0: |          | 76/? [01:02<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 76, loss 6.760648250579834\n",
      "Epoch 0: |          | 77/? [01:03<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 77, loss 6.716292381286621\n",
      "Epoch 0: |          | 78/? [01:04<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 78, loss 6.438317775726318\n",
      "Epoch 0: |          | 79/? [01:05<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 79, loss 6.402099609375\n",
      "Epoch 0: |          | 80/? [01:06<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 80, loss 6.3582048416137695\n",
      "Epoch 0: |          | 81/? [01:06<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 81, loss 6.269822120666504\n",
      "Epoch 0: |          | 82/? [01:07<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 82, loss 7.003969669342041\n",
      "Epoch 0: |          | 83/? [01:08<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 83, loss 6.148567199707031\n",
      "Epoch 0: |          | 84/? [01:09<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 84, loss 6.167891025543213\n",
      "Epoch 0: |          | 85/? [01:10<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 85, loss 6.225176811218262\n",
      "Epoch 0: |          | 86/? [01:11<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 86, loss 5.975375175476074\n",
      "Epoch 0: |          | 87/? [01:12<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 87, loss 6.273975372314453\n",
      "Epoch 0: |          | 88/? [01:12<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 88, loss 7.096259117126465\n",
      "Epoch 0: |          | 89/? [01:13<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 89, loss 6.752864837646484\n",
      "Epoch 0: |          | 90/? [01:14<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 90, loss 6.6097846031188965\n",
      "Epoch 0: |          | 91/? [01:15<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 91, loss 6.906783103942871\n",
      "Epoch 0: |          | 92/? [01:16<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 92, loss 7.165170192718506\n",
      "Epoch 0: |          | 93/? [01:17<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 93, loss 6.834191799163818\n",
      "Epoch 0: |          | 94/? [01:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 94, loss 6.894183158874512\n",
      "Epoch 0: |          | 95/? [01:18<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 95, loss 7.821335792541504\n",
      "Epoch 0: |          | 96/? [01:19<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 96, loss 6.29082727432251\n",
      "Epoch 0: |          | 97/? [01:20<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 97, loss 6.409989833831787\n",
      "Epoch 0: |          | 98/? [01:21<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 98, loss 6.675455570220947\n",
      "Epoch 0: |          | 99/? [01:21<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 99, loss 6.666255950927734\n",
      "Epoch 0: |          | 100/? [01:22<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 100, loss 6.776808261871338\n",
      "Epoch 0: |          | 101/? [01:23<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 101, loss 6.499902248382568\n",
      "Epoch 0: |          | 102/? [01:24<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 102, loss 6.583159446716309\n",
      "Epoch 0: |          | 103/? [01:25<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 103, loss 6.211285591125488\n",
      "Epoch 0: |          | 104/? [01:26<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 104, loss 6.603228569030762\n",
      "Epoch 0: |          | 105/? [01:26<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 105, loss 6.558821201324463\n",
      "Epoch 0: |          | 106/? [01:27<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 106, loss 6.444422245025635\n",
      "Epoch 0: |          | 107/? [01:28<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 107, loss 6.7286505699157715\n",
      "Epoch 0: |          | 108/? [01:29<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 108, loss 6.73852014541626\n",
      "Epoch 0: |          | 109/? [01:29<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 109, loss 5.753487586975098\n",
      "Epoch 0: |          | 110/? [01:30<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 110, loss 6.560904026031494\n",
      "Epoch 0: |          | 111/? [01:31<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 111, loss 6.877739906311035\n",
      "Epoch 0: |          | 112/? [01:32<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 112, loss 6.252599716186523\n",
      "Epoch 0: |          | 113/? [01:33<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 113, loss 5.529427528381348\n",
      "Epoch 0: |          | 114/? [01:34<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 114, loss 6.549729824066162\n",
      "Epoch 0: |          | 115/? [01:35<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 115, loss 6.597156524658203\n",
      "Epoch 0: |          | 116/? [01:35<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 116, loss 6.472556114196777\n",
      "Epoch 0: |          | 117/? [01:36<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 117, loss 6.533596038818359\n",
      "Epoch 0: |          | 118/? [01:37<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 118, loss 6.613687992095947\n",
      "Epoch 0: |          | 119/? [01:38<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 119, loss 6.8048810958862305\n",
      "Epoch 0: |          | 120/? [01:39<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 120, loss 6.484889030456543\n",
      "Epoch 0: |          | 121/? [01:39<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 121, loss 6.312967777252197\n",
      "Epoch 0: |          | 122/? [01:40<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 122, loss 6.115049362182617\n",
      "Epoch 0: |          | 123/? [01:41<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 123, loss 6.2244672775268555\n",
      "Epoch 0: |          | 124/? [01:42<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 124, loss 6.503072261810303\n",
      "Epoch 0: |          | 125/? [01:42<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 125, loss 6.446662902832031\n",
      "Epoch 0: |          | 126/? [01:43<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 126, loss 6.668115139007568\n",
      "Epoch 0: |          | 127/? [01:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 127, loss 6.501393795013428\n",
      "Epoch 0: |          | 128/? [01:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 128, loss 5.716094970703125\n",
      "Epoch 0: |          | 129/? [01:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 129, loss 6.4164719581604\n",
      "Epoch 0: |          | 130/? [01:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 130, loss 5.62555456161499\n",
      "Epoch 0: |          | 131/? [01:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 131, loss 6.32401180267334\n",
      "Epoch 0: |          | 132/? [01:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 132, loss 6.405393123626709\n",
      "Epoch 0: |          | 133/? [01:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 133, loss 6.334227561950684\n",
      "Epoch 0: |          | 134/? [01:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 134, loss 6.149979591369629\n",
      "Epoch 0: |          | 135/? [01:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 135, loss 6.538128852844238\n",
      "Epoch 0: |          | 136/? [01:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 136, loss 6.5048065185546875\n",
      "Epoch 0: |          | 137/? [01:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 137, loss 5.386275291442871\n",
      "Epoch 0: |          | 138/? [01:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 138, loss 6.125616073608398\n",
      "Epoch 0: |          | 139/? [01:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 139, loss 6.7167510986328125\n",
      "Epoch 0: |          | 140/? [01:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 140, loss 6.194840431213379\n",
      "Epoch 0: |          | 141/? [01:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 141, loss 5.966736793518066\n",
      "Epoch 0: |          | 142/? [01:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 142, loss 7.515782833099365\n",
      "Epoch 0: |          | 143/? [01:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 143, loss 7.148024082183838\n",
      "Epoch 0: |          | 144/? [01:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 144, loss 6.0666375160217285\n",
      "Epoch 0: |          | 145/? [01:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 145, loss 6.049836158752441\n",
      "Epoch 0: |          | 146/? [01:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 146, loss 6.203680038452148\n",
      "Epoch 0: |          | 147/? [02:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 147, loss 6.407238006591797\n",
      "Epoch 0: |          | 148/? [02:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 148, loss 6.190789222717285\n",
      "Epoch 0: |          | 149/? [02:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 149, loss 5.409086227416992\n",
      "Epoch 0: |          | 150/? [02:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 150, loss 6.3441996574401855\n",
      "Epoch 0: |          | 151/? [02:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 151, loss 6.390162944793701\n",
      "Epoch 0: |          | 152/? [02:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 152, loss 6.504125118255615\n",
      "Epoch 0: |          | 153/? [02:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 153, loss 5.613409996032715\n",
      "Epoch 0: |          | 154/? [02:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 154, loss 6.634072780609131\n",
      "Epoch 0: |          | 155/? [02:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 155, loss 6.514363765716553\n",
      "Epoch 0: |          | 156/? [02:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 156, loss 5.553805351257324\n",
      "Epoch 0: |          | 157/? [02:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 157, loss 6.353274345397949\n",
      "Epoch 0: |          | 158/? [02:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 158, loss 6.298624038696289\n",
      "Epoch 0: |          | 159/? [02:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 159, loss 6.132051467895508\n",
      "Epoch 0: |          | 160/? [02:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 160, loss 5.7593207359313965\n",
      "Epoch 0: |          | 161/? [02:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 161, loss 6.409396171569824\n",
      "Epoch 0: |          | 162/? [02:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 162, loss 6.570105075836182\n",
      "Epoch 0: |          | 163/? [02:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 163, loss 5.624650001525879\n",
      "Epoch 0: |          | 164/? [02:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 164, loss 6.030155181884766\n",
      "Epoch 0: |          | 165/? [02:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 165, loss 6.693373203277588\n",
      "Epoch 0: |          | 166/? [02:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 166, loss 6.395026206970215\n",
      "Epoch 0: |          | 167/? [02:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 167, loss 6.499029636383057\n",
      "Epoch 0: |          | 168/? [02:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 168, loss 6.12394380569458\n",
      "Epoch 0: |          | 169/? [02:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 169, loss 5.621379852294922\n",
      "Epoch 0: |          | 170/? [02:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 170, loss 6.0154924392700195\n",
      "Epoch 0: |          | 171/? [02:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 171, loss 6.2715582847595215\n",
      "Epoch 0: |          | 172/? [02:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 172, loss 5.961061477661133\n",
      "Epoch 0: |          | 173/? [02:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 173, loss 6.911476135253906\n",
      "Epoch 0: |          | 174/? [02:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 174, loss 6.950402736663818\n",
      "Epoch 0: |          | 175/? [02:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 175, loss 6.7484450340271\n",
      "Epoch 0: |          | 176/? [02:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 176, loss 5.844649314880371\n",
      "Epoch 0: |          | 177/? [02:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 177, loss 6.128169059753418\n",
      "Epoch 0: |          | 178/? [02:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 178, loss 5.942831516265869\n",
      "Epoch 0: |          | 179/? [02:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 179, loss 6.524683952331543\n",
      "Epoch 0: |          | 180/? [02:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 180, loss 6.038578987121582\n",
      "Epoch 0: |          | 181/? [02:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 181, loss 5.752294540405273\n",
      "Epoch 0: |          | 182/? [02:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 182, loss 6.263919353485107\n",
      "Epoch 0: |          | 183/? [02:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 183, loss 5.719512939453125\n",
      "Epoch 0: |          | 184/? [02:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 184, loss 5.789355278015137\n",
      "Epoch 0: |          | 185/? [02:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 185, loss 6.713236331939697\n",
      "Epoch 0: |          | 186/? [02:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 186, loss 6.005843162536621\n",
      "Epoch 0: |          | 187/? [02:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 187, loss 6.548968315124512\n",
      "Epoch 0: |          | 188/? [02:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 188, loss 5.9422502517700195\n",
      "Epoch 0: |          | 189/? [02:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 189, loss 6.549761772155762\n",
      "Epoch 0: |          | 190/? [02:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 190, loss 5.994146823883057\n",
      "Epoch 0: |          | 191/? [02:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 191, loss 6.83990478515625\n",
      "Epoch 0: |          | 192/? [02:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 192, loss 6.7369585037231445\n",
      "Epoch 0: |          | 193/? [02:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 193, loss 5.795897483825684\n",
      "Epoch 0: |          | 194/? [02:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 194, loss 5.760348320007324\n",
      "Epoch 0: |          | 195/? [02:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 195, loss 6.4088287353515625\n",
      "Epoch 0: |          | 196/? [02:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 196, loss 6.399662017822266\n",
      "Epoch 0: |          | 197/? [02:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 197, loss 6.256860733032227\n",
      "Epoch 0: |          | 198/? [02:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 198, loss 5.783848762512207\n",
      "Epoch 0: |          | 199/? [02:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 199, loss 6.210365295410156\n",
      "Epoch 0: |          | 200/? [02:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 200, loss 6.416901588439941\n",
      "Epoch 0: |          | 201/? [02:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 201, loss 6.490237236022949\n",
      "Epoch 0: |          | 202/? [02:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 202, loss 6.244266033172607\n",
      "Epoch 0: |          | 203/? [02:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 203, loss 6.363085746765137\n",
      "Epoch 0: |          | 204/? [02:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 204, loss 6.365777969360352\n",
      "Epoch 0: |          | 205/? [02:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 205, loss 5.7314863204956055\n",
      "Epoch 0: |          | 206/? [02:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 206, loss 5.724129676818848\n",
      "Epoch 0: |          | 207/? [02:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 207, loss 6.2112603187561035\n",
      "Epoch 0: |          | 208/? [02:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 208, loss 6.007048606872559\n",
      "Epoch 0: |          | 209/? [02:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 209, loss 6.024186134338379\n",
      "Epoch 0: |          | 210/? [02:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 210, loss 6.607388496398926\n",
      "Epoch 0: |          | 211/? [02:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 211, loss 5.9110331535339355\n",
      "Epoch 0: |          | 212/? [02:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 212, loss 6.138566970825195\n",
      "Epoch 0: |          | 213/? [02:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 213, loss 5.881956577301025\n",
      "Epoch 0: |          | 214/? [02:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 214, loss 6.073701858520508\n",
      "Epoch 0: |          | 215/? [02:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 215, loss 5.592489242553711\n",
      "Epoch 0: |          | 216/? [02:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 216, loss 6.2889580726623535\n",
      "Epoch 0: |          | 217/? [02:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 217, loss 5.975572109222412\n",
      "Epoch 0: |          | 218/? [02:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 218, loss 6.347977161407471\n",
      "Epoch 0: |          | 219/? [02:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 219, loss 6.17614221572876\n",
      "Epoch 0: |          | 220/? [02:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 220, loss 6.175739288330078\n",
      "Epoch 0: |          | 221/? [03:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 221, loss 5.844816207885742\n",
      "Epoch 0: |          | 222/? [03:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 222, loss 5.195931434631348\n",
      "Epoch 0: |          | 223/? [03:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 223, loss 6.407834053039551\n",
      "Epoch 0: |          | 224/? [03:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 224, loss 6.389720439910889\n",
      "Epoch 0: |          | 225/? [03:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 225, loss 6.168387413024902\n",
      "Epoch 0: |          | 226/? [03:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 226, loss 5.955743312835693\n",
      "Epoch 0: |          | 227/? [03:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 227, loss 6.32739782333374\n",
      "Epoch 0: |          | 228/? [03:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 228, loss 6.0815749168396\n",
      "Epoch 0: |          | 229/? [03:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 229, loss 6.318901538848877\n",
      "Epoch 0: |          | 230/? [03:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 230, loss 5.8988823890686035\n",
      "Epoch 0: |          | 231/? [03:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 231, loss 5.924350738525391\n",
      "Epoch 0: |          | 232/? [03:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 232, loss 5.888279914855957\n",
      "Epoch 0: |          | 233/? [03:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 233, loss 6.609652042388916\n",
      "Epoch 0: |          | 234/? [03:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 234, loss 6.4931230545043945\n",
      "Epoch 0: |          | 235/? [03:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 235, loss 6.420882225036621\n",
      "Epoch 0: |          | 236/? [03:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 236, loss 6.128935813903809\n",
      "Epoch 0: |          | 237/? [03:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 237, loss 6.121934413909912\n",
      "Epoch 0: |          | 238/? [03:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 238, loss 6.233870029449463\n",
      "Epoch 0: |          | 239/? [03:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 239, loss 6.028725624084473\n",
      "Epoch 0: |          | 240/? [03:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 240, loss 5.528563022613525\n",
      "Epoch 0: |          | 241/? [03:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 241, loss 6.172572135925293\n",
      "Epoch 0: |          | 242/? [03:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 242, loss 6.447775363922119\n",
      "Epoch 0: |          | 243/? [03:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 243, loss 5.178183078765869\n",
      "Epoch 0: |          | 244/? [03:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 244, loss 5.859704971313477\n",
      "Epoch 0: |          | 245/? [03:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 245, loss 6.013367176055908\n",
      "Epoch 0: |          | 246/? [03:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 246, loss 6.1981000900268555\n",
      "Epoch 0: |          | 247/? [03:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 247, loss 6.311807155609131\n",
      "Epoch 0: |          | 248/? [03:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 248, loss 5.723635196685791\n",
      "Epoch 0: |          | 249/? [03:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 249, loss 5.347912788391113\n",
      "Epoch 0: |          | 250/? [03:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 250, loss 5.926299095153809\n",
      "Epoch 0: |          | 251/? [03:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 251, loss 6.206784248352051\n",
      "Epoch 0: |          | 252/? [03:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 252, loss 5.935914516448975\n",
      "Epoch 0: |          | 253/? [03:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 253, loss 6.41457986831665\n",
      "Epoch 0: |          | 254/? [03:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 254, loss 6.443110466003418\n",
      "Epoch 0: |          | 255/? [03:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 255, loss 5.952178001403809\n",
      "Epoch 0: |          | 256/? [03:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 256, loss 8.556191444396973\n",
      "Epoch 0: |          | 257/? [03:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 257, loss 5.701922416687012\n",
      "Epoch 0: |          | 258/? [03:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 258, loss 5.975674629211426\n",
      "Epoch 0: |          | 259/? [03:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 259, loss 5.872920989990234\n",
      "Epoch 0: |          | 260/? [03:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 260, loss 5.80913782119751\n",
      "Epoch 0: |          | 261/? [03:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 261, loss 6.050973415374756\n",
      "Epoch 0: |          | 262/? [03:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 262, loss 6.245973587036133\n",
      "Epoch 0: |          | 263/? [03:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 263, loss 5.90825080871582\n",
      "Epoch 0: |          | 264/? [03:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 264, loss 6.170335292816162\n",
      "Epoch 0: |          | 265/? [03:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 265, loss 5.27129602432251\n",
      "Epoch 0: |          | 266/? [03:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 266, loss 5.929932594299316\n",
      "Epoch 0: |          | 267/? [03:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 267, loss 5.458209037780762\n",
      "Epoch 0: |          | 268/? [03:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 268, loss 5.752837181091309\n",
      "Epoch 0: |          | 269/? [03:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 269, loss 6.167655944824219\n",
      "Epoch 0: |          | 270/? [03:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 270, loss 5.779836654663086\n",
      "Epoch 0: |          | 271/? [03:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 271, loss 6.499319553375244\n",
      "Epoch 0: |          | 272/? [03:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 272, loss 6.437932014465332\n",
      "Epoch 0: |          | 273/? [03:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 273, loss 5.477102756500244\n",
      "Epoch 0: |          | 274/? [03:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 274, loss 6.6959123611450195\n",
      "Epoch 0: |          | 275/? [03:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 275, loss 6.081070423126221\n",
      "Epoch 0: |          | 276/? [03:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 276, loss 5.180712699890137\n",
      "Epoch 0: |          | 277/? [03:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 277, loss 5.897528648376465\n",
      "Epoch 0: |          | 278/? [03:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 278, loss 4.892611980438232\n",
      "Epoch 0: |          | 279/? [03:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 279, loss 5.681970596313477\n",
      "Epoch 0: |          | 280/? [03:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 280, loss 5.28255558013916\n",
      "Epoch 0: |          | 281/? [03:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 281, loss 6.10887336730957\n",
      "Epoch 0: |          | 282/? [03:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 282, loss 5.731071472167969\n",
      "Epoch 0: |          | 283/? [03:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 283, loss 5.968132972717285\n",
      "Epoch 0: |          | 284/? [03:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 284, loss 5.7621283531188965\n",
      "Epoch 0: |          | 285/? [03:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 285, loss 5.004292964935303\n",
      "Epoch 0: |          | 286/? [03:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 286, loss 5.753246307373047\n",
      "Epoch 0: |          | 287/? [03:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 287, loss 5.491034984588623\n",
      "Epoch 0: |          | 288/? [03:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 288, loss 5.403726100921631\n",
      "Epoch 0: |          | 289/? [03:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 289, loss 5.699728965759277\n",
      "Epoch 0: |          | 290/? [03:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 290, loss 5.611414909362793\n",
      "Epoch 0: |          | 291/? [03:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 291, loss 5.868304252624512\n",
      "Epoch 0: |          | 292/? [03:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 292, loss 5.4368696212768555\n",
      "Epoch 0: |          | 293/? [03:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 293, loss 6.023932456970215\n",
      "Epoch 0: |          | 294/? [03:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 294, loss 5.619913101196289\n",
      "Epoch 0: |          | 295/? [03:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 295, loss 6.077829837799072\n",
      "Epoch 0: |          | 296/? [04:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 296, loss 5.556387901306152\n",
      "Epoch 0: |          | 297/? [04:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 297, loss 6.207398891448975\n",
      "Epoch 0: |          | 298/? [04:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 298, loss 6.023388385772705\n",
      "Epoch 0: |          | 299/? [04:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 299, loss 6.925135135650635\n",
      "Epoch 0: |          | 300/? [04:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 300, loss 6.08201265335083\n",
      "Epoch 0: |          | 301/? [04:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 301, loss 5.66957950592041\n",
      "Epoch 0: |          | 302/? [04:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 302, loss 6.040026664733887\n",
      "Epoch 0: |          | 303/? [04:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 303, loss 5.7935285568237305\n",
      "Epoch 0: |          | 304/? [04:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 304, loss 6.12518835067749\n",
      "Epoch 0: |          | 305/? [04:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 305, loss 6.244256019592285\n",
      "Epoch 0: |          | 306/? [04:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 306, loss 5.79986572265625\n",
      "Epoch 0: |          | 307/? [04:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 307, loss 6.006124973297119\n",
      "Epoch 0: |          | 308/? [04:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 308, loss 6.221970081329346\n",
      "Epoch 0: |          | 309/? [04:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 309, loss 6.133692741394043\n",
      "Epoch 0: |          | 310/? [04:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 310, loss 6.4643144607543945\n",
      "Epoch 0: |          | 311/? [04:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 311, loss 5.887777805328369\n",
      "Epoch 0: |          | 312/? [04:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 312, loss 6.239095687866211\n",
      "Epoch 0: |          | 313/? [04:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 313, loss 5.482334136962891\n",
      "Epoch 0: |          | 314/? [04:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 314, loss 6.022700309753418\n",
      "Epoch 0: |          | 315/? [04:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 315, loss 5.529486656188965\n",
      "Epoch 0: |          | 316/? [04:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 316, loss 6.297730445861816\n",
      "Epoch 0: |          | 317/? [04:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 317, loss 6.029844760894775\n",
      "Epoch 0: |          | 318/? [04:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 318, loss 6.1909332275390625\n",
      "Epoch 0: |          | 319/? [04:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 319, loss 5.276196479797363\n",
      "Epoch 0: |          | 320/? [04:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 320, loss 5.771982669830322\n",
      "Epoch 0: |          | 321/? [04:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 321, loss 5.479741096496582\n",
      "Epoch 0: |          | 322/? [04:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 322, loss 6.146750450134277\n",
      "Epoch 0: |          | 323/? [04:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 323, loss 6.4196367263793945\n",
      "Epoch 0: |          | 324/? [04:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 324, loss 5.79616641998291\n",
      "Epoch 0: |          | 325/? [04:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 325, loss 6.551453590393066\n",
      "Epoch 0: |          | 326/? [04:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 326, loss 5.852267265319824\n",
      "Epoch 0: |          | 327/? [04:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 327, loss 5.769606590270996\n",
      "Epoch 0: |          | 328/? [04:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 328, loss 5.326419353485107\n",
      "Epoch 0: |          | 329/? [04:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 329, loss 6.011495590209961\n",
      "Epoch 0: |          | 330/? [04:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 330, loss 6.370179176330566\n",
      "Epoch 0: |          | 331/? [04:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 331, loss 4.608878135681152\n",
      "Epoch 0: |          | 332/? [04:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 332, loss 5.783942222595215\n",
      "Epoch 0: |          | 333/? [04:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 333, loss 5.544473171234131\n",
      "Epoch 0: |          | 334/? [04:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 334, loss 6.75076150894165\n",
      "Epoch 0: |          | 335/? [04:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 335, loss 6.834737300872803\n",
      "Epoch 0: |          | 336/? [04:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 336, loss 6.29374885559082\n",
      "Epoch 0: |          | 337/? [04:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 337, loss 6.973551273345947\n",
      "Epoch 0: |          | 338/? [04:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 338, loss 6.526419162750244\n",
      "Epoch 0: |          | 339/? [04:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 339, loss 5.654247760772705\n",
      "Epoch 0: |          | 340/? [04:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 340, loss 6.111262321472168\n",
      "Epoch 0: |          | 341/? [04:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 341, loss 5.500268936157227\n",
      "Epoch 0: |          | 342/? [04:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 342, loss 6.015036106109619\n",
      "Epoch 0: |          | 343/? [04:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 343, loss 5.828909397125244\n",
      "Epoch 0: |          | 344/? [04:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 344, loss 6.376787185668945\n",
      "Epoch 0: |          | 345/? [04:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 345, loss 5.9566569328308105\n",
      "Epoch 0: |          | 346/? [04:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 346, loss 6.10753059387207\n",
      "Epoch 0: |          | 347/? [04:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 347, loss 5.575537204742432\n",
      "Epoch 0: |          | 348/? [04:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 348, loss 4.981561183929443\n",
      "Epoch 0: |          | 349/? [04:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 349, loss 4.679168224334717\n",
      "Epoch 0: |          | 350/? [04:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 350, loss 6.278787612915039\n",
      "Epoch 0: |          | 351/? [04:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 351, loss 6.209963321685791\n",
      "Epoch 0: |          | 352/? [04:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 352, loss 5.441645622253418\n",
      "Epoch 0: |          | 353/? [04:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 353, loss 5.1969122886657715\n",
      "Epoch 0: |          | 354/? [04:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 354, loss 5.5197434425354\n",
      "Epoch 0: |          | 355/? [04:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 355, loss 5.918471813201904\n",
      "Epoch 0: |          | 356/? [04:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 356, loss 5.92080545425415\n",
      "Epoch 0: |          | 357/? [04:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 357, loss 5.572604179382324\n",
      "Epoch 0: |          | 358/? [04:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 358, loss 5.410065650939941\n",
      "Epoch 0: |          | 359/? [04:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 359, loss 6.152225017547607\n",
      "Epoch 0: |          | 360/? [04:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 360, loss 5.618594169616699\n",
      "Epoch 0: |          | 361/? [04:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 361, loss 5.602255344390869\n",
      "Epoch 0: |          | 362/? [04:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 362, loss 5.535359859466553\n",
      "Epoch 0: |          | 363/? [04:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 363, loss 5.519534587860107\n",
      "Epoch 0: |          | 364/? [04:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 364, loss 6.090226173400879\n",
      "Epoch 0: |          | 365/? [04:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 365, loss 6.146973609924316\n",
      "Epoch 0: |          | 366/? [04:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 366, loss 5.719020366668701\n",
      "Epoch 0: |          | 367/? [04:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 367, loss 5.853418350219727\n",
      "Epoch 0: |          | 368/? [04:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 368, loss 5.376677513122559\n",
      "Epoch 0: |          | 369/? [04:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 369, loss 5.705757141113281\n",
      "Epoch 0: |          | 370/? [05:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 370, loss 5.259791851043701\n",
      "Epoch 0: |          | 371/? [05:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 371, loss 6.344725608825684\n",
      "Epoch 0: |          | 372/? [05:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 372, loss 6.02174186706543\n",
      "Epoch 0: |          | 373/? [05:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 373, loss 5.810129165649414\n",
      "Epoch 0: |          | 374/? [05:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 374, loss 5.649747371673584\n",
      "Epoch 0: |          | 375/? [05:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 375, loss 6.240920066833496\n",
      "Epoch 0: |          | 376/? [05:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 376, loss 5.609897613525391\n",
      "Epoch 0: |          | 377/? [05:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 377, loss 5.869363307952881\n",
      "Epoch 0: |          | 378/? [05:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 378, loss 6.051103591918945\n",
      "Epoch 0: |          | 379/? [05:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 379, loss 5.925854682922363\n",
      "Epoch 0: |          | 380/? [05:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 380, loss 6.056063652038574\n",
      "Epoch 0: |          | 381/? [05:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 381, loss 5.899880409240723\n",
      "Epoch 0: |          | 382/? [05:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 382, loss 5.6514668464660645\n",
      "Epoch 0: |          | 383/? [05:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 383, loss 5.630465984344482\n",
      "Epoch 0: |          | 384/? [05:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 384, loss 6.189591407775879\n",
      "Epoch 0: |          | 385/? [05:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 385, loss 5.858078956604004\n",
      "Epoch 0: |          | 386/? [05:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 386, loss 4.456263542175293\n",
      "Epoch 0: |          | 387/? [05:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 387, loss 5.38856840133667\n",
      "Epoch 0: |          | 388/? [05:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 388, loss 6.231630802154541\n",
      "Epoch 0: |          | 389/? [05:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 389, loss 6.218434810638428\n",
      "Epoch 0: |          | 390/? [05:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 390, loss 5.686765193939209\n",
      "Epoch 0: |          | 391/? [05:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 391, loss 6.034674644470215\n",
      "Epoch 0: |          | 392/? [05:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 392, loss 6.02430534362793\n",
      "Epoch 0: |          | 393/? [05:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 393, loss 6.056183338165283\n",
      "Epoch 0: |          | 394/? [05:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 394, loss 5.801605224609375\n",
      "Epoch 0: |          | 395/? [05:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 395, loss 6.148559093475342\n",
      "Epoch 0: |          | 396/? [05:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 396, loss 5.946237087249756\n",
      "Epoch 0: |          | 397/? [05:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 397, loss 5.938710689544678\n",
      "Epoch 0: |          | 398/? [05:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 398, loss 5.662060260772705\n",
      "Epoch 0: |          | 399/? [05:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 399, loss 5.924076080322266\n",
      "Epoch 0: |          | 400/? [05:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 400, loss 5.8255181312561035\n",
      "Epoch 0: |          | 401/? [05:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 401, loss 5.603968143463135\n",
      "Epoch 0: |          | 402/? [05:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 402, loss 6.28549861907959\n",
      "Epoch 0: |          | 403/? [05:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 403, loss 5.813617706298828\n",
      "Epoch 0: |          | 404/? [05:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 404, loss 5.344840049743652\n",
      "Epoch 0: |          | 405/? [05:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 405, loss 5.287346363067627\n",
      "Epoch 0: |          | 406/? [05:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 406, loss 5.737840175628662\n",
      "Epoch 0: |          | 407/? [05:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 407, loss 5.6041669845581055\n",
      "Epoch 0: |          | 408/? [05:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 408, loss 5.927086353302002\n",
      "Epoch 0: |          | 409/? [05:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 409, loss 5.9772844314575195\n",
      "Epoch 0: |          | 410/? [05:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 410, loss 5.833552360534668\n",
      "Epoch 0: |          | 411/? [05:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 411, loss 5.428164482116699\n",
      "Epoch 0: |          | 412/? [05:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 412, loss 5.267870903015137\n",
      "Epoch 0: |          | 413/? [05:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 413, loss 5.836418628692627\n",
      "Epoch 0: |          | 414/? [05:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 414, loss 5.2742156982421875\n",
      "Epoch 0: |          | 415/? [05:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 415, loss 5.780669212341309\n",
      "Epoch 0: |          | 416/? [05:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 416, loss 6.121771335601807\n",
      "Epoch 0: |          | 417/? [05:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 417, loss 6.470789909362793\n",
      "Epoch 0: |          | 418/? [05:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 418, loss 6.016961097717285\n",
      "Epoch 0: |          | 419/? [05:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 419, loss 6.196690559387207\n",
      "Epoch 0: |          | 420/? [05:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 420, loss 5.723702430725098\n",
      "Epoch 0: |          | 421/? [05:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 421, loss 6.267624855041504\n",
      "Epoch 0: |          | 422/? [05:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 422, loss 5.833733558654785\n",
      "Epoch 0: |          | 423/? [05:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 423, loss 5.259525299072266\n",
      "Epoch 0: |          | 424/? [05:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 424, loss 6.424025535583496\n",
      "Epoch 0: |          | 425/? [05:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 425, loss 6.477428436279297\n",
      "Epoch 0: |          | 426/? [05:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 426, loss 5.549643516540527\n",
      "Epoch 0: |          | 427/? [05:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 427, loss 5.6511406898498535\n",
      "Epoch 0: |          | 428/? [05:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 428, loss 6.425347328186035\n",
      "Epoch 0: |          | 429/? [05:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 429, loss 5.230918884277344\n",
      "Epoch 0: |          | 430/? [05:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 430, loss 6.014577388763428\n",
      "Epoch 0: |          | 431/? [05:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 431, loss 5.726552486419678\n",
      "Epoch 0: |          | 432/? [05:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 432, loss 5.823981761932373\n",
      "Epoch 0: |          | 433/? [05:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 433, loss 5.920156002044678\n",
      "Epoch 0: |          | 434/? [05:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 434, loss 5.797652721405029\n",
      "Epoch 0: |          | 435/? [05:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 435, loss 5.591113090515137\n",
      "Epoch 0: |          | 436/? [05:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 436, loss 5.965015888214111\n",
      "Epoch 0: |          | 437/? [05:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 437, loss 5.938621997833252\n",
      "Epoch 0: |          | 438/? [05:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 438, loss 5.539207935333252\n",
      "Epoch 0: |          | 439/? [05:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 439, loss 5.999926567077637\n",
      "Epoch 0: |          | 440/? [05:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 440, loss 6.081936836242676\n",
      "Epoch 0: |          | 441/? [05:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 441, loss 6.163262844085693\n",
      "Epoch 0: |          | 442/? [05:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 442, loss 5.637425899505615\n",
      "Epoch 0: |          | 443/? [05:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 443, loss 6.047459125518799\n",
      "Epoch 0: |          | 444/? [05:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 444, loss 5.7088236808776855\n",
      "Epoch 0: |          | 445/? [06:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 445, loss 6.423367500305176\n",
      "Epoch 0: |          | 446/? [06:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 446, loss 5.793637752532959\n",
      "Epoch 0: |          | 447/? [06:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 447, loss 6.1841607093811035\n",
      "Epoch 0: |          | 448/? [06:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 448, loss 5.696606636047363\n",
      "Epoch 0: |          | 449/? [06:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 449, loss 5.593010902404785\n",
      "Epoch 0: |          | 450/? [06:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 450, loss 6.014639377593994\n",
      "Epoch 0: |          | 451/? [06:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 451, loss 5.616970062255859\n",
      "Epoch 0: |          | 452/? [06:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 452, loss 5.506507873535156\n",
      "Epoch 0: |          | 453/? [06:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 453, loss 6.104167461395264\n",
      "Epoch 0: |          | 454/? [06:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 454, loss 5.429970741271973\n",
      "Epoch 0: |          | 455/? [06:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 455, loss 5.872005462646484\n",
      "Epoch 0: |          | 456/? [06:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 456, loss 5.257423400878906\n",
      "Epoch 0: |          | 457/? [06:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 457, loss 5.6817626953125\n",
      "Epoch 0: |          | 458/? [06:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 458, loss 6.0369977951049805\n",
      "Epoch 0: |          | 459/? [06:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 459, loss 6.069029331207275\n",
      "Epoch 0: |          | 460/? [06:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 460, loss 5.829513072967529\n",
      "Epoch 0: |          | 461/? [06:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 461, loss 5.775862693786621\n",
      "Epoch 0: |          | 462/? [06:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 462, loss 5.721530437469482\n",
      "Epoch 0: |          | 463/? [06:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 463, loss 5.607956886291504\n",
      "Epoch 0: |          | 464/? [06:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 464, loss 5.246682167053223\n",
      "Epoch 0: |          | 465/? [06:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 465, loss 5.5784502029418945\n",
      "Epoch 0: |          | 466/? [06:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 466, loss 5.88773775100708\n",
      "Epoch 0: |          | 467/? [06:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 467, loss 5.859620094299316\n",
      "Epoch 0: |          | 468/? [06:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 468, loss 5.809023857116699\n",
      "Epoch 0: |          | 469/? [06:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 469, loss 5.803828239440918\n",
      "Epoch 0: |          | 470/? [06:20<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 470, loss 5.100184440612793\n",
      "Epoch 0: |          | 471/? [06:21<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 471, loss 5.608556747436523\n",
      "Epoch 0: |          | 472/? [06:22<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 472, loss 5.399425506591797\n",
      "Epoch 0: |          | 473/? [06:22<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 473, loss 5.343808650970459\n",
      "Epoch 0: |          | 474/? [06:23<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 474, loss 5.2195234298706055\n",
      "Epoch 0: |          | 475/? [06:24<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 475, loss 6.369530200958252\n",
      "Epoch 0: |          | 476/? [06:25<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 476, loss 5.321767330169678\n",
      "Epoch 0: |          | 477/? [06:25<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 477, loss 4.960803031921387\n",
      "Epoch 0: |          | 478/? [06:26<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 478, loss 4.911850452423096\n",
      "Epoch 0: |          | 479/? [06:27<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 479, loss 5.777198791503906\n",
      "Epoch 0: |          | 480/? [06:28<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 480, loss 6.04156494140625\n",
      "Epoch 0: |          | 481/? [06:28<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 481, loss 5.952389717102051\n",
      "Epoch 0: |          | 482/? [06:29<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 482, loss 5.8700995445251465\n",
      "Epoch 0: |          | 483/? [06:30<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 483, loss 5.467816352844238\n",
      "Epoch 0: |          | 484/? [06:31<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 484, loss 6.101202011108398\n",
      "Epoch 0: |          | 485/? [06:32<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 485, loss 5.957687854766846\n",
      "Epoch 0: |          | 486/? [06:32<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 486, loss 5.9149322509765625\n",
      "Epoch 0: |          | 487/? [06:33<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 487, loss 5.79564905166626\n",
      "Epoch 0: |          | 488/? [06:34<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 488, loss 5.429052829742432\n",
      "Epoch 0: |          | 489/? [06:35<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 489, loss 4.851192951202393\n",
      "Epoch 0: |          | 490/? [06:36<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 490, loss 5.7652177810668945\n",
      "Epoch 0: |          | 491/? [06:37<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 491, loss 5.525022506713867\n",
      "Epoch 0: |          | 492/? [06:37<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 492, loss 5.077301979064941\n",
      "Epoch 0: |          | 493/? [06:38<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 493, loss 5.972277641296387\n",
      "Epoch 0: |          | 494/? [06:39<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 494, loss 5.882068157196045\n",
      "Epoch 0: |          | 495/? [06:40<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 495, loss 5.994507312774658\n",
      "Epoch 0: |          | 496/? [06:41<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 496, loss 5.4637556076049805\n",
      "Epoch 0: |          | 497/? [06:41<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 497, loss 5.89937686920166\n",
      "Epoch 0: |          | 498/? [06:42<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 498, loss 5.718827247619629\n",
      "Epoch 0: |          | 499/? [06:43<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 499, loss 5.645786762237549\n",
      "Epoch 0: |          | 500/? [06:44<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 500, loss 5.596749782562256\n",
      "Epoch 0: |          | 501/? [06:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 501, loss 5.152096271514893\n",
      "Epoch 0: |          | 502/? [06:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 502, loss 5.687416076660156\n",
      "Epoch 0: |          | 503/? [06:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 503, loss 5.991402626037598\n",
      "Epoch 0: |          | 504/? [06:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 504, loss 5.607377529144287\n",
      "Epoch 0: |          | 505/? [06:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 505, loss 4.7198896408081055\n",
      "Epoch 0: |          | 506/? [06:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 506, loss 5.471621990203857\n",
      "Epoch 0: |          | 507/? [06:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 507, loss 5.54332971572876\n",
      "Epoch 0: |          | 508/? [06:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 508, loss 5.843621730804443\n",
      "Epoch 0: |          | 509/? [06:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 509, loss 5.238010406494141\n",
      "Epoch 0: |          | 510/? [06:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 510, loss 6.333136558532715\n",
      "Epoch 0: |          | 511/? [06:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 511, loss 5.7664055824279785\n",
      "Epoch 0: |          | 512/? [06:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 512, loss 5.066202640533447\n",
      "Epoch 0: |          | 513/? [07:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 513, loss 5.379521369934082\n",
      "Epoch 0: |          | 514/? [07:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 514, loss 5.485919952392578\n",
      "Epoch 0: |          | 515/? [07:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 515, loss 5.228816032409668\n",
      "Epoch 0: |          | 516/? [07:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 516, loss 5.574934959411621\n",
      "Epoch 0: |          | 517/? [07:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 517, loss 5.907828330993652\n",
      "Epoch 0: |          | 518/? [07:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 518, loss 5.5206217765808105\n",
      "Epoch 0: |          | 519/? [07:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 519, loss 5.954484939575195\n",
      "Epoch 0: |          | 520/? [07:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 520, loss 5.567500591278076\n",
      "Epoch 0: |          | 521/? [07:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 521, loss 5.5099263191223145\n",
      "Epoch 0: |          | 522/? [07:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 522, loss 6.1437273025512695\n",
      "Epoch 0: |          | 523/? [07:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 523, loss 6.077685356140137\n",
      "Epoch 0: |          | 524/? [07:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 524, loss 6.041534900665283\n",
      "Epoch 0: |          | 525/? [07:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 525, loss 5.55601167678833\n",
      "Epoch 0: |          | 526/? [07:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 526, loss 5.437981605529785\n",
      "Epoch 0: |          | 527/? [07:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 527, loss 5.664480209350586\n",
      "Epoch 0: |          | 528/? [07:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 528, loss 5.882835865020752\n",
      "Epoch 0: |          | 529/? [07:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 529, loss 5.408656120300293\n",
      "Epoch 0: |          | 530/? [07:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 530, loss 5.9877119064331055\n",
      "Epoch 0: |          | 531/? [07:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 531, loss 5.382114887237549\n",
      "Epoch 0: |          | 532/? [07:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 532, loss 5.710558891296387\n",
      "Epoch 0: |          | 533/? [07:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 533, loss 5.562966346740723\n",
      "Epoch 0: |          | 534/? [07:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 534, loss 5.318093299865723\n",
      "Epoch 0: |          | 535/? [07:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 535, loss 6.203307151794434\n",
      "Epoch 0: |          | 536/? [07:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 536, loss 5.990184783935547\n",
      "Epoch 0: |          | 537/? [07:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 537, loss 5.701161861419678\n",
      "Epoch 0: |          | 538/? [07:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 538, loss 5.5059309005737305\n",
      "Epoch 0: |          | 539/? [07:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 539, loss 5.410600185394287\n",
      "Epoch 0: |          | 540/? [07:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 540, loss 6.014645576477051\n",
      "Epoch 0: |          | 541/? [07:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 541, loss 5.663913726806641\n",
      "Epoch 0: |          | 542/? [07:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 542, loss 5.504583835601807\n",
      "Epoch 0: |          | 543/? [07:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 543, loss 5.825414180755615\n",
      "Epoch 0: |          | 544/? [07:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 544, loss 5.620413780212402\n",
      "Epoch 0: |          | 545/? [07:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 545, loss 4.765280246734619\n",
      "Epoch 0: |          | 546/? [07:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 546, loss 5.66165828704834\n",
      "Epoch 0: |          | 547/? [07:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 547, loss 5.972452640533447\n",
      "Epoch 0: |          | 548/? [07:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 548, loss 5.841792106628418\n",
      "Epoch 0: |          | 549/? [07:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 549, loss 5.590912342071533\n",
      "Epoch 0: |          | 550/? [07:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 550, loss 6.0669050216674805\n",
      "Epoch 0: |          | 551/? [07:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 551, loss 5.617170810699463\n",
      "Epoch 0: |          | 552/? [07:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 552, loss 5.832865238189697\n",
      "Epoch 0: |          | 553/? [07:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 553, loss 4.968566417694092\n",
      "Epoch 0: |          | 554/? [07:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 554, loss 5.8537187576293945\n",
      "Epoch 0: |          | 555/? [07:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 555, loss 6.108071804046631\n",
      "Epoch 0: |          | 556/? [07:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 556, loss 5.4810895919799805\n",
      "Epoch 0: |          | 557/? [07:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 557, loss 5.2678327560424805\n",
      "Epoch 0: |          | 558/? [07:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 558, loss 5.3239288330078125\n",
      "Epoch 0: |          | 559/? [07:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 559, loss 5.4854631423950195\n",
      "Epoch 0: |          | 560/? [07:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 560, loss 4.784146785736084\n",
      "Epoch 0: |          | 561/? [07:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 561, loss 5.039876461029053\n",
      "Epoch 0: |          | 562/? [07:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 562, loss 5.694468021392822\n",
      "Epoch 0: |          | 563/? [07:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 563, loss 4.847708702087402\n",
      "Epoch 0: |          | 564/? [07:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 564, loss 5.632473468780518\n",
      "Epoch 0: |          | 565/? [07:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 565, loss 6.06723165512085\n",
      "Epoch 0: |          | 566/? [07:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 566, loss 5.854085445404053\n",
      "Epoch 0: |          | 567/? [07:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 567, loss 5.8786725997924805\n",
      "Epoch 0: |          | 568/? [07:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 568, loss 5.1749043464660645\n",
      "Epoch 0: |          | 569/? [07:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 569, loss 5.933588981628418\n",
      "Epoch 0: |          | 570/? [07:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 570, loss 5.697268009185791\n",
      "Epoch 0: |          | 571/? [07:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 571, loss 5.2935075759887695\n",
      "Epoch 0: |          | 572/? [07:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 572, loss 6.137164115905762\n",
      "Epoch 0: |          | 573/? [07:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 573, loss 4.547586441040039\n",
      "Epoch 0: |          | 574/? [07:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 574, loss 5.878386497497559\n",
      "Epoch 0: |          | 575/? [07:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 575, loss 5.163050651550293\n",
      "Epoch 0: |          | 576/? [07:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 576, loss 5.519747257232666\n",
      "Epoch 0: |          | 577/? [07:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 577, loss 5.668492317199707\n",
      "Epoch 0: |          | 578/? [07:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 578, loss 5.902274131774902\n",
      "Epoch 0: |          | 579/? [07:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 579, loss 4.895247936248779\n",
      "Epoch 0: |          | 580/? [07:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 580, loss 5.738879203796387\n",
      "Epoch 0: |          | 581/? [07:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 581, loss 5.818669319152832\n",
      "Epoch 0: |          | 582/? [07:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 582, loss 5.730547904968262\n",
      "Epoch 0: |          | 583/? [07:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 583, loss 5.472975254058838\n",
      "Epoch 0: |          | 584/? [07:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 584, loss 5.554543495178223\n",
      "Epoch 0: |          | 585/? [07:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 585, loss 5.860505104064941\n",
      "Epoch 0: |          | 586/? [07:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 586, loss 5.828902244567871\n",
      "Epoch 0: |          | 587/? [07:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 587, loss 5.611021518707275\n",
      "Epoch 0: |          | 588/? [08:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 588, loss 5.742938041687012\n",
      "Epoch 0: |          | 589/? [08:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 589, loss 5.235723972320557\n",
      "Epoch 0: |          | 590/? [08:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 590, loss 5.935722351074219\n",
      "Epoch 0: |          | 591/? [08:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 591, loss 5.776350975036621\n",
      "Epoch 0: |          | 592/? [08:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 592, loss 5.496399402618408\n",
      "Epoch 0: |          | 593/? [08:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 593, loss 5.422451496124268\n",
      "Epoch 0: |          | 594/? [08:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 594, loss 6.284224510192871\n",
      "Epoch 0: |          | 595/? [08:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 595, loss 4.972813606262207\n",
      "Epoch 0: |          | 596/? [08:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 596, loss 5.131392478942871\n",
      "Epoch 0: |          | 597/? [08:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 597, loss 5.472659111022949\n",
      "Epoch 0: |          | 598/? [08:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 598, loss 5.873624324798584\n",
      "Epoch 0: |          | 599/? [08:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 599, loss 5.681741237640381\n",
      "Epoch 0: |          | 600/? [08:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 600, loss 5.094973564147949\n",
      "Epoch 0: |          | 601/? [08:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 601, loss 5.539244651794434\n",
      "Epoch 0: |          | 602/? [08:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 602, loss 5.004782676696777\n",
      "Epoch 0: |          | 603/? [08:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 603, loss 5.551077842712402\n",
      "Epoch 0: |          | 604/? [08:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 604, loss 8.474374771118164\n",
      "Epoch 0: |          | 605/? [08:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 605, loss 4.985870361328125\n",
      "Epoch 0: |          | 606/? [08:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 606, loss 5.387104511260986\n",
      "Epoch 0: |          | 607/? [08:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 607, loss 5.906098365783691\n",
      "Epoch 0: |          | 608/? [08:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 608, loss 5.3172736167907715\n",
      "Epoch 0: |          | 609/? [08:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 609, loss 5.349738121032715\n",
      "Epoch 0: |          | 610/? [08:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 610, loss 5.4098005294799805\n",
      "Epoch 0: |          | 611/? [08:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 611, loss 5.589206695556641\n",
      "Epoch 0: |          | 612/? [08:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 612, loss 5.3522419929504395\n",
      "Epoch 0: |          | 613/? [08:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 613, loss 5.688398361206055\n",
      "Epoch 0: |          | 614/? [08:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 614, loss 5.207263469696045\n",
      "Epoch 0: |          | 615/? [08:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 615, loss 5.839437961578369\n",
      "Epoch 0: |          | 616/? [08:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 616, loss 6.207127094268799\n",
      "Epoch 0: |          | 617/? [08:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 617, loss 5.178854942321777\n",
      "Epoch 0: |          | 618/? [08:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 618, loss 5.659409523010254\n",
      "Epoch 0: |          | 619/? [08:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 619, loss 5.6159563064575195\n",
      "Epoch 0: |          | 620/? [08:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 620, loss 5.866890907287598\n",
      "Epoch 0: |          | 621/? [08:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 621, loss 5.0911149978637695\n",
      "Epoch 0: |          | 622/? [08:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 622, loss 4.918852806091309\n",
      "Epoch 0: |          | 623/? [08:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 623, loss 4.5984721183776855\n",
      "Epoch 0: |          | 624/? [08:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 624, loss 4.102956771850586\n",
      "Epoch 0: |          | 625/? [08:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 625, loss 6.006689071655273\n",
      "Epoch 0: |          | 626/? [08:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 626, loss 5.489651679992676\n",
      "Epoch 0: |          | 627/? [08:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 627, loss 5.403596878051758\n",
      "Epoch 0: |          | 628/? [08:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 628, loss 5.228017807006836\n",
      "Epoch 0: |          | 629/? [08:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 629, loss 5.754322528839111\n",
      "Epoch 0: |          | 630/? [08:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 630, loss 5.444971561431885\n",
      "Epoch 0: |          | 631/? [08:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 631, loss 5.702298164367676\n",
      "Epoch 0: |          | 632/? [08:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 632, loss 4.614211559295654\n",
      "Epoch 0: |          | 633/? [08:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 633, loss 5.7726287841796875\n",
      "Epoch 0: |          | 634/? [08:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 634, loss 5.289617538452148\n",
      "Epoch 0: |          | 635/? [08:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 635, loss 5.003951072692871\n",
      "Epoch 0: |          | 636/? [08:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 636, loss 5.489392280578613\n",
      "Epoch 0: |          | 637/? [08:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 637, loss 5.270622253417969\n",
      "Epoch 0: |          | 638/? [08:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 638, loss 5.510749816894531\n",
      "Epoch 0: |          | 639/? [08:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 639, loss 5.182471752166748\n",
      "Epoch 0: |          | 640/? [08:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 640, loss 5.999817848205566\n",
      "Epoch 0: |          | 641/? [08:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 641, loss 5.2083306312561035\n",
      "Epoch 0: |          | 642/? [08:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 642, loss 5.665094375610352\n",
      "Epoch 0: |          | 643/? [08:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 643, loss 5.856612205505371\n",
      "Epoch 0: |          | 644/? [08:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 644, loss 5.321891784667969\n",
      "Epoch 0: |          | 645/? [08:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 645, loss 5.243256568908691\n",
      "Epoch 0: |          | 646/? [08:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 646, loss 5.281374931335449\n",
      "Epoch 0: |          | 647/? [08:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 647, loss 6.075784206390381\n",
      "Epoch 0: |          | 648/? [08:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 648, loss 5.822913646697998\n",
      "Epoch 0: |          | 649/? [08:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 649, loss 6.414984703063965\n",
      "Epoch 0: |          | 650/? [08:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 650, loss 5.901832580566406\n",
      "Epoch 0: |          | 651/? [08:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 651, loss 6.0434770584106445\n",
      "Epoch 0: |          | 652/? [08:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 652, loss 5.238709926605225\n",
      "Epoch 0: |          | 653/? [08:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 653, loss 5.339946746826172\n",
      "Epoch 0: |          | 654/? [08:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 654, loss 5.7919206619262695\n",
      "Epoch 0: |          | 655/? [08:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 655, loss 5.485228538513184\n",
      "Epoch 0: |          | 656/? [08:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 656, loss 4.959972858428955\n",
      "Epoch 0: |          | 657/? [08:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 657, loss 7.93657922744751\n",
      "Epoch 0: |          | 658/? [08:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 658, loss 5.53670597076416\n",
      "Epoch 0: |          | 659/? [08:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 659, loss 5.509316444396973\n",
      "Epoch 0: |          | 660/? [08:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 660, loss 5.905879020690918\n",
      "Epoch 0: |          | 661/? [08:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 661, loss 5.687729835510254\n",
      "Epoch 0: |          | 662/? [08:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 662, loss 5.51522159576416\n",
      "Epoch 0: |          | 663/? [08:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 663, loss 5.30658483505249\n",
      "Epoch 0: |          | 664/? [09:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 664, loss 5.2204718589782715\n",
      "Epoch 0: |          | 665/? [09:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 665, loss 5.559053897857666\n",
      "Epoch 0: |          | 666/? [09:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 666, loss 5.476119518280029\n",
      "Epoch 0: |          | 667/? [09:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 667, loss 6.216425895690918\n",
      "Epoch 0: |          | 668/? [09:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 668, loss 5.021641731262207\n",
      "Epoch 0: |          | 669/? [09:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 669, loss 5.260247230529785\n",
      "Epoch 0: |          | 670/? [09:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 670, loss 5.9315032958984375\n",
      "Epoch 0: |          | 671/? [09:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 671, loss 5.624521255493164\n",
      "Epoch 0: |          | 672/? [09:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 672, loss 5.6002655029296875\n",
      "Epoch 0: |          | 673/? [09:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 673, loss 5.367294788360596\n",
      "Epoch 0: |          | 674/? [09:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 674, loss 4.3332109451293945\n",
      "Epoch 0: |          | 675/? [09:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 675, loss 2.860220193862915\n",
      "Epoch 0: |          | 676/? [09:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 676, loss 2.5141522884368896\n",
      "Epoch 0: |          | 677/? [09:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 677, loss 2.094329833984375\n",
      "Epoch 0: |          | 678/? [09:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 678, loss 3.1381916999816895\n",
      "Epoch 0: |          | 679/? [09:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 679, loss 4.952631950378418\n",
      "Epoch 0: |          | 680/? [09:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 680, loss 5.533645153045654\n",
      "Epoch 0: |          | 681/? [09:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 681, loss 4.629390239715576\n",
      "Epoch 0: |          | 682/? [09:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 682, loss 5.334853172302246\n",
      "Epoch 0: |          | 683/? [09:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 683, loss 5.015111923217773\n",
      "Epoch 0: |          | 684/? [09:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 684, loss 6.031195640563965\n",
      "Epoch 0: |          | 685/? [09:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 685, loss 5.72156286239624\n",
      "Epoch 0: |          | 686/? [09:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 686, loss 5.076003074645996\n",
      "Epoch 0: |          | 687/? [09:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 687, loss 5.9401960372924805\n",
      "Epoch 0: |          | 688/? [09:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 688, loss 5.917876243591309\n",
      "Epoch 0: |          | 689/? [09:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 689, loss 5.719071388244629\n",
      "Epoch 0: |          | 690/? [09:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 690, loss 5.8693952560424805\n",
      "Epoch 0: |          | 691/? [09:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 691, loss 5.669732093811035\n",
      "Epoch 0: |          | 692/? [09:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 692, loss 5.373565196990967\n",
      "Epoch 0: |          | 693/? [09:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 693, loss 5.865340232849121\n",
      "Epoch 0: |          | 694/? [09:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 694, loss 5.358670234680176\n",
      "Epoch 0: |          | 695/? [09:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 695, loss 6.02431583404541\n",
      "Epoch 0: |          | 696/? [09:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 696, loss 4.818984031677246\n",
      "Epoch 0: |          | 697/? [09:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 697, loss 5.528355121612549\n",
      "Epoch 0: |          | 698/? [09:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 698, loss 4.578963279724121\n",
      "Epoch 0: |          | 699/? [09:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 699, loss 5.515089511871338\n",
      "Epoch 0: |          | 700/? [09:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 700, loss 5.749447345733643\n",
      "Epoch 0: |          | 701/? [09:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 701, loss 5.164711952209473\n",
      "Epoch 0: |          | 702/? [09:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 702, loss 5.328652381896973\n",
      "Epoch 0: |          | 703/? [09:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 703, loss 5.6151442527771\n",
      "Epoch 0: |          | 704/? [09:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 704, loss 5.592307090759277\n",
      "Epoch 0: |          | 705/? [09:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 705, loss 5.105806350708008\n",
      "Epoch 0: |          | 706/? [09:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 706, loss 5.234866142272949\n",
      "Epoch 0: |          | 707/? [09:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 707, loss 5.618227958679199\n",
      "Epoch 0: |          | 708/? [09:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 708, loss 5.477025985717773\n",
      "Epoch 0: |          | 709/? [09:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 709, loss 5.266617298126221\n",
      "Epoch 0: |          | 710/? [09:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 710, loss 5.840146541595459\n",
      "Epoch 0: |          | 711/? [09:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 711, loss 6.234673500061035\n",
      "Epoch 0: |          | 712/? [09:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 712, loss 5.737572193145752\n",
      "Epoch 0: |          | 713/? [09:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 713, loss 5.624386787414551\n",
      "Epoch 0: |          | 714/? [09:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 714, loss 5.953104019165039\n",
      "Epoch 0: |          | 715/? [09:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 715, loss 4.596850395202637\n",
      "Epoch 0: |          | 716/? [09:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 716, loss 5.534512042999268\n",
      "Epoch 0: |          | 717/? [09:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 717, loss 5.210357189178467\n",
      "Epoch 0: |          | 718/? [09:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 718, loss 4.722998142242432\n",
      "Epoch 0: |          | 719/? [09:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 719, loss 5.3049845695495605\n",
      "Epoch 0: |          | 720/? [09:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 720, loss 5.1570000648498535\n",
      "Epoch 0: |          | 721/? [09:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 721, loss 5.858370304107666\n",
      "Epoch 0: |          | 722/? [09:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 722, loss 4.8208208084106445\n",
      "Epoch 0: |          | 723/? [09:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 723, loss 5.54416561126709\n",
      "Epoch 0: |          | 724/? [09:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 724, loss 5.610477924346924\n",
      "Epoch 0: |          | 725/? [09:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 725, loss 5.129228591918945\n",
      "Epoch 0: |          | 726/? [09:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 726, loss 5.279664516448975\n",
      "Epoch 0: |          | 727/? [09:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 727, loss 4.9021711349487305\n",
      "Epoch 0: |          | 728/? [09:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 728, loss 4.783382415771484\n",
      "Epoch 0: |          | 729/? [09:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 729, loss 5.146182060241699\n",
      "Epoch 0: |          | 730/? [09:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 730, loss 5.204728126525879\n",
      "Epoch 0: |          | 731/? [09:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 731, loss 5.395882606506348\n",
      "Epoch 0: |          | 732/? [09:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 732, loss 5.788630962371826\n",
      "Epoch 0: |          | 733/? [09:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 733, loss 5.27632999420166\n",
      "Epoch 0: |          | 734/? [09:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 734, loss 5.70977258682251\n",
      "Epoch 0: |          | 735/? [09:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 735, loss 5.582883834838867\n",
      "Epoch 0: |          | 736/? [09:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 736, loss 4.873753547668457\n",
      "Epoch 0: |          | 737/? [09:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 737, loss 5.750082015991211\n",
      "Epoch 0: |          | 738/? [10:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 738, loss 4.887001991271973\n",
      "Epoch 0: |          | 739/? [10:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 739, loss 5.599045753479004\n",
      "Epoch 0: |          | 740/? [10:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 740, loss 5.009853839874268\n",
      "Epoch 0: |          | 741/? [10:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 741, loss 5.354680061340332\n",
      "Epoch 0: |          | 742/? [10:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 742, loss 5.7960615158081055\n",
      "Epoch 0: |          | 743/? [10:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 743, loss 5.580174446105957\n",
      "Epoch 0: |          | 744/? [10:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 744, loss 5.466811180114746\n",
      "Epoch 0: |          | 745/? [10:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 745, loss 5.114466667175293\n",
      "Epoch 0: |          | 746/? [10:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 746, loss 5.363156318664551\n",
      "Epoch 0: |          | 747/? [10:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 747, loss 5.329859733581543\n",
      "Epoch 0: |          | 748/? [10:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 748, loss 5.186460494995117\n",
      "Epoch 0: |          | 749/? [10:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 749, loss 5.538210868835449\n",
      "Epoch 0: |          | 750/? [10:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 750, loss 5.856947422027588\n",
      "Epoch 0: |          | 751/? [10:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 751, loss 4.696465015411377\n",
      "Epoch 0: |          | 752/? [10:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 752, loss 5.795674800872803\n",
      "Epoch 0: |          | 753/? [10:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 753, loss 5.119442939758301\n",
      "Epoch 0: |          | 754/? [10:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 754, loss 5.476996898651123\n",
      "Epoch 0: |          | 755/? [10:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 755, loss 5.003771781921387\n",
      "Epoch 0: |          | 756/? [10:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 756, loss 5.453027248382568\n",
      "Epoch 0: |          | 757/? [10:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 757, loss 5.401941299438477\n",
      "Epoch 0: |          | 758/? [10:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 758, loss 5.05012845993042\n",
      "Epoch 0: |          | 759/? [10:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 759, loss 5.389413833618164\n",
      "Epoch 0: |          | 760/? [10:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 760, loss 5.598079204559326\n",
      "Epoch 0: |          | 761/? [10:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 761, loss 5.69028902053833\n",
      "Epoch 0: |          | 762/? [10:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 762, loss 5.333929061889648\n",
      "Epoch 0: |          | 763/? [10:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 763, loss 5.912430763244629\n",
      "Epoch 0: |          | 764/? [10:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 764, loss 5.8394622802734375\n",
      "Epoch 0: |          | 765/? [10:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 765, loss 5.445313453674316\n",
      "Epoch 0: |          | 766/? [10:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 766, loss 5.922490119934082\n",
      "Epoch 0: |          | 767/? [10:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 767, loss 6.056840419769287\n",
      "Epoch 0: |          | 768/? [10:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 768, loss 5.493472099304199\n",
      "Epoch 0: |          | 769/? [10:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 769, loss 4.557048320770264\n",
      "Epoch 0: |          | 770/? [10:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 770, loss 5.233562469482422\n",
      "Epoch 0: |          | 771/? [10:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 771, loss 6.00492000579834\n",
      "Epoch 0: |          | 772/? [10:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 772, loss 5.730591773986816\n",
      "Epoch 0: |          | 773/? [10:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 773, loss 5.322907447814941\n",
      "Epoch 0: |          | 774/? [10:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 774, loss 5.35986328125\n",
      "Epoch 0: |          | 775/? [10:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 775, loss 5.930113792419434\n",
      "Epoch 0: |          | 776/? [10:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 776, loss 5.375496864318848\n",
      "Epoch 0: |          | 777/? [10:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 777, loss 5.330728054046631\n",
      "Epoch 0: |          | 778/? [10:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 778, loss 5.742478370666504\n",
      "Epoch 0: |          | 779/? [10:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 779, loss 6.113963603973389\n",
      "Epoch 0: |          | 780/? [10:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 780, loss 4.869594097137451\n",
      "Epoch 0: |          | 781/? [10:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 781, loss 5.11429500579834\n",
      "Epoch 0: |          | 782/? [10:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 782, loss 5.7009358406066895\n",
      "Epoch 0: |          | 783/? [10:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 783, loss 5.607274532318115\n",
      "Epoch 0: |          | 784/? [10:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 784, loss 5.114802837371826\n",
      "Epoch 0: |          | 785/? [10:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 785, loss 5.117795944213867\n",
      "Epoch 0: |          | 786/? [10:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 786, loss 5.8686203956604\n",
      "Epoch 0: |          | 787/? [10:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 787, loss 5.9555182456970215\n",
      "Epoch 0: |          | 788/? [10:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 788, loss 4.267166614532471\n",
      "Epoch 0: |          | 789/? [10:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 789, loss 5.364194869995117\n",
      "Epoch 0: |          | 790/? [10:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 790, loss 6.159371376037598\n",
      "Epoch 0: |          | 791/? [10:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 791, loss 5.980263710021973\n",
      "Epoch 0: |          | 792/? [10:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 792, loss 4.873240947723389\n",
      "Epoch 0: |          | 793/? [10:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 793, loss 5.4736247062683105\n",
      "Epoch 0: |          | 794/? [10:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 794, loss 5.8327436447143555\n",
      "Epoch 0: |          | 795/? [10:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 795, loss 5.4414238929748535\n",
      "Epoch 0: |          | 796/? [10:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 796, loss 5.7232279777526855\n",
      "Epoch 0: |          | 797/? [10:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 797, loss 4.700378894805908\n",
      "Epoch 0: |          | 798/? [10:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 798, loss 4.699416160583496\n",
      "Epoch 0: |          | 799/? [10:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 799, loss 5.79373836517334\n",
      "Epoch 0: |          | 800/? [10:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 800, loss 5.571507453918457\n",
      "Epoch 0: |          | 801/? [10:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 801, loss 4.968176364898682\n",
      "Epoch 0: |          | 802/? [10:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 802, loss 5.474885940551758\n",
      "Epoch 0: |          | 803/? [10:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 803, loss 5.426146507263184\n",
      "Epoch 0: |          | 804/? [10:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 804, loss 5.527812957763672\n",
      "Epoch 0: |          | 805/? [10:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 805, loss 6.047200679779053\n",
      "Epoch 0: |          | 806/? [10:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 806, loss 5.946329593658447\n",
      "Epoch 0: |          | 807/? [10:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 807, loss 5.507012367248535\n",
      "Epoch 0: |          | 808/? [10:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 808, loss 4.946045875549316\n",
      "Epoch 0: |          | 809/? [10:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 809, loss 5.555907726287842\n",
      "Epoch 0: |          | 810/? [10:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 810, loss 5.466401100158691\n",
      "Epoch 0: |          | 811/? [11:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 811, loss 5.661803722381592\n",
      "Epoch 0: |          | 812/? [11:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 812, loss 6.52536678314209\n",
      "Epoch 0: |          | 813/? [11:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 813, loss 5.90410041809082\n",
      "Epoch 0: |          | 814/? [11:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 814, loss 5.014833927154541\n",
      "Epoch 0: |          | 815/? [11:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 815, loss 5.734185218811035\n",
      "Epoch 0: |          | 816/? [11:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 816, loss 5.6170172691345215\n",
      "Epoch 0: |          | 817/? [11:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 817, loss 4.851515769958496\n",
      "Epoch 0: |          | 818/? [11:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 818, loss 5.878479957580566\n",
      "Epoch 0: |          | 819/? [11:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 819, loss 5.587596893310547\n",
      "Epoch 0: |          | 820/? [11:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 820, loss 5.491504192352295\n",
      "Epoch 0: |          | 821/? [11:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 821, loss 5.582261085510254\n",
      "Epoch 0: |          | 822/? [11:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 822, loss 4.946959972381592\n",
      "Epoch 0: |          | 823/? [11:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 823, loss 4.883823394775391\n",
      "Epoch 0: |          | 824/? [11:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 824, loss 5.528999328613281\n",
      "Epoch 0: |          | 825/? [11:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 825, loss 5.080121040344238\n",
      "Epoch 0: |          | 826/? [11:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 826, loss 5.406458854675293\n",
      "Epoch 0: |          | 827/? [11:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 827, loss 5.170008659362793\n",
      "Epoch 0: |          | 828/? [11:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 828, loss 5.7167582511901855\n",
      "Epoch 0: |          | 829/? [11:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 829, loss 5.351236820220947\n",
      "Epoch 0: |          | 830/? [11:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 830, loss 5.913365364074707\n",
      "Epoch 0: |          | 831/? [11:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 831, loss 3.588498592376709\n",
      "Epoch 0: |          | 832/? [11:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 832, loss 5.373066425323486\n",
      "Epoch 0: |          | 833/? [11:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 833, loss 5.150835990905762\n",
      "Epoch 0: |          | 834/? [11:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 834, loss 5.9078874588012695\n",
      "Epoch 0: |          | 835/? [11:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 835, loss 5.542458534240723\n",
      "Epoch 0: |          | 836/? [11:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 836, loss 6.022217273712158\n",
      "Epoch 0: |          | 837/? [11:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 837, loss 5.539735317230225\n",
      "Epoch 0: |          | 838/? [11:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 838, loss 4.704615116119385\n",
      "Epoch 0: |          | 839/? [11:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 839, loss 5.018975734710693\n",
      "Epoch 0: |          | 840/? [11:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 840, loss 5.689625263214111\n",
      "Epoch 0: |          | 841/? [11:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 841, loss 5.696554660797119\n",
      "Epoch 0: |          | 842/? [11:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 842, loss 5.358897686004639\n",
      "Epoch 0: |          | 843/? [11:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 843, loss 5.707569122314453\n",
      "Epoch 0: |          | 844/? [11:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 844, loss 5.008744239807129\n",
      "Epoch 0: |          | 845/? [11:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 845, loss 5.522219181060791\n",
      "Epoch 0: |          | 846/? [11:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 846, loss 6.141279220581055\n",
      "Epoch 0: |          | 847/? [11:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 847, loss 5.4991865158081055\n",
      "Epoch 0: |          | 848/? [11:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 848, loss 4.941450595855713\n",
      "Epoch 0: |          | 849/? [11:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 849, loss 5.233776092529297\n",
      "Epoch 0: |          | 850/? [11:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 850, loss 5.148620128631592\n",
      "Epoch 0: |          | 851/? [11:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 851, loss 5.61543083190918\n",
      "Epoch 0: |          | 852/? [11:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 852, loss 5.475518226623535\n",
      "Epoch 0: |          | 853/? [11:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 853, loss 5.641838073730469\n",
      "Epoch 0: |          | 854/? [11:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 854, loss 4.628824710845947\n",
      "Epoch 0: |          | 855/? [11:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 855, loss 5.157265663146973\n",
      "Epoch 0: |          | 856/? [11:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 856, loss 5.015626907348633\n",
      "Epoch 0: |          | 857/? [11:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 857, loss 5.625411510467529\n",
      "Epoch 0: |          | 858/? [11:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 858, loss 5.459197998046875\n",
      "Epoch 0: |          | 859/? [11:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 859, loss 5.493956089019775\n",
      "Epoch 0: |          | 860/? [11:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 860, loss 5.929394721984863\n",
      "Epoch 0: |          | 861/? [11:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 861, loss 4.979262828826904\n",
      "Epoch 0: |          | 862/? [11:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 862, loss 5.605406761169434\n",
      "Epoch 0: |          | 863/? [11:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 863, loss 4.620086669921875\n",
      "Epoch 0: |          | 864/? [11:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 864, loss 5.6378021240234375\n",
      "Epoch 0: |          | 865/? [11:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 865, loss 5.441355228424072\n",
      "Epoch 0: |          | 866/? [11:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 866, loss 5.047087669372559\n",
      "Epoch 0: |          | 867/? [11:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 867, loss 4.807704925537109\n",
      "Epoch 0: |          | 868/? [11:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 868, loss 5.611564636230469\n",
      "Epoch 0: |          | 869/? [11:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 869, loss 5.442465782165527\n",
      "Epoch 0: |          | 870/? [11:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 870, loss 5.017022132873535\n",
      "Epoch 0: |          | 871/? [11:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 871, loss 5.566012382507324\n",
      "Epoch 0: |          | 872/? [11:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 872, loss 5.41798734664917\n",
      "Epoch 0: |          | 873/? [11:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 873, loss 5.338426113128662\n",
      "Epoch 0: |          | 874/? [11:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 874, loss 4.722266674041748\n",
      "Epoch 0: |          | 875/? [11:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 875, loss 5.509988784790039\n",
      "Epoch 0: |          | 876/? [11:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 876, loss 5.374152183532715\n",
      "Epoch 0: |          | 877/? [11:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 877, loss 5.357004642486572\n",
      "Epoch 0: |          | 878/? [11:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 878, loss 4.850365161895752\n",
      "Epoch 0: |          | 879/? [11:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 879, loss 5.0575666427612305\n",
      "Epoch 0: |          | 880/? [11:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 880, loss 5.955557823181152\n",
      "Epoch 0: |          | 881/? [11:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 881, loss 5.385994911193848\n",
      "Epoch 0: |          | 882/? [11:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 882, loss 5.298847675323486\n",
      "Epoch 0: |          | 883/? [11:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 883, loss 5.338111400604248\n",
      "Epoch 0: |          | 884/? [12:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 884, loss 5.4387125968933105\n",
      "Epoch 0: |          | 885/? [12:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 885, loss 5.199179172515869\n",
      "Epoch 0: |          | 886/? [12:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 886, loss 5.667088508605957\n",
      "Epoch 0: |          | 887/? [12:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 887, loss 5.859172344207764\n",
      "Epoch 0: |          | 888/? [12:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 888, loss 5.5540900230407715\n",
      "Epoch 0: |          | 889/? [12:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 889, loss 5.385109901428223\n",
      "Epoch 0: |          | 890/? [12:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 890, loss 5.819759845733643\n",
      "Epoch 0: |          | 891/? [12:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 891, loss 4.977629661560059\n",
      "Epoch 0: |          | 892/? [12:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 892, loss 5.81060266494751\n",
      "Epoch 0: |          | 893/? [12:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 893, loss 5.1311540603637695\n",
      "Epoch 0: |          | 894/? [12:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 894, loss 4.901973724365234\n",
      "Epoch 0: |          | 895/? [12:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 895, loss 5.819773197174072\n",
      "Epoch 0: |          | 896/? [12:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 896, loss 5.513845443725586\n",
      "Epoch 0: |          | 897/? [12:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 897, loss 5.616225719451904\n",
      "Epoch 0: |          | 898/? [12:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 898, loss 5.589058876037598\n",
      "Epoch 0: |          | 899/? [12:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 899, loss 5.315898895263672\n",
      "Epoch 0: |          | 900/? [12:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 900, loss 5.114770412445068\n",
      "Epoch 0: |          | 901/? [12:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 901, loss 5.693282127380371\n",
      "Epoch 0: |          | 902/? [12:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 902, loss 5.67890739440918\n",
      "Epoch 0: |          | 903/? [12:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 903, loss 4.998488426208496\n",
      "Epoch 0: |          | 904/? [12:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 904, loss 5.365415096282959\n",
      "Epoch 0: |          | 905/? [12:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 905, loss 5.632184028625488\n",
      "Epoch 0: |          | 906/? [12:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 906, loss 5.31313943862915\n",
      "Epoch 0: |          | 907/? [12:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 907, loss 5.439633846282959\n",
      "Epoch 0: |          | 908/? [12:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 908, loss 5.541932106018066\n",
      "Epoch 0: |          | 909/? [12:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 909, loss 5.50548791885376\n",
      "Epoch 0: |          | 910/? [12:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 910, loss 5.23805046081543\n",
      "Epoch 0: |          | 911/? [12:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 911, loss 5.220827102661133\n",
      "Epoch 0: |          | 912/? [12:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 912, loss 5.220589637756348\n",
      "Epoch 0: |          | 913/? [12:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 913, loss 5.227474689483643\n",
      "Epoch 0: |          | 914/? [12:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 914, loss 5.667771339416504\n",
      "Epoch 0: |          | 915/? [12:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 915, loss 5.581581115722656\n",
      "Epoch 0: |          | 916/? [12:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 916, loss 5.242282390594482\n",
      "Epoch 0: |          | 917/? [12:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 917, loss 5.333691596984863\n",
      "Epoch 0: |          | 918/? [12:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 918, loss 5.240054130554199\n",
      "Epoch 0: |          | 919/? [12:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 919, loss 5.112586498260498\n",
      "Epoch 0: |          | 920/? [12:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 920, loss 5.3858795166015625\n",
      "Epoch 0: |          | 921/? [12:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 921, loss 5.1880340576171875\n",
      "Epoch 0: |          | 922/? [12:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 922, loss 5.335896968841553\n",
      "Epoch 0: |          | 923/? [12:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 923, loss 5.113396644592285\n",
      "Epoch 0: |          | 924/? [12:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 924, loss 5.1934919357299805\n",
      "Epoch 0: |          | 925/? [12:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 925, loss 5.391125679016113\n",
      "Epoch 0: |          | 926/? [12:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 926, loss 5.319790363311768\n",
      "Epoch 0: |          | 927/? [12:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 927, loss 5.4013566970825195\n",
      "Epoch 0: |          | 928/? [12:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 928, loss 4.936028957366943\n",
      "Epoch 0: |          | 929/? [12:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 929, loss 5.128331184387207\n",
      "Epoch 0: |          | 930/? [12:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 930, loss 4.844562530517578\n",
      "Epoch 0: |          | 931/? [12:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 931, loss 4.551640510559082\n",
      "Epoch 0: |          | 932/? [12:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 932, loss 5.300605773925781\n",
      "Epoch 0: |          | 933/? [12:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 933, loss 5.123416423797607\n",
      "Epoch 0: |          | 934/? [12:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 934, loss 5.899786949157715\n",
      "Epoch 0: |          | 935/? [12:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 935, loss 5.967156887054443\n",
      "Epoch 0: |          | 936/? [12:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 936, loss 5.37748908996582\n",
      "Epoch 0: |          | 937/? [12:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 937, loss 5.960204124450684\n",
      "Epoch 0: |          | 938/? [12:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 938, loss 5.109314918518066\n",
      "Epoch 0: |          | 939/? [12:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 939, loss 5.405552387237549\n",
      "Epoch 0: |          | 940/? [12:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 940, loss 5.7872700691223145\n",
      "Epoch 0: |          | 941/? [12:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 941, loss 5.018288612365723\n",
      "Epoch 0: |          | 942/? [12:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 942, loss 4.770509243011475\n",
      "Epoch 0: |          | 943/? [12:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 943, loss 5.593767166137695\n",
      "Epoch 0: |          | 944/? [12:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 944, loss 4.55128288269043\n",
      "Epoch 0: |          | 945/? [12:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 945, loss 5.23533821105957\n",
      "Epoch 0: |          | 946/? [12:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 946, loss 5.349045276641846\n",
      "Epoch 0: |          | 947/? [12:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 947, loss 5.302351951599121\n",
      "Epoch 0: |          | 948/? [12:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 948, loss 5.294065475463867\n",
      "Epoch 0: |          | 949/? [12:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 949, loss 5.179054260253906\n",
      "Epoch 0: |          | 950/? [12:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 950, loss 5.072454452514648\n",
      "Epoch 0: |          | 951/? [12:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 951, loss 5.912770748138428\n",
      "Epoch 0: |          | 952/? [12:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 952, loss 5.687593460083008\n",
      "Epoch 0: |          | 953/? [12:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 953, loss 6.219467639923096\n",
      "Epoch 0: |          | 954/? [12:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 954, loss 5.455103874206543\n",
      "Epoch 0: |          | 955/? [12:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 955, loss 5.973563194274902\n",
      "Epoch 0: |          | 956/? [12:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 956, loss 5.101168155670166\n",
      "Epoch 0: |          | 957/? [12:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 957, loss 5.640178680419922\n",
      "Epoch 0: |          | 958/? [12:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 958, loss 6.014928340911865\n",
      "Epoch 0: |          | 959/? [13:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 959, loss 6.236110687255859\n",
      "Epoch 0: |          | 960/? [13:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 960, loss 5.814597129821777\n",
      "Epoch 0: |          | 961/? [13:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 961, loss 5.717583179473877\n",
      "Epoch 0: |          | 962/? [13:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 962, loss 5.5439558029174805\n",
      "Epoch 0: |          | 963/? [13:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 963, loss 4.9893999099731445\n",
      "Epoch 0: |          | 964/? [13:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 964, loss 5.527182579040527\n",
      "Epoch 0: |          | 965/? [13:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 965, loss 5.209347724914551\n",
      "Epoch 0: |          | 966/? [13:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 966, loss 4.973355293273926\n",
      "Epoch 0: |          | 967/? [13:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 967, loss 5.2301716804504395\n",
      "Epoch 0: |          | 968/? [13:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 968, loss 5.3425116539001465\n",
      "Epoch 0: |          | 969/? [13:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 969, loss 4.897238731384277\n",
      "Epoch 0: |          | 970/? [13:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 970, loss 5.4785871505737305\n",
      "Epoch 0: |          | 971/? [13:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 971, loss 6.227494239807129\n",
      "Epoch 0: |          | 972/? [13:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 972, loss 5.285476207733154\n",
      "Epoch 0: |          | 973/? [13:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 973, loss 5.737224578857422\n",
      "Epoch 0: |          | 974/? [13:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 974, loss 5.190789699554443\n",
      "Epoch 0: |          | 975/? [13:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 975, loss 5.356500148773193\n",
      "Epoch 0: |          | 976/? [13:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 976, loss 5.444126129150391\n",
      "Epoch 0: |          | 977/? [13:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 977, loss 6.002750396728516\n",
      "Epoch 0: |          | 978/? [13:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 978, loss 5.802117347717285\n",
      "Epoch 0: |          | 979/? [13:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 979, loss 5.705315589904785\n",
      "Epoch 0: |          | 980/? [13:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 980, loss 5.149957180023193\n",
      "Epoch 0: |          | 981/? [13:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 981, loss 4.5259857177734375\n",
      "Epoch 0: |          | 982/? [13:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 982, loss 5.3855390548706055\n",
      "Epoch 0: |          | 983/? [13:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 983, loss 5.716288089752197\n",
      "Epoch 0: |          | 984/? [13:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 984, loss 4.633906364440918\n",
      "Epoch 0: |          | 985/? [13:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 985, loss 5.0731000900268555\n",
      "Epoch 0: |          | 986/? [13:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 986, loss 5.007599830627441\n",
      "Epoch 0: |          | 987/? [13:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 987, loss 5.073805332183838\n",
      "Epoch 0: |          | 988/? [13:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 988, loss 5.654455184936523\n",
      "Epoch 0: |          | 989/? [13:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 989, loss 5.315481662750244\n",
      "Epoch 0: |          | 990/? [13:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 990, loss 4.352801322937012\n",
      "Epoch 0: |          | 991/? [13:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 991, loss 5.113941192626953\n",
      "Epoch 0: |          | 992/? [13:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 992, loss 5.947590351104736\n",
      "Epoch 0: |          | 993/? [13:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 993, loss 5.043504238128662\n",
      "Epoch 0: |          | 994/? [13:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 994, loss 5.130123138427734\n",
      "Epoch 0: |          | 995/? [13:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 995, loss 5.6073713302612305\n",
      "Epoch 0: |          | 996/? [13:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 996, loss 5.514841079711914\n",
      "Epoch 0: |          | 997/? [13:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 997, loss 5.109129905700684\n",
      "Epoch 0: |          | 998/? [13:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 998, loss 5.382630348205566\n",
      "Epoch 0: |          | 999/? [13:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 999, loss 5.850205421447754\n",
      "Epoch 0: |          | 1000/? [13:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 1000, loss 5.329292297363281\n",
      "Epoch 0: |          | 1001/? [13:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1001, loss 5.592450141906738\n",
      "Epoch 0: |          | 1002/? [13:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1002, loss 5.674727439880371\n",
      "Epoch 0: |          | 1003/? [13:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1003, loss 5.591828346252441\n",
      "Epoch 0: |          | 1004/? [13:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1004, loss 4.538832664489746\n",
      "Epoch 0: |          | 1005/? [13:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1005, loss 5.12720251083374\n",
      "Epoch 0: |          | 1006/? [13:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1006, loss 5.6572747230529785\n",
      "Epoch 0: |          | 1007/? [13:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1007, loss 5.276072025299072\n",
      "Epoch 0: |          | 1008/? [13:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1008, loss 5.192209720611572\n",
      "Epoch 0: |          | 1009/? [13:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1009, loss 5.696181774139404\n",
      "Epoch 0: |          | 1010/? [13:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1010, loss 4.608116149902344\n",
      "Epoch 0: |          | 1011/? [13:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1011, loss 5.377662658691406\n",
      "Epoch 0: |          | 1012/? [13:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1012, loss 5.064587116241455\n",
      "Epoch 0: |          | 1013/? [13:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1013, loss 5.529282093048096\n",
      "Epoch 0: |          | 1014/? [13:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1014, loss 5.679267883300781\n",
      "Epoch 0: |          | 1015/? [13:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1015, loss 5.387723922729492\n",
      "Epoch 0: |          | 1016/? [13:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1016, loss 5.413459777832031\n",
      "Epoch 0: |          | 1017/? [13:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1017, loss 4.83635139465332\n",
      "Epoch 0: |          | 1018/? [13:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1018, loss 5.384902000427246\n",
      "Epoch 0: |          | 1019/? [13:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1019, loss 5.323790550231934\n",
      "Epoch 0: |          | 1020/? [13:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1020, loss 4.847562789916992\n",
      "Epoch 0: |          | 1021/? [13:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1021, loss 4.993163108825684\n",
      "Epoch 0: |          | 1022/? [13:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1022, loss 4.694328784942627\n",
      "Epoch 0: |          | 1023/? [13:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1023, loss 4.373943328857422\n",
      "Epoch 0: |          | 1024/? [13:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1024, loss 5.220324516296387\n",
      "Epoch 0: |          | 1025/? [13:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1025, loss 5.219889163970947\n",
      "Epoch 0: |          | 1026/? [14:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1026, loss 3.8365235328674316\n",
      "Epoch 0: |          | 1027/? [14:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1027, loss 5.9513258934021\n",
      "Epoch 0: |          | 1028/? [14:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1028, loss 5.278771877288818\n",
      "Epoch 0: |          | 1029/? [14:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1029, loss 5.134883880615234\n",
      "Epoch 0: |          | 1030/? [14:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1030, loss 4.8563714027404785\n",
      "Epoch 0: |          | 1031/? [14:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1031, loss 4.921840667724609\n",
      "Epoch 0: |          | 1032/? [14:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1032, loss 5.817437648773193\n",
      "Epoch 0: |          | 1033/? [14:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1033, loss 5.697661876678467\n",
      "Epoch 0: |          | 1034/? [14:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1034, loss 5.128747463226318\n",
      "Epoch 0: |          | 1035/? [14:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1035, loss 5.060297012329102\n",
      "Epoch 0: |          | 1036/? [14:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1036, loss 4.749032020568848\n",
      "Epoch 0: |          | 1037/? [14:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1037, loss 5.667658805847168\n",
      "Epoch 0: |          | 1038/? [14:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1038, loss 5.762668609619141\n",
      "Epoch 0: |          | 1039/? [14:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1039, loss 5.8765668869018555\n",
      "Epoch 0: |          | 1040/? [14:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1040, loss 5.34023904800415\n",
      "Epoch 0: |          | 1041/? [14:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1041, loss 5.705921649932861\n",
      "Epoch 0: |          | 1042/? [14:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1042, loss 5.313633918762207\n",
      "Epoch 0: |          | 1043/? [14:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1043, loss 5.525386333465576\n",
      "Epoch 0: |          | 1044/? [14:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1044, loss 5.084625720977783\n",
      "Epoch 0: |          | 1045/? [14:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1045, loss 4.666763782501221\n",
      "Epoch 0: |          | 1046/? [14:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1046, loss 4.489441871643066\n",
      "Epoch 0: |          | 1047/? [14:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1047, loss 5.895993709564209\n",
      "Epoch 0: |          | 1048/? [14:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1048, loss 4.970871925354004\n",
      "Epoch 0: |          | 1049/? [14:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1049, loss 5.1839094161987305\n",
      "Epoch 0: |          | 1050/? [14:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1050, loss 4.930117607116699\n",
      "Epoch 0: |          | 1051/? [14:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1051, loss 5.054076194763184\n",
      "Epoch 0: |          | 1052/? [14:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1052, loss 5.593998908996582\n",
      "Epoch 0: |          | 1053/? [14:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1053, loss 5.580362319946289\n",
      "Epoch 0: |          | 1054/? [14:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1054, loss 4.917304515838623\n",
      "Epoch 0: |          | 1055/? [14:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1055, loss 4.740593910217285\n",
      "Epoch 0: |          | 1056/? [14:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1056, loss 4.863272190093994\n",
      "Epoch 0: |          | 1057/? [14:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1057, loss 5.501001358032227\n",
      "Epoch 0: |          | 1058/? [14:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1058, loss 5.012236595153809\n",
      "Epoch 0: |          | 1059/? [14:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1059, loss 5.727375030517578\n",
      "Epoch 0: |          | 1060/? [14:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1060, loss 5.592264175415039\n",
      "Epoch 0: |          | 1061/? [14:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1061, loss 4.109938621520996\n",
      "Epoch 0: |          | 1062/? [14:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1062, loss 5.846802234649658\n",
      "Epoch 0: |          | 1063/? [14:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1063, loss 5.273367881774902\n",
      "Epoch 0: |          | 1064/? [14:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1064, loss 5.517393112182617\n",
      "Epoch 0: |          | 1065/? [14:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1065, loss 4.359847068786621\n",
      "Epoch 0: |          | 1066/? [14:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1066, loss 5.363338470458984\n",
      "Epoch 0: |          | 1067/? [14:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1067, loss 4.670121192932129\n",
      "Epoch 0: |          | 1068/? [14:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1068, loss 5.003037929534912\n",
      "Epoch 0: |          | 1069/? [14:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1069, loss 5.314789772033691\n",
      "Epoch 0: |          | 1070/? [14:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1070, loss 5.157134532928467\n",
      "Epoch 0: |          | 1071/? [14:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1071, loss 5.385441780090332\n",
      "Epoch 0: |          | 1072/? [14:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1072, loss 5.463881015777588\n",
      "Epoch 0: |          | 1073/? [14:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1073, loss 5.916184425354004\n",
      "Epoch 0: |          | 1074/? [14:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1074, loss 5.1812286376953125\n",
      "Epoch 0: |          | 1075/? [14:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1075, loss 4.725826263427734\n",
      "Epoch 0: |          | 1076/? [14:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1076, loss 5.721076965332031\n",
      "Epoch 0: |          | 1077/? [14:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1077, loss 5.169663906097412\n",
      "Epoch 0: |          | 1078/? [14:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1078, loss 5.229341506958008\n",
      "Epoch 0: |          | 1079/? [14:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1079, loss 6.318957328796387\n",
      "Epoch 0: |          | 1080/? [14:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1080, loss 5.440110206604004\n",
      "Epoch 0: |          | 1081/? [14:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1081, loss 5.581582069396973\n",
      "Epoch 0: |          | 1082/? [14:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1082, loss 4.850460529327393\n",
      "Epoch 0: |          | 1083/? [14:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1083, loss 4.975619316101074\n",
      "Epoch 0: |          | 1084/? [14:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1084, loss 4.5529279708862305\n",
      "Epoch 0: |          | 1085/? [14:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1085, loss 5.068064212799072\n",
      "Epoch 0: |          | 1086/? [14:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1086, loss 5.308196067810059\n",
      "Epoch 0: |          | 1087/? [14:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1087, loss 5.9615983963012695\n",
      "Epoch 0: |          | 1088/? [14:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1088, loss 5.519591331481934\n",
      "Epoch 0: |          | 1089/? [14:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1089, loss 5.979390621185303\n",
      "Epoch 0: |          | 1090/? [14:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1090, loss 5.646551609039307\n",
      "Epoch 0: |          | 1091/? [14:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1091, loss 5.170454978942871\n",
      "Epoch 0: |          | 1092/? [14:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1092, loss 5.34818172454834\n",
      "Epoch 0: |          | 1093/? [14:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1093, loss 4.864631652832031\n",
      "Epoch 0: |          | 1094/? [14:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1094, loss 5.296046733856201\n",
      "Epoch 0: |          | 1095/? [14:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1095, loss 5.388603210449219\n",
      "Epoch 0: |          | 1096/? [14:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1096, loss 5.510256767272949\n",
      "Epoch 0: |          | 1097/? [14:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1097, loss 5.122129440307617\n",
      "Epoch 0: |          | 1098/? [14:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1098, loss 4.202176094055176\n",
      "Epoch 0: |          | 1099/? [14:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1099, loss 5.127054691314697\n",
      "Epoch 0: |          | 1100/? [15:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1100, loss 5.409573554992676\n",
      "Epoch 0: |          | 1101/? [15:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1101, loss 4.814360618591309\n",
      "Epoch 0: |          | 1102/? [15:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1102, loss 5.773034572601318\n",
      "Epoch 0: |          | 1103/? [15:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1103, loss 6.5905303955078125\n",
      "Epoch 0: |          | 1104/? [15:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1104, loss 5.559553623199463\n",
      "Epoch 0: |          | 1105/? [15:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1105, loss 5.517391681671143\n",
      "Epoch 0: |          | 1106/? [15:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1106, loss 5.0961503982543945\n",
      "Epoch 0: |          | 1107/? [15:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1107, loss 5.14654016494751\n",
      "Epoch 0: |          | 1108/? [15:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1108, loss 5.06404447555542\n",
      "Epoch 0: |          | 1109/? [15:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1109, loss 4.562893867492676\n",
      "Epoch 0: |          | 1110/? [15:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1110, loss 5.860217094421387\n",
      "Epoch 0: |          | 1111/? [15:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1111, loss 5.45481538772583\n",
      "Epoch 0: |          | 1112/? [15:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1112, loss 5.355237007141113\n",
      "Epoch 0: |          | 1113/? [15:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1113, loss 5.004354953765869\n",
      "Epoch 0: |          | 1114/? [15:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1114, loss 4.505448341369629\n",
      "Epoch 0: |          | 1115/? [15:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1115, loss 4.089249610900879\n",
      "Epoch 0: |          | 1116/? [15:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1116, loss 4.815788745880127\n",
      "Epoch 0: |          | 1117/? [15:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1117, loss 5.055968761444092\n",
      "Epoch 0: |          | 1118/? [15:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1118, loss 4.963329315185547\n",
      "Epoch 0: |          | 1119/? [15:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1119, loss 5.7495856285095215\n",
      "Epoch 0: |          | 1120/? [15:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1120, loss 5.305228233337402\n",
      "Epoch 0: |          | 1121/? [15:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1121, loss 5.534013748168945\n",
      "Epoch 0: |          | 1122/? [15:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1122, loss 5.579628944396973\n",
      "Epoch 0: |          | 1123/? [15:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1123, loss 5.219479560852051\n",
      "Epoch 0: |          | 1124/? [15:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1124, loss 5.5838093757629395\n",
      "Epoch 0: |          | 1125/? [15:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1125, loss 4.705694198608398\n",
      "Epoch 0: |          | 1126/? [15:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1126, loss 4.7193379402160645\n",
      "Epoch 0: |          | 1127/? [15:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1127, loss 5.093406677246094\n",
      "Epoch 0: |          | 1128/? [15:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1128, loss 5.049862861633301\n",
      "Epoch 0: |          | 1129/? [15:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1129, loss 5.268404960632324\n",
      "Epoch 0: |          | 1130/? [15:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1130, loss 5.3923234939575195\n",
      "Epoch 0: |          | 1131/? [15:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1131, loss 5.508820533752441\n",
      "Epoch 0: |          | 1132/? [15:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1132, loss 3.9841666221618652\n",
      "Epoch 0: |          | 1133/? [15:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1133, loss 5.24675178527832\n",
      "Epoch 0: |          | 1134/? [15:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1134, loss 4.879093170166016\n",
      "Epoch 0: |          | 1135/? [15:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1135, loss 5.495133876800537\n",
      "Epoch 0: |          | 1136/? [15:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1136, loss 5.175583839416504\n",
      "Epoch 0: |          | 1137/? [15:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1137, loss 5.1970720291137695\n",
      "Epoch 0: |          | 1138/? [15:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1138, loss 5.816824913024902\n",
      "Epoch 0: |          | 1139/? [15:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1139, loss 6.733071804046631\n",
      "Epoch 0: |          | 1140/? [15:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1140, loss 5.425531387329102\n",
      "Epoch 0: |          | 1141/? [15:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1141, loss 5.657880783081055\n",
      "Epoch 0: |          | 1142/? [15:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1142, loss 5.580486297607422\n",
      "Epoch 0: |          | 1143/? [15:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1143, loss 5.706439018249512\n",
      "Epoch 0: |          | 1144/? [15:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1144, loss 5.361969947814941\n",
      "Epoch 0: |          | 1145/? [15:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1145, loss 5.108303070068359\n",
      "Epoch 0: |          | 1146/? [15:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1146, loss 5.442855358123779\n",
      "Epoch 0: |          | 1147/? [15:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1147, loss 4.5115838050842285\n",
      "Epoch 0: |          | 1148/? [15:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1148, loss 4.945503234863281\n",
      "Epoch 0: |          | 1149/? [15:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1149, loss 6.307051181793213\n",
      "Epoch 0: |          | 1150/? [15:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1150, loss 5.380609035491943\n",
      "Epoch 0: |          | 1151/? [15:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1151, loss 5.809179782867432\n",
      "Epoch 0: |          | 1152/? [15:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1152, loss 4.903853893280029\n",
      "Epoch 0: |          | 1153/? [15:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1153, loss 5.2982892990112305\n",
      "Epoch 0: |          | 1154/? [15:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1154, loss 4.920499324798584\n",
      "Epoch 0: |          | 1155/? [15:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1155, loss 5.247078895568848\n",
      "Epoch 0: |          | 1156/? [15:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1156, loss 5.445400238037109\n",
      "Epoch 0: |          | 1157/? [15:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1157, loss 5.458306789398193\n",
      "Epoch 0: |          | 1158/? [15:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1158, loss 6.0831804275512695\n",
      "Epoch 0: |          | 1159/? [15:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1159, loss 4.249764442443848\n",
      "Epoch 0: |          | 1160/? [15:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1160, loss 5.640756607055664\n",
      "Epoch 0: |          | 1161/? [15:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1161, loss 5.5975751876831055\n",
      "Epoch 0: |          | 1162/? [15:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1162, loss 5.440459728240967\n",
      "Epoch 0: |          | 1163/? [15:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1163, loss 5.839261531829834\n",
      "Epoch 0: |          | 1164/? [15:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1164, loss 5.751039981842041\n",
      "Epoch 0: |          | 1165/? [15:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1165, loss 4.7204694747924805\n",
      "Epoch 0: |          | 1166/? [15:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1166, loss 5.395464897155762\n",
      "Epoch 0: |          | 1167/? [15:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1167, loss 5.69150972366333\n",
      "Epoch 0: |          | 1168/? [15:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1168, loss 6.042662620544434\n",
      "Epoch 0: |          | 1169/? [15:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1169, loss 4.80359411239624\n",
      "Epoch 0: |          | 1170/? [15:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1170, loss 5.436948776245117\n",
      "Epoch 0: |          | 1171/? [15:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1171, loss 5.300307273864746\n",
      "Epoch 0: |          | 1172/? [15:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1172, loss 4.54393196105957\n",
      "Epoch 0: |          | 1173/? [15:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1173, loss 5.399228096008301\n",
      "Epoch 0: |          | 1174/? [16:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1174, loss 4.82593297958374\n",
      "Epoch 0: |          | 1175/? [16:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1175, loss 5.641220569610596\n",
      "Epoch 0: |          | 1176/? [16:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1176, loss 5.518824577331543\n",
      "Epoch 0: |          | 1177/? [16:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1177, loss 5.641718864440918\n",
      "Epoch 0: |          | 1178/? [16:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1178, loss 4.987672328948975\n",
      "Epoch 0: |          | 1179/? [16:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1179, loss 5.483645439147949\n",
      "Epoch 0: |          | 1180/? [16:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1180, loss 5.302788257598877\n",
      "Epoch 0: |          | 1181/? [16:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1181, loss 5.435269355773926\n",
      "Epoch 0: |          | 1182/? [16:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1182, loss 5.175215244293213\n",
      "Epoch 0: |          | 1183/? [16:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1183, loss 4.953501224517822\n",
      "Epoch 0: |          | 1184/? [16:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1184, loss 4.997452735900879\n",
      "Epoch 0: |          | 1185/? [16:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1185, loss 5.220264911651611\n",
      "Epoch 0: |          | 1186/? [16:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1186, loss 5.320658206939697\n",
      "Epoch 0: |          | 1187/? [16:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1187, loss 5.100763320922852\n",
      "Epoch 0: |          | 1188/? [16:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1188, loss 5.546298980712891\n",
      "Epoch 0: |          | 1189/? [16:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1189, loss 5.612429141998291\n",
      "Epoch 0: |          | 1190/? [16:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1190, loss 5.055073261260986\n",
      "Epoch 0: |          | 1191/? [16:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1191, loss 5.071171760559082\n",
      "Epoch 0: |          | 1192/? [16:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1192, loss 5.47492790222168\n",
      "Epoch 0: |          | 1193/? [16:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1193, loss 4.928553581237793\n",
      "Epoch 0: |          | 1194/? [16:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1194, loss 4.38505744934082\n",
      "Epoch 0: |          | 1195/? [16:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1195, loss 5.250988960266113\n",
      "Epoch 0: |          | 1196/? [16:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1196, loss 5.364659786224365\n",
      "Epoch 0: |          | 1197/? [16:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1197, loss 5.318653583526611\n",
      "Epoch 0: |          | 1198/? [16:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1198, loss 5.23403263092041\n",
      "Epoch 0: |          | 1199/? [16:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1199, loss 5.454360008239746\n",
      "Epoch 0: |          | 1200/? [16:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1200, loss 4.620126724243164\n",
      "Epoch 0: |          | 1201/? [16:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1201, loss 5.4722514152526855\n",
      "Epoch 0: |          | 1202/? [16:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1202, loss 4.9315266609191895\n",
      "Epoch 0: |          | 1203/? [16:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1203, loss 5.057955741882324\n",
      "Epoch 0: |          | 1204/? [16:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1204, loss 4.36070442199707\n",
      "Epoch 0: |          | 1205/? [16:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1205, loss 5.101587295532227\n",
      "Epoch 0: |          | 1206/? [16:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1206, loss 5.126665115356445\n",
      "Epoch 0: |          | 1207/? [16:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1207, loss 5.592052459716797\n",
      "Epoch 0: |          | 1208/? [16:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1208, loss 5.6617889404296875\n",
      "Epoch 0: |          | 1209/? [16:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1209, loss 5.240910053253174\n",
      "Epoch 0: |          | 1210/? [16:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1210, loss 5.5735063552856445\n",
      "Epoch 0: |          | 1211/? [16:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1211, loss 5.586808204650879\n",
      "Epoch 0: |          | 1212/? [16:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1212, loss 5.344986915588379\n",
      "Epoch 0: |          | 1213/? [16:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1213, loss 5.033695220947266\n",
      "Epoch 0: |          | 1214/? [16:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1214, loss 5.816231727600098\n",
      "Epoch 0: |          | 1215/? [16:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1215, loss 4.901398658752441\n",
      "Epoch 0: |          | 1216/? [16:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1216, loss 5.4359893798828125\n",
      "Epoch 0: |          | 1217/? [16:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1217, loss 5.523583889007568\n",
      "Epoch 0: |          | 1218/? [16:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1218, loss 5.470643520355225\n",
      "Epoch 0: |          | 1219/? [16:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1219, loss 5.032664775848389\n",
      "Epoch 0: |          | 1220/? [16:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1220, loss 5.873330116271973\n",
      "Epoch 0: |          | 1221/? [16:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1221, loss 5.667996406555176\n",
      "Epoch 0: |          | 1222/? [16:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1222, loss 4.138840675354004\n",
      "Epoch 0: |          | 1223/? [16:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1223, loss 4.40330171585083\n",
      "Epoch 0: |          | 1224/? [16:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1224, loss 4.83736515045166\n",
      "Epoch 0: |          | 1225/? [16:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1225, loss 5.482425689697266\n",
      "Epoch 0: |          | 1226/? [16:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1226, loss 5.577429294586182\n",
      "Epoch 0: |          | 1227/? [16:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1227, loss 5.124382495880127\n",
      "Epoch 0: |          | 1228/? [16:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1228, loss 5.23321008682251\n",
      "Epoch 0: |          | 1229/? [16:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1229, loss 4.706366539001465\n",
      "Epoch 0: |          | 1230/? [16:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1230, loss 5.349597454071045\n",
      "Epoch 0: |          | 1231/? [16:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1231, loss 5.39401388168335\n",
      "Epoch 0: |          | 1232/? [16:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1232, loss 5.529758930206299\n",
      "Epoch 0: |          | 1233/? [16:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1233, loss 5.391082763671875\n",
      "Epoch 0: |          | 1234/? [16:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1234, loss 4.131885051727295\n",
      "Epoch 0: |          | 1235/? [16:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1235, loss 5.306149005889893\n",
      "Epoch 0: |          | 1236/? [16:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1236, loss 4.729857444763184\n",
      "Epoch 0: |          | 1237/? [16:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1237, loss 5.293280601501465\n",
      "Epoch 0: |          | 1238/? [16:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1238, loss 5.023959159851074\n",
      "Epoch 0: |          | 1239/? [16:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1239, loss 5.018547058105469\n",
      "Epoch 0: |          | 1240/? [16:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1240, loss 5.71605110168457\n",
      "Epoch 0: |          | 1241/? [16:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1241, loss 5.161637306213379\n",
      "Epoch 0: |          | 1242/? [16:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1242, loss 4.995190620422363\n",
      "Epoch 0: |          | 1243/? [16:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1243, loss 4.714601039886475\n",
      "Epoch 0: |          | 1244/? [16:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1244, loss 4.954442024230957\n",
      "Epoch 0: |          | 1245/? [16:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1245, loss 4.525518417358398\n",
      "Epoch 0: |          | 1246/? [17:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1246, loss 5.430017471313477\n",
      "Epoch 0: |          | 1247/? [17:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1247, loss 5.390618324279785\n",
      "Epoch 0: |          | 1248/? [17:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1248, loss 4.810784339904785\n",
      "Epoch 0: |          | 1249/? [17:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1249, loss 4.915114402770996\n",
      "Epoch 0: |          | 1250/? [17:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1250, loss 5.078835964202881\n",
      "Epoch 0: |          | 1251/? [17:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1251, loss 4.859832763671875\n",
      "Epoch 0: |          | 1252/? [17:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1252, loss 5.643635272979736\n",
      "Epoch 0: |          | 1253/? [17:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1253, loss 5.153294086456299\n",
      "Epoch 0: |          | 1254/? [17:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1254, loss 4.254480838775635\n",
      "Epoch 0: |          | 1255/? [17:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1255, loss 5.730116367340088\n",
      "Epoch 0: |          | 1256/? [17:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1256, loss 4.745254993438721\n",
      "Epoch 0: |          | 1257/? [17:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1257, loss 4.840959072113037\n",
      "Epoch 0: |          | 1258/? [17:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1258, loss 5.524287223815918\n",
      "Epoch 0: |          | 1259/? [17:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1259, loss 5.349067211151123\n",
      "Epoch 0: |          | 1260/? [17:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1260, loss 5.632445812225342\n",
      "Epoch 0: |          | 1261/? [17:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1261, loss 5.114729881286621\n",
      "Epoch 0: |          | 1262/? [17:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1262, loss 4.978686332702637\n",
      "Epoch 0: |          | 1263/? [17:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1263, loss 5.506272315979004\n",
      "Epoch 0: |          | 1264/? [17:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1264, loss 5.713553428649902\n",
      "Epoch 0: |          | 1265/? [17:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1265, loss 5.388038158416748\n",
      "Epoch 0: |          | 1266/? [17:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1266, loss 5.003792762756348\n",
      "Epoch 0: |          | 1267/? [17:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1267, loss 5.149682521820068\n",
      "Epoch 0: |          | 1268/? [17:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1268, loss 5.028461456298828\n",
      "Epoch 0: |          | 1269/? [17:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1269, loss 4.532253265380859\n",
      "Epoch 0: |          | 1270/? [17:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1270, loss 4.901851177215576\n",
      "Epoch 0: |          | 1271/? [17:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1271, loss 5.062051773071289\n",
      "Epoch 0: |          | 1272/? [17:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1272, loss 4.552056789398193\n",
      "Epoch 0: |          | 1273/? [17:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1273, loss 5.505562782287598\n",
      "Epoch 0: |          | 1274/? [17:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1274, loss 4.258021354675293\n",
      "Epoch 0: |          | 1275/? [17:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1275, loss 4.736253261566162\n",
      "Epoch 0: |          | 1276/? [17:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1276, loss 5.241755485534668\n",
      "Epoch 0: |          | 1277/? [17:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1277, loss 4.73654317855835\n",
      "Epoch 0: |          | 1278/? [17:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1278, loss 5.404629707336426\n",
      "Epoch 0: |          | 1279/? [17:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1279, loss 5.3907880783081055\n",
      "Epoch 0: |          | 1280/? [17:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1280, loss 4.426975250244141\n",
      "Epoch 0: |          | 1281/? [17:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1281, loss 4.779297828674316\n",
      "Epoch 0: |          | 1282/? [17:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1282, loss 4.5602827072143555\n",
      "Epoch 0: |          | 1283/? [17:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1283, loss 5.418219566345215\n",
      "Epoch 0: |          | 1284/? [17:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1284, loss 4.343779563903809\n",
      "Epoch 0: |          | 1285/? [17:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1285, loss 5.666621208190918\n",
      "Epoch 0: |          | 1286/? [17:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1286, loss 4.003663063049316\n",
      "Epoch 0: |          | 1287/? [17:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1287, loss 5.439633369445801\n",
      "Epoch 0: |          | 1288/? [17:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1288, loss 5.297019004821777\n",
      "Epoch 0: |          | 1289/? [17:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1289, loss 4.103568077087402\n",
      "Epoch 0: |          | 1290/? [17:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1290, loss 5.395528793334961\n",
      "Epoch 0: |          | 1291/? [17:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1291, loss 6.188660621643066\n",
      "Epoch 0: |          | 1292/? [17:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1292, loss 5.635201454162598\n",
      "Epoch 0: |          | 1293/? [17:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1293, loss 4.889761924743652\n",
      "Epoch 0: |          | 1294/? [17:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1294, loss 5.185117244720459\n",
      "Epoch 0: |          | 1295/? [17:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1295, loss 5.107059001922607\n",
      "Epoch 0: |          | 1296/? [17:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1296, loss 4.293513774871826\n",
      "Epoch 0: |          | 1297/? [17:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1297, loss 5.433770656585693\n",
      "Epoch 0: |          | 1298/? [17:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1298, loss 5.183027744293213\n",
      "Epoch 0: |          | 1299/? [17:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1299, loss 4.035243034362793\n",
      "Epoch 0: |          | 1300/? [17:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1300, loss 5.306239128112793\n",
      "Epoch 0: |          | 1301/? [17:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1301, loss 4.9715495109558105\n",
      "Epoch 0: |          | 1302/? [17:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1302, loss 5.032899379730225\n",
      "Epoch 0: |          | 1303/? [17:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1303, loss 4.8079447746276855\n",
      "Epoch 0: |          | 1304/? [17:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1304, loss 5.650494575500488\n",
      "Epoch 0: |          | 1305/? [17:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1305, loss 4.439882278442383\n",
      "Epoch 0: |          | 1306/? [17:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1306, loss 5.050359725952148\n",
      "Epoch 0: |          | 1307/? [17:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1307, loss 4.517744064331055\n",
      "Epoch 0: |          | 1308/? [17:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1308, loss 4.4703874588012695\n",
      "Epoch 0: |          | 1309/? [17:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1309, loss 4.909707069396973\n",
      "Epoch 0: |          | 1310/? [17:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1310, loss 5.239642143249512\n",
      "Epoch 0: |          | 1311/? [17:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1311, loss 4.595777988433838\n",
      "Epoch 0: |          | 1312/? [17:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1312, loss 4.35321569442749\n",
      "Epoch 0: |          | 1313/? [17:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1313, loss 5.579770088195801\n",
      "Epoch 0: |          | 1314/? [17:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1314, loss 4.710984230041504\n",
      "Epoch 0: |          | 1315/? [17:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1315, loss 5.55088996887207\n",
      "Epoch 0: |          | 1316/? [17:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1316, loss 5.256943225860596\n",
      "Epoch 0: |          | 1317/? [17:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1317, loss 4.808931350708008\n",
      "Epoch 0: |          | 1318/? [17:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1318, loss 5.166859149932861\n",
      "Epoch 0: |          | 1319/? [18:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1319, loss 5.267114162445068\n",
      "Epoch 0: |          | 1320/? [18:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1320, loss 4.892098426818848\n",
      "Epoch 0: |          | 1321/? [18:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1321, loss 5.318058013916016\n",
      "Epoch 0: |          | 1322/? [18:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1322, loss 5.470487594604492\n",
      "Epoch 0: |          | 1323/? [18:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1323, loss 4.699163913726807\n",
      "Epoch 0: |          | 1324/? [18:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1324, loss 5.697576999664307\n",
      "Epoch 0: |          | 1325/? [18:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1325, loss 5.725220203399658\n",
      "Epoch 0: |          | 1326/? [18:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1326, loss 5.193442344665527\n",
      "Epoch 0: |          | 1327/? [18:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1327, loss 5.0517191886901855\n",
      "Epoch 0: |          | 1328/? [18:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1328, loss 4.73179292678833\n",
      "Epoch 0: |          | 1329/? [18:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1329, loss 5.474646091461182\n",
      "Epoch 0: |          | 1330/? [18:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1330, loss 5.184152126312256\n",
      "Epoch 0: |          | 1331/? [18:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1331, loss 5.238987922668457\n",
      "Epoch 0: |          | 1332/? [18:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1332, loss 5.054959774017334\n",
      "Epoch 0: |          | 1333/? [18:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1333, loss 4.890490531921387\n",
      "Epoch 0: |          | 1334/? [18:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1334, loss 5.001800060272217\n",
      "Epoch 0: |          | 1335/? [18:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1335, loss 5.074738502502441\n",
      "Epoch 0: |          | 1336/? [18:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1336, loss 4.655372142791748\n",
      "Epoch 0: |          | 1337/? [18:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1337, loss 5.439881324768066\n",
      "Epoch 0: |          | 1338/? [18:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1338, loss 4.2907395362854\n",
      "Epoch 0: |          | 1339/? [18:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1339, loss 4.993449687957764\n",
      "Epoch 0: |          | 1340/? [18:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1340, loss 4.406987190246582\n",
      "Epoch 0: |          | 1341/? [18:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1341, loss 5.294796943664551\n",
      "Epoch 0: |          | 1342/? [18:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1342, loss 5.5890212059021\n",
      "Epoch 0: |          | 1343/? [18:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1343, loss 4.905961036682129\n",
      "Epoch 0: |          | 1344/? [18:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1344, loss 5.031094074249268\n",
      "Epoch 0: |          | 1345/? [18:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1345, loss 5.14840030670166\n",
      "Epoch 0: |          | 1346/? [18:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1346, loss 6.449709415435791\n",
      "Epoch 0: |          | 1347/? [18:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1347, loss 5.583110332489014\n",
      "Epoch 0: |          | 1348/? [18:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1348, loss 5.969061851501465\n",
      "Epoch 0: |          | 1349/? [18:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1349, loss 5.467104434967041\n",
      "Epoch 0: |          | 1350/? [18:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1350, loss 6.028729438781738\n",
      "Epoch 0: |          | 1351/? [18:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1351, loss 5.493439197540283\n",
      "Epoch 0: |          | 1352/? [18:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1352, loss 4.396944522857666\n",
      "Epoch 0: |          | 1353/? [18:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1353, loss 4.735905647277832\n",
      "Epoch 0: |          | 1354/? [18:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1354, loss 5.687050819396973\n",
      "Epoch 0: |          | 1355/? [18:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1355, loss 5.639523983001709\n",
      "Epoch 0: |          | 1356/? [18:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1356, loss 5.233735084533691\n",
      "Epoch 0: |          | 1357/? [18:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1357, loss 4.889534950256348\n",
      "Epoch 0: |          | 1358/? [18:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1358, loss 5.133683204650879\n",
      "Epoch 0: |          | 1359/? [18:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1359, loss 4.9140214920043945\n",
      "Epoch 0: |          | 1360/? [18:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1360, loss 5.1760663986206055\n",
      "Epoch 0: |          | 1361/? [18:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1361, loss 5.143482685089111\n",
      "Epoch 0: |          | 1362/? [18:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1362, loss 5.028353691101074\n",
      "Epoch 0: |          | 1363/? [18:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1363, loss 4.379049301147461\n",
      "Epoch 0: |          | 1364/? [18:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1364, loss 4.956576347351074\n",
      "Epoch 0: |          | 1365/? [18:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1365, loss 4.546612739562988\n",
      "Epoch 0: |          | 1366/? [18:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1366, loss 5.318797588348389\n",
      "Epoch 0: |          | 1367/? [18:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1367, loss 4.590060234069824\n",
      "Epoch 0: |          | 1368/? [18:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1368, loss 4.442855358123779\n",
      "Epoch 0: |          | 1369/? [18:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1369, loss 5.095372200012207\n",
      "Epoch 0: |          | 1370/? [18:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1370, loss 4.501025199890137\n",
      "Epoch 0: |          | 1371/? [18:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1371, loss 5.7249956130981445\n",
      "Epoch 0: |          | 1372/? [18:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1372, loss 4.856011390686035\n",
      "Epoch 0: |          | 1373/? [18:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1373, loss 5.568792819976807\n",
      "Epoch 0: |          | 1374/? [18:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1374, loss 4.45607852935791\n",
      "Epoch 0: |          | 1375/? [18:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1375, loss 5.132929801940918\n",
      "Epoch 0: |          | 1376/? [18:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1376, loss 5.187031269073486\n",
      "Epoch 0: |          | 1377/? [18:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1377, loss 5.005084037780762\n",
      "Epoch 0: |          | 1378/? [18:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1378, loss 5.467773914337158\n",
      "Epoch 0: |          | 1379/? [18:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1379, loss 5.163086891174316\n",
      "Epoch 0: |          | 1380/? [18:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1380, loss 5.189284801483154\n",
      "Epoch 0: |          | 1381/? [18:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1381, loss 5.37825345993042\n",
      "Epoch 0: |          | 1382/? [18:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1382, loss 4.81534481048584\n",
      "Epoch 0: |          | 1383/? [18:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1383, loss 5.256289482116699\n",
      "Epoch 0: |          | 1384/? [18:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1384, loss 5.205147743225098\n",
      "Epoch 0: |          | 1385/? [18:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1385, loss 5.0422492027282715\n",
      "Epoch 0: |          | 1386/? [18:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1386, loss 4.983321666717529\n",
      "Epoch 0: |          | 1387/? [18:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1387, loss 5.0756378173828125\n",
      "Epoch 0: |          | 1388/? [18:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1388, loss 4.249040603637695\n",
      "Epoch 0: |          | 1389/? [18:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1389, loss 5.326887130737305\n",
      "Epoch 0: |          | 1390/? [18:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1390, loss 5.637641906738281\n",
      "Epoch 0: |          | 1391/? [18:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1391, loss 5.270392894744873\n",
      "Epoch 0: |          | 1392/? [18:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1392, loss 4.665350437164307\n",
      "Epoch 0: |          | 1393/? [19:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1393, loss 4.967292785644531\n",
      "Epoch 0: |          | 1394/? [19:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1394, loss 4.817766189575195\n",
      "Epoch 0: |          | 1395/? [19:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1395, loss 5.247600555419922\n",
      "Epoch 0: |          | 1396/? [19:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1396, loss 5.231575965881348\n",
      "Epoch 0: |          | 1397/? [19:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1397, loss 4.219149589538574\n",
      "Epoch 0: |          | 1398/? [19:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1398, loss 5.468973159790039\n",
      "Epoch 0: |          | 1399/? [19:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1399, loss 5.635961532592773\n",
      "Epoch 0: |          | 1400/? [19:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1400, loss 4.505109786987305\n",
      "Epoch 0: |          | 1401/? [19:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1401, loss 5.603255748748779\n",
      "Epoch 0: |          | 1402/? [19:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1402, loss 5.111412525177002\n",
      "Epoch 0: |          | 1403/? [19:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1403, loss 5.263411045074463\n",
      "Epoch 0: |          | 1404/? [19:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1404, loss 5.2000041007995605\n",
      "Epoch 0: |          | 1405/? [19:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1405, loss 5.602557182312012\n",
      "Epoch 0: |          | 1406/? [19:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1406, loss 5.410688877105713\n",
      "Epoch 0: |          | 1407/? [19:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1407, loss 5.651264190673828\n",
      "Epoch 0: |          | 1408/? [19:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1408, loss 4.816583633422852\n",
      "Epoch 0: |          | 1409/? [19:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1409, loss 5.044737815856934\n",
      "Epoch 0: |          | 1410/? [19:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1410, loss 5.097635269165039\n",
      "Epoch 0: |          | 1411/? [19:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1411, loss 5.449533462524414\n",
      "Epoch 0: |          | 1412/? [19:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1412, loss 4.696156978607178\n",
      "Epoch 0: |          | 1413/? [19:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1413, loss 4.639109134674072\n",
      "Epoch 0: |          | 1414/? [19:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1414, loss 4.624438762664795\n",
      "Epoch 0: |          | 1415/? [19:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1415, loss 5.1209797859191895\n",
      "Epoch 0: |          | 1416/? [19:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1416, loss 5.505334854125977\n",
      "Epoch 0: |          | 1417/? [19:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1417, loss 4.953238010406494\n",
      "Epoch 0: |          | 1418/? [19:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1418, loss 5.309823036193848\n",
      "Epoch 0: |          | 1419/? [19:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1419, loss 5.001383304595947\n",
      "Epoch 0: |          | 1420/? [19:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1420, loss 4.997040748596191\n",
      "Epoch 0: |          | 1421/? [19:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1421, loss 4.382894992828369\n",
      "Epoch 0: |          | 1422/? [19:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1422, loss 5.413417816162109\n",
      "Epoch 0: |          | 1423/? [19:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1423, loss 5.497010707855225\n",
      "Epoch 0: |          | 1424/? [19:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1424, loss 4.975937843322754\n",
      "Epoch 0: |          | 1425/? [19:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1425, loss 5.248939514160156\n",
      "Epoch 0: |          | 1426/? [19:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1426, loss 4.814873218536377\n",
      "Epoch 0: |          | 1427/? [19:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1427, loss 5.32174015045166\n",
      "Epoch 0: |          | 1428/? [19:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1428, loss 5.412829399108887\n",
      "Epoch 0: |          | 1429/? [19:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1429, loss 5.325736045837402\n",
      "Epoch 0: |          | 1430/? [19:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1430, loss 5.391043663024902\n",
      "Epoch 0: |          | 1431/? [19:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1431, loss 5.221233367919922\n",
      "Epoch 0: |          | 1432/? [19:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1432, loss 5.140814781188965\n",
      "Epoch 0: |          | 1433/? [19:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1433, loss 5.03178071975708\n",
      "Epoch 0: |          | 1434/? [19:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1434, loss 5.131436347961426\n",
      "Epoch 0: |          | 1435/? [19:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1435, loss 4.706385135650635\n",
      "Epoch 0: |          | 1436/? [19:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1436, loss 5.0692138671875\n",
      "Epoch 0: |          | 1437/? [19:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1437, loss 4.29910945892334\n",
      "Epoch 0: |          | 1438/? [19:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1438, loss 6.206684112548828\n",
      "Epoch 0: |          | 1439/? [19:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1439, loss 5.364195823669434\n",
      "Epoch 0: |          | 1440/? [19:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1440, loss 5.272377967834473\n",
      "Epoch 0: |          | 1441/? [19:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1441, loss 5.516592979431152\n",
      "Epoch 0: |          | 1442/? [19:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1442, loss 5.659506797790527\n",
      "Epoch 0: |          | 1443/? [19:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1443, loss 4.635995388031006\n",
      "Epoch 0: |          | 1444/? [19:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1444, loss 4.824368476867676\n",
      "Epoch 0: |          | 1445/? [19:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1445, loss 5.487098217010498\n",
      "Epoch 0: |          | 1446/? [19:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1446, loss 5.0379862785339355\n",
      "Epoch 0: |          | 1447/? [19:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1447, loss 5.108706474304199\n",
      "Epoch 0: |          | 1448/? [19:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1448, loss 4.870520114898682\n",
      "Epoch 0: |          | 1449/? [19:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1449, loss 5.2210693359375\n",
      "Epoch 0: |          | 1450/? [19:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1450, loss 5.385170936584473\n",
      "Epoch 0: |          | 1451/? [19:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1451, loss 5.679701805114746\n",
      "Epoch 0: |          | 1452/? [19:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1452, loss 5.145267963409424\n",
      "Epoch 0: |          | 1453/? [19:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1453, loss 4.326554775238037\n",
      "Epoch 0: |          | 1454/? [19:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1454, loss 4.98303747177124\n",
      "Epoch 0: |          | 1455/? [19:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1455, loss 5.166745185852051\n",
      "Epoch 0: |          | 1456/? [19:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1456, loss 4.768649101257324\n",
      "Epoch 0: |          | 1457/? [19:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1457, loss 4.910886287689209\n",
      "Epoch 0: |          | 1458/? [19:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1458, loss 4.9126482009887695\n",
      "Epoch 0: |          | 1459/? [19:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1459, loss 5.282968044281006\n",
      "Epoch 0: |          | 1460/? [19:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1460, loss 5.061326026916504\n",
      "Epoch 0: |          | 1461/? [19:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1461, loss 5.261015892028809\n",
      "Epoch 0: |          | 1462/? [19:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1462, loss 5.5234599113464355\n",
      "Epoch 0: |          | 1463/? [19:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1463, loss 5.4218363761901855\n",
      "Epoch 0: |          | 1464/? [19:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1464, loss 4.736837863922119\n",
      "Epoch 0: |          | 1465/? [19:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1465, loss 4.9957475662231445\n",
      "Epoch 0: |          | 1466/? [20:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1466, loss 4.6327314376831055\n",
      "Epoch 0: |          | 1467/? [20:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1467, loss 5.494442939758301\n",
      "Epoch 0: |          | 1468/? [20:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1468, loss 5.200439453125\n",
      "Epoch 0: |          | 1469/? [20:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1469, loss 4.511603832244873\n",
      "Epoch 0: |          | 1470/? [20:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1470, loss 5.406586647033691\n",
      "Epoch 0: |          | 1471/? [20:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1471, loss 5.318408489227295\n",
      "Epoch 0: |          | 1472/? [20:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1472, loss 4.993213653564453\n",
      "Epoch 0: |          | 1473/? [20:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1473, loss 4.932754993438721\n",
      "Epoch 0: |          | 1474/? [20:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1474, loss 4.7360310554504395\n",
      "Epoch 0: |          | 1475/? [20:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1475, loss 4.467228412628174\n",
      "Epoch 0: |          | 1476/? [20:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1476, loss 5.088474750518799\n",
      "Epoch 0: |          | 1477/? [20:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1477, loss 5.129250526428223\n",
      "Epoch 0: |          | 1478/? [20:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1478, loss 4.969168663024902\n",
      "Epoch 0: |          | 1479/? [20:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1479, loss 5.677060127258301\n",
      "Epoch 0: |          | 1480/? [20:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1480, loss 5.281744003295898\n",
      "Epoch 0: |          | 1481/? [20:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1481, loss 5.030396461486816\n",
      "Epoch 0: |          | 1482/? [20:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1482, loss 5.122774600982666\n",
      "Epoch 0: |          | 1483/? [20:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1483, loss 4.587088108062744\n",
      "Epoch 0: |          | 1484/? [20:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1484, loss 4.872483730316162\n",
      "Epoch 0: |          | 1485/? [20:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1485, loss 5.284473896026611\n",
      "Epoch 0: |          | 1486/? [20:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1486, loss 4.994555950164795\n",
      "Epoch 0: |          | 1487/? [20:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1487, loss 4.565483093261719\n",
      "Epoch 0: |          | 1488/? [20:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1488, loss 5.288864612579346\n",
      "Epoch 0: |          | 1489/? [20:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1489, loss 5.1738176345825195\n",
      "Epoch 0: |          | 1490/? [20:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1490, loss 5.118620872497559\n",
      "Epoch 0: |          | 1491/? [20:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1491, loss 3.6992175579071045\n",
      "Epoch 0: |          | 1492/? [20:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1492, loss 4.509267807006836\n",
      "Epoch 0: |          | 1493/? [20:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1493, loss 4.778509140014648\n",
      "Epoch 0: |          | 1494/? [20:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1494, loss 4.9703593254089355\n",
      "Epoch 0: |          | 1495/? [20:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1495, loss 4.877535820007324\n",
      "Epoch 0: |          | 1496/? [20:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1496, loss 5.283547878265381\n",
      "Epoch 0: |          | 1497/? [20:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1497, loss 4.362563610076904\n",
      "Epoch 0: |          | 1498/? [20:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1498, loss 4.731282711029053\n",
      "Epoch 0: |          | 1499/? [20:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1499, loss 5.4424028396606445\n",
      "Epoch 0: |          | 1500/? [20:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1500, loss 5.285432815551758\n",
      "Epoch 0: |          | 1501/? [20:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1501, loss 4.998958110809326\n",
      "Epoch 0: |          | 1502/? [20:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1502, loss 5.211178302764893\n",
      "Epoch 0: |          | 1503/? [20:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1503, loss 4.7760910987854\n",
      "Epoch 0: |          | 1504/? [20:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1504, loss 5.439953327178955\n",
      "Epoch 0: |          | 1505/? [20:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1505, loss 5.429077625274658\n",
      "Epoch 0: |          | 1506/? [20:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1506, loss 5.096560001373291\n",
      "Epoch 0: |          | 1507/? [20:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1507, loss 4.756295204162598\n",
      "Epoch 0: |          | 1508/? [20:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1508, loss 4.961964130401611\n",
      "Epoch 0: |          | 1509/? [20:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1509, loss 4.914708137512207\n",
      "Epoch 0: |          | 1510/? [20:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1510, loss 5.271336555480957\n",
      "Epoch 0: |          | 1511/? [20:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1511, loss 4.706214904785156\n",
      "Epoch 0: |          | 1512/? [20:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1512, loss 5.669672966003418\n",
      "Epoch 0: |          | 1513/? [20:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1513, loss 5.4962005615234375\n",
      "Epoch 0: |          | 1514/? [20:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1514, loss 4.582162857055664\n",
      "Epoch 0: |          | 1515/? [20:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1515, loss 5.75888204574585\n",
      "Epoch 0: |          | 1516/? [20:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1516, loss 5.249028205871582\n",
      "Epoch 0: |          | 1517/? [20:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1517, loss 4.831752300262451\n",
      "Epoch 0: |          | 1518/? [20:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1518, loss 4.616010665893555\n",
      "Epoch 0: |          | 1519/? [20:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1519, loss 5.221735000610352\n",
      "Epoch 0: |          | 1520/? [20:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1520, loss 5.556387901306152\n",
      "Epoch 0: |          | 1521/? [20:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1521, loss 5.012440204620361\n",
      "Epoch 0: |          | 1522/? [20:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1522, loss 4.635933876037598\n",
      "Epoch 0: |          | 1523/? [20:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1523, loss 5.032581329345703\n",
      "Epoch 0: |          | 1524/? [20:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1524, loss 5.063022136688232\n",
      "Epoch 0: |          | 1525/? [20:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1525, loss 4.943523406982422\n",
      "Epoch 0: |          | 1526/? [20:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1526, loss 5.258271217346191\n",
      "Epoch 0: |          | 1527/? [20:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1527, loss 5.233292102813721\n",
      "Epoch 0: |          | 1528/? [20:55<00:00,  1.22it/s, v_num=30]ERROR: Input has inproper shape\n",
      "Epoch 0: |          | 1529/? [20:55<00:00,  1.22it/s, v_num=30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komputer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:132: `training_step` returned `None`. If this was on purpose, ignore this warning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VALIDATION: Batch 0, loss 5.40457010269165\n",
      "   VALIDATION: Batch 1, loss 4.248114585876465\n",
      "   VALIDATION: Batch 2, loss 5.5528974533081055\n",
      "   VALIDATION: Batch 3, loss 5.249224662780762\n",
      "   VALIDATION: Batch 4, loss 4.815767765045166\n",
      "   VALIDATION: Batch 5, loss 4.330968379974365\n",
      "   VALIDATION: Batch 6, loss 4.641824245452881\n",
      "   VALIDATION: Batch 7, loss 5.290759086608887\n",
      "   VALIDATION: Batch 8, loss 5.2831878662109375\n",
      "   VALIDATION: Batch 9, loss 5.276829242706299\n",
      "   VALIDATION: Batch 10, loss 5.089653015136719\n",
      "   VALIDATION: Batch 11, loss 4.591521263122559\n",
      "   VALIDATION: Batch 12, loss 5.114561557769775\n",
      "   VALIDATION: Batch 13, loss 5.373461723327637\n",
      "   VALIDATION: Batch 14, loss 5.020101547241211\n",
      "   VALIDATION: Batch 15, loss 4.8066606521606445\n",
      "   VALIDATION: Batch 16, loss 5.305370330810547\n",
      "   VALIDATION: Batch 17, loss 4.9691901206970215\n",
      "   VALIDATION: Batch 18, loss 4.105074405670166\n",
      "   VALIDATION: Batch 19, loss 5.231184005737305\n",
      "   VALIDATION: Batch 20, loss 5.386232376098633\n",
      "   VALIDATION: Batch 21, loss 5.613448143005371\n",
      "   VALIDATION: Batch 22, loss 5.2335004806518555\n",
      "   VALIDATION: Batch 23, loss 4.795811176300049\n",
      "   VALIDATION: Batch 24, loss 4.722841262817383\n",
      "   VALIDATION: Batch 25, loss 5.140699863433838\n",
      "   VALIDATION: Batch 26, loss 5.2465009689331055\n",
      "   VALIDATION: Batch 27, loss 5.223319053649902\n",
      "   VALIDATION: Batch 28, loss 4.881065368652344\n",
      "   VALIDATION: Batch 29, loss 5.252628803253174\n",
      "   VALIDATION: Batch 30, loss 4.774787902832031\n",
      "   VALIDATION: Batch 31, loss 5.006185531616211\n",
      "   VALIDATION: Batch 32, loss 5.69116735458374\n",
      "   VALIDATION: Batch 33, loss 3.726940870285034\n",
      "   VALIDATION: Batch 34, loss 4.914531230926514\n",
      "   VALIDATION: Batch 35, loss 5.2679643630981445\n",
      "   VALIDATION: Batch 36, loss 4.823400497436523\n",
      "   VALIDATION: Batch 37, loss 4.562178611755371\n",
      "   VALIDATION: Batch 38, loss 4.585891246795654\n",
      "   VALIDATION: Batch 39, loss 5.002264976501465\n",
      "   VALIDATION: Batch 40, loss 5.185469150543213\n",
      "   VALIDATION: Batch 41, loss 4.078338146209717\n",
      "   VALIDATION: Batch 42, loss 5.194619178771973\n",
      "   VALIDATION: Batch 43, loss 5.286362648010254\n",
      "   VALIDATION: Batch 44, loss 4.796147346496582\n",
      "   VALIDATION: Batch 45, loss 5.3497514724731445\n",
      "   VALIDATION: Batch 46, loss 4.528165817260742\n",
      "   VALIDATION: Batch 47, loss 5.459774971008301\n",
      "   VALIDATION: Batch 48, loss 5.504951477050781\n",
      "   VALIDATION: Batch 49, loss 5.234779357910156\n",
      "   VALIDATION: Batch 50, loss 5.139756202697754\n",
      "   VALIDATION: Batch 51, loss 5.603680610656738\n",
      "   VALIDATION: Batch 52, loss 4.608059883117676\n",
      "   VALIDATION: Batch 53, loss 4.564032554626465\n",
      "   VALIDATION: Batch 54, loss 4.6575927734375\n",
      "   VALIDATION: Batch 55, loss 5.5708746910095215\n",
      "   VALIDATION: Batch 56, loss 5.00649356842041\n",
      "   VALIDATION: Batch 57, loss 6.435723304748535\n",
      "   VALIDATION: Batch 58, loss 4.956568241119385\n",
      "   VALIDATION: Batch 59, loss 4.6183648109436035\n",
      "   VALIDATION: Batch 60, loss 4.0538787841796875\n",
      "   VALIDATION: Batch 61, loss 4.993014812469482\n",
      "   VALIDATION: Batch 62, loss 5.026001930236816\n",
      "   VALIDATION: Batch 63, loss 5.437361717224121\n",
      "   VALIDATION: Batch 64, loss 5.275073051452637\n",
      "   VALIDATION: Batch 65, loss 4.4386396408081055\n",
      "   VALIDATION: Batch 66, loss 5.317906379699707\n",
      "   VALIDATION: Batch 67, loss 4.707106113433838\n",
      "   VALIDATION: Batch 68, loss 4.93603515625\n",
      "   VALIDATION: Batch 69, loss 5.270302772521973\n",
      "   VALIDATION: Batch 70, loss 5.469751358032227\n",
      "   VALIDATION: Batch 71, loss 4.832329750061035\n",
      "   VALIDATION: Batch 72, loss 5.792352676391602\n",
      "   VALIDATION: Batch 73, loss 4.598449230194092\n",
      "   VALIDATION: Batch 74, loss 5.223846912384033\n",
      "   VALIDATION: Batch 75, loss 5.32805871963501\n",
      "   VALIDATION: Batch 76, loss 5.160836219787598\n",
      "   VALIDATION: Batch 77, loss 5.290176868438721\n",
      "   VALIDATION: Batch 78, loss 5.0948262214660645\n",
      "   VALIDATION: Batch 79, loss 5.021884918212891\n",
      "   VALIDATION: Batch 80, loss 5.300923824310303\n",
      "   VALIDATION: Batch 81, loss 4.910599708557129\n",
      "   VALIDATION: Batch 82, loss 5.287312984466553\n",
      "   VALIDATION: Batch 83, loss 4.614780902862549\n",
      "   VALIDATION: Batch 84, loss 5.235941410064697\n",
      "   VALIDATION: Batch 85, loss 5.061514854431152\n",
      "   VALIDATION: Batch 86, loss 4.982087135314941\n",
      "   VALIDATION: Batch 87, loss 4.8473734855651855\n",
      "   VALIDATION: Batch 88, loss 4.433253288269043\n",
      "   VALIDATION: Batch 89, loss 4.723921298980713\n",
      "   VALIDATION: Batch 90, loss 4.977994441986084\n",
      "   VALIDATION: Batch 91, loss 5.35097074508667\n",
      "   VALIDATION: Batch 92, loss 5.058778285980225\n",
      "   VALIDATION: Batch 93, loss 5.443671226501465\n",
      "   VALIDATION: Batch 94, loss 4.9636406898498535\n",
      "   VALIDATION: Batch 95, loss 4.409210205078125\n",
      "   VALIDATION: Batch 96, loss 4.897543907165527\n",
      "   VALIDATION: Batch 97, loss 4.759416103363037\n",
      "   VALIDATION: Batch 98, loss 5.151024341583252\n",
      "   VALIDATION: Batch 99, loss 5.233410835266113\n",
      "   VALIDATION: Batch 100, loss 5.61043643951416\n",
      "   VALIDATION: Batch 101, loss 4.207549571990967\n",
      "   VALIDATION: Batch 102, loss 5.716222763061523\n",
      "   VALIDATION: Batch 103, loss 5.5544023513793945\n",
      "   VALIDATION: Batch 104, loss 4.517226219177246\n",
      "   VALIDATION: Batch 105, loss 5.156702518463135\n",
      "   VALIDATION: Batch 106, loss 4.828958511352539\n",
      "   VALIDATION: Batch 107, loss 4.99526309967041\n",
      "   VALIDATION: Batch 108, loss 4.724596977233887\n",
      "   VALIDATION: Batch 109, loss 5.319106578826904\n",
      "   VALIDATION: Batch 110, loss 5.172843933105469\n",
      "   VALIDATION: Batch 111, loss 5.393540859222412\n",
      "   VALIDATION: Batch 112, loss 6.073901653289795\n",
      "   VALIDATION: Batch 113, loss 5.586872577667236\n",
      "   VALIDATION: Batch 114, loss 5.218636512756348\n",
      "   VALIDATION: Batch 115, loss 4.767114639282227\n",
      "   VALIDATION: Batch 116, loss 4.543267726898193\n",
      "   VALIDATION: Batch 117, loss 5.265489101409912\n",
      "   VALIDATION: Batch 118, loss 5.515381813049316\n",
      "   VALIDATION: Batch 119, loss 4.558505058288574\n",
      "   VALIDATION: Batch 120, loss 4.063989162445068\n",
      "   VALIDATION: Batch 121, loss 4.523280143737793\n",
      "   VALIDATION: Batch 122, loss 5.030507564544678\n",
      "   VALIDATION: Batch 123, loss 5.079512119293213\n",
      "   VALIDATION: Batch 124, loss 4.213926315307617\n",
      "   VALIDATION: Batch 125, loss 4.987331867218018\n",
      "   VALIDATION: Batch 126, loss 5.172768592834473\n",
      "   VALIDATION: Batch 127, loss 5.091264247894287\n",
      "   VALIDATION: Batch 128, loss 5.093937397003174\n",
      "   VALIDATION: Batch 129, loss 4.837181091308594\n",
      "   VALIDATION: Batch 130, loss 4.377354621887207\n",
      "   VALIDATION: Batch 131, loss 4.265456676483154\n",
      "   VALIDATION: Batch 132, loss 4.964169502258301\n",
      "   VALIDATION: Batch 133, loss 5.193112373352051\n",
      "   VALIDATION: Batch 134, loss 5.185214996337891\n",
      "   VALIDATION: Batch 135, loss 5.4409589767456055\n",
      "   VALIDATION: Batch 136, loss 5.405272483825684\n",
      "   VALIDATION: Batch 137, loss 5.34207010269165\n",
      "   VALIDATION: Batch 138, loss 4.973635196685791\n",
      "   VALIDATION: Batch 139, loss 5.437115669250488\n",
      "   VALIDATION: Batch 140, loss 4.3805341720581055\n",
      "   VALIDATION: Batch 141, loss 5.283985614776611\n",
      "   VALIDATION: Batch 142, loss 4.039797782897949\n",
      "   VALIDATION: Batch 143, loss 4.950934410095215\n",
      "   VALIDATION: Batch 144, loss 5.276005744934082\n",
      "   VALIDATION: Batch 145, loss 5.043525695800781\n",
      "   VALIDATION: Batch 146, loss 4.908897399902344\n",
      "   VALIDATION: Batch 147, loss 5.265763282775879\n",
      "   VALIDATION: Batch 148, loss 5.243694305419922\n",
      "   VALIDATION: Batch 149, loss 5.649601936340332\n",
      "   VALIDATION: Batch 150, loss 5.482026100158691\n",
      "   VALIDATION: Batch 151, loss 5.463521480560303\n",
      "   VALIDATION: Batch 152, loss 5.034775733947754\n",
      "   VALIDATION: Batch 153, loss 5.201235771179199\n",
      "   VALIDATION: Batch 154, loss 5.07200288772583\n",
      "   VALIDATION: Batch 155, loss 4.686959266662598\n",
      "   VALIDATION: Batch 156, loss 5.478910446166992\n",
      "   VALIDATION: Batch 157, loss 5.348073959350586\n",
      "   VALIDATION: Batch 158, loss 4.4200663566589355\n",
      "   VALIDATION: Batch 159, loss 4.980660915374756\n",
      "   VALIDATION: Batch 160, loss 5.545085430145264\n",
      "   VALIDATION: Batch 161, loss 5.644454002380371\n",
      "   VALIDATION: Batch 162, loss 5.140432357788086\n",
      "   VALIDATION: Batch 163, loss 4.4739532470703125\n",
      "   VALIDATION: Batch 164, loss 4.962960243225098\n",
      "   VALIDATION: Batch 165, loss 5.4334845542907715\n",
      "   VALIDATION: Batch 166, loss 5.07776403427124\n",
      "   VALIDATION: Batch 167, loss 5.299447536468506\n",
      "   VALIDATION: Batch 168, loss 4.119771480560303\n",
      "   VALIDATION: Batch 169, loss 4.800114154815674\n",
      "   VALIDATION: Batch 170, loss 5.017498970031738\n",
      "   VALIDATION: Batch 171, loss 5.069645404815674\n",
      "   VALIDATION: Batch 172, loss 5.170638084411621\n",
      "   VALIDATION: Batch 173, loss 5.254773139953613\n",
      "   VALIDATION: Batch 174, loss 5.472365379333496\n",
      "   VALIDATION: Batch 175, loss 5.075304985046387\n",
      "   VALIDATION: Batch 176, loss 4.826411724090576\n",
      "   VALIDATION: Batch 177, loss 5.2367658615112305\n",
      "   VALIDATION: Batch 178, loss 5.865970134735107\n",
      "   VALIDATION: Batch 179, loss 5.192252159118652\n",
      "   VALIDATION: Batch 180, loss 4.74920129776001\n",
      "   VALIDATION: Batch 181, loss 4.972448825836182\n",
      "   VALIDATION: Batch 182, loss 5.1567301750183105\n",
      "   VALIDATION: Batch 183, loss 4.157052993774414\n",
      "   VALIDATION: Batch 184, loss 3.9612879753112793\n",
      "   VALIDATION: Batch 185, loss 4.6703619956970215\n",
      "   VALIDATION: Batch 186, loss 4.706272125244141\n",
      "   VALIDATION: Batch 187, loss 5.04500150680542\n",
      "   VALIDATION: Batch 188, loss 5.2654595375061035\n",
      "   VALIDATION: Batch 189, loss 4.569262504577637\n",
      "   VALIDATION: Batch 190, loss 4.5504631996154785\n",
      "   VALIDATION: Batch 191, loss 5.190729141235352\n",
      "   VALIDATION: Batch 192, loss 5.69538688659668\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=30]              TRRAINING: Batch 0, loss 5.124616622924805\n",
      "Epoch 1: |          | 1/? [00:01<00:00,  0.93it/s, v_num=30]   TRRAINING: Batch 1, loss 4.620667457580566\n",
      "Epoch 1: |          | 2/? [00:01<00:00,  1.04it/s, v_num=30]   TRRAINING: Batch 2, loss 4.783786296844482\n",
      "Epoch 1: |          | 3/? [00:02<00:00,  1.07it/s, v_num=30]   TRRAINING: Batch 3, loss 4.3832502365112305\n",
      "Epoch 1: |          | 4/? [00:03<00:00,  1.11it/s, v_num=30]   TRRAINING: Batch 4, loss 4.824938774108887\n",
      "Epoch 1: |          | 5/? [00:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 5, loss 5.7576518058776855\n",
      "Epoch 1: |          | 6/? [00:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 6, loss 5.596787929534912\n",
      "Epoch 1: |          | 7/? [00:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 7, loss 4.850621223449707\n",
      "Epoch 1: |          | 8/? [00:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 8, loss 4.8030571937561035\n",
      "Epoch 1: |          | 9/? [00:07<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 9, loss 5.053652286529541\n",
      "Epoch 1: |          | 10/? [00:08<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 10, loss 5.301239967346191\n",
      "Epoch 1: |          | 11/? [00:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 11, loss 5.208227634429932\n",
      "Epoch 1: |          | 12/? [00:09<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 12, loss 6.464047431945801\n",
      "Epoch 1: |          | 13/? [00:10<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 13, loss 5.030869483947754\n",
      "Epoch 1: |          | 14/? [00:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 14, loss 5.282915115356445\n",
      "Epoch 1: |          | 15/? [00:12<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 15, loss 4.617665767669678\n",
      "Epoch 1: |          | 16/? [00:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 16, loss 4.176753520965576\n",
      "Epoch 1: |          | 17/? [00:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 17, loss 5.582406520843506\n",
      "Epoch 1: |          | 18/? [00:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 18, loss 5.019366264343262\n",
      "Epoch 1: |          | 19/? [00:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 19, loss 4.824959754943848\n",
      "Epoch 1: |          | 20/? [00:16<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 20, loss 5.196412086486816\n",
      "Epoch 1: |          | 21/? [00:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 21, loss 5.263623237609863\n",
      "Epoch 1: |          | 22/? [00:18<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 22, loss 5.07188081741333\n",
      "Epoch 1: |          | 23/? [00:19<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 23, loss 4.3831586837768555\n",
      "Epoch 1: |          | 24/? [00:20<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 24, loss 5.054959297180176\n",
      "Epoch 1: |          | 25/? [00:20<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 25, loss 5.024358749389648\n",
      "Epoch 1: |          | 26/? [00:21<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 26, loss 4.844211578369141\n",
      "Epoch 1: |          | 27/? [00:22<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 27, loss 4.607954978942871\n",
      "Epoch 1: |          | 28/? [00:23<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 28, loss 5.644873142242432\n",
      "Epoch 1: |          | 29/? [00:23<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 29, loss 5.116253852844238\n",
      "Epoch 1: |          | 30/? [00:24<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 30, loss 4.986763954162598\n",
      "Epoch 1: |          | 31/? [00:25<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 31, loss 5.63701057434082\n",
      "Epoch 1: |          | 32/? [00:26<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 32, loss 5.123185157775879\n",
      "Epoch 1: |          | 33/? [00:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 33, loss 4.939614772796631\n",
      "Epoch 1: |          | 34/? [00:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 34, loss 4.851012706756592\n",
      "Epoch 1: |          | 35/? [00:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 35, loss 4.137035369873047\n",
      "Epoch 1: |          | 36/? [00:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 36, loss 5.281473159790039\n",
      "Epoch 1: |          | 37/? [00:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 37, loss 5.286159038543701\n",
      "Epoch 1: |          | 38/? [00:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 38, loss 5.562593936920166\n",
      "Epoch 1: |          | 39/? [00:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 39, loss 5.475681781768799\n",
      "Epoch 1: |          | 40/? [00:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 40, loss 4.96828556060791\n",
      "Epoch 1: |          | 41/? [00:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 41, loss 4.860898971557617\n",
      "Epoch 1: |          | 42/? [00:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 42, loss 4.814301013946533\n",
      "Epoch 1: |          | 43/? [00:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 43, loss 4.864794731140137\n",
      "Epoch 1: |          | 44/? [00:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 44, loss 4.283493995666504\n",
      "Epoch 1: |          | 45/? [00:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 45, loss 4.550638675689697\n",
      "Epoch 1: |          | 46/? [00:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 46, loss 5.734898567199707\n",
      "Epoch 1: |          | 47/? [00:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 47, loss 4.814155578613281\n",
      "Epoch 1: |          | 48/? [00:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 48, loss 4.512907981872559\n",
      "Epoch 1: |          | 49/? [00:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 49, loss 4.849611282348633\n",
      "Epoch 1: |          | 50/? [00:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 50, loss 4.873549461364746\n",
      "Epoch 1: |          | 51/? [00:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 51, loss 4.950908660888672\n",
      "Epoch 1: |          | 52/? [00:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 52, loss 5.3607025146484375\n",
      "Epoch 1: |          | 53/? [00:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 53, loss 4.951754570007324\n",
      "Epoch 1: |          | 54/? [00:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 54, loss 4.895485877990723\n",
      "Epoch 1: |          | 55/? [00:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 55, loss 5.014893054962158\n",
      "Epoch 1: |          | 56/? [00:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 56, loss 5.057307243347168\n",
      "Epoch 1: |          | 57/? [00:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 57, loss 5.038294315338135\n",
      "Epoch 1: |          | 58/? [00:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 58, loss 6.265873908996582\n",
      "Epoch 1: |          | 59/? [00:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 59, loss 5.211551666259766\n",
      "Epoch 1: |          | 60/? [00:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 60, loss 5.3143792152404785\n",
      "Epoch 1: |          | 61/? [00:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 61, loss 5.3442583084106445\n",
      "Epoch 1: |          | 62/? [00:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 62, loss 4.843520164489746\n",
      "Epoch 1: |          | 63/? [00:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 63, loss 5.1065168380737305\n",
      "Epoch 1: |          | 64/? [00:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 64, loss 4.957764625549316\n",
      "Epoch 1: |          | 65/? [00:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 65, loss 5.029073238372803\n",
      "Epoch 1: |          | 66/? [00:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 66, loss 4.402145862579346\n",
      "Epoch 1: |          | 67/? [00:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 67, loss 5.0132551193237305\n",
      "Epoch 1: |          | 68/? [00:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 68, loss 5.331243515014648\n",
      "Epoch 1: |          | 69/? [00:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 69, loss 4.9876389503479\n",
      "Epoch 1: |          | 70/? [00:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 70, loss 4.636710166931152\n",
      "Epoch 1: |          | 71/? [00:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 71, loss 4.557159900665283\n",
      "Epoch 1: |          | 72/? [00:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 72, loss 5.215465068817139\n",
      "Epoch 1: |          | 73/? [00:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 73, loss 5.263589859008789\n",
      "Epoch 1: |          | 74/? [01:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 74, loss 4.647341251373291\n",
      "Epoch 1: |          | 75/? [01:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 75, loss 4.870926856994629\n",
      "Epoch 1: |          | 76/? [01:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 76, loss 4.904004096984863\n",
      "Epoch 1: |          | 77/? [01:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 77, loss 5.094699382781982\n",
      "Epoch 1: |          | 78/? [01:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 78, loss 4.564766883850098\n",
      "Epoch 1: |          | 79/? [01:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 79, loss 4.882749080657959\n",
      "Epoch 1: |          | 80/? [01:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 80, loss 4.697890281677246\n",
      "Epoch 1: |          | 81/? [01:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 81, loss 4.348377704620361\n",
      "Epoch 1: |          | 82/? [01:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 82, loss 5.309205055236816\n",
      "Epoch 1: |          | 83/? [01:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 83, loss 4.464010715484619\n",
      "Epoch 1: |          | 84/? [01:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 84, loss 4.337569236755371\n",
      "Epoch 1: |          | 85/? [01:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 85, loss 4.389328956604004\n",
      "Epoch 1: |          | 86/? [01:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 86, loss 4.373190879821777\n",
      "Epoch 1: |          | 87/? [01:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 87, loss 4.493931293487549\n",
      "Epoch 1: |          | 88/? [01:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 88, loss 5.513249397277832\n",
      "Epoch 1: |          | 89/? [01:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 89, loss 5.21076774597168\n",
      "Epoch 1: |          | 90/? [01:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 90, loss 5.098221302032471\n",
      "Epoch 1: |          | 91/? [01:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 91, loss 5.016051292419434\n",
      "Epoch 1: |          | 92/? [01:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 92, loss 5.424507141113281\n",
      "Epoch 1: |          | 93/? [01:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 93, loss 5.435267448425293\n",
      "Epoch 1: |          | 94/? [01:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 94, loss 5.320667743682861\n",
      "Epoch 1: |          | 95/? [01:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 95, loss 6.300199031829834\n",
      "Epoch 1: |          | 96/? [01:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 96, loss 4.68093204498291\n",
      "Epoch 1: |          | 97/? [01:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 97, loss 4.7121806144714355\n",
      "Epoch 1: |          | 98/? [01:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 98, loss 5.171156406402588\n",
      "Epoch 1: |          | 99/? [01:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 99, loss 5.280643939971924\n",
      "Epoch 1: |          | 100/? [01:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 100, loss 5.339175224304199\n",
      "Epoch 1: |          | 101/? [01:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 101, loss 4.945537567138672\n",
      "Epoch 1: |          | 102/? [01:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 102, loss 4.877646446228027\n",
      "Epoch 1: |          | 103/? [01:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 103, loss 4.573052406311035\n",
      "Epoch 1: |          | 104/? [01:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 104, loss 5.1343607902526855\n",
      "Epoch 1: |          | 105/? [01:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 105, loss 4.917642116546631\n",
      "Epoch 1: |          | 106/? [01:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 106, loss 4.958028316497803\n",
      "Epoch 1: |          | 107/? [01:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 107, loss 5.172979831695557\n",
      "Epoch 1: |          | 108/? [01:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 108, loss 5.106640338897705\n",
      "Epoch 1: |          | 109/? [01:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 109, loss 4.501278400421143\n",
      "Epoch 1: |          | 110/? [01:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 110, loss 5.15277624130249\n",
      "Epoch 1: |          | 111/? [01:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 111, loss 5.608119010925293\n",
      "Epoch 1: |          | 112/? [01:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 112, loss 4.607949256896973\n",
      "Epoch 1: |          | 113/? [01:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 113, loss 4.021083831787109\n",
      "Epoch 1: |          | 114/? [01:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 114, loss 5.236014366149902\n",
      "Epoch 1: |          | 115/? [01:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 115, loss 5.3316569328308105\n",
      "Epoch 1: |          | 116/? [01:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 116, loss 5.055588722229004\n",
      "Epoch 1: |          | 117/? [01:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 117, loss 5.024792671203613\n",
      "Epoch 1: |          | 118/? [01:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 118, loss 5.267370223999023\n",
      "Epoch 1: |          | 119/? [01:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 119, loss 5.437924385070801\n",
      "Epoch 1: |          | 120/? [01:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 120, loss 5.180485725402832\n",
      "Epoch 1: |          | 121/? [01:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 121, loss 4.87589168548584\n",
      "Epoch 1: |          | 122/? [01:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 122, loss 4.365063190460205\n",
      "Epoch 1: |          | 123/? [01:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 123, loss 4.844878196716309\n",
      "Epoch 1: |          | 124/? [01:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 124, loss 5.141900062561035\n",
      "Epoch 1: |          | 125/? [01:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 125, loss 4.911313533782959\n",
      "Epoch 1: |          | 126/? [01:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 126, loss 5.326119899749756\n",
      "Epoch 1: |          | 127/? [01:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 127, loss 5.24933385848999\n",
      "Epoch 1: |          | 128/? [01:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 128, loss 4.292674541473389\n",
      "Epoch 1: |          | 129/? [01:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 129, loss 5.0704827308654785\n",
      "Epoch 1: |          | 130/? [01:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 130, loss 3.933825731277466\n",
      "Epoch 1: |          | 131/? [01:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 131, loss 4.91360330581665\n",
      "Epoch 1: |          | 132/? [01:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 132, loss 4.926820755004883\n",
      "Epoch 1: |          | 133/? [01:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 133, loss 4.965882778167725\n",
      "Epoch 1: |          | 134/? [01:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 134, loss 5.020044326782227\n",
      "Epoch 1: |          | 135/? [01:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 135, loss 5.274664878845215\n",
      "Epoch 1: |          | 136/? [01:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 136, loss 5.179959297180176\n",
      "Epoch 1: |          | 137/? [01:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 137, loss 3.9974327087402344\n",
      "Epoch 1: |          | 138/? [01:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 138, loss 4.773133754730225\n",
      "Epoch 1: |          | 139/? [01:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 139, loss 5.349549770355225\n",
      "Epoch 1: |          | 140/? [01:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 140, loss 4.3414530754089355\n",
      "Epoch 1: |          | 141/? [01:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 141, loss 4.511580467224121\n",
      "Epoch 1: |          | 142/? [01:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 142, loss 6.276266098022461\n",
      "Epoch 1: |          | 143/? [01:55<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 143, loss 5.854153633117676\n",
      "Epoch 1: |          | 144/? [01:56<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 144, loss 4.857152462005615\n",
      "Epoch 1: |          | 145/? [01:57<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 145, loss 4.4647393226623535\n",
      "Epoch 1: |          | 146/? [01:58<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 146, loss 4.722298622131348\n",
      "Epoch 1: |          | 147/? [01:59<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 147, loss 4.974690914154053\n",
      "Epoch 1: |          | 148/? [01:59<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 148, loss 4.739211559295654\n",
      "Epoch 1: |          | 149/? [02:00<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 149, loss 4.063177585601807\n",
      "Epoch 1: |          | 150/? [02:00<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 150, loss 5.021432399749756\n",
      "Epoch 1: |          | 151/? [02:01<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 151, loss 5.105448246002197\n",
      "Epoch 1: |          | 152/? [02:02<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 152, loss 5.2017083168029785\n",
      "Epoch 1: |          | 153/? [02:03<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 153, loss 4.199882507324219\n",
      "Epoch 1: |          | 154/? [02:04<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 154, loss 5.399685382843018\n",
      "Epoch 1: |          | 155/? [02:05<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 155, loss 5.019930839538574\n",
      "Epoch 1: |          | 156/? [02:05<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 156, loss 4.195219993591309\n",
      "Epoch 1: |          | 157/? [02:06<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 157, loss 5.020129203796387\n",
      "Epoch 1: |          | 158/? [02:07<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 158, loss 5.05672550201416\n",
      "Epoch 1: |          | 159/? [02:08<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 159, loss 4.815499782562256\n",
      "Epoch 1: |          | 160/? [02:09<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 160, loss 4.483241081237793\n",
      "Epoch 1: |          | 161/? [02:09<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 161, loss 5.098844528198242\n",
      "Epoch 1: |          | 162/? [02:10<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 162, loss 5.140524864196777\n",
      "Epoch 1: |          | 163/? [02:11<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 163, loss 4.091962814331055\n",
      "Epoch 1: |          | 164/? [02:12<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 164, loss 4.543346881866455\n",
      "Epoch 1: |          | 165/? [02:12<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 165, loss 5.458429336547852\n",
      "Epoch 1: |          | 166/? [02:13<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 166, loss 5.142574310302734\n",
      "Epoch 1: |          | 167/? [02:14<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 167, loss 5.216823101043701\n",
      "Epoch 1: |          | 168/? [02:15<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 168, loss 4.771612644195557\n",
      "Epoch 1: |          | 169/? [02:16<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 169, loss 4.417922019958496\n",
      "Epoch 1: |          | 170/? [02:17<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 170, loss 4.666485786437988\n",
      "Epoch 1: |          | 171/? [02:17<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 171, loss 5.119381904602051\n",
      "Epoch 1: |          | 172/? [02:18<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 172, loss 4.724404335021973\n",
      "Epoch 1: |          | 173/? [02:19<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 173, loss 5.705910682678223\n",
      "Epoch 1: |          | 174/? [02:20<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 174, loss 5.780927658081055\n",
      "Epoch 1: |          | 175/? [02:21<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 175, loss 5.661136150360107\n",
      "Epoch 1: |          | 176/? [02:21<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 176, loss 4.678420066833496\n",
      "Epoch 1: |          | 177/? [02:22<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 177, loss 4.750399589538574\n",
      "Epoch 1: |          | 178/? [02:23<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 178, loss 4.590993881225586\n",
      "Epoch 1: |          | 179/? [02:24<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 179, loss 5.321560382843018\n",
      "Epoch 1: |          | 180/? [02:25<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 180, loss 4.762936592102051\n",
      "Epoch 1: |          | 181/? [02:25<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 181, loss 4.539576530456543\n",
      "Epoch 1: |          | 182/? [02:26<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 182, loss 5.052035331726074\n",
      "Epoch 1: |          | 183/? [02:27<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 183, loss 4.461712837219238\n",
      "Epoch 1: |          | 184/? [02:28<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 184, loss 4.573069095611572\n",
      "Epoch 1: |          | 185/? [02:29<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 185, loss 5.391739845275879\n",
      "Epoch 1: |          | 186/? [02:29<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 186, loss 4.716031074523926\n",
      "Epoch 1: |          | 187/? [02:30<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 187, loss 5.30535364151001\n",
      "Epoch 1: |          | 188/? [02:31<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 188, loss 4.713258743286133\n",
      "Epoch 1: |          | 189/? [02:32<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 189, loss 5.361967086791992\n",
      "Epoch 1: |          | 190/? [02:33<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 190, loss 4.771707534790039\n",
      "Epoch 1: |          | 191/? [02:34<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 191, loss 5.699622631072998\n",
      "Epoch 1: |          | 192/? [02:34<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 192, loss 5.540750503540039\n",
      "Epoch 1: |          | 193/? [02:35<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 193, loss 4.610127925872803\n",
      "Epoch 1: |          | 194/? [02:36<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 194, loss 4.591790199279785\n",
      "Epoch 1: |          | 195/? [02:37<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 195, loss 5.259450435638428\n",
      "Epoch 1: |          | 196/? [02:38<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 196, loss 5.2535505294799805\n",
      "Epoch 1: |          | 197/? [02:39<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 197, loss 5.004799842834473\n",
      "Epoch 1: |          | 198/? [02:39<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 198, loss 4.465494632720947\n",
      "Epoch 1: |          | 199/? [02:40<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 199, loss 5.146903038024902\n",
      "Epoch 1: |          | 200/? [02:41<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 200, loss 4.8707380294799805\n",
      "Epoch 1: |          | 201/? [02:42<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 201, loss 5.198168754577637\n",
      "Epoch 1: |          | 202/? [02:43<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 202, loss 5.092016220092773\n",
      "Epoch 1: |          | 203/? [02:44<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 203, loss 5.010655403137207\n",
      "Epoch 1: |          | 204/? [02:44<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 204, loss 5.028645038604736\n",
      "Epoch 1: |          | 205/? [02:45<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 205, loss 4.620095252990723\n",
      "Epoch 1: |          | 206/? [02:46<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 206, loss 4.5945587158203125\n",
      "Epoch 1: |          | 207/? [02:47<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 207, loss 5.064860820770264\n",
      "Epoch 1: |          | 208/? [02:48<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 208, loss 5.032075881958008\n",
      "Epoch 1: |          | 209/? [02:48<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 209, loss 4.8220343589782715\n",
      "Epoch 1: |          | 210/? [02:49<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 210, loss 5.514197826385498\n",
      "Epoch 1: |          | 211/? [02:50<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 211, loss 4.785218715667725\n",
      "Epoch 1: |          | 212/? [02:51<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 212, loss 4.963373184204102\n",
      "Epoch 1: |          | 213/? [02:52<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 213, loss 4.7742719650268555\n",
      "Epoch 1: |          | 214/? [02:53<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 214, loss 4.810644149780273\n",
      "Epoch 1: |          | 215/? [02:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 215, loss 4.396010398864746\n",
      "Epoch 1: |          | 216/? [02:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 216, loss 5.143136024475098\n",
      "Epoch 1: |          | 217/? [02:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 217, loss 4.946048259735107\n",
      "Epoch 1: |          | 218/? [02:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 218, loss 5.117835998535156\n",
      "Epoch 1: |          | 219/? [02:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 219, loss 4.9369215965271\n",
      "Epoch 1: |          | 220/? [02:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 220, loss 5.074622631072998\n",
      "Epoch 1: |          | 221/? [02:58<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 221, loss 4.752333641052246\n",
      "Epoch 1: |          | 222/? [02:59<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 222, loss 3.9589684009552\n",
      "Epoch 1: |          | 223/? [03:00<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 223, loss 5.442670822143555\n",
      "Epoch 1: |          | 224/? [03:01<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 224, loss 5.314996242523193\n",
      "Epoch 1: |          | 225/? [03:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 225, loss 5.01872444152832\n",
      "Epoch 1: |          | 226/? [03:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 226, loss 4.785205364227295\n",
      "Epoch 1: |          | 227/? [03:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 227, loss 5.2103118896484375\n",
      "Epoch 1: |          | 228/? [03:04<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 228, loss 4.810763835906982\n",
      "Epoch 1: |          | 229/? [03:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 229, loss 5.079604625701904\n",
      "Epoch 1: |          | 230/? [03:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 230, loss 4.850449562072754\n",
      "Epoch 1: |          | 231/? [03:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 231, loss 4.774453639984131\n",
      "Epoch 1: |          | 232/? [03:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 232, loss 4.666589736938477\n",
      "Epoch 1: |          | 233/? [03:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 233, loss 5.416206359863281\n",
      "Epoch 1: |          | 234/? [03:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 234, loss 5.378108978271484\n",
      "Epoch 1: |          | 235/? [03:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 235, loss 5.368070125579834\n",
      "Epoch 1: |          | 236/? [03:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 236, loss 4.881430625915527\n",
      "Epoch 1: |          | 237/? [03:11<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 237, loss 5.009383201599121\n",
      "Epoch 1: |          | 238/? [03:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 238, loss 5.222304344177246\n",
      "Epoch 1: |          | 239/? [03:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 239, loss 4.844344139099121\n",
      "Epoch 1: |          | 240/? [03:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 240, loss 4.165509223937988\n",
      "Epoch 1: |          | 241/? [03:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 241, loss 4.992833614349365\n",
      "Epoch 1: |          | 242/? [03:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 242, loss 5.328469276428223\n",
      "Epoch 1: |          | 243/? [03:16<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 243, loss 3.9548885822296143\n",
      "Epoch 1: |          | 244/? [03:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 244, loss 4.661329746246338\n",
      "Epoch 1: |          | 245/? [03:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 245, loss 4.889578819274902\n",
      "Epoch 1: |          | 246/? [03:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 246, loss 5.12085485458374\n",
      "Epoch 1: |          | 247/? [03:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 247, loss 5.182360649108887\n",
      "Epoch 1: |          | 248/? [03:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 248, loss 4.5907883644104\n",
      "Epoch 1: |          | 249/? [03:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 249, loss 4.269138336181641\n",
      "Epoch 1: |          | 250/? [03:22<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 250, loss 4.939065456390381\n",
      "Epoch 1: |          | 251/? [03:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 251, loss 5.040373802185059\n",
      "Epoch 1: |          | 252/? [03:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 252, loss 4.738947868347168\n",
      "Epoch 1: |          | 253/? [03:24<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 253, loss 5.506961345672607\n",
      "Epoch 1: |          | 254/? [03:25<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 254, loss 5.351181983947754\n",
      "Epoch 1: |          | 255/? [03:26<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 255, loss 4.837292671203613\n",
      "Epoch 1: |          | 256/? [03:27<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 256, loss 7.147091865539551\n",
      "Epoch 1: |          | 257/? [03:27<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 257, loss 4.690265655517578\n",
      "Epoch 1: |          | 258/? [03:28<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 258, loss 4.841678619384766\n",
      "Epoch 1: |          | 259/? [03:29<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 259, loss 4.62930154800415\n",
      "Epoch 1: |          | 260/? [03:29<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 260, loss 4.605050563812256\n",
      "Epoch 1: |          | 261/? [03:30<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 261, loss 4.735518455505371\n",
      "Epoch 1: |          | 262/? [03:31<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 262, loss 5.132791042327881\n",
      "Epoch 1: |          | 263/? [03:32<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 263, loss 4.859588146209717\n",
      "Epoch 1: |          | 264/? [03:33<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 264, loss 4.958804130554199\n",
      "Epoch 1: |          | 265/? [03:33<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 265, loss 4.298320293426514\n",
      "Epoch 1: |          | 266/? [03:34<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 266, loss 4.744668960571289\n",
      "Epoch 1: |          | 267/? [03:35<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 267, loss 4.3977885246276855\n",
      "Epoch 1: |          | 268/? [03:36<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 268, loss 4.7471513748168945\n",
      "Epoch 1: |          | 269/? [03:37<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 269, loss 5.188807487487793\n",
      "Epoch 1: |          | 270/? [03:38<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 270, loss 4.69754695892334\n",
      "Epoch 1: |          | 271/? [03:38<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 271, loss 5.459506034851074\n",
      "Epoch 1: |          | 272/? [03:39<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 272, loss 5.338791847229004\n",
      "Epoch 1: |          | 273/? [03:40<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 273, loss 4.412120819091797\n",
      "Epoch 1: |          | 274/? [03:41<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 274, loss 5.616693019866943\n",
      "Epoch 1: |          | 275/? [03:42<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 275, loss 4.97258186340332\n",
      "Epoch 1: |          | 276/? [03:43<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 276, loss 4.235437870025635\n",
      "Epoch 1: |          | 277/? [03:43<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 277, loss 4.866911888122559\n",
      "Epoch 1: |          | 278/? [03:44<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 278, loss 3.8624839782714844\n",
      "Epoch 1: |          | 279/? [03:45<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 279, loss 4.7319016456604\n",
      "Epoch 1: |          | 280/? [03:46<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 280, loss 4.265832901000977\n",
      "Epoch 1: |          | 281/? [03:46<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 281, loss 5.16812801361084\n",
      "Epoch 1: |          | 282/? [03:47<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 282, loss 4.745519638061523\n",
      "Epoch 1: |          | 283/? [03:48<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 283, loss 4.818733215332031\n",
      "Epoch 1: |          | 284/? [03:49<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 284, loss 4.661633491516113\n",
      "Epoch 1: |          | 285/? [03:50<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 285, loss 4.030364036560059\n",
      "Epoch 1: |          | 286/? [03:51<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 286, loss 4.707821369171143\n",
      "Epoch 1: |          | 287/? [03:52<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 287, loss 4.549490928649902\n",
      "Epoch 1: |          | 288/? [03:53<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 288, loss 4.500617504119873\n",
      "Epoch 1: |          | 289/? [03:53<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 289, loss 4.57046365737915\n",
      "Epoch 1: |          | 290/? [03:54<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 290, loss 4.247458457946777\n",
      "Epoch 1: |          | 291/? [03:55<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 291, loss 4.889557361602783\n",
      "Epoch 1: |          | 292/? [03:56<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 292, loss 4.510148525238037\n",
      "Epoch 1: |          | 293/? [03:57<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 293, loss 4.969527244567871\n",
      "Epoch 1: |          | 294/? [03:57<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 294, loss 4.684615135192871\n",
      "Epoch 1: |          | 295/? [03:58<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 295, loss 5.091556072235107\n",
      "Epoch 1: |          | 296/? [03:59<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 296, loss 4.530158996582031\n",
      "Epoch 1: |          | 297/? [04:00<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 297, loss 5.258620262145996\n",
      "Epoch 1: |          | 298/? [04:00<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 298, loss 4.98598575592041\n",
      "Epoch 1: |          | 299/? [04:01<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 299, loss 5.937376976013184\n",
      "Epoch 1: |          | 300/? [04:02<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 300, loss 5.019765853881836\n",
      "Epoch 1: |          | 301/? [04:03<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 301, loss 4.656294822692871\n",
      "Epoch 1: |          | 302/? [04:04<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 302, loss 5.146293640136719\n",
      "Epoch 1: |          | 303/? [04:05<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 303, loss 4.829499244689941\n",
      "Epoch 1: |          | 304/? [04:05<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 304, loss 5.113123893737793\n",
      "Epoch 1: |          | 305/? [04:06<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 305, loss 5.222207546234131\n",
      "Epoch 1: |          | 306/? [04:07<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 306, loss 4.827746391296387\n",
      "Epoch 1: |          | 307/? [04:08<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 307, loss 5.103604316711426\n",
      "Epoch 1: |          | 308/? [04:09<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 308, loss 5.219865322113037\n",
      "Epoch 1: |          | 309/? [04:10<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 309, loss 4.96945333480835\n",
      "Epoch 1: |          | 310/? [04:10<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 310, loss 5.432318687438965\n",
      "Epoch 1: |          | 311/? [04:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 311, loss 4.874238967895508\n",
      "Epoch 1: |          | 312/? [04:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 312, loss 5.099041938781738\n",
      "Epoch 1: |          | 313/? [04:13<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 313, loss 4.511715888977051\n",
      "Epoch 1: |          | 314/? [04:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 314, loss 4.978301048278809\n",
      "Epoch 1: |          | 315/? [04:14<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 315, loss 4.483730792999268\n",
      "Epoch 1: |          | 316/? [04:15<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 316, loss 5.301629066467285\n",
      "Epoch 1: |          | 317/? [04:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 317, loss 5.000479698181152\n",
      "Epoch 1: |          | 318/? [04:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 318, loss 5.202707767486572\n",
      "Epoch 1: |          | 319/? [04:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 319, loss 4.321507930755615\n",
      "Epoch 1: |          | 320/? [04:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 320, loss 4.778801918029785\n",
      "Epoch 1: |          | 321/? [04:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 321, loss 4.4940314292907715\n",
      "Epoch 1: |          | 322/? [04:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 322, loss 5.251588821411133\n",
      "Epoch 1: |          | 323/? [04:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 323, loss 5.402836799621582\n",
      "Epoch 1: |          | 324/? [04:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 324, loss 4.840710163116455\n",
      "Epoch 1: |          | 325/? [04:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 325, loss 5.4839372634887695\n",
      "Epoch 1: |          | 326/? [04:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 326, loss 4.9873528480529785\n",
      "Epoch 1: |          | 327/? [04:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 327, loss 4.80873441696167\n",
      "Epoch 1: |          | 328/? [04:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 328, loss 4.429742336273193\n",
      "Epoch 1: |          | 329/? [04:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 329, loss 5.071433067321777\n",
      "Epoch 1: |          | 330/? [04:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 330, loss 5.51556921005249\n",
      "Epoch 1: |          | 331/? [04:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 331, loss 3.6407928466796875\n",
      "Epoch 1: |          | 332/? [04:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 332, loss 4.934446334838867\n",
      "Epoch 1: |          | 333/? [04:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 333, loss 4.689735412597656\n",
      "Epoch 1: |          | 334/? [04:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 334, loss 5.8557448387146\n",
      "Epoch 1: |          | 335/? [04:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 335, loss 5.630125999450684\n",
      "Epoch 1: |          | 336/? [04:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 336, loss 5.339650630950928\n",
      "Epoch 1: |          | 337/? [04:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 337, loss 6.088126182556152\n",
      "Epoch 1: |          | 338/? [04:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 338, loss 5.583421230316162\n",
      "Epoch 1: |          | 339/? [04:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 339, loss 4.598902225494385\n",
      "Epoch 1: |          | 340/? [04:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 340, loss 5.013175964355469\n",
      "Epoch 1: |          | 341/? [04:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 341, loss 4.294503688812256\n",
      "Epoch 1: |          | 342/? [04:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 342, loss 5.036772727966309\n",
      "Epoch 1: |          | 343/? [04:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 343, loss 4.766660690307617\n",
      "Epoch 1: |          | 344/? [04:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 344, loss 5.505293846130371\n",
      "Epoch 1: |          | 345/? [04:39<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 345, loss 4.8145341873168945\n",
      "Epoch 1: |          | 346/? [04:40<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 346, loss 5.0573906898498535\n",
      "Epoch 1: |          | 347/? [04:40<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 347, loss 4.621138095855713\n",
      "Epoch 1: |          | 348/? [04:41<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 348, loss 3.9271652698516846\n",
      "Epoch 1: |          | 349/? [04:42<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 349, loss 3.7391133308410645\n",
      "Epoch 1: |          | 350/? [04:43<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 350, loss 5.327249050140381\n",
      "Epoch 1: |          | 351/? [04:43<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 351, loss 5.30136251449585\n",
      "Epoch 1: |          | 352/? [04:44<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 352, loss 4.511016845703125\n",
      "Epoch 1: |          | 353/? [04:45<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 353, loss 4.20798397064209\n",
      "Epoch 1: |          | 354/? [04:46<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 354, loss 4.607176780700684\n",
      "Epoch 1: |          | 355/? [04:47<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 355, loss 4.987654685974121\n",
      "Epoch 1: |          | 356/? [04:47<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 356, loss 4.959824562072754\n",
      "Epoch 1: |          | 357/? [04:48<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 357, loss 4.497747421264648\n",
      "Epoch 1: |          | 358/? [04:49<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 358, loss 4.419284820556641\n",
      "Epoch 1: |          | 359/? [04:50<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 359, loss 5.1673784255981445\n",
      "Epoch 1: |          | 360/? [04:51<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 360, loss 4.612423896789551\n",
      "Epoch 1: |          | 361/? [04:52<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 361, loss 4.694530487060547\n",
      "Epoch 1: |          | 362/? [04:52<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 362, loss 4.568127632141113\n",
      "Epoch 1: |          | 363/? [04:53<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 363, loss 4.495749473571777\n",
      "Epoch 1: |          | 364/? [04:54<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 364, loss 5.147100448608398\n",
      "Epoch 1: |          | 365/? [04:55<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 365, loss 5.215365886688232\n",
      "Epoch 1: |          | 366/? [04:56<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 366, loss 4.8512067794799805\n",
      "Epoch 1: |          | 367/? [04:56<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 367, loss 4.935707092285156\n",
      "Epoch 1: |          | 368/? [04:57<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 368, loss 4.341086387634277\n",
      "Epoch 1: |          | 369/? [04:58<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 369, loss 4.778458118438721\n",
      "Epoch 1: |          | 370/? [04:59<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 370, loss 4.259840488433838\n",
      "Epoch 1: |          | 371/? [05:00<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 371, loss 5.378229141235352\n",
      "Epoch 1: |          | 372/? [05:00<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 372, loss 4.967855453491211\n",
      "Epoch 1: |          | 373/? [05:01<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 373, loss 4.9077677726745605\n",
      "Epoch 1: |          | 374/? [05:02<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 374, loss 4.686840057373047\n",
      "Epoch 1: |          | 375/? [05:03<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 375, loss 5.397788047790527\n",
      "Epoch 1: |          | 376/? [05:03<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 376, loss 4.727606296539307\n",
      "Epoch 1: |          | 377/? [05:04<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 377, loss 4.9343953132629395\n",
      "Epoch 1: |          | 378/? [05:05<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 378, loss 5.1202497482299805\n",
      "Epoch 1: |          | 379/? [05:06<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 379, loss 4.9371795654296875\n",
      "Epoch 1: |          | 380/? [05:07<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 380, loss 5.0839643478393555\n",
      "Epoch 1: |          | 381/? [05:08<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 381, loss 4.983423709869385\n",
      "Epoch 1: |          | 382/? [05:09<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 382, loss 4.67734432220459\n",
      "Epoch 1: |          | 383/? [05:10<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 383, loss 4.710841178894043\n",
      "Epoch 1: |          | 384/? [05:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 384, loss 5.284167289733887\n",
      "Epoch 1: |          | 385/? [05:11<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 385, loss 4.800494194030762\n",
      "Epoch 1: |          | 386/? [05:12<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 386, loss 3.5833027362823486\n",
      "Epoch 1: |          | 387/? [05:12<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 387, loss 4.59128475189209\n",
      "Epoch 1: |          | 388/? [05:13<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 388, loss 5.2717790603637695\n",
      "Epoch 1: |          | 389/? [05:14<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 389, loss 5.310594081878662\n",
      "Epoch 1: |          | 390/? [05:15<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 390, loss 4.6258063316345215\n",
      "Epoch 1: |          | 391/? [05:16<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 391, loss 5.166927337646484\n",
      "Epoch 1: |          | 392/? [05:16<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 392, loss 5.162820816040039\n",
      "Epoch 1: |          | 393/? [05:17<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 393, loss 5.24388313293457\n",
      "Epoch 1: |          | 394/? [05:18<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 394, loss 4.840235710144043\n",
      "Epoch 1: |          | 395/? [05:19<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 395, loss 5.15898323059082\n",
      "Epoch 1: |          | 396/? [05:20<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 396, loss 5.087771415710449\n",
      "Epoch 1: |          | 397/? [05:21<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 397, loss 4.898520469665527\n",
      "Epoch 1: |          | 398/? [05:21<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 398, loss 4.658944129943848\n",
      "Epoch 1: |          | 399/? [05:22<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 399, loss 4.831973075866699\n",
      "Epoch 1: |          | 400/? [05:23<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 400, loss 4.891146659851074\n",
      "Epoch 1: |          | 401/? [05:24<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 401, loss 4.696277141571045\n",
      "Epoch 1: |          | 402/? [05:25<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 402, loss 5.426400184631348\n",
      "Epoch 1: |          | 403/? [05:26<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 403, loss 4.960091590881348\n",
      "Epoch 1: |          | 404/? [05:26<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 404, loss 4.436362266540527\n",
      "Epoch 1: |          | 405/? [05:27<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 405, loss 4.413466453552246\n",
      "Epoch 1: |          | 406/? [05:28<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 406, loss 4.84699010848999\n",
      "Epoch 1: |          | 407/? [05:29<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 407, loss 4.820903778076172\n",
      "Epoch 1: |          | 408/? [05:30<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 408, loss 5.155643939971924\n",
      "Epoch 1: |          | 409/? [05:30<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 409, loss 5.12396764755249\n",
      "Epoch 1: |          | 410/? [05:31<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 410, loss 4.866363048553467\n",
      "Epoch 1: |          | 411/? [05:32<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 411, loss 4.664302825927734\n",
      "Epoch 1: |          | 412/? [05:33<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 412, loss 4.082533836364746\n",
      "Epoch 1: |          | 413/? [05:34<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 413, loss 4.956844329833984\n",
      "Epoch 1: |          | 414/? [05:34<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 414, loss 4.401592254638672\n",
      "Epoch 1: |          | 415/? [05:35<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 415, loss 4.897454261779785\n",
      "Epoch 1: |          | 416/? [05:36<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 416, loss 5.30777645111084\n",
      "Epoch 1: |          | 417/? [05:37<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 417, loss 5.652336597442627\n",
      "Epoch 1: |          | 418/? [05:38<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 418, loss 5.138274192810059\n",
      "Epoch 1: |          | 419/? [05:38<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 419, loss 5.238209247589111\n",
      "Epoch 1: |          | 420/? [05:39<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 420, loss 4.901852130889893\n",
      "Epoch 1: |          | 421/? [05:40<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 421, loss 5.4703049659729\n",
      "Epoch 1: |          | 422/? [05:41<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 422, loss 4.992581844329834\n",
      "Epoch 1: |          | 423/? [05:41<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 423, loss 4.460927486419678\n",
      "Epoch 1: |          | 424/? [05:42<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 424, loss 5.4471869468688965\n",
      "Epoch 1: |          | 425/? [05:43<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 425, loss 5.357577323913574\n",
      "Epoch 1: |          | 426/? [05:44<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 426, loss 4.703474998474121\n",
      "Epoch 1: |          | 427/? [05:44<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 427, loss 4.653153419494629\n",
      "Epoch 1: |          | 428/? [05:45<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 428, loss 5.591144561767578\n",
      "Epoch 1: |          | 429/? [05:46<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 429, loss 4.307744979858398\n",
      "Epoch 1: |          | 430/? [05:47<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 430, loss 5.076811790466309\n",
      "Epoch 1: |          | 431/? [05:48<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 431, loss 4.89176082611084\n",
      "Epoch 1: |          | 432/? [05:49<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 432, loss 4.9899797439575195\n",
      "Epoch 1: |          | 433/? [05:49<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 433, loss 4.877313137054443\n",
      "Epoch 1: |          | 434/? [05:50<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 434, loss 4.876059055328369\n",
      "Epoch 1: |          | 435/? [05:51<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 435, loss 4.558978080749512\n",
      "Epoch 1: |          | 436/? [05:52<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 436, loss 5.036493301391602\n",
      "Epoch 1: |          | 437/? [05:53<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 437, loss 5.084322929382324\n",
      "Epoch 1: |          | 438/? [05:54<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 438, loss 4.660693645477295\n",
      "Epoch 1: |          | 439/? [05:54<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 439, loss 4.888919353485107\n",
      "Epoch 1: |          | 440/? [05:55<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 440, loss 4.8633928298950195\n",
      "Epoch 1: |          | 441/? [05:56<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 441, loss 5.10929012298584\n",
      "Epoch 1: |          | 442/? [05:56<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 442, loss 4.744164943695068\n",
      "Epoch 1: |          | 443/? [05:57<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 443, loss 5.088165283203125\n",
      "Epoch 1: |          | 444/? [05:58<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 444, loss 4.856142044067383\n",
      "Epoch 1: |          | 445/? [05:59<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 445, loss 5.719179630279541\n",
      "Epoch 1: |          | 446/? [06:00<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 446, loss 4.8281097412109375\n",
      "Epoch 1: |          | 447/? [06:00<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 447, loss 5.411705493927002\n",
      "Epoch 1: |          | 448/? [06:01<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 448, loss 4.70892858505249\n",
      "Epoch 1: |          | 449/? [06:02<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 449, loss 4.753295421600342\n",
      "Epoch 1: |          | 450/? [06:03<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 450, loss 5.186081886291504\n",
      "Epoch 1: |          | 451/? [06:04<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 451, loss 4.789886951446533\n",
      "Epoch 1: |          | 452/? [06:05<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 452, loss 4.576927185058594\n",
      "Epoch 1: |          | 453/? [06:06<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 453, loss 5.2434282302856445\n",
      "Epoch 1: |          | 454/? [06:06<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 454, loss 4.598589897155762\n",
      "Epoch 1: |          | 455/? [06:07<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 455, loss 5.035764694213867\n",
      "Epoch 1: |          | 456/? [06:08<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 456, loss 4.453513145446777\n",
      "Epoch 1: |          | 457/? [06:09<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 457, loss 4.797237396240234\n",
      "Epoch 1: |          | 458/? [06:10<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 458, loss 5.201142311096191\n",
      "Epoch 1: |          | 459/? [06:10<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 459, loss 5.207149982452393\n",
      "Epoch 1: |          | 460/? [06:11<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 460, loss 4.9887213706970215\n",
      "Epoch 1: |          | 461/? [06:12<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 461, loss 4.965616703033447\n",
      "Epoch 1: |          | 462/? [06:13<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 462, loss 4.944822311401367\n",
      "Epoch 1: |          | 463/? [06:14<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 463, loss 4.838665962219238\n",
      "Epoch 1: |          | 464/? [06:14<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 464, loss 4.334245681762695\n",
      "Epoch 1: |          | 465/? [06:15<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 465, loss 4.632724285125732\n",
      "Epoch 1: |          | 466/? [06:16<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 466, loss 5.072383880615234\n",
      "Epoch 1: |          | 467/? [06:17<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 467, loss 4.9308342933654785\n",
      "Epoch 1: |          | 468/? [06:17<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 468, loss 4.837255001068115\n",
      "Epoch 1: |          | 469/? [06:18<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 469, loss 4.9817214012146\n",
      "Epoch 1: |          | 470/? [06:19<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 470, loss 4.100386142730713\n",
      "Epoch 1: |          | 471/? [06:20<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 471, loss 4.899846076965332\n",
      "Epoch 1: |          | 472/? [06:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 472, loss 4.554598331451416\n",
      "Epoch 1: |          | 473/? [06:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 473, loss 4.598452568054199\n",
      "Epoch 1: |          | 474/? [06:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 474, loss 4.192821979522705\n",
      "Epoch 1: |          | 475/? [06:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 475, loss 5.5807576179504395\n",
      "Epoch 1: |          | 476/? [06:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 476, loss 4.449756145477295\n",
      "Epoch 1: |          | 477/? [06:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 477, loss 3.991436004638672\n",
      "Epoch 1: |          | 478/? [06:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 478, loss 4.135891914367676\n",
      "Epoch 1: |          | 479/? [06:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 479, loss 4.865033149719238\n",
      "Epoch 1: |          | 480/? [06:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 480, loss 5.058838844299316\n",
      "Epoch 1: |          | 481/? [06:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 481, loss 4.710652828216553\n",
      "Epoch 1: |          | 482/? [06:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 482, loss 4.762691020965576\n",
      "Epoch 1: |          | 483/? [06:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 483, loss 4.414790153503418\n",
      "Epoch 1: |          | 484/? [06:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 484, loss 5.291092395782471\n",
      "Epoch 1: |          | 485/? [06:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 485, loss 5.001942157745361\n",
      "Epoch 1: |          | 486/? [06:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 486, loss 4.9532880783081055\n",
      "Epoch 1: |          | 487/? [06:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 487, loss 5.02804708480835\n",
      "Epoch 1: |          | 488/? [06:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 488, loss 4.652132987976074\n",
      "Epoch 1: |          | 489/? [06:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 489, loss 4.159935474395752\n",
      "Epoch 1: |          | 490/? [06:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 490, loss 4.993180751800537\n",
      "Epoch 1: |          | 491/? [06:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 491, loss 4.640415191650391\n",
      "Epoch 1: |          | 492/? [06:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 492, loss 4.184763431549072\n",
      "Epoch 1: |          | 493/? [06:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 493, loss 5.189875602722168\n",
      "Epoch 1: |          | 494/? [06:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 494, loss 5.058988094329834\n",
      "Epoch 1: |          | 495/? [06:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 495, loss 5.0349931716918945\n",
      "Epoch 1: |          | 496/? [06:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 496, loss 4.656964302062988\n",
      "Epoch 1: |          | 497/? [06:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 497, loss 5.114566326141357\n",
      "Epoch 1: |          | 498/? [06:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 498, loss 4.851991176605225\n",
      "Epoch 1: |          | 499/? [06:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 499, loss 4.795645713806152\n",
      "Epoch 1: |          | 500/? [06:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 500, loss 4.794761657714844\n",
      "Epoch 1: |          | 501/? [06:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 501, loss 4.398538112640381\n",
      "Epoch 1: |          | 502/? [06:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 502, loss 4.874678611755371\n",
      "Epoch 1: |          | 503/? [06:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 503, loss 5.021466255187988\n",
      "Epoch 1: |          | 504/? [06:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 504, loss 4.744655132293701\n",
      "Epoch 1: |          | 505/? [06:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 505, loss 3.9784629344940186\n",
      "Epoch 1: |          | 506/? [06:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 506, loss 4.70155143737793\n",
      "Epoch 1: |          | 507/? [06:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 507, loss 4.769721984863281\n",
      "Epoch 1: |          | 508/? [06:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 508, loss 5.111603736877441\n",
      "Epoch 1: |          | 509/? [06:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 509, loss 4.40960693359375\n",
      "Epoch 1: |          | 510/? [06:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 510, loss 5.056525230407715\n",
      "Epoch 1: |          | 511/? [06:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 511, loss 4.931606769561768\n",
      "Epoch 1: |          | 512/? [06:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 512, loss 4.270961284637451\n",
      "Epoch 1: |          | 513/? [06:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 513, loss 4.552957534790039\n",
      "Epoch 1: |          | 514/? [07:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 514, loss 4.63336181640625\n",
      "Epoch 1: |          | 515/? [07:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 515, loss 4.368605613708496\n",
      "Epoch 1: |          | 516/? [07:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 516, loss 4.6403021812438965\n",
      "Epoch 1: |          | 517/? [07:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 517, loss 5.0356125831604\n",
      "Epoch 1: |          | 518/? [07:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 518, loss 4.5680155754089355\n",
      "Epoch 1: |          | 519/? [07:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 519, loss 5.014161586761475\n",
      "Epoch 1: |          | 520/? [07:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 520, loss 4.724630832672119\n",
      "Epoch 1: |          | 521/? [07:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 521, loss 4.720620632171631\n",
      "Epoch 1: |          | 522/? [07:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 522, loss 5.375246524810791\n",
      "Epoch 1: |          | 523/? [07:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 523, loss 5.369473457336426\n",
      "Epoch 1: |          | 524/? [07:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 524, loss 5.172636032104492\n",
      "Epoch 1: |          | 525/? [07:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 525, loss 4.708836555480957\n",
      "Epoch 1: |          | 526/? [07:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 526, loss 4.564456939697266\n",
      "Epoch 1: |          | 527/? [07:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 527, loss 5.011718273162842\n",
      "Epoch 1: |          | 528/? [07:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 528, loss 5.025209426879883\n",
      "Epoch 1: |          | 529/? [07:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 529, loss 4.554073333740234\n",
      "Epoch 1: |          | 530/? [07:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 530, loss 5.132326126098633\n",
      "Epoch 1: |          | 531/? [07:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 531, loss 4.575745582580566\n",
      "Epoch 1: |          | 532/? [07:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 532, loss 4.9006853103637695\n",
      "Epoch 1: |          | 533/? [07:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 533, loss 4.620768070220947\n",
      "Epoch 1: |          | 534/? [07:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 534, loss 4.518010139465332\n",
      "Epoch 1: |          | 535/? [07:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 535, loss 5.316929340362549\n",
      "Epoch 1: |          | 536/? [07:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 536, loss 5.262768745422363\n",
      "Epoch 1: |          | 537/? [07:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 537, loss 4.900282859802246\n",
      "Epoch 1: |          | 538/? [07:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 538, loss 4.538740634918213\n",
      "Epoch 1: |          | 539/? [07:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 539, loss 4.592865467071533\n",
      "Epoch 1: |          | 540/? [07:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 540, loss 5.181731700897217\n",
      "Epoch 1: |          | 541/? [07:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 541, loss 4.817368984222412\n",
      "Epoch 1: |          | 542/? [07:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 542, loss 4.629650115966797\n",
      "Epoch 1: |          | 543/? [07:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 543, loss 4.988632678985596\n",
      "Epoch 1: |          | 544/? [07:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 544, loss 4.83087158203125\n",
      "Epoch 1: |          | 545/? [07:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 545, loss 3.965087890625\n",
      "Epoch 1: |          | 546/? [07:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 546, loss 4.946567058563232\n",
      "Epoch 1: |          | 547/? [07:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 547, loss 5.279305458068848\n",
      "Epoch 1: |          | 548/? [07:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 548, loss 5.092333793640137\n",
      "Epoch 1: |          | 549/? [07:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 549, loss 4.844344139099121\n",
      "Epoch 1: |          | 550/? [07:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 550, loss 5.285919666290283\n",
      "Epoch 1: |          | 551/? [07:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 551, loss 4.829334259033203\n",
      "Epoch 1: |          | 552/? [07:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 552, loss 5.0562744140625\n",
      "Epoch 1: |          | 553/? [07:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 553, loss 4.223113059997559\n",
      "Epoch 1: |          | 554/? [07:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 554, loss 4.942594528198242\n",
      "Epoch 1: |          | 555/? [07:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 555, loss 5.32541561126709\n",
      "Epoch 1: |          | 556/? [07:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 556, loss 4.848830699920654\n",
      "Epoch 1: |          | 557/? [07:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 557, loss 4.4874725341796875\n",
      "Epoch 1: |          | 558/? [07:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 558, loss 4.532629489898682\n",
      "Epoch 1: |          | 559/? [07:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 559, loss 4.655097961425781\n",
      "Epoch 1: |          | 560/? [07:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 560, loss 4.026760101318359\n",
      "Epoch 1: |          | 561/? [07:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 561, loss 4.282224655151367\n",
      "Epoch 1: |          | 562/? [07:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 562, loss 4.994510650634766\n",
      "Epoch 1: |          | 563/? [07:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 563, loss 4.063821315765381\n",
      "Epoch 1: |          | 564/? [07:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 564, loss 4.804596900939941\n",
      "Epoch 1: |          | 565/? [07:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 565, loss 5.2734150886535645\n",
      "Epoch 1: |          | 566/? [07:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 566, loss 5.101089954376221\n",
      "Epoch 1: |          | 567/? [07:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 567, loss 5.178742408752441\n",
      "Epoch 1: |          | 568/? [07:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 568, loss 4.3425211906433105\n",
      "Epoch 1: |          | 569/? [07:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 569, loss 4.991546154022217\n",
      "Epoch 1: |          | 570/? [07:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 570, loss 4.927342891693115\n",
      "Epoch 1: |          | 571/? [07:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 571, loss 4.468043804168701\n",
      "Epoch 1: |          | 572/? [07:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 572, loss 5.466945648193359\n",
      "Epoch 1: |          | 573/? [07:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 573, loss 3.7351551055908203\n",
      "Epoch 1: |          | 574/? [07:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 574, loss 5.097654342651367\n",
      "Epoch 1: |          | 575/? [07:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 575, loss 4.4332380294799805\n",
      "Epoch 1: |          | 576/? [07:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 576, loss 4.6695122718811035\n",
      "Epoch 1: |          | 577/? [07:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 577, loss 4.90861701965332\n",
      "Epoch 1: |          | 578/? [07:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 578, loss 5.113389015197754\n",
      "Epoch 1: |          | 579/? [07:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 579, loss 4.091910362243652\n",
      "Epoch 1: |          | 580/? [07:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 580, loss 4.933856964111328\n",
      "Epoch 1: |          | 581/? [07:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 581, loss 5.0040717124938965\n",
      "Epoch 1: |          | 582/? [07:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 582, loss 4.972380638122559\n",
      "Epoch 1: |          | 583/? [07:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 583, loss 4.70205545425415\n",
      "Epoch 1: |          | 584/? [07:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 584, loss 4.883547782897949\n",
      "Epoch 1: |          | 585/? [07:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 585, loss 5.065476417541504\n",
      "Epoch 1: |          | 586/? [07:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 586, loss 5.057899475097656\n",
      "Epoch 1: |          | 587/? [07:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 587, loss 4.868821144104004\n",
      "Epoch 1: |          | 588/? [07:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 588, loss 5.024238586425781\n",
      "Epoch 1: |          | 589/? [08:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 589, loss 4.4266791343688965\n",
      "Epoch 1: |          | 590/? [08:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 590, loss 5.121667385101318\n",
      "Epoch 1: |          | 591/? [08:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 591, loss 4.940321922302246\n",
      "Epoch 1: |          | 592/? [08:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 592, loss 4.662293434143066\n",
      "Epoch 1: |          | 593/? [08:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 593, loss 4.758206367492676\n",
      "Epoch 1: |          | 594/? [08:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 594, loss 5.638986587524414\n",
      "Epoch 1: |          | 595/? [08:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 595, loss 4.231268405914307\n",
      "Epoch 1: |          | 596/? [08:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 596, loss 4.364654064178467\n",
      "Epoch 1: |          | 597/? [08:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 597, loss 4.736128807067871\n",
      "Epoch 1: |          | 598/? [08:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 598, loss 5.145541667938232\n",
      "Epoch 1: |          | 599/? [08:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 599, loss 4.819550037384033\n",
      "Epoch 1: |          | 600/? [08:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 600, loss 4.460037708282471\n",
      "Epoch 1: |          | 601/? [08:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 601, loss 4.8383989334106445\n",
      "Epoch 1: |          | 602/? [08:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 602, loss 4.324517726898193\n",
      "Epoch 1: |          | 603/? [08:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 603, loss 4.663527488708496\n",
      "Epoch 1: |          | 604/? [08:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 604, loss 7.563580513000488\n",
      "Epoch 1: |          | 605/? [08:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 605, loss 4.25956392288208\n",
      "Epoch 1: |          | 606/? [08:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 606, loss 4.55767297744751\n",
      "Epoch 1: |          | 607/? [08:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 607, loss 5.009598731994629\n",
      "Epoch 1: |          | 608/? [08:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 608, loss 4.643740653991699\n",
      "Epoch 1: |          | 609/? [08:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 609, loss 4.62539005279541\n",
      "Epoch 1: |          | 610/? [08:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 610, loss 4.677243232727051\n",
      "Epoch 1: |          | 611/? [08:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 611, loss 4.856651782989502\n",
      "Epoch 1: |          | 612/? [08:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 612, loss 4.574639320373535\n",
      "Epoch 1: |          | 613/? [08:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 613, loss 4.919884204864502\n",
      "Epoch 1: |          | 614/? [08:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 614, loss 4.5820841789245605\n",
      "Epoch 1: |          | 615/? [08:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 615, loss 5.141974449157715\n",
      "Epoch 1: |          | 616/? [08:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 616, loss 5.478462219238281\n",
      "Epoch 1: |          | 617/? [08:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 617, loss 4.393715858459473\n",
      "Epoch 1: |          | 618/? [08:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 618, loss 4.940777778625488\n",
      "Epoch 1: |          | 619/? [08:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 619, loss 4.812046527862549\n",
      "Epoch 1: |          | 620/? [08:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 620, loss 5.1003570556640625\n",
      "Epoch 1: |          | 621/? [08:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 621, loss 4.407235622406006\n",
      "Epoch 1: |          | 622/? [08:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 622, loss 4.192043781280518\n",
      "Epoch 1: |          | 623/? [08:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 623, loss 3.8859095573425293\n",
      "Epoch 1: |          | 624/? [08:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 624, loss 3.4634060859680176\n",
      "Epoch 1: |          | 625/? [08:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 625, loss 5.203365325927734\n",
      "Epoch 1: |          | 626/? [08:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 626, loss 4.711264133453369\n",
      "Epoch 1: |          | 627/? [08:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 627, loss 4.692727088928223\n",
      "Epoch 1: |          | 628/? [08:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 628, loss 4.582842826843262\n",
      "Epoch 1: |          | 629/? [08:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 629, loss 5.03938102722168\n",
      "Epoch 1: |          | 630/? [08:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 630, loss 4.783984184265137\n",
      "Epoch 1: |          | 631/? [08:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 631, loss 4.977766990661621\n",
      "Epoch 1: |          | 632/? [08:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 632, loss 3.974569797515869\n",
      "Epoch 1: |          | 633/? [08:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 633, loss 5.068005561828613\n",
      "Epoch 1: |          | 634/? [08:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 634, loss 4.550266742706299\n",
      "Epoch 1: |          | 635/? [08:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 635, loss 4.302948951721191\n",
      "Epoch 1: |          | 636/? [08:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 636, loss 4.770724296569824\n",
      "Epoch 1: |          | 637/? [08:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 637, loss 4.58054256439209\n",
      "Epoch 1: |          | 638/? [08:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 638, loss 4.825056076049805\n",
      "Epoch 1: |          | 639/? [08:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 639, loss 4.522878646850586\n",
      "Epoch 1: |          | 640/? [08:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 640, loss 5.218283653259277\n",
      "Epoch 1: |          | 641/? [08:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 641, loss 4.376177787780762\n",
      "Epoch 1: |          | 642/? [08:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 642, loss 4.939755916595459\n",
      "Epoch 1: |          | 643/? [08:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 643, loss 4.956203937530518\n",
      "Epoch 1: |          | 644/? [08:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 644, loss 4.693399906158447\n",
      "Epoch 1: |          | 645/? [08:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 645, loss 4.546597957611084\n",
      "Epoch 1: |          | 646/? [08:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 646, loss 4.547475337982178\n",
      "Epoch 1: |          | 647/? [08:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 647, loss 5.261709690093994\n",
      "Epoch 1: |          | 648/? [08:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 648, loss 4.814431190490723\n",
      "Epoch 1: |          | 649/? [08:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 649, loss 5.158011436462402\n",
      "Epoch 1: |          | 650/? [08:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 650, loss 5.175642490386963\n",
      "Epoch 1: |          | 651/? [08:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 651, loss 5.3124284744262695\n",
      "Epoch 1: |          | 652/? [08:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 652, loss 4.597662448883057\n",
      "Epoch 1: |          | 653/? [08:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 653, loss 4.717560768127441\n",
      "Epoch 1: |          | 654/? [08:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 654, loss 5.068356513977051\n",
      "Epoch 1: |          | 655/? [08:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 655, loss 4.700104713439941\n",
      "Epoch 1: |          | 656/? [08:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 656, loss 4.258177757263184\n",
      "Epoch 1: |          | 657/? [08:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 657, loss 6.961023807525635\n",
      "Epoch 1: |          | 658/? [08:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 658, loss 4.623099327087402\n",
      "Epoch 1: |          | 659/? [08:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 659, loss 4.749459266662598\n",
      "Epoch 1: |          | 660/? [08:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 660, loss 5.160156726837158\n",
      "Epoch 1: |          | 661/? [08:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 661, loss 5.052255153656006\n",
      "Epoch 1: |          | 662/? [08:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 662, loss 4.875425815582275\n",
      "Epoch 1: |          | 663/? [08:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 663, loss 4.536440372467041\n",
      "Epoch 1: |          | 664/? [09:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 664, loss 4.526820659637451\n",
      "Epoch 1: |          | 665/? [09:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 665, loss 4.866547584533691\n",
      "Epoch 1: |          | 666/? [09:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 666, loss 4.654328346252441\n",
      "Epoch 1: |          | 667/? [09:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 667, loss 5.543353080749512\n",
      "Epoch 1: |          | 668/? [09:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 668, loss 4.230848789215088\n",
      "Epoch 1: |          | 669/? [09:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 669, loss 4.510841369628906\n",
      "Epoch 1: |          | 670/? [09:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 670, loss 5.17557430267334\n",
      "Epoch 1: |          | 671/? [09:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 671, loss 4.932399272918701\n",
      "Epoch 1: |          | 672/? [09:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 672, loss 4.897693157196045\n",
      "Epoch 1: |          | 673/? [09:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 673, loss 4.728033065795898\n",
      "Epoch 1: |          | 674/? [09:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 674, loss 3.3845410346984863\n",
      "Epoch 1: |          | 675/? [09:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 675, loss 1.8429495096206665\n",
      "Epoch 1: |          | 676/? [09:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 676, loss 1.4933507442474365\n",
      "Epoch 1: |          | 677/? [09:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 677, loss 1.165056824684143\n",
      "Epoch 1: |          | 678/? [09:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 678, loss 2.4152395725250244\n",
      "Epoch 1: |          | 679/? [09:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 679, loss 4.194736003875732\n",
      "Epoch 1: |          | 680/? [09:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 680, loss 4.771696090698242\n",
      "Epoch 1: |          | 681/? [09:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 681, loss 3.9887795448303223\n",
      "Epoch 1: |          | 682/? [09:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 682, loss 4.606062889099121\n",
      "Epoch 1: |          | 683/? [09:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 683, loss 4.2060651779174805\n",
      "Epoch 1: |          | 684/? [09:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 684, loss 5.376330852508545\n",
      "Epoch 1: |          | 685/? [09:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 685, loss 4.94488000869751\n",
      "Epoch 1: |          | 686/? [09:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 686, loss 4.411413192749023\n",
      "Epoch 1: |          | 687/? [09:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 687, loss 5.176309108734131\n",
      "Epoch 1: |          | 688/? [09:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 688, loss 4.922685146331787\n",
      "Epoch 1: |          | 689/? [09:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 689, loss 4.8659138679504395\n",
      "Epoch 1: |          | 690/? [09:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 690, loss 5.206369400024414\n",
      "Epoch 1: |          | 691/? [09:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 691, loss 4.9086713790893555\n",
      "Epoch 1: |          | 692/? [09:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 692, loss 4.714983940124512\n",
      "Epoch 1: |          | 693/? [09:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 693, loss 5.208191871643066\n",
      "Epoch 1: |          | 694/? [09:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 694, loss 4.539112567901611\n",
      "Epoch 1: |          | 695/? [09:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 695, loss 5.262670993804932\n",
      "Epoch 1: |          | 696/? [09:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 696, loss 4.265398025512695\n",
      "Epoch 1: |          | 697/? [09:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 697, loss 4.743175506591797\n",
      "Epoch 1: |          | 698/? [09:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 698, loss 3.8616511821746826\n",
      "Epoch 1: |          | 699/? [09:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 699, loss 4.832821846008301\n",
      "Epoch 1: |          | 700/? [09:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 700, loss 5.0533552169799805\n",
      "Epoch 1: |          | 701/? [09:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 701, loss 4.492936134338379\n",
      "Epoch 1: |          | 702/? [09:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 702, loss 4.728754997253418\n",
      "Epoch 1: |          | 703/? [09:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 703, loss 4.896248817443848\n",
      "Epoch 1: |          | 704/? [09:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 704, loss 4.856695652008057\n",
      "Epoch 1: |          | 705/? [09:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 705, loss 4.370269775390625\n",
      "Epoch 1: |          | 706/? [09:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 706, loss 4.503747940063477\n",
      "Epoch 1: |          | 707/? [09:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 707, loss 4.934349536895752\n",
      "Epoch 1: |          | 708/? [09:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 708, loss 4.7449517250061035\n",
      "Epoch 1: |          | 709/? [09:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 709, loss 4.5973029136657715\n",
      "Epoch 1: |          | 710/? [09:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 710, loss 5.219470977783203\n",
      "Epoch 1: |          | 711/? [09:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 711, loss 5.457910537719727\n",
      "Epoch 1: |          | 712/? [09:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 712, loss 5.061606407165527\n",
      "Epoch 1: |          | 713/? [09:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 713, loss 4.998835563659668\n",
      "Epoch 1: |          | 714/? [09:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 714, loss 5.190795421600342\n",
      "Epoch 1: |          | 715/? [09:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 715, loss 3.9398398399353027\n",
      "Epoch 1: |          | 716/? [09:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 716, loss 4.820440769195557\n",
      "Epoch 1: |          | 717/? [09:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 717, loss 4.544396877288818\n",
      "Epoch 1: |          | 718/? [09:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 718, loss 4.114405155181885\n",
      "Epoch 1: |          | 719/? [09:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 719, loss 4.670199394226074\n",
      "Epoch 1: |          | 720/? [09:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 720, loss 4.444218158721924\n",
      "Epoch 1: |          | 721/? [09:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 721, loss 5.1513471603393555\n",
      "Epoch 1: |          | 722/? [09:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 722, loss 4.166264533996582\n",
      "Epoch 1: |          | 723/? [09:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 723, loss 4.834198951721191\n",
      "Epoch 1: |          | 724/? [09:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 724, loss 4.781390190124512\n",
      "Epoch 1: |          | 725/? [09:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 725, loss 4.4599761962890625\n",
      "Epoch 1: |          | 726/? [09:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 726, loss 4.585690021514893\n",
      "Epoch 1: |          | 727/? [09:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 727, loss 4.265311241149902\n",
      "Epoch 1: |          | 728/? [09:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 728, loss 4.13243293762207\n",
      "Epoch 1: |          | 729/? [09:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 729, loss 4.508023738861084\n",
      "Epoch 1: |          | 730/? [09:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 730, loss 4.538293361663818\n",
      "Epoch 1: |          | 731/? [09:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 731, loss 4.749965190887451\n",
      "Epoch 1: |          | 732/? [09:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 732, loss 5.087380409240723\n",
      "Epoch 1: |          | 733/? [09:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 733, loss 4.646406173706055\n",
      "Epoch 1: |          | 734/? [09:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 734, loss 5.034182548522949\n",
      "Epoch 1: |          | 735/? [09:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 735, loss 4.896676063537598\n",
      "Epoch 1: |          | 736/? [09:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 736, loss 4.3604230880737305\n",
      "Epoch 1: |          | 737/? [09:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 737, loss 5.136723518371582\n",
      "Epoch 1: |          | 738/? [09:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 738, loss 4.2680511474609375\n",
      "Epoch 1: |          | 739/? [10:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 739, loss 4.893570423126221\n",
      "Epoch 1: |          | 740/? [10:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 740, loss 4.370223045349121\n",
      "Epoch 1: |          | 741/? [10:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 741, loss 4.678346633911133\n",
      "Epoch 1: |          | 742/? [10:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 742, loss 5.160143852233887\n",
      "Epoch 1: |          | 743/? [10:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 743, loss 4.924109935760498\n",
      "Epoch 1: |          | 744/? [10:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 744, loss 4.807681083679199\n",
      "Epoch 1: |          | 745/? [10:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 745, loss 4.453452110290527\n",
      "Epoch 1: |          | 746/? [10:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 746, loss 4.758976936340332\n",
      "Epoch 1: |          | 747/? [10:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 747, loss 4.610163688659668\n",
      "Epoch 1: |          | 748/? [10:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 748, loss 4.291677474975586\n",
      "Epoch 1: |          | 749/? [10:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 749, loss 4.788107872009277\n",
      "Epoch 1: |          | 750/? [10:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 750, loss 5.168946266174316\n",
      "Epoch 1: |          | 751/? [10:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 751, loss 3.741973876953125\n",
      "Epoch 1: |          | 752/? [10:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 752, loss 5.006280422210693\n",
      "Epoch 1: |          | 753/? [10:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 753, loss 4.232018947601318\n",
      "Epoch 1: |          | 754/? [10:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 754, loss 4.673287391662598\n",
      "Epoch 1: |          | 755/? [10:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 755, loss 4.277676582336426\n",
      "Epoch 1: |          | 756/? [10:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 756, loss 4.740018367767334\n",
      "Epoch 1: |          | 757/? [10:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 757, loss 4.728785514831543\n",
      "Epoch 1: |          | 758/? [10:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 758, loss 4.382946968078613\n",
      "Epoch 1: |          | 759/? [10:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 759, loss 4.576727390289307\n",
      "Epoch 1: |          | 760/? [10:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 760, loss 4.885310173034668\n",
      "Epoch 1: |          | 761/? [10:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 761, loss 4.981714248657227\n",
      "Epoch 1: |          | 762/? [10:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 762, loss 4.647308349609375\n",
      "Epoch 1: |          | 763/? [10:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 763, loss 5.070025444030762\n",
      "Epoch 1: |          | 764/? [10:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 764, loss 5.1947221755981445\n",
      "Epoch 1: |          | 765/? [10:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 765, loss 4.835861682891846\n",
      "Epoch 1: |          | 766/? [10:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 766, loss 5.252251625061035\n",
      "Epoch 1: |          | 767/? [10:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 767, loss 5.387119293212891\n",
      "Epoch 1: |          | 768/? [10:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 768, loss 4.808955192565918\n",
      "Epoch 1: |          | 769/? [10:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 769, loss 3.923671007156372\n",
      "Epoch 1: |          | 770/? [10:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 770, loss 4.543740749359131\n",
      "Epoch 1: |          | 771/? [10:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 771, loss 5.350461483001709\n",
      "Epoch 1: |          | 772/? [10:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 772, loss 5.076092720031738\n",
      "Epoch 1: |          | 773/? [10:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 773, loss 4.628935813903809\n",
      "Epoch 1: |          | 774/? [10:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 774, loss 4.734388828277588\n",
      "Epoch 1: |          | 775/? [10:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 775, loss 5.283961296081543\n",
      "Epoch 1: |          | 776/? [10:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 776, loss 4.707791328430176\n",
      "Epoch 1: |          | 777/? [10:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 777, loss 4.711220741271973\n",
      "Epoch 1: |          | 778/? [10:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 778, loss 5.081833839416504\n",
      "Epoch 1: |          | 779/? [10:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 779, loss 5.473297595977783\n",
      "Epoch 1: |          | 780/? [10:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 780, loss 4.222103595733643\n",
      "Epoch 1: |          | 781/? [10:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 781, loss 4.466592311859131\n",
      "Epoch 1: |          | 782/? [10:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 782, loss 4.925456523895264\n",
      "Epoch 1: |          | 783/? [10:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 783, loss 4.951786518096924\n",
      "Epoch 1: |          | 784/? [10:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 784, loss 4.469259262084961\n",
      "Epoch 1: |          | 785/? [10:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 785, loss 4.388197898864746\n",
      "Epoch 1: |          | 786/? [10:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 786, loss 5.230183124542236\n",
      "Epoch 1: |          | 787/? [10:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 787, loss 5.2533698081970215\n",
      "Epoch 1: |          | 788/? [10:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 788, loss 3.3009750843048096\n",
      "Epoch 1: |          | 789/? [10:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 789, loss 4.691161632537842\n",
      "Epoch 1: |          | 790/? [10:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 790, loss 5.509149551391602\n",
      "Epoch 1: |          | 791/? [10:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 791, loss 5.279386520385742\n",
      "Epoch 1: |          | 792/? [10:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 792, loss 4.230870246887207\n",
      "Epoch 1: |          | 793/? [10:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 793, loss 4.813900470733643\n",
      "Epoch 1: |          | 794/? [10:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 794, loss 5.209622383117676\n",
      "Epoch 1: |          | 795/? [10:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 795, loss 4.701542854309082\n",
      "Epoch 1: |          | 796/? [10:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 796, loss 5.094647407531738\n",
      "Epoch 1: |          | 797/? [10:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 797, loss 3.9509787559509277\n",
      "Epoch 1: |          | 798/? [10:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 798, loss 4.103115081787109\n",
      "Epoch 1: |          | 799/? [10:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 799, loss 5.132511138916016\n",
      "Epoch 1: |          | 800/? [10:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 800, loss 4.905513286590576\n",
      "Epoch 1: |          | 801/? [10:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 801, loss 4.296611309051514\n",
      "Epoch 1: |          | 802/? [10:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 802, loss 4.794530868530273\n",
      "Epoch 1: |          | 803/? [10:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 803, loss 4.6208953857421875\n",
      "Epoch 1: |          | 804/? [10:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 804, loss 4.878710746765137\n",
      "Epoch 1: |          | 805/? [10:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 805, loss 5.2033467292785645\n",
      "Epoch 1: |          | 806/? [10:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 806, loss 5.34535551071167\n",
      "Epoch 1: |          | 807/? [10:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 807, loss 4.832964897155762\n",
      "Epoch 1: |          | 808/? [10:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 808, loss 4.280139923095703\n",
      "Epoch 1: |          | 809/? [10:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 809, loss 4.88577938079834\n",
      "Epoch 1: |          | 810/? [10:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 810, loss 4.758940696716309\n",
      "Epoch 1: |          | 811/? [10:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 811, loss 5.0568108558654785\n",
      "Epoch 1: |          | 812/? [11:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 812, loss 5.733584880828857\n",
      "Epoch 1: |          | 813/? [11:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 813, loss 5.301182746887207\n",
      "Epoch 1: |          | 814/? [11:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 814, loss 4.270352363586426\n",
      "Epoch 1: |          | 815/? [11:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 815, loss 5.076617240905762\n",
      "Epoch 1: |          | 816/? [11:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 816, loss 4.942618370056152\n",
      "Epoch 1: |          | 817/? [11:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 817, loss 4.188079357147217\n",
      "Epoch 1: |          | 818/? [11:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 818, loss 5.261332035064697\n",
      "Epoch 1: |          | 819/? [11:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 819, loss 4.956275939941406\n",
      "Epoch 1: |          | 820/? [11:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 820, loss 4.822056770324707\n",
      "Epoch 1: |          | 821/? [11:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 821, loss 4.767061710357666\n",
      "Epoch 1: |          | 822/? [11:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 822, loss 4.299108982086182\n",
      "Epoch 1: |          | 823/? [11:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 823, loss 4.2581868171691895\n",
      "Epoch 1: |          | 824/? [11:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 824, loss 4.820207118988037\n",
      "Epoch 1: |          | 825/? [11:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 825, loss 4.3823041915893555\n",
      "Epoch 1: |          | 826/? [11:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 826, loss 4.850203990936279\n",
      "Epoch 1: |          | 827/? [11:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 827, loss 4.516983985900879\n",
      "Epoch 1: |          | 828/? [11:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 828, loss 5.0726318359375\n",
      "Epoch 1: |          | 829/? [11:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 829, loss 4.6768412590026855\n",
      "Epoch 1: |          | 830/? [11:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 830, loss 5.317019939422607\n",
      "Epoch 1: |          | 831/? [11:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 831, loss 3.02992582321167\n",
      "Epoch 1: |          | 832/? [11:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 832, loss 4.729992866516113\n",
      "Epoch 1: |          | 833/? [11:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 833, loss 4.554531097412109\n",
      "Epoch 1: |          | 834/? [11:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 834, loss 5.313737392425537\n",
      "Epoch 1: |          | 835/? [11:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 835, loss 4.834763050079346\n",
      "Epoch 1: |          | 836/? [11:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 836, loss 5.351324558258057\n",
      "Epoch 1: |          | 837/? [11:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 837, loss 4.8313703536987305\n",
      "Epoch 1: |          | 838/? [11:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 838, loss 4.077179908752441\n",
      "Epoch 1: |          | 839/? [11:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 839, loss 4.382813930511475\n",
      "Epoch 1: |          | 840/? [11:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 840, loss 5.006038188934326\n",
      "Epoch 1: |          | 841/? [11:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 841, loss 5.07324743270874\n",
      "Epoch 1: |          | 842/? [11:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 842, loss 4.7640252113342285\n",
      "Epoch 1: |          | 843/? [11:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 843, loss 5.105042457580566\n",
      "Epoch 1: |          | 844/? [11:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 844, loss 4.397404670715332\n",
      "Epoch 1: |          | 845/? [11:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 845, loss 4.847282409667969\n",
      "Epoch 1: |          | 846/? [11:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 846, loss 5.4326653480529785\n",
      "Epoch 1: |          | 847/? [11:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 847, loss 4.904019355773926\n",
      "Epoch 1: |          | 848/? [11:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 848, loss 4.339938163757324\n",
      "Epoch 1: |          | 849/? [11:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 849, loss 4.5412468910217285\n",
      "Epoch 1: |          | 850/? [11:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 850, loss 4.552857875823975\n",
      "Epoch 1: |          | 851/? [11:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 851, loss 5.004199028015137\n",
      "Epoch 1: |          | 852/? [11:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 852, loss 4.918762683868408\n",
      "Epoch 1: |          | 853/? [11:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 853, loss 4.983592987060547\n",
      "Epoch 1: |          | 854/? [11:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 854, loss 4.008657932281494\n",
      "Epoch 1: |          | 855/? [11:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 855, loss 4.451367378234863\n",
      "Epoch 1: |          | 856/? [11:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 856, loss 4.374905109405518\n",
      "Epoch 1: |          | 857/? [11:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 857, loss 4.997504234313965\n",
      "Epoch 1: |          | 858/? [11:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 858, loss 4.873383045196533\n",
      "Epoch 1: |          | 859/? [11:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 859, loss 4.894144058227539\n",
      "Epoch 1: |          | 860/? [11:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 860, loss 5.26568078994751\n",
      "Epoch 1: |          | 861/? [11:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 861, loss 4.414112091064453\n",
      "Epoch 1: |          | 862/? [11:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 862, loss 4.940150260925293\n",
      "Epoch 1: |          | 863/? [11:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 863, loss 4.031553745269775\n",
      "Epoch 1: |          | 864/? [11:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 864, loss 4.937915802001953\n",
      "Epoch 1: |          | 865/? [11:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 865, loss 4.783262729644775\n",
      "Epoch 1: |          | 866/? [11:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 866, loss 3.9448819160461426\n",
      "Epoch 1: |          | 867/? [11:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 867, loss 4.040555953979492\n",
      "Epoch 1: |          | 868/? [11:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 868, loss 4.924956798553467\n",
      "Epoch 1: |          | 869/? [11:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 869, loss 4.838183879852295\n",
      "Epoch 1: |          | 870/? [11:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 870, loss 4.383251190185547\n",
      "Epoch 1: |          | 871/? [11:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 871, loss 4.925615310668945\n",
      "Epoch 1: |          | 872/? [11:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 872, loss 4.783782958984375\n",
      "Epoch 1: |          | 873/? [11:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 873, loss 4.643490314483643\n",
      "Epoch 1: |          | 874/? [11:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 874, loss 4.145138263702393\n",
      "Epoch 1: |          | 875/? [11:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 875, loss 4.908139228820801\n",
      "Epoch 1: |          | 876/? [11:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 876, loss 4.673678398132324\n",
      "Epoch 1: |          | 877/? [11:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 877, loss 4.788996696472168\n",
      "Epoch 1: |          | 878/? [11:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 878, loss 4.244844913482666\n",
      "Epoch 1: |          | 879/? [11:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 879, loss 4.3445963859558105\n",
      "Epoch 1: |          | 880/? [11:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 880, loss 5.420834064483643\n",
      "Epoch 1: |          | 881/? [11:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 881, loss 4.835916042327881\n",
      "Epoch 1: |          | 882/? [11:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 882, loss 4.648604869842529\n",
      "Epoch 1: |          | 883/? [11:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 883, loss 4.72667932510376\n",
      "Epoch 1: |          | 884/? [11:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 884, loss 4.85206937789917\n",
      "Epoch 1: |          | 885/? [12:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 885, loss 4.552212238311768\n",
      "Epoch 1: |          | 886/? [12:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 886, loss 5.19074010848999\n",
      "Epoch 1: |          | 887/? [12:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 887, loss 5.241179943084717\n",
      "Epoch 1: |          | 888/? [12:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 888, loss 4.925077438354492\n",
      "Epoch 1: |          | 889/? [12:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 889, loss 4.689349174499512\n",
      "Epoch 1: |          | 890/? [12:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 890, loss 5.010649681091309\n",
      "Epoch 1: |          | 891/? [12:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 891, loss 4.359169006347656\n",
      "Epoch 1: |          | 892/? [12:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 892, loss 5.195866107940674\n",
      "Epoch 1: |          | 893/? [12:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 893, loss 4.5795087814331055\n",
      "Epoch 1: |          | 894/? [12:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 894, loss 4.264432907104492\n",
      "Epoch 1: |          | 895/? [12:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 895, loss 5.276604652404785\n",
      "Epoch 1: |          | 896/? [12:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 896, loss 4.930022239685059\n",
      "Epoch 1: |          | 897/? [12:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 897, loss 4.945620536804199\n",
      "Epoch 1: |          | 898/? [12:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 898, loss 4.923861503601074\n",
      "Epoch 1: |          | 899/? [12:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 899, loss 4.668978691101074\n",
      "Epoch 1: |          | 900/? [12:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 900, loss 4.50283145904541\n",
      "Epoch 1: |          | 901/? [12:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 901, loss 5.051634788513184\n",
      "Epoch 1: |          | 902/? [12:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 902, loss 5.052109241485596\n",
      "Epoch 1: |          | 903/? [12:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 903, loss 4.323156833648682\n",
      "Epoch 1: |          | 904/? [12:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 904, loss 4.78985595703125\n",
      "Epoch 1: |          | 905/? [12:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 905, loss 5.062517166137695\n",
      "Epoch 1: |          | 906/? [12:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 906, loss 4.694533824920654\n",
      "Epoch 1: |          | 907/? [12:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 907, loss 4.83628511428833\n",
      "Epoch 1: |          | 908/? [12:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 908, loss 4.91471004486084\n",
      "Epoch 1: |          | 909/? [12:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 909, loss 4.904694080352783\n",
      "Epoch 1: |          | 910/? [12:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 910, loss 4.6762566566467285\n",
      "Epoch 1: |          | 911/? [12:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 911, loss 4.659789085388184\n",
      "Epoch 1: |          | 912/? [12:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 912, loss 4.623713970184326\n",
      "Epoch 1: |          | 913/? [12:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 913, loss 4.6091108322143555\n",
      "Epoch 1: |          | 914/? [12:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 914, loss 5.032803058624268\n",
      "Epoch 1: |          | 915/? [12:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 915, loss 4.939492225646973\n",
      "Epoch 1: |          | 916/? [12:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 916, loss 4.696750164031982\n",
      "Epoch 1: |          | 917/? [12:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 917, loss 4.738432884216309\n",
      "Epoch 1: |          | 918/? [12:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 918, loss 4.567708969116211\n",
      "Epoch 1: |          | 919/? [12:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 919, loss 4.497189521789551\n",
      "Epoch 1: |          | 920/? [12:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 920, loss 4.722237586975098\n",
      "Epoch 1: |          | 921/? [12:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 921, loss 4.595340728759766\n",
      "Epoch 1: |          | 922/? [12:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 922, loss 4.750308036804199\n",
      "Epoch 1: |          | 923/? [12:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 923, loss 4.566853046417236\n",
      "Epoch 1: |          | 924/? [12:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 924, loss 4.561394691467285\n",
      "Epoch 1: |          | 925/? [12:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 925, loss 4.845619201660156\n",
      "Epoch 1: |          | 926/? [12:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 926, loss 4.717253684997559\n",
      "Epoch 1: |          | 927/? [12:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 927, loss 4.8534674644470215\n",
      "Epoch 1: |          | 928/? [12:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 928, loss 4.284111976623535\n",
      "Epoch 1: |          | 929/? [12:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 929, loss 4.456127166748047\n",
      "Epoch 1: |          | 930/? [12:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 930, loss 4.312407970428467\n",
      "Epoch 1: |          | 931/? [12:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 931, loss 4.002537727355957\n",
      "Epoch 1: |          | 932/? [12:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 932, loss 4.753449440002441\n",
      "Epoch 1: |          | 933/? [12:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 933, loss 4.560174942016602\n",
      "Epoch 1: |          | 934/? [12:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 934, loss 5.25241756439209\n",
      "Epoch 1: |          | 935/? [12:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 935, loss 5.408511161804199\n",
      "Epoch 1: |          | 936/? [12:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 936, loss 4.728686809539795\n",
      "Epoch 1: |          | 937/? [12:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 937, loss 5.2381367683410645\n",
      "Epoch 1: |          | 938/? [12:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 938, loss 4.524937629699707\n",
      "Epoch 1: |          | 939/? [12:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 939, loss 4.851542949676514\n",
      "Epoch 1: |          | 940/? [12:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 940, loss 5.185097694396973\n",
      "Epoch 1: |          | 941/? [12:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 941, loss 4.463413238525391\n",
      "Epoch 1: |          | 942/? [12:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 942, loss 4.055703163146973\n",
      "Epoch 1: |          | 943/? [12:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 943, loss 4.990746021270752\n",
      "Epoch 1: |          | 944/? [12:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 944, loss 3.9582676887512207\n",
      "Epoch 1: |          | 945/? [12:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 945, loss 4.619304656982422\n",
      "Epoch 1: |          | 946/? [12:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 946, loss 4.726978778839111\n",
      "Epoch 1: |          | 947/? [12:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 947, loss 4.567318916320801\n",
      "Epoch 1: |          | 948/? [12:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 948, loss 4.7227373123168945\n",
      "Epoch 1: |          | 949/? [12:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 949, loss 4.588761329650879\n",
      "Epoch 1: |          | 950/? [12:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 950, loss 4.434027671813965\n",
      "Epoch 1: |          | 951/? [12:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 951, loss 5.283254146575928\n",
      "Epoch 1: |          | 952/? [12:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 952, loss 5.116846084594727\n",
      "Epoch 1: |          | 953/? [12:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 953, loss 5.608153343200684\n",
      "Epoch 1: |          | 954/? [12:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 954, loss 4.732212066650391\n",
      "Epoch 1: |          | 955/? [12:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 955, loss 5.3968400955200195\n",
      "Epoch 1: |          | 956/? [12:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 956, loss 4.5403056144714355\n",
      "Epoch 1: |          | 957/? [12:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 957, loss 4.98948335647583\n",
      "Epoch 1: |          | 958/? [12:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 958, loss 5.337446689605713\n",
      "Epoch 1: |          | 959/? [12:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 959, loss 5.410325527191162\n",
      "Epoch 1: |          | 960/? [13:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 960, loss 5.217928409576416\n",
      "Epoch 1: |          | 961/? [13:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 961, loss 5.192196846008301\n",
      "Epoch 1: |          | 962/? [13:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 962, loss 4.925809383392334\n",
      "Epoch 1: |          | 963/? [13:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 963, loss 4.411008834838867\n",
      "Epoch 1: |          | 964/? [13:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 964, loss 4.969851493835449\n",
      "Epoch 1: |          | 965/? [13:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 965, loss 4.595860004425049\n",
      "Epoch 1: |          | 966/? [13:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 966, loss 4.373430252075195\n",
      "Epoch 1: |          | 967/? [13:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 967, loss 4.623836994171143\n",
      "Epoch 1: |          | 968/? [13:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 968, loss 4.686874866485596\n",
      "Epoch 1: |          | 969/? [13:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 969, loss 4.343051433563232\n",
      "Epoch 1: |          | 970/? [13:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 970, loss 4.884765148162842\n",
      "Epoch 1: |          | 971/? [13:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 971, loss 5.56325626373291\n",
      "Epoch 1: |          | 972/? [13:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 972, loss 4.604188919067383\n",
      "Epoch 1: |          | 973/? [13:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 973, loss 5.005263805389404\n",
      "Epoch 1: |          | 974/? [13:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 974, loss 4.723384857177734\n",
      "Epoch 1: |          | 975/? [13:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 975, loss 4.754312038421631\n",
      "Epoch 1: |          | 976/? [13:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 976, loss 4.848454475402832\n",
      "Epoch 1: |          | 977/? [13:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 977, loss 5.44846248626709\n",
      "Epoch 1: |          | 978/? [13:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 978, loss 5.169008255004883\n",
      "Epoch 1: |          | 979/? [13:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 979, loss 5.191595554351807\n",
      "Epoch 1: |          | 980/? [13:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 980, loss 4.434783935546875\n",
      "Epoch 1: |          | 981/? [13:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 981, loss 3.9879558086395264\n",
      "Epoch 1: |          | 982/? [13:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 982, loss 4.787463188171387\n",
      "Epoch 1: |          | 983/? [13:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 983, loss 5.144615173339844\n",
      "Epoch 1: |          | 984/? [13:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 984, loss 4.119499206542969\n",
      "Epoch 1: |          | 985/? [13:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 985, loss 4.505349636077881\n",
      "Epoch 1: |          | 986/? [13:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 986, loss 4.447307586669922\n",
      "Epoch 1: |          | 987/? [13:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 987, loss 4.297023296356201\n",
      "Epoch 1: |          | 988/? [13:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 988, loss 5.058874130249023\n",
      "Epoch 1: |          | 989/? [13:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 989, loss 4.738412380218506\n",
      "Epoch 1: |          | 990/? [13:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 990, loss 3.815476179122925\n",
      "Epoch 1: |          | 991/? [13:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 991, loss 4.633993148803711\n",
      "Epoch 1: |          | 992/? [13:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 992, loss 5.421113014221191\n",
      "Epoch 1: |          | 993/? [13:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 993, loss 4.448753356933594\n",
      "Epoch 1: |          | 994/? [13:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 994, loss 4.5789475440979\n",
      "Epoch 1: |          | 995/? [13:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 995, loss 5.04935359954834\n",
      "Epoch 1: |          | 996/? [13:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 996, loss 5.012919902801514\n",
      "Epoch 1: |          | 997/? [13:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 997, loss 4.550648212432861\n",
      "Epoch 1: |          | 998/? [13:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 998, loss 4.811405181884766\n",
      "Epoch 1: |          | 999/? [13:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 999, loss 5.14949893951416\n",
      "Epoch 1: |          | 1000/? [13:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1000, loss 4.623225212097168\n",
      "Epoch 1: |          | 1001/? [13:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1001, loss 5.048808574676514\n",
      "Epoch 1: |          | 1002/? [13:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1002, loss 5.117717742919922\n",
      "Epoch 1: |          | 1003/? [13:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1003, loss 5.076204776763916\n",
      "Epoch 1: |          | 1004/? [13:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1004, loss 3.926366090774536\n",
      "Epoch 1: |          | 1005/? [13:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1005, loss 4.61185884475708\n",
      "Epoch 1: |          | 1006/? [13:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1006, loss 5.0536394119262695\n",
      "Epoch 1: |          | 1007/? [13:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1007, loss 4.696069717407227\n",
      "Epoch 1: |          | 1008/? [13:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1008, loss 4.620276927947998\n",
      "Epoch 1: |          | 1009/? [13:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1009, loss 5.079940319061279\n",
      "Epoch 1: |          | 1010/? [13:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1010, loss 4.07181978225708\n",
      "Epoch 1: |          | 1011/? [13:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1011, loss 4.7697858810424805\n",
      "Epoch 1: |          | 1012/? [13:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1012, loss 4.430157661437988\n",
      "Epoch 1: |          | 1013/? [13:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1013, loss 4.874755859375\n",
      "Epoch 1: |          | 1014/? [13:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1014, loss 5.0528950691223145\n",
      "Epoch 1: |          | 1015/? [13:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1015, loss 4.774933338165283\n",
      "Epoch 1: |          | 1016/? [13:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1016, loss 4.773064136505127\n",
      "Epoch 1: |          | 1017/? [13:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1017, loss 4.39875602722168\n",
      "Epoch 1: |          | 1018/? [13:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1018, loss 4.759402275085449\n",
      "Epoch 1: |          | 1019/? [13:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1019, loss 4.704718112945557\n",
      "Epoch 1: |          | 1020/? [13:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1020, loss 4.214846611022949\n",
      "Epoch 1: |          | 1021/? [13:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1021, loss 4.42582368850708\n",
      "Epoch 1: |          | 1022/? [13:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1022, loss 4.146353721618652\n",
      "Epoch 1: |          | 1023/? [13:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1023, loss 3.844364881515503\n",
      "Epoch 1: |          | 1024/? [13:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1024, loss 4.568831443786621\n",
      "Epoch 1: |          | 1025/? [13:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1025, loss 4.553720951080322\n",
      "Epoch 1: |          | 1026/? [13:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1026, loss 3.361781358718872\n",
      "Epoch 1: |          | 1027/? [13:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1027, loss 4.906089782714844\n",
      "Epoch 1: |          | 1028/? [14:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1028, loss 4.583777904510498\n",
      "Epoch 1: |          | 1029/? [14:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1029, loss 4.5532402992248535\n",
      "Epoch 1: |          | 1030/? [14:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1030, loss 4.179976463317871\n",
      "Epoch 1: |          | 1031/? [14:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1031, loss 4.282943248748779\n",
      "Epoch 1: |          | 1032/? [14:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1032, loss 5.080065727233887\n",
      "Epoch 1: |          | 1033/? [14:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1033, loss 5.124999046325684\n",
      "Epoch 1: |          | 1034/? [14:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1034, loss 4.484230995178223\n",
      "Epoch 1: |          | 1035/? [14:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1035, loss 4.514944076538086\n",
      "Epoch 1: |          | 1036/? [14:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1036, loss 4.26322078704834\n",
      "Epoch 1: |          | 1037/? [14:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1037, loss 5.075909614562988\n",
      "Epoch 1: |          | 1038/? [14:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1038, loss 5.23920202255249\n",
      "Epoch 1: |          | 1039/? [14:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1039, loss 5.361243724822998\n",
      "Epoch 1: |          | 1040/? [14:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1040, loss 4.770029067993164\n",
      "Epoch 1: |          | 1041/? [14:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1041, loss 5.187053203582764\n",
      "Epoch 1: |          | 1042/? [14:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1042, loss 4.680438041687012\n",
      "Epoch 1: |          | 1043/? [14:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1043, loss 4.966195583343506\n",
      "Epoch 1: |          | 1044/? [14:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1044, loss 4.489388942718506\n",
      "Epoch 1: |          | 1045/? [14:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1045, loss 4.115475177764893\n",
      "Epoch 1: |          | 1046/? [14:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1046, loss 3.927588939666748\n",
      "Epoch 1: |          | 1047/? [14:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1047, loss 5.304884433746338\n",
      "Epoch 1: |          | 1048/? [14:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1048, loss 4.440086841583252\n",
      "Epoch 1: |          | 1049/? [14:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1049, loss 4.660097122192383\n",
      "Epoch 1: |          | 1050/? [14:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1050, loss 4.292953968048096\n",
      "Epoch 1: |          | 1051/? [14:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1051, loss 4.475944519042969\n",
      "Epoch 1: |          | 1052/? [14:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1052, loss 5.027146339416504\n",
      "Epoch 1: |          | 1053/? [14:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1053, loss 5.06724739074707\n",
      "Epoch 1: |          | 1054/? [14:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1054, loss 4.418097019195557\n",
      "Epoch 1: |          | 1055/? [14:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1055, loss 4.189496040344238\n",
      "Epoch 1: |          | 1056/? [14:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1056, loss 4.226075649261475\n",
      "Epoch 1: |          | 1057/? [14:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1057, loss 4.916861534118652\n",
      "Epoch 1: |          | 1058/? [14:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1058, loss 4.450558185577393\n",
      "Epoch 1: |          | 1059/? [14:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1059, loss 5.150053024291992\n",
      "Epoch 1: |          | 1060/? [14:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1060, loss 5.009596824645996\n",
      "Epoch 1: |          | 1061/? [14:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1061, loss 3.498678684234619\n",
      "Epoch 1: |          | 1062/? [14:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1062, loss 4.94209623336792\n",
      "Epoch 1: |          | 1063/? [14:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1063, loss 4.691817760467529\n",
      "Epoch 1: |          | 1064/? [14:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1064, loss 4.917442798614502\n",
      "Epoch 1: |          | 1065/? [14:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1065, loss 3.6330108642578125\n",
      "Epoch 1: |          | 1066/? [14:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1066, loss 4.8069329261779785\n",
      "Epoch 1: |          | 1067/? [14:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1067, loss 4.146313667297363\n",
      "Epoch 1: |          | 1068/? [14:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1068, loss 4.366273403167725\n",
      "Epoch 1: |          | 1069/? [14:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1069, loss 4.820772171020508\n",
      "Epoch 1: |          | 1070/? [14:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1070, loss 4.602029323577881\n",
      "Epoch 1: |          | 1071/? [14:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1071, loss 4.893799304962158\n",
      "Epoch 1: |          | 1072/? [14:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1072, loss 4.909242153167725\n",
      "Epoch 1: |          | 1073/? [14:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1073, loss 5.340114593505859\n",
      "Epoch 1: |          | 1074/? [14:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1074, loss 4.574366569519043\n",
      "Epoch 1: |          | 1075/? [14:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1075, loss 4.258113384246826\n",
      "Epoch 1: |          | 1076/? [14:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1076, loss 5.138033390045166\n",
      "Epoch 1: |          | 1077/? [14:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1077, loss 4.520554065704346\n",
      "Epoch 1: |          | 1078/? [14:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1078, loss 4.640873908996582\n",
      "Epoch 1: |          | 1079/? [14:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1079, loss 5.368152618408203\n",
      "Epoch 1: |          | 1080/? [14:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1080, loss 4.76467752456665\n",
      "Epoch 1: |          | 1081/? [14:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1081, loss 4.997729301452637\n",
      "Epoch 1: |          | 1082/? [14:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1082, loss 4.308521270751953\n",
      "Epoch 1: |          | 1083/? [14:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1083, loss 4.259839057922363\n",
      "Epoch 1: |          | 1084/? [14:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1084, loss 3.9580910205841064\n",
      "Epoch 1: |          | 1085/? [14:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1085, loss 4.491741180419922\n",
      "Epoch 1: |          | 1086/? [14:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1086, loss 4.773512840270996\n",
      "Epoch 1: |          | 1087/? [14:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1087, loss 5.3687214851379395\n",
      "Epoch 1: |          | 1088/? [14:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1088, loss 4.926278114318848\n",
      "Epoch 1: |          | 1089/? [14:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1089, loss 5.261750221252441\n",
      "Epoch 1: |          | 1090/? [14:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1090, loss 4.91844367980957\n",
      "Epoch 1: |          | 1091/? [14:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1091, loss 4.532048225402832\n",
      "Epoch 1: |          | 1092/? [14:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1092, loss 4.738363742828369\n",
      "Epoch 1: |          | 1093/? [14:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1093, loss 4.255947589874268\n",
      "Epoch 1: |          | 1094/? [14:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1094, loss 4.744959831237793\n",
      "Epoch 1: |          | 1095/? [14:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1095, loss 4.832857608795166\n",
      "Epoch 1: |          | 1096/? [14:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1096, loss 4.980880260467529\n",
      "Epoch 1: |          | 1097/? [14:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1097, loss 4.589409828186035\n",
      "Epoch 1: |          | 1098/? [14:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1098, loss 3.735381603240967\n",
      "Epoch 1: |          | 1099/? [14:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1099, loss 4.540430068969727\n",
      "Epoch 1: |          | 1100/? [14:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1100, loss 4.843686103820801\n",
      "Epoch 1: |          | 1101/? [15:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1101, loss 4.259819507598877\n",
      "Epoch 1: |          | 1102/? [15:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1102, loss 5.229602813720703\n",
      "Epoch 1: |          | 1103/? [15:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1103, loss 5.976221084594727\n",
      "Epoch 1: |          | 1104/? [15:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1104, loss 4.993644714355469\n",
      "Epoch 1: |          | 1105/? [15:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1105, loss 5.016446113586426\n",
      "Epoch 1: |          | 1106/? [15:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1106, loss 4.514103412628174\n",
      "Epoch 1: |          | 1107/? [15:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1107, loss 4.610896110534668\n",
      "Epoch 1: |          | 1108/? [15:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1108, loss 4.563309669494629\n",
      "Epoch 1: |          | 1109/? [15:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1109, loss 4.079837799072266\n",
      "Epoch 1: |          | 1110/? [15:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1110, loss 5.3609771728515625\n",
      "Epoch 1: |          | 1111/? [15:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1111, loss 4.910948276519775\n",
      "Epoch 1: |          | 1112/? [15:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1112, loss 4.810985088348389\n",
      "Epoch 1: |          | 1113/? [15:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1113, loss 4.488052845001221\n",
      "Epoch 1: |          | 1114/? [15:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1114, loss 3.9619572162628174\n",
      "Epoch 1: |          | 1115/? [15:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1115, loss 3.6120312213897705\n",
      "Epoch 1: |          | 1116/? [15:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1116, loss 4.202543258666992\n",
      "Epoch 1: |          | 1117/? [15:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1117, loss 4.499699592590332\n",
      "Epoch 1: |          | 1118/? [15:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1118, loss 4.3896660804748535\n",
      "Epoch 1: |          | 1119/? [15:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1119, loss 5.188941478729248\n",
      "Epoch 1: |          | 1120/? [15:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1120, loss 4.691723823547363\n",
      "Epoch 1: |          | 1121/? [15:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1121, loss 4.986250877380371\n",
      "Epoch 1: |          | 1122/? [15:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1122, loss 4.793356418609619\n",
      "Epoch 1: |          | 1123/? [15:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1123, loss 4.659161567687988\n",
      "Epoch 1: |          | 1124/? [15:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1124, loss 5.043883323669434\n",
      "Epoch 1: |          | 1125/? [15:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1125, loss 4.237099647521973\n",
      "Epoch 1: |          | 1126/? [15:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1126, loss 4.210667133331299\n",
      "Epoch 1: |          | 1127/? [15:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1127, loss 4.526697635650635\n",
      "Epoch 1: |          | 1128/? [15:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1128, loss 4.565279483795166\n",
      "Epoch 1: |          | 1129/? [15:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1129, loss 4.702149391174316\n",
      "Epoch 1: |          | 1130/? [15:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1130, loss 4.862643718719482\n",
      "Epoch 1: |          | 1131/? [15:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1131, loss 4.948910713195801\n",
      "Epoch 1: |          | 1132/? [15:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1132, loss 3.5447402000427246\n",
      "Epoch 1: |          | 1133/? [15:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1133, loss 4.719923496246338\n",
      "Epoch 1: |          | 1134/? [15:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1134, loss 4.292574882507324\n",
      "Epoch 1: |          | 1135/? [15:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1135, loss 4.985220909118652\n",
      "Epoch 1: |          | 1136/? [15:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1136, loss 4.635425090789795\n",
      "Epoch 1: |          | 1137/? [15:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1137, loss 4.676273822784424\n",
      "Epoch 1: |          | 1138/? [15:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1138, loss 5.225630760192871\n",
      "Epoch 1: |          | 1139/? [15:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1139, loss 5.672182559967041\n",
      "Epoch 1: |          | 1140/? [15:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1140, loss 4.98728084564209\n",
      "Epoch 1: |          | 1141/? [15:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1141, loss 5.059767246246338\n",
      "Epoch 1: |          | 1142/? [15:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1142, loss 5.087686061859131\n",
      "Epoch 1: |          | 1143/? [15:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1143, loss 5.1435160636901855\n",
      "Epoch 1: |          | 1144/? [15:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1144, loss 4.687949180603027\n",
      "Epoch 1: |          | 1145/? [15:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1145, loss 4.625900745391846\n",
      "Epoch 1: |          | 1146/? [15:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1146, loss 4.694712162017822\n",
      "Epoch 1: |          | 1147/? [15:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1147, loss 4.062386989593506\n",
      "Epoch 1: |          | 1148/? [15:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1148, loss 4.381363868713379\n",
      "Epoch 1: |          | 1149/? [15:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1149, loss 5.46706485748291\n",
      "Epoch 1: |          | 1150/? [15:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1150, loss 4.8103203773498535\n",
      "Epoch 1: |          | 1151/? [15:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1151, loss 5.239799499511719\n",
      "Epoch 1: |          | 1152/? [15:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1152, loss 4.365802764892578\n",
      "Epoch 1: |          | 1153/? [15:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1153, loss 4.729192733764648\n",
      "Epoch 1: |          | 1154/? [15:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1154, loss 4.338741302490234\n",
      "Epoch 1: |          | 1155/? [15:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1155, loss 4.655900001525879\n",
      "Epoch 1: |          | 1156/? [15:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1156, loss 4.864096641540527\n",
      "Epoch 1: |          | 1157/? [15:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1157, loss 4.8928632736206055\n",
      "Epoch 1: |          | 1158/? [15:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1158, loss 5.31685733795166\n",
      "Epoch 1: |          | 1159/? [15:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1159, loss 3.748192548751831\n",
      "Epoch 1: |          | 1160/? [15:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1160, loss 5.089362144470215\n",
      "Epoch 1: |          | 1161/? [15:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1161, loss 5.002285957336426\n",
      "Epoch 1: |          | 1162/? [15:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1162, loss 4.888762950897217\n",
      "Epoch 1: |          | 1163/? [15:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1163, loss 5.365658760070801\n",
      "Epoch 1: |          | 1164/? [15:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1164, loss 5.238615036010742\n",
      "Epoch 1: |          | 1165/? [15:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1165, loss 4.196561813354492\n",
      "Epoch 1: |          | 1166/? [15:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1166, loss 4.860476493835449\n",
      "Epoch 1: |          | 1167/? [15:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1167, loss 5.100972652435303\n",
      "Epoch 1: |          | 1168/? [15:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1168, loss 5.442872047424316\n",
      "Epoch 1: |          | 1169/? [15:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1169, loss 4.298012733459473\n",
      "Epoch 1: |          | 1170/? [15:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1170, loss 4.938608169555664\n",
      "Epoch 1: |          | 1171/? [15:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1171, loss 4.551671504974365\n",
      "Epoch 1: |          | 1172/? [15:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1172, loss 4.119787693023682\n",
      "Epoch 1: |          | 1173/? [15:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1173, loss 4.858454704284668\n",
      "Epoch 1: |          | 1174/? [15:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1174, loss 4.306516170501709\n",
      "Epoch 1: |          | 1175/? [15:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1175, loss 5.024916648864746\n",
      "Epoch 1: |          | 1176/? [16:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1176, loss 5.021523475646973\n",
      "Epoch 1: |          | 1177/? [16:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1177, loss 5.120570659637451\n",
      "Epoch 1: |          | 1178/? [16:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1178, loss 4.488531589508057\n",
      "Epoch 1: |          | 1179/? [16:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1179, loss 5.034641265869141\n",
      "Epoch 1: |          | 1180/? [16:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1180, loss 4.8024396896362305\n",
      "Epoch 1: |          | 1181/? [16:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1181, loss 4.876525402069092\n",
      "Epoch 1: |          | 1182/? [16:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1182, loss 4.690290927886963\n",
      "Epoch 1: |          | 1183/? [16:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1183, loss 4.3919854164123535\n",
      "Epoch 1: |          | 1184/? [16:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1184, loss 4.559820652008057\n",
      "Epoch 1: |          | 1185/? [16:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1185, loss 4.618666648864746\n",
      "Epoch 1: |          | 1186/? [16:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1186, loss 4.801442623138428\n",
      "Epoch 1: |          | 1187/? [16:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1187, loss 4.586150169372559\n",
      "Epoch 1: |          | 1188/? [16:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1188, loss 5.026348114013672\n",
      "Epoch 1: |          | 1189/? [16:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1189, loss 5.035162925720215\n",
      "Epoch 1: |          | 1190/? [16:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1190, loss 4.594863414764404\n",
      "Epoch 1: |          | 1191/? [16:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1191, loss 4.583405017852783\n",
      "Epoch 1: |          | 1192/? [16:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1192, loss 4.96010684967041\n",
      "Epoch 1: |          | 1193/? [16:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1193, loss 4.396010875701904\n",
      "Epoch 1: |          | 1194/? [16:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1194, loss 3.9104163646698\n",
      "Epoch 1: |          | 1195/? [16:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1195, loss 4.724998474121094\n",
      "Epoch 1: |          | 1196/? [16:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1196, loss 4.854422092437744\n",
      "Epoch 1: |          | 1197/? [16:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1197, loss 4.764728546142578\n",
      "Epoch 1: |          | 1198/? [16:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1198, loss 4.763933181762695\n",
      "Epoch 1: |          | 1199/? [16:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1199, loss 4.970584392547607\n",
      "Epoch 1: |          | 1200/? [16:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1200, loss 4.15936803817749\n",
      "Epoch 1: |          | 1201/? [16:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1201, loss 4.9045023918151855\n",
      "Epoch 1: |          | 1202/? [16:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1202, loss 4.434617519378662\n",
      "Epoch 1: |          | 1203/? [16:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1203, loss 4.520232200622559\n",
      "Epoch 1: |          | 1204/? [16:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1204, loss 3.9042983055114746\n",
      "Epoch 1: |          | 1205/? [16:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1205, loss 4.603298664093018\n",
      "Epoch 1: |          | 1206/? [16:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1206, loss 4.614296913146973\n",
      "Epoch 1: |          | 1207/? [16:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1207, loss 4.993772029876709\n",
      "Epoch 1: |          | 1208/? [16:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1208, loss 5.153606414794922\n",
      "Epoch 1: |          | 1209/? [16:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1209, loss 4.709877967834473\n",
      "Epoch 1: |          | 1210/? [16:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1210, loss 5.040513038635254\n",
      "Epoch 1: |          | 1211/? [16:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1211, loss 5.046084880828857\n",
      "Epoch 1: |          | 1212/? [16:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1212, loss 4.802239418029785\n",
      "Epoch 1: |          | 1213/? [16:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1213, loss 4.5186028480529785\n",
      "Epoch 1: |          | 1214/? [16:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1214, loss 5.300060272216797\n",
      "Epoch 1: |          | 1215/? [16:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1215, loss 4.4152936935424805\n",
      "Epoch 1: |          | 1216/? [16:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1216, loss 4.838309288024902\n",
      "Epoch 1: |          | 1217/? [16:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1217, loss 4.978884696960449\n",
      "Epoch 1: |          | 1218/? [16:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1218, loss 4.935603618621826\n",
      "Epoch 1: |          | 1219/? [16:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1219, loss 4.470698356628418\n",
      "Epoch 1: |          | 1220/? [16:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1220, loss 5.290793418884277\n",
      "Epoch 1: |          | 1221/? [16:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1221, loss 5.014914035797119\n",
      "Epoch 1: |          | 1222/? [16:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1222, loss 3.695519208908081\n",
      "Epoch 1: |          | 1223/? [16:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1223, loss 3.870182752609253\n",
      "Epoch 1: |          | 1224/? [16:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1224, loss 4.347507953643799\n",
      "Epoch 1: |          | 1225/? [16:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1225, loss 5.028120994567871\n",
      "Epoch 1: |          | 1226/? [16:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1226, loss 5.077008247375488\n",
      "Epoch 1: |          | 1227/? [16:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1227, loss 4.6173200607299805\n",
      "Epoch 1: |          | 1228/? [16:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1228, loss 4.678258419036865\n",
      "Epoch 1: |          | 1229/? [16:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1229, loss 4.121744155883789\n",
      "Epoch 1: |          | 1230/? [16:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1230, loss 4.82517147064209\n",
      "Epoch 1: |          | 1231/? [16:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1231, loss 4.827164649963379\n",
      "Epoch 1: |          | 1232/? [16:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1232, loss 5.042413234710693\n",
      "Epoch 1: |          | 1233/? [16:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1233, loss 4.829612731933594\n",
      "Epoch 1: |          | 1234/? [16:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1234, loss 3.6618144512176514\n",
      "Epoch 1: |          | 1235/? [16:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1235, loss 4.811005115509033\n",
      "Epoch 1: |          | 1236/? [16:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1236, loss 4.249442100524902\n",
      "Epoch 1: |          | 1237/? [16:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1237, loss 4.688141345977783\n",
      "Epoch 1: |          | 1238/? [16:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1238, loss 4.546011447906494\n",
      "Epoch 1: |          | 1239/? [16:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1239, loss 4.491513252258301\n",
      "Epoch 1: |          | 1240/? [16:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1240, loss 5.195549964904785\n",
      "Epoch 1: |          | 1241/? [16:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1241, loss 4.636624336242676\n",
      "Epoch 1: |          | 1242/? [16:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1242, loss 4.539515495300293\n",
      "Epoch 1: |          | 1243/? [16:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1243, loss 4.250432014465332\n",
      "Epoch 1: |          | 1244/? [16:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1244, loss 4.475394248962402\n",
      "Epoch 1: |          | 1245/? [16:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1245, loss 3.9813590049743652\n",
      "Epoch 1: |          | 1246/? [16:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1246, loss 4.872341632843018\n",
      "Epoch 1: |          | 1247/? [17:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1247, loss 4.87956428527832\n",
      "Epoch 1: |          | 1248/? [17:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1248, loss 4.285545825958252\n",
      "Epoch 1: |          | 1249/? [17:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1249, loss 4.478399753570557\n",
      "Epoch 1: |          | 1250/? [17:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1250, loss 4.6525654792785645\n",
      "Epoch 1: |          | 1251/? [17:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1251, loss 4.402718544006348\n",
      "Epoch 1: |          | 1252/? [17:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1252, loss 5.18234920501709\n",
      "Epoch 1: |          | 1253/? [17:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1253, loss 4.578665256500244\n",
      "Epoch 1: |          | 1254/? [17:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1254, loss 3.789783477783203\n",
      "Epoch 1: |          | 1255/? [17:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1255, loss 5.266757965087891\n",
      "Epoch 1: |          | 1256/? [17:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1256, loss 4.26816463470459\n",
      "Epoch 1: |          | 1257/? [17:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1257, loss 4.326350212097168\n",
      "Epoch 1: |          | 1258/? [17:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1258, loss 5.034482479095459\n",
      "Epoch 1: |          | 1259/? [17:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1259, loss 4.821619033813477\n",
      "Epoch 1: |          | 1260/? [17:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1260, loss 5.160796165466309\n",
      "Epoch 1: |          | 1261/? [17:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1261, loss 4.604083061218262\n",
      "Epoch 1: |          | 1262/? [17:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1262, loss 4.423768520355225\n",
      "Epoch 1: |          | 1263/? [17:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1263, loss 4.951354026794434\n",
      "Epoch 1: |          | 1264/? [17:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1264, loss 5.166754722595215\n",
      "Epoch 1: |          | 1265/? [17:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1265, loss 4.924859523773193\n",
      "Epoch 1: |          | 1266/? [17:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1266, loss 4.531449317932129\n",
      "Epoch 1: |          | 1267/? [17:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1267, loss 4.664412498474121\n",
      "Epoch 1: |          | 1268/? [17:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1268, loss 4.5149335861206055\n",
      "Epoch 1: |          | 1269/? [17:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1269, loss 4.015284061431885\n",
      "Epoch 1: |          | 1270/? [17:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1270, loss 4.419853210449219\n",
      "Epoch 1: |          | 1271/? [17:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1271, loss 4.6154279708862305\n",
      "Epoch 1: |          | 1272/? [17:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1272, loss 4.103504657745361\n",
      "Epoch 1: |          | 1273/? [17:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1273, loss 5.028317451477051\n",
      "Epoch 1: |          | 1274/? [17:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1274, loss 3.731210708618164\n",
      "Epoch 1: |          | 1275/? [17:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1275, loss 4.263400077819824\n",
      "Epoch 1: |          | 1276/? [17:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1276, loss 4.7318596839904785\n",
      "Epoch 1: |          | 1277/? [17:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1277, loss 4.288247108459473\n",
      "Epoch 1: |          | 1278/? [17:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1278, loss 4.592165946960449\n",
      "Epoch 1: |          | 1279/? [17:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1279, loss 4.85996150970459\n",
      "Epoch 1: |          | 1280/? [17:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1280, loss 3.931252956390381\n",
      "Epoch 1: |          | 1281/? [17:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1281, loss 4.366387367248535\n",
      "Epoch 1: |          | 1282/? [17:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1282, loss 4.0976128578186035\n",
      "Epoch 1: |          | 1283/? [17:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1283, loss 4.963481903076172\n",
      "Epoch 1: |          | 1284/? [17:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1284, loss 3.877487897872925\n",
      "Epoch 1: |          | 1285/? [17:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1285, loss 5.150141716003418\n",
      "Epoch 1: |          | 1286/? [17:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1286, loss 3.4993622303009033\n",
      "Epoch 1: |          | 1287/? [17:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1287, loss 4.946775436401367\n",
      "Epoch 1: |          | 1288/? [17:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1288, loss 4.773067951202393\n",
      "Epoch 1: |          | 1289/? [17:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1289, loss 3.6924023628234863\n",
      "Epoch 1: |          | 1290/? [17:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1290, loss 4.819460868835449\n",
      "Epoch 1: |          | 1291/? [17:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1291, loss 5.674140930175781\n",
      "Epoch 1: |          | 1292/? [17:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1292, loss 5.096244812011719\n",
      "Epoch 1: |          | 1293/? [17:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1293, loss 4.382817268371582\n",
      "Epoch 1: |          | 1294/? [17:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1294, loss 4.6311211585998535\n",
      "Epoch 1: |          | 1295/? [17:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1295, loss 4.65028190612793\n",
      "Epoch 1: |          | 1296/? [17:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1296, loss 3.801436185836792\n",
      "Epoch 1: |          | 1297/? [17:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1297, loss 4.941501617431641\n",
      "Epoch 1: |          | 1298/? [17:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1298, loss 4.661368370056152\n",
      "Epoch 1: |          | 1299/? [17:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1299, loss 3.588160753250122\n",
      "Epoch 1: |          | 1300/? [17:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1300, loss 4.722599983215332\n",
      "Epoch 1: |          | 1301/? [17:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1301, loss 4.4353108406066895\n",
      "Epoch 1: |          | 1302/? [17:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1302, loss 4.570920467376709\n",
      "Epoch 1: |          | 1303/? [17:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1303, loss 4.328039169311523\n",
      "Epoch 1: |          | 1304/? [17:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1304, loss 5.188963413238525\n",
      "Epoch 1: |          | 1305/? [17:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1305, loss 3.934896945953369\n",
      "Epoch 1: |          | 1306/? [17:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1306, loss 4.591205596923828\n",
      "Epoch 1: |          | 1307/? [17:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1307, loss 4.075393199920654\n",
      "Epoch 1: |          | 1308/? [17:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1308, loss 4.065859794616699\n",
      "Epoch 1: |          | 1309/? [17:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1309, loss 4.319849491119385\n",
      "Epoch 1: |          | 1310/? [17:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1310, loss 4.762663841247559\n",
      "Epoch 1: |          | 1311/? [17:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1311, loss 4.130993366241455\n",
      "Epoch 1: |          | 1312/? [17:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1312, loss 3.901651382446289\n",
      "Epoch 1: |          | 1313/? [17:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1313, loss 5.110468864440918\n",
      "Epoch 1: |          | 1314/? [17:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1314, loss 4.2487030029296875\n",
      "Epoch 1: |          | 1315/? [17:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1315, loss 5.086934566497803\n",
      "Epoch 1: |          | 1316/? [17:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1316, loss 4.7937726974487305\n",
      "Epoch 1: |          | 1317/? [17:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1317, loss 4.398700714111328\n",
      "Epoch 1: |          | 1318/? [17:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1318, loss 4.656998634338379\n",
      "Epoch 1: |          | 1319/? [17:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1319, loss 4.811797618865967\n",
      "Epoch 1: |          | 1320/? [18:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1320, loss 4.3880295753479\n",
      "Epoch 1: |          | 1321/? [18:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1321, loss 4.791104316711426\n",
      "Epoch 1: |          | 1322/? [18:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1322, loss 4.928624153137207\n",
      "Epoch 1: |          | 1323/? [18:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1323, loss 4.243340015411377\n",
      "Epoch 1: |          | 1324/? [18:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1324, loss 5.190051078796387\n",
      "Epoch 1: |          | 1325/? [18:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1325, loss 5.2702202796936035\n",
      "Epoch 1: |          | 1326/? [18:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1326, loss 4.701858997344971\n",
      "Epoch 1: |          | 1327/? [18:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1327, loss 4.55544900894165\n",
      "Epoch 1: |          | 1328/? [18:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1328, loss 4.218687534332275\n",
      "Epoch 1: |          | 1329/? [18:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1329, loss 4.986908435821533\n",
      "Epoch 1: |          | 1330/? [18:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1330, loss 4.652040481567383\n",
      "Epoch 1: |          | 1331/? [18:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1331, loss 4.698829174041748\n",
      "Epoch 1: |          | 1332/? [18:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1332, loss 4.539109706878662\n",
      "Epoch 1: |          | 1333/? [18:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1333, loss 4.377263069152832\n",
      "Epoch 1: |          | 1334/? [18:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1334, loss 4.504199028015137\n",
      "Epoch 1: |          | 1335/? [18:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1335, loss 4.5448126792907715\n",
      "Epoch 1: |          | 1336/? [18:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1336, loss 4.116096019744873\n",
      "Epoch 1: |          | 1337/? [18:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1337, loss 4.828385353088379\n",
      "Epoch 1: |          | 1338/? [18:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1338, loss 3.8560824394226074\n",
      "Epoch 1: |          | 1339/? [18:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1339, loss 4.534348964691162\n",
      "Epoch 1: |          | 1340/? [18:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1340, loss 3.899430513381958\n",
      "Epoch 1: |          | 1341/? [18:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1341, loss 4.812430381774902\n",
      "Epoch 1: |          | 1342/? [18:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1342, loss 5.0905280113220215\n",
      "Epoch 1: |          | 1343/? [18:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1343, loss 4.440804958343506\n",
      "Epoch 1: |          | 1344/? [18:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1344, loss 4.584749698638916\n",
      "Epoch 1: |          | 1345/? [18:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1345, loss 4.698254585266113\n",
      "Epoch 1: |          | 1346/? [18:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1346, loss 5.9373579025268555\n",
      "Epoch 1: |          | 1347/? [18:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1347, loss 5.049715995788574\n",
      "Epoch 1: |          | 1348/? [18:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1348, loss 5.3523149490356445\n",
      "Epoch 1: |          | 1349/? [18:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1349, loss 4.914278030395508\n",
      "Epoch 1: |          | 1350/? [18:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1350, loss 5.289366722106934\n",
      "Epoch 1: |          | 1351/? [18:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1351, loss 4.962845802307129\n",
      "Epoch 1: |          | 1352/? [18:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1352, loss 3.9642271995544434\n",
      "Epoch 1: |          | 1353/? [18:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1353, loss 4.263398170471191\n",
      "Epoch 1: |          | 1354/? [18:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1354, loss 4.957128524780273\n",
      "Epoch 1: |          | 1355/? [18:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1355, loss 5.080941200256348\n",
      "Epoch 1: |          | 1356/? [18:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1356, loss 4.7244462966918945\n",
      "Epoch 1: |          | 1357/? [18:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1357, loss 4.4127302169799805\n",
      "Epoch 1: |          | 1358/? [18:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1358, loss 4.635901927947998\n",
      "Epoch 1: |          | 1359/? [18:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1359, loss 4.487910270690918\n",
      "Epoch 1: |          | 1360/? [18:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1360, loss 4.751654624938965\n",
      "Epoch 1: |          | 1361/? [18:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1361, loss 4.652433395385742\n",
      "Epoch 1: |          | 1362/? [18:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1362, loss 4.542342185974121\n",
      "Epoch 1: |          | 1363/? [18:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1363, loss 3.893059492111206\n",
      "Epoch 1: |          | 1364/? [18:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1364, loss 4.4933671951293945\n",
      "Epoch 1: |          | 1365/? [18:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1365, loss 4.081523418426514\n",
      "Epoch 1: |          | 1366/? [18:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1366, loss 4.847404479980469\n",
      "Epoch 1: |          | 1367/? [18:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1367, loss 4.143980026245117\n",
      "Epoch 1: |          | 1368/? [18:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1368, loss 3.9158005714416504\n",
      "Epoch 1: |          | 1369/? [18:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1369, loss 4.623440742492676\n",
      "Epoch 1: |          | 1370/? [18:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1370, loss 4.078647613525391\n",
      "Epoch 1: |          | 1371/? [18:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1371, loss 5.153946876525879\n",
      "Epoch 1: |          | 1372/? [18:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1372, loss 4.472827911376953\n",
      "Epoch 1: |          | 1373/? [18:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1373, loss 5.071561336517334\n",
      "Epoch 1: |          | 1374/? [18:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1374, loss 4.016778469085693\n",
      "Epoch 1: |          | 1375/? [18:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1375, loss 4.685317039489746\n",
      "Epoch 1: |          | 1376/? [18:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1376, loss 4.652986526489258\n",
      "Epoch 1: |          | 1377/? [18:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1377, loss 4.536652565002441\n",
      "Epoch 1: |          | 1378/? [18:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1378, loss 4.860836982727051\n",
      "Epoch 1: |          | 1379/? [18:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1379, loss 4.6640520095825195\n",
      "Epoch 1: |          | 1380/? [18:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1380, loss 4.7510809898376465\n",
      "Epoch 1: |          | 1381/? [18:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1381, loss 4.88961124420166\n",
      "Epoch 1: |          | 1382/? [18:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1382, loss 4.3309454917907715\n",
      "Epoch 1: |          | 1383/? [18:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1383, loss 4.677197456359863\n",
      "Epoch 1: |          | 1384/? [18:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1384, loss 4.655168056488037\n",
      "Epoch 1: |          | 1385/? [18:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1385, loss 4.571128845214844\n",
      "Epoch 1: |          | 1386/? [18:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1386, loss 4.539820671081543\n",
      "Epoch 1: |          | 1387/? [18:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1387, loss 4.614383220672607\n",
      "Epoch 1: |          | 1388/? [18:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1388, loss 3.863720417022705\n",
      "Epoch 1: |          | 1389/? [18:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1389, loss 4.854788780212402\n",
      "Epoch 1: |          | 1390/? [18:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1390, loss 5.145691394805908\n",
      "Epoch 1: |          | 1391/? [18:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1391, loss 4.756103515625\n",
      "Epoch 1: |          | 1392/? [18:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1392, loss 4.183987617492676\n",
      "Epoch 1: |          | 1393/? [18:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1393, loss 4.508628845214844\n",
      "Epoch 1: |          | 1394/? [19:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1394, loss 4.2428507804870605\n",
      "Epoch 1: |          | 1395/? [19:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1395, loss 4.769692420959473\n",
      "Epoch 1: |          | 1396/? [19:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1396, loss 4.7161149978637695\n",
      "Epoch 1: |          | 1397/? [19:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1397, loss 3.7954020500183105\n",
      "Epoch 1: |          | 1398/? [19:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1398, loss 5.0404510498046875\n",
      "Epoch 1: |          | 1399/? [19:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1399, loss 5.138299465179443\n",
      "Epoch 1: |          | 1400/? [19:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1400, loss 4.087191581726074\n",
      "Epoch 1: |          | 1401/? [19:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1401, loss 5.125516891479492\n",
      "Epoch 1: |          | 1402/? [19:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1402, loss 4.56198787689209\n",
      "Epoch 1: |          | 1403/? [19:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1403, loss 4.771022796630859\n",
      "Epoch 1: |          | 1404/? [19:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1404, loss 4.71421480178833\n",
      "Epoch 1: |          | 1405/? [19:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1405, loss 5.131167888641357\n",
      "Epoch 1: |          | 1406/? [19:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1406, loss 4.984771251678467\n",
      "Epoch 1: |          | 1407/? [19:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1407, loss 5.175265312194824\n",
      "Epoch 1: |          | 1408/? [19:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1408, loss 4.343492031097412\n",
      "Epoch 1: |          | 1409/? [19:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1409, loss 4.471005439758301\n",
      "Epoch 1: |          | 1410/? [19:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1410, loss 4.553107261657715\n",
      "Epoch 1: |          | 1411/? [19:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1411, loss 4.94326639175415\n",
      "Epoch 1: |          | 1412/? [19:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1412, loss 4.309713840484619\n",
      "Epoch 1: |          | 1413/? [19:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1413, loss 4.142232894897461\n",
      "Epoch 1: |          | 1414/? [19:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1414, loss 4.239665985107422\n",
      "Epoch 1: |          | 1415/? [19:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1415, loss 4.672390460968018\n",
      "Epoch 1: |          | 1416/? [19:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1416, loss 5.034432411193848\n",
      "Epoch 1: |          | 1417/? [19:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1417, loss 4.555747032165527\n",
      "Epoch 1: |          | 1418/? [19:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1418, loss 4.860471248626709\n",
      "Epoch 1: |          | 1419/? [19:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1419, loss 4.50328254699707\n",
      "Epoch 1: |          | 1420/? [19:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1420, loss 4.460999488830566\n",
      "Epoch 1: |          | 1421/? [19:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1421, loss 3.956774950027466\n",
      "Epoch 1: |          | 1422/? [19:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1422, loss 4.956225872039795\n",
      "Epoch 1: |          | 1423/? [19:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1423, loss 4.968752384185791\n",
      "Epoch 1: |          | 1424/? [19:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1424, loss 4.477263927459717\n",
      "Epoch 1: |          | 1425/? [19:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1425, loss 4.762238502502441\n",
      "Epoch 1: |          | 1426/? [19:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1426, loss 4.275116443634033\n",
      "Epoch 1: |          | 1427/? [19:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1427, loss 4.9137797355651855\n",
      "Epoch 1: |          | 1428/? [19:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1428, loss 4.89640998840332\n",
      "Epoch 1: |          | 1429/? [19:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1429, loss 4.831287860870361\n",
      "Epoch 1: |          | 1430/? [19:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1430, loss 4.944325923919678\n",
      "Epoch 1: |          | 1431/? [19:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1431, loss 4.709133625030518\n",
      "Epoch 1: |          | 1432/? [19:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1432, loss 4.63970947265625\n",
      "Epoch 1: |          | 1433/? [19:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1433, loss 4.58275032043457\n",
      "Epoch 1: |          | 1434/? [19:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1434, loss 4.696793079376221\n",
      "Epoch 1: |          | 1435/? [19:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1435, loss 4.25237512588501\n",
      "Epoch 1: |          | 1436/? [19:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1436, loss 4.62573766708374\n",
      "Epoch 1: |          | 1437/? [19:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1437, loss 3.8789665699005127\n",
      "Epoch 1: |          | 1438/? [19:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1438, loss 5.716424942016602\n",
      "Epoch 1: |          | 1439/? [19:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1439, loss 4.812958717346191\n",
      "Epoch 1: |          | 1440/? [19:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1440, loss 4.803674697875977\n",
      "Epoch 1: |          | 1441/? [19:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1441, loss 5.138307571411133\n",
      "Epoch 1: |          | 1442/? [19:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1442, loss 5.174747467041016\n",
      "Epoch 1: |          | 1443/? [19:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1443, loss 4.185492515563965\n",
      "Epoch 1: |          | 1444/? [19:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1444, loss 4.339312553405762\n",
      "Epoch 1: |          | 1445/? [19:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1445, loss 4.96353006362915\n",
      "Epoch 1: |          | 1446/? [19:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1446, loss 4.482593536376953\n",
      "Epoch 1: |          | 1447/? [19:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1447, loss 4.578460693359375\n",
      "Epoch 1: |          | 1448/? [19:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1448, loss 4.405050754547119\n",
      "Epoch 1: |          | 1449/? [19:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1449, loss 4.77496337890625\n",
      "Epoch 1: |          | 1450/? [19:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1450, loss 4.848452091217041\n",
      "Epoch 1: |          | 1451/? [19:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1451, loss 5.180514335632324\n",
      "Epoch 1: |          | 1452/? [19:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1452, loss 4.675749778747559\n",
      "Epoch 1: |          | 1453/? [19:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1453, loss 3.8820576667785645\n",
      "Epoch 1: |          | 1454/? [19:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1454, loss 4.5345258712768555\n",
      "Epoch 1: |          | 1455/? [19:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1455, loss 4.735124588012695\n",
      "Epoch 1: |          | 1456/? [19:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1456, loss 4.213105201721191\n",
      "Epoch 1: |          | 1457/? [19:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1457, loss 4.410662651062012\n",
      "Epoch 1: |          | 1458/? [19:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1458, loss 4.492819309234619\n",
      "Epoch 1: |          | 1459/? [19:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1459, loss 4.850234508514404\n",
      "Epoch 1: |          | 1460/? [19:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1460, loss 4.642331123352051\n",
      "Epoch 1: |          | 1461/? [19:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1461, loss 4.746902942657471\n",
      "Epoch 1: |          | 1462/? [19:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1462, loss 5.043549537658691\n",
      "Epoch 1: |          | 1463/? [19:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1463, loss 4.944592475891113\n",
      "Epoch 1: |          | 1464/? [19:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1464, loss 4.226727485656738\n",
      "Epoch 1: |          | 1465/? [19:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1465, loss 4.5738630294799805\n",
      "Epoch 1: |          | 1466/? [19:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1466, loss 4.20028018951416\n",
      "Epoch 1: |          | 1467/? [20:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1467, loss 5.019084453582764\n",
      "Epoch 1: |          | 1468/? [20:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1468, loss 4.626886367797852\n",
      "Epoch 1: |          | 1469/? [20:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1469, loss 4.109539985656738\n",
      "Epoch 1: |          | 1470/? [20:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1470, loss 4.838654518127441\n",
      "Epoch 1: |          | 1471/? [20:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1471, loss 4.912224292755127\n",
      "Epoch 1: |          | 1472/? [20:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1472, loss 4.634295463562012\n",
      "Epoch 1: |          | 1473/? [20:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1473, loss 4.453936576843262\n",
      "Epoch 1: |          | 1474/? [20:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1474, loss 4.357859134674072\n",
      "Epoch 1: |          | 1475/? [20:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1475, loss 4.048466682434082\n",
      "Epoch 1: |          | 1476/? [20:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1476, loss 4.620518684387207\n",
      "Epoch 1: |          | 1477/? [20:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1477, loss 4.650982856750488\n",
      "Epoch 1: |          | 1478/? [20:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1478, loss 4.4951491355896\n",
      "Epoch 1: |          | 1479/? [20:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1479, loss 5.140574932098389\n",
      "Epoch 1: |          | 1480/? [20:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1480, loss 4.877450466156006\n",
      "Epoch 1: |          | 1481/? [20:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1481, loss 4.542134761810303\n",
      "Epoch 1: |          | 1482/? [20:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1482, loss 4.634364128112793\n",
      "Epoch 1: |          | 1483/? [20:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1483, loss 4.189452171325684\n",
      "Epoch 1: |          | 1484/? [20:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1484, loss 4.439813613891602\n",
      "Epoch 1: |          | 1485/? [20:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1485, loss 4.751321792602539\n",
      "Epoch 1: |          | 1486/? [20:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1486, loss 4.585818767547607\n",
      "Epoch 1: |          | 1487/? [20:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1487, loss 4.116855621337891\n",
      "Epoch 1: |          | 1488/? [20:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1488, loss 4.822295188903809\n",
      "Epoch 1: |          | 1489/? [20:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1489, loss 4.705320358276367\n",
      "Epoch 1: |          | 1490/? [20:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1490, loss 4.636751651763916\n",
      "Epoch 1: |          | 1491/? [20:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1491, loss 3.3835437297821045\n",
      "Epoch 1: |          | 1492/? [20:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1492, loss 4.087079048156738\n",
      "Epoch 1: |          | 1493/? [20:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1493, loss 4.091761589050293\n",
      "Epoch 1: |          | 1494/? [20:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1494, loss 4.5590996742248535\n",
      "Epoch 1: |          | 1495/? [20:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1495, loss 4.455328941345215\n",
      "Epoch 1: |          | 1496/? [20:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1496, loss 4.765795707702637\n",
      "Epoch 1: |          | 1497/? [20:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1497, loss 3.9345569610595703\n",
      "Epoch 1: |          | 1498/? [20:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1498, loss 4.312225818634033\n",
      "Epoch 1: |          | 1499/? [20:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1499, loss 4.957116603851318\n",
      "Epoch 1: |          | 1500/? [20:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1500, loss 4.8251848220825195\n",
      "Epoch 1: |          | 1501/? [20:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1501, loss 4.611560821533203\n",
      "Epoch 1: |          | 1502/? [20:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1502, loss 4.790198802947998\n",
      "Epoch 1: |          | 1503/? [20:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1503, loss 4.368927955627441\n",
      "Epoch 1: |          | 1504/? [20:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1504, loss 5.027500629425049\n",
      "Epoch 1: |          | 1505/? [20:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1505, loss 4.982234001159668\n",
      "Epoch 1: |          | 1506/? [20:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1506, loss 4.581221580505371\n",
      "Epoch 1: |          | 1507/? [20:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1507, loss 4.3771796226501465\n",
      "Epoch 1: |          | 1508/? [20:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1508, loss 4.547154426574707\n",
      "Epoch 1: |          | 1509/? [20:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1509, loss 4.483123302459717\n",
      "Epoch 1: |          | 1510/? [20:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1510, loss 4.798136234283447\n",
      "Epoch 1: |          | 1511/? [20:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1511, loss 4.272904396057129\n",
      "Epoch 1: |          | 1512/? [20:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1512, loss 5.165783882141113\n",
      "Epoch 1: |          | 1513/? [20:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1513, loss 5.113430976867676\n",
      "Epoch 1: |          | 1514/? [20:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1514, loss 4.170931339263916\n",
      "Epoch 1: |          | 1515/? [20:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1515, loss 5.227312088012695\n",
      "Epoch 1: |          | 1516/? [20:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1516, loss 4.900631904602051\n",
      "Epoch 1: |          | 1517/? [20:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1517, loss 4.407430648803711\n",
      "Epoch 1: |          | 1518/? [20:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1518, loss 4.209017753601074\n",
      "Epoch 1: |          | 1519/? [20:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1519, loss 4.785648822784424\n",
      "Epoch 1: |          | 1520/? [20:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1520, loss 5.175694465637207\n",
      "Epoch 1: |          | 1521/? [20:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1521, loss 4.553637981414795\n",
      "Epoch 1: |          | 1522/? [20:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1522, loss 4.254685401916504\n",
      "Epoch 1: |          | 1523/? [20:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1523, loss 4.64504861831665\n",
      "Epoch 1: |          | 1524/? [20:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1524, loss 4.5880632400512695\n",
      "Epoch 1: |          | 1525/? [20:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1525, loss 4.4045209884643555\n",
      "Epoch 1: |          | 1526/? [20:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1526, loss 4.837144374847412\n",
      "Epoch 1: |          | 1527/? [20:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1527, loss 4.835068702697754\n",
      "Epoch 1: |          | 1528/? [20:54<00:00,  1.22it/s, v_num=30]ERROR: Input has inproper shape\n",
      "Epoch 1: |          | 1529/? [20:54<00:00,  1.22it/s, v_num=30]   VALIDATION: Batch 0, loss 5.0797247886657715\n",
      "   VALIDATION: Batch 1, loss 3.9362525939941406\n",
      "   VALIDATION: Batch 2, loss 5.201282501220703\n",
      "   VALIDATION: Batch 3, loss 4.8812150955200195\n",
      "   VALIDATION: Batch 4, loss 4.481404781341553\n",
      "   VALIDATION: Batch 5, loss 4.047629356384277\n",
      "   VALIDATION: Batch 6, loss 4.348517417907715\n",
      "   VALIDATION: Batch 7, loss 5.006699562072754\n",
      "   VALIDATION: Batch 8, loss 4.97277307510376\n",
      "   VALIDATION: Batch 9, loss 4.976264953613281\n",
      "   VALIDATION: Batch 10, loss 4.760079383850098\n",
      "   VALIDATION: Batch 11, loss 4.314364910125732\n",
      "   VALIDATION: Batch 12, loss 4.731099605560303\n",
      "   VALIDATION: Batch 13, loss 5.086112976074219\n",
      "   VALIDATION: Batch 14, loss 4.646618843078613\n",
      "   VALIDATION: Batch 15, loss 4.4634504318237305\n",
      "   VALIDATION: Batch 16, loss 4.926567554473877\n",
      "   VALIDATION: Batch 17, loss 4.603703498840332\n",
      "   VALIDATION: Batch 18, loss 3.8589916229248047\n",
      "   VALIDATION: Batch 19, loss 4.909070014953613\n",
      "   VALIDATION: Batch 20, loss 5.085320949554443\n",
      "   VALIDATION: Batch 21, loss 5.2912163734436035\n",
      "   VALIDATION: Batch 22, loss 4.955117225646973\n",
      "   VALIDATION: Batch 23, loss 4.466128349304199\n",
      "   VALIDATION: Batch 24, loss 4.3799052238464355\n",
      "   VALIDATION: Batch 25, loss 4.795987129211426\n",
      "   VALIDATION: Batch 26, loss 4.980044841766357\n",
      "   VALIDATION: Batch 27, loss 4.885998725891113\n",
      "   VALIDATION: Batch 28, loss 4.567684650421143\n",
      "   VALIDATION: Batch 29, loss 4.90822696685791\n",
      "   VALIDATION: Batch 30, loss 4.393944263458252\n",
      "   VALIDATION: Batch 31, loss 4.730507850646973\n",
      "   VALIDATION: Batch 32, loss 5.327294826507568\n",
      "   VALIDATION: Batch 33, loss 3.446915864944458\n",
      "   VALIDATION: Batch 34, loss 4.672573089599609\n",
      "   VALIDATION: Batch 35, loss 4.921276569366455\n",
      "   VALIDATION: Batch 36, loss 4.455105304718018\n",
      "   VALIDATION: Batch 37, loss 4.231490135192871\n",
      "   VALIDATION: Batch 38, loss 4.298848628997803\n",
      "   VALIDATION: Batch 39, loss 4.710718631744385\n",
      "   VALIDATION: Batch 40, loss 4.851288318634033\n",
      "   VALIDATION: Batch 41, loss 3.7128663063049316\n",
      "   VALIDATION: Batch 42, loss 4.941866874694824\n",
      "   VALIDATION: Batch 43, loss 4.957634449005127\n",
      "   VALIDATION: Batch 44, loss 4.471072196960449\n",
      "   VALIDATION: Batch 45, loss 5.042029857635498\n",
      "   VALIDATION: Batch 46, loss 4.133145332336426\n",
      "   VALIDATION: Batch 47, loss 5.131301403045654\n",
      "   VALIDATION: Batch 48, loss 5.178711891174316\n",
      "   VALIDATION: Batch 49, loss 4.8614044189453125\n",
      "   VALIDATION: Batch 50, loss 4.786959171295166\n",
      "   VALIDATION: Batch 51, loss 5.28256368637085\n",
      "   VALIDATION: Batch 52, loss 4.33252477645874\n",
      "   VALIDATION: Batch 53, loss 4.287564277648926\n",
      "   VALIDATION: Batch 54, loss 4.353606224060059\n",
      "   VALIDATION: Batch 55, loss 5.279504299163818\n",
      "   VALIDATION: Batch 56, loss 4.611800193786621\n",
      "   VALIDATION: Batch 57, loss 5.9417290687561035\n",
      "   VALIDATION: Batch 58, loss 4.635207653045654\n",
      "   VALIDATION: Batch 59, loss 4.318575859069824\n",
      "   VALIDATION: Batch 60, loss 3.7921557426452637\n",
      "   VALIDATION: Batch 61, loss 4.677357196807861\n",
      "   VALIDATION: Batch 62, loss 4.699049472808838\n",
      "   VALIDATION: Batch 63, loss 5.158456325531006\n",
      "   VALIDATION: Batch 64, loss 4.992069721221924\n",
      "   VALIDATION: Batch 65, loss 4.102592468261719\n",
      "   VALIDATION: Batch 66, loss 5.023221492767334\n",
      "   VALIDATION: Batch 67, loss 4.425463676452637\n",
      "   VALIDATION: Batch 68, loss 4.608638763427734\n",
      "   VALIDATION: Batch 69, loss 4.960759162902832\n",
      "   VALIDATION: Batch 70, loss 5.104270935058594\n",
      "   VALIDATION: Batch 71, loss 4.524729251861572\n",
      "   VALIDATION: Batch 72, loss 5.4458441734313965\n",
      "   VALIDATION: Batch 73, loss 4.207170486450195\n",
      "   VALIDATION: Batch 74, loss 4.874454021453857\n",
      "   VALIDATION: Batch 75, loss 4.953530311584473\n",
      "   VALIDATION: Batch 76, loss 4.817123889923096\n",
      "   VALIDATION: Batch 77, loss 4.97479248046875\n",
      "   VALIDATION: Batch 78, loss 4.765999794006348\n",
      "   VALIDATION: Batch 79, loss 4.702949523925781\n",
      "   VALIDATION: Batch 80, loss 4.921059608459473\n",
      "   VALIDATION: Batch 81, loss 4.5991435050964355\n",
      "   VALIDATION: Batch 82, loss 4.930818557739258\n",
      "   VALIDATION: Batch 83, loss 4.279085159301758\n",
      "   VALIDATION: Batch 84, loss 4.953554630279541\n",
      "   VALIDATION: Batch 85, loss 4.7447919845581055\n",
      "   VALIDATION: Batch 86, loss 4.6543779373168945\n",
      "   VALIDATION: Batch 87, loss 4.533795356750488\n",
      "   VALIDATION: Batch 88, loss 4.082183837890625\n",
      "   VALIDATION: Batch 89, loss 4.399075508117676\n",
      "   VALIDATION: Batch 90, loss 4.690868377685547\n",
      "   VALIDATION: Batch 91, loss 4.949857234954834\n",
      "   VALIDATION: Batch 92, loss 4.769793510437012\n",
      "   VALIDATION: Batch 93, loss 5.1319427490234375\n",
      "   VALIDATION: Batch 94, loss 4.688860893249512\n",
      "   VALIDATION: Batch 95, loss 4.082615375518799\n",
      "   VALIDATION: Batch 96, loss 4.592151165008545\n",
      "   VALIDATION: Batch 97, loss 4.3853840827941895\n",
      "   VALIDATION: Batch 98, loss 4.875502586364746\n",
      "   VALIDATION: Batch 99, loss 4.9361677169799805\n",
      "   VALIDATION: Batch 100, loss 5.306302547454834\n",
      "   VALIDATION: Batch 101, loss 3.8943068981170654\n",
      "   VALIDATION: Batch 102, loss 5.351529598236084\n",
      "   VALIDATION: Batch 103, loss 5.2923150062561035\n",
      "   VALIDATION: Batch 104, loss 4.235604286193848\n",
      "   VALIDATION: Batch 105, loss 4.828251838684082\n",
      "   VALIDATION: Batch 106, loss 4.591491222381592\n",
      "   VALIDATION: Batch 107, loss 4.694472789764404\n",
      "   VALIDATION: Batch 108, loss 4.411705017089844\n",
      "   VALIDATION: Batch 109, loss 5.001530647277832\n",
      "   VALIDATION: Batch 110, loss 4.831382751464844\n",
      "   VALIDATION: Batch 111, loss 5.075150012969971\n",
      "   VALIDATION: Batch 112, loss 5.751837253570557\n",
      "   VALIDATION: Batch 113, loss 5.232447147369385\n",
      "   VALIDATION: Batch 114, loss 4.9383931159973145\n",
      "   VALIDATION: Batch 115, loss 4.452258110046387\n",
      "   VALIDATION: Batch 116, loss 4.24790096282959\n",
      "   VALIDATION: Batch 117, loss 4.958582878112793\n",
      "   VALIDATION: Batch 118, loss 5.1494059562683105\n",
      "   VALIDATION: Batch 119, loss 4.228914260864258\n",
      "   VALIDATION: Batch 120, loss 3.811483860015869\n",
      "   VALIDATION: Batch 121, loss 4.213704586029053\n",
      "   VALIDATION: Batch 122, loss 4.6399149894714355\n",
      "   VALIDATION: Batch 123, loss 4.752926349639893\n",
      "   VALIDATION: Batch 124, loss 3.9075779914855957\n",
      "   VALIDATION: Batch 125, loss 4.661699295043945\n",
      "   VALIDATION: Batch 126, loss 4.854531764984131\n",
      "   VALIDATION: Batch 127, loss 4.744453430175781\n",
      "   VALIDATION: Batch 128, loss 4.733767032623291\n",
      "   VALIDATION: Batch 129, loss 4.480608940124512\n",
      "   VALIDATION: Batch 130, loss 4.0156097412109375\n",
      "   VALIDATION: Batch 131, loss 3.9983608722686768\n",
      "   VALIDATION: Batch 132, loss 4.6367034912109375\n",
      "   VALIDATION: Batch 133, loss 4.882214546203613\n",
      "   VALIDATION: Batch 134, loss 4.842257976531982\n",
      "   VALIDATION: Batch 135, loss 5.061739921569824\n",
      "   VALIDATION: Batch 136, loss 5.1184282302856445\n",
      "   VALIDATION: Batch 137, loss 4.968475341796875\n",
      "   VALIDATION: Batch 138, loss 4.679938316345215\n",
      "   VALIDATION: Batch 139, loss 5.08743953704834\n",
      "   VALIDATION: Batch 140, loss 4.085426330566406\n",
      "   VALIDATION: Batch 141, loss 5.015664100646973\n",
      "   VALIDATION: Batch 142, loss 3.786518096923828\n",
      "   VALIDATION: Batch 143, loss 4.642586708068848\n",
      "   VALIDATION: Batch 144, loss 4.93464469909668\n",
      "   VALIDATION: Batch 145, loss 4.681146144866943\n",
      "   VALIDATION: Batch 146, loss 4.539131164550781\n",
      "   VALIDATION: Batch 147, loss 4.895421028137207\n",
      "   VALIDATION: Batch 148, loss 4.942643642425537\n",
      "   VALIDATION: Batch 149, loss 5.378247261047363\n",
      "   VALIDATION: Batch 150, loss 5.144753456115723\n",
      "   VALIDATION: Batch 151, loss 5.222447395324707\n",
      "   VALIDATION: Batch 152, loss 4.689603328704834\n",
      "   VALIDATION: Batch 153, loss 4.881237983703613\n",
      "   VALIDATION: Batch 154, loss 4.7471394538879395\n",
      "   VALIDATION: Batch 155, loss 4.414251327514648\n",
      "   VALIDATION: Batch 156, loss 5.152647018432617\n",
      "   VALIDATION: Batch 157, loss 4.943792343139648\n",
      "   VALIDATION: Batch 158, loss 4.125855922698975\n",
      "   VALIDATION: Batch 159, loss 4.637786865234375\n",
      "   VALIDATION: Batch 160, loss 5.110655784606934\n",
      "   VALIDATION: Batch 161, loss 5.299380302429199\n",
      "   VALIDATION: Batch 162, loss 4.762346267700195\n",
      "   VALIDATION: Batch 163, loss 4.1701507568359375\n",
      "   VALIDATION: Batch 164, loss 4.60777473449707\n",
      "   VALIDATION: Batch 165, loss 5.148749828338623\n",
      "   VALIDATION: Batch 166, loss 4.632496356964111\n",
      "   VALIDATION: Batch 167, loss 4.98393440246582\n",
      "   VALIDATION: Batch 168, loss 3.8379108905792236\n",
      "   VALIDATION: Batch 169, loss 4.4734787940979\n",
      "   VALIDATION: Batch 170, loss 4.744309902191162\n",
      "   VALIDATION: Batch 171, loss 4.822394371032715\n",
      "   VALIDATION: Batch 172, loss 4.720607280731201\n",
      "   VALIDATION: Batch 173, loss 4.719138145446777\n",
      "   VALIDATION: Batch 174, loss 5.052853584289551\n",
      "   VALIDATION: Batch 175, loss 4.760403633117676\n",
      "   VALIDATION: Batch 176, loss 4.528644561767578\n",
      "   VALIDATION: Batch 177, loss 4.943342685699463\n",
      "   VALIDATION: Batch 178, loss 5.537455081939697\n",
      "   VALIDATION: Batch 179, loss 4.861341953277588\n",
      "   VALIDATION: Batch 180, loss 4.450993537902832\n",
      "   VALIDATION: Batch 181, loss 4.60024356842041\n",
      "   VALIDATION: Batch 182, loss 4.806819438934326\n",
      "   VALIDATION: Batch 183, loss 3.8314414024353027\n",
      "   VALIDATION: Batch 184, loss 3.6200814247131348\n",
      "   VALIDATION: Batch 185, loss 4.356828212738037\n",
      "   VALIDATION: Batch 186, loss 4.394404888153076\n",
      "   VALIDATION: Batch 187, loss 4.60060977935791\n",
      "   VALIDATION: Batch 188, loss 4.946646213531494\n",
      "   VALIDATION: Batch 189, loss 4.243630886077881\n",
      "   VALIDATION: Batch 190, loss 4.265848159790039\n",
      "   VALIDATION: Batch 191, loss 4.835524559020996\n",
      "   VALIDATION: Batch 192, loss 5.3152899742126465\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=30]              TRRAINING: Batch 0, loss 4.760138511657715\n",
      "Epoch 2: |          | 1/? [00:01<00:00,  0.91it/s, v_num=30]   TRRAINING: Batch 1, loss 4.271914005279541\n",
      "Epoch 2: |          | 2/? [00:01<00:00,  1.03it/s, v_num=30]   TRRAINING: Batch 2, loss 4.401094913482666\n",
      "Epoch 2: |          | 3/? [00:02<00:00,  1.06it/s, v_num=30]   TRRAINING: Batch 3, loss 4.010287284851074\n",
      "Epoch 2: |          | 4/? [00:03<00:00,  1.10it/s, v_num=30]   TRRAINING: Batch 4, loss 4.455416679382324\n",
      "Epoch 2: |          | 5/? [00:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 5, loss 5.348731994628906\n",
      "Epoch 2: |          | 6/? [00:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 6, loss 5.0965399742126465\n",
      "Epoch 2: |          | 7/? [00:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 7, loss 4.36190128326416\n",
      "Epoch 2: |          | 8/? [00:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 8, loss 4.358336448669434\n",
      "Epoch 2: |          | 9/? [00:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 9, loss 4.625422954559326\n",
      "Epoch 2: |          | 10/? [00:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 10, loss 4.891002655029297\n",
      "Epoch 2: |          | 11/? [00:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 11, loss 4.8123779296875\n",
      "Epoch 2: |          | 12/? [00:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 12, loss 5.973304748535156\n",
      "Epoch 2: |          | 13/? [00:10<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 13, loss 4.647668838500977\n",
      "Epoch 2: |          | 14/? [00:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 14, loss 4.87639856338501\n",
      "Epoch 2: |          | 15/? [00:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 15, loss 4.16164493560791\n",
      "Epoch 2: |          | 16/? [00:13<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 16, loss 3.8290112018585205\n",
      "Epoch 2: |          | 17/? [00:14<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 17, loss 5.157059192657471\n",
      "Epoch 2: |          | 18/? [00:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 18, loss 4.596325397491455\n",
      "Epoch 2: |          | 19/? [00:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 19, loss 4.429712295532227\n",
      "Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 20, loss 4.731388092041016\n",
      "Epoch 2: |          | 21/? [00:17<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 21, loss 4.835721015930176\n",
      "Epoch 2: |          | 22/? [00:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 22, loss 4.668805122375488\n",
      "Epoch 2: |          | 23/? [00:19<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 23, loss 4.006957054138184\n",
      "Epoch 2: |          | 24/? [00:20<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 24, loss 4.6565327644348145\n",
      "Epoch 2: |          | 25/? [00:20<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 25, loss 4.592010974884033\n",
      "Epoch 2: |          | 26/? [00:21<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 26, loss 4.380573272705078\n",
      "Epoch 2: |          | 27/? [00:22<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 27, loss 4.217127799987793\n",
      "Epoch 2: |          | 28/? [00:23<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 28, loss 5.251534461975098\n",
      "Epoch 2: |          | 29/? [00:23<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 29, loss 4.6639509201049805\n",
      "Epoch 2: |          | 30/? [00:24<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 30, loss 4.524289131164551\n",
      "Epoch 2: |          | 31/? [00:25<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 31, loss 5.189974784851074\n",
      "Epoch 2: |          | 32/? [00:26<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 32, loss 4.6952009201049805\n",
      "Epoch 2: |          | 33/? [00:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 33, loss 4.519142150878906\n",
      "Epoch 2: |          | 34/? [00:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 34, loss 4.425726890563965\n",
      "Epoch 2: |          | 35/? [00:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 35, loss 3.7735276222229004\n",
      "Epoch 2: |          | 36/? [00:29<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 36, loss 4.836340427398682\n",
      "Epoch 2: |          | 37/? [00:30<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 37, loss 4.883374214172363\n",
      "Epoch 2: |          | 38/? [00:31<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 38, loss 5.195550441741943\n",
      "Epoch 2: |          | 39/? [00:32<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 39, loss 5.114691257476807\n",
      "Epoch 2: |          | 40/? [00:32<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 40, loss 4.529995918273926\n",
      "Epoch 2: |          | 41/? [00:33<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 41, loss 4.47349214553833\n",
      "Epoch 2: |          | 42/? [00:34<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 42, loss 4.3241376876831055\n",
      "Epoch 2: |          | 43/? [00:35<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 43, loss 4.464341163635254\n",
      "Epoch 2: |          | 44/? [00:36<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 44, loss 3.8229587078094482\n",
      "Epoch 2: |          | 45/? [00:37<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 45, loss 3.958280563354492\n",
      "Epoch 2: |          | 46/? [00:38<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 46, loss 5.260558128356934\n",
      "Epoch 2: |          | 47/? [00:38<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 47, loss 4.372873783111572\n",
      "Epoch 2: |          | 48/? [00:39<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 48, loss 4.043978691101074\n",
      "Epoch 2: |          | 49/? [00:40<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 49, loss 4.519140720367432\n",
      "Epoch 2: |          | 50/? [00:41<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 50, loss 4.456604957580566\n",
      "Epoch 2: |          | 51/? [00:42<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 51, loss 4.495410442352295\n",
      "Epoch 2: |          | 52/? [00:42<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 52, loss 5.035688877105713\n",
      "Epoch 2: |          | 53/? [00:43<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 53, loss 4.641687393188477\n",
      "Epoch 2: |          | 54/? [00:44<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 54, loss 4.543705940246582\n",
      "Epoch 2: |          | 55/? [00:45<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 55, loss 4.60188627243042\n",
      "Epoch 2: |          | 56/? [00:46<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 56, loss 4.713644027709961\n",
      "Epoch 2: |          | 57/? [00:47<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 57, loss 4.647727012634277\n",
      "Epoch 2: |          | 58/? [00:48<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 58, loss 5.922457695007324\n",
      "Epoch 2: |          | 59/? [00:48<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 59, loss 4.74676513671875\n",
      "Epoch 2: |          | 60/? [00:49<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 60, loss 4.888474464416504\n",
      "Epoch 2: |          | 61/? [00:50<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 61, loss 4.912184238433838\n",
      "Epoch 2: |          | 62/? [00:51<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 62, loss 4.460775852203369\n",
      "Epoch 2: |          | 63/? [00:52<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 63, loss 4.695897102355957\n",
      "Epoch 2: |          | 64/? [00:52<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 64, loss 4.504895210266113\n",
      "Epoch 2: |          | 65/? [00:53<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 65, loss 4.610350608825684\n",
      "Epoch 2: |          | 66/? [00:54<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 66, loss 3.915860414505005\n",
      "Epoch 2: |          | 67/? [00:55<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 67, loss 4.637240409851074\n",
      "Epoch 2: |          | 68/? [00:56<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 68, loss 4.841256141662598\n",
      "Epoch 2: |          | 69/? [00:56<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 69, loss 4.552216053009033\n",
      "Epoch 2: |          | 70/? [00:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 70, loss 4.230725288391113\n",
      "Epoch 2: |          | 71/? [00:58<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 71, loss 4.176419734954834\n",
      "Epoch 2: |          | 72/? [00:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 72, loss 4.729182243347168\n",
      "Epoch 2: |          | 73/? [00:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 73, loss 4.786756992340088\n",
      "Epoch 2: |          | 74/? [01:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 74, loss 4.266415119171143\n",
      "Epoch 2: |          | 75/? [01:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 75, loss 4.488409519195557\n",
      "Epoch 2: |          | 76/? [01:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 76, loss 4.490647315979004\n",
      "Epoch 2: |          | 77/? [01:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 77, loss 4.558487892150879\n",
      "Epoch 2: |          | 78/? [01:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 78, loss 4.203465461730957\n",
      "Epoch 2: |          | 79/? [01:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 79, loss 4.51431131362915\n",
      "Epoch 2: |          | 80/? [01:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 80, loss 4.357884883880615\n",
      "Epoch 2: |          | 81/? [01:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 81, loss 3.9256725311279297\n",
      "Epoch 2: |          | 82/? [01:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 82, loss 4.886719703674316\n",
      "Epoch 2: |          | 83/? [01:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 83, loss 4.087502479553223\n",
      "Epoch 2: |          | 84/? [01:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 84, loss 3.9388797283172607\n",
      "Epoch 2: |          | 85/? [01:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 85, loss 3.9526073932647705\n",
      "Epoch 2: |          | 86/? [01:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 86, loss 3.986227512359619\n",
      "Epoch 2: |          | 87/? [01:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 87, loss 4.1281890869140625\n",
      "Epoch 2: |          | 88/? [01:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 88, loss 5.091053009033203\n",
      "Epoch 2: |          | 89/? [01:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 89, loss 4.831839561462402\n",
      "Epoch 2: |          | 90/? [01:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 90, loss 4.697126865386963\n",
      "Epoch 2: |          | 91/? [01:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 91, loss 4.5272417068481445\n",
      "Epoch 2: |          | 92/? [01:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 92, loss 4.897572040557861\n",
      "Epoch 2: |          | 93/? [01:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 93, loss 5.008264541625977\n",
      "Epoch 2: |          | 94/? [01:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 94, loss 4.853209018707275\n",
      "Epoch 2: |          | 95/? [01:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 95, loss 5.4916486740112305\n",
      "Epoch 2: |          | 96/? [01:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 96, loss 4.286443710327148\n",
      "Epoch 2: |          | 97/? [01:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 97, loss 4.238487720489502\n",
      "Epoch 2: |          | 98/? [01:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 98, loss 4.685664176940918\n",
      "Epoch 2: |          | 99/? [01:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 99, loss 4.852644920349121\n",
      "Epoch 2: |          | 100/? [01:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 100, loss 4.9375715255737305\n",
      "Epoch 2: |          | 101/? [01:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 101, loss 4.534252166748047\n",
      "Epoch 2: |          | 102/? [01:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 102, loss 4.4701409339904785\n",
      "Epoch 2: |          | 103/? [01:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 103, loss 4.21810245513916\n",
      "Epoch 2: |          | 104/? [01:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 104, loss 4.670224189758301\n",
      "Epoch 2: |          | 105/? [01:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 105, loss 4.503179550170898\n",
      "Epoch 2: |          | 106/? [01:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 106, loss 4.5609235763549805\n",
      "Epoch 2: |          | 107/? [01:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 107, loss 4.755853176116943\n",
      "Epoch 2: |          | 108/? [01:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 108, loss 4.647404670715332\n",
      "Epoch 2: |          | 109/? [01:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 109, loss 4.20372200012207\n",
      "Epoch 2: |          | 110/? [01:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 110, loss 4.69169807434082\n",
      "Epoch 2: |          | 111/? [01:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 111, loss 5.238937854766846\n",
      "Epoch 2: |          | 112/? [01:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 112, loss 4.081655979156494\n",
      "Epoch 2: |          | 113/? [01:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 113, loss 3.562243938446045\n",
      "Epoch 2: |          | 114/? [01:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 114, loss 4.827223777770996\n",
      "Epoch 2: |          | 115/? [01:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 115, loss 4.986863136291504\n",
      "Epoch 2: |          | 116/? [01:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 116, loss 4.314082145690918\n",
      "Epoch 2: |          | 117/? [01:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 117, loss 4.242405891418457\n",
      "Epoch 2: |          | 118/? [01:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 118, loss 4.8248467445373535\n",
      "Epoch 2: |          | 119/? [01:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 119, loss 5.031147003173828\n",
      "Epoch 2: |          | 120/? [01:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 120, loss 4.803281784057617\n",
      "Epoch 2: |          | 121/? [01:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 121, loss 4.520986557006836\n",
      "Epoch 2: |          | 122/? [01:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 122, loss 3.9691321849823\n",
      "Epoch 2: |          | 123/? [01:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 123, loss 4.458005428314209\n",
      "Epoch 2: |          | 124/? [01:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 124, loss 4.669143199920654\n",
      "Epoch 2: |          | 125/? [01:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 125, loss 4.443602085113525\n",
      "Epoch 2: |          | 126/? [01:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 126, loss 4.898151874542236\n",
      "Epoch 2: |          | 127/? [01:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 127, loss 4.874871730804443\n",
      "Epoch 2: |          | 128/? [01:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 128, loss 3.8931522369384766\n",
      "Epoch 2: |          | 129/? [01:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 129, loss 4.657552242279053\n",
      "Epoch 2: |          | 130/? [01:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 130, loss 3.5766806602478027\n",
      "Epoch 2: |          | 131/? [01:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 131, loss 4.532569408416748\n",
      "Epoch 2: |          | 132/? [01:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 132, loss 4.51814603805542\n",
      "Epoch 2: |          | 133/? [01:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 133, loss 4.580449104309082\n",
      "Epoch 2: |          | 134/? [01:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 134, loss 4.655587196350098\n",
      "Epoch 2: |          | 135/? [01:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 135, loss 4.865565776824951\n",
      "Epoch 2: |          | 136/? [01:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 136, loss 4.7600297927856445\n",
      "Epoch 2: |          | 137/? [01:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 137, loss 3.6395645141601562\n",
      "Epoch 2: |          | 138/? [01:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 138, loss 4.424689292907715\n",
      "Epoch 2: |          | 139/? [01:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 139, loss 4.920328140258789\n",
      "Epoch 2: |          | 140/? [01:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 140, loss 3.9331390857696533\n",
      "Epoch 2: |          | 141/? [01:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 141, loss 4.1594343185424805\n",
      "Epoch 2: |          | 142/? [01:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 142, loss 5.804769992828369\n",
      "Epoch 2: |          | 143/? [01:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 143, loss 5.397927284240723\n",
      "Epoch 2: |          | 144/? [01:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 144, loss 4.488281726837158\n",
      "Epoch 2: |          | 145/? [01:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 145, loss 4.060593605041504\n",
      "Epoch 2: |          | 146/? [01:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 146, loss 4.2410101890563965\n",
      "Epoch 2: |          | 147/? [01:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 147, loss 4.530838966369629\n",
      "Epoch 2: |          | 148/? [02:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 148, loss 4.237622261047363\n",
      "Epoch 2: |          | 149/? [02:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 149, loss 3.7419826984405518\n",
      "Epoch 2: |          | 150/? [02:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 150, loss 4.650323390960693\n",
      "Epoch 2: |          | 151/? [02:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 151, loss 4.715526103973389\n",
      "Epoch 2: |          | 152/? [02:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 152, loss 4.801224708557129\n",
      "Epoch 2: |          | 153/? [02:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 153, loss 3.8194260597229004\n",
      "Epoch 2: |          | 154/? [02:04<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 154, loss 5.044360160827637\n",
      "Epoch 2: |          | 155/? [02:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 155, loss 4.61631965637207\n",
      "Epoch 2: |          | 156/? [02:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 156, loss 3.9094672203063965\n",
      "Epoch 2: |          | 157/? [02:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 157, loss 4.658281326293945\n",
      "Epoch 2: |          | 158/? [02:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 158, loss 4.714955806732178\n",
      "Epoch 2: |          | 159/? [02:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 159, loss 4.427743911743164\n",
      "Epoch 2: |          | 160/? [02:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 160, loss 4.136941909790039\n",
      "Epoch 2: |          | 161/? [02:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 161, loss 4.64644718170166\n",
      "Epoch 2: |          | 162/? [02:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 162, loss 4.730704307556152\n",
      "Epoch 2: |          | 163/? [02:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 163, loss 3.67558217048645\n",
      "Epoch 2: |          | 164/? [02:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 164, loss 4.128939628601074\n",
      "Epoch 2: |          | 165/? [02:13<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 165, loss 5.072976589202881\n",
      "Epoch 2: |          | 166/? [02:14<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 166, loss 4.734257221221924\n",
      "Epoch 2: |          | 167/? [02:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 167, loss 4.8399481773376465\n",
      "Epoch 2: |          | 168/? [02:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 168, loss 4.365936756134033\n",
      "Epoch 2: |          | 169/? [02:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 169, loss 3.9907538890838623\n",
      "Epoch 2: |          | 170/? [02:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 170, loss 4.267216205596924\n",
      "Epoch 2: |          | 171/? [02:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 171, loss 4.689028739929199\n",
      "Epoch 2: |          | 172/? [02:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 172, loss 4.347460746765137\n",
      "Epoch 2: |          | 173/? [02:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 173, loss 5.237730979919434\n",
      "Epoch 2: |          | 174/? [02:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 174, loss 5.182830333709717\n",
      "Epoch 2: |          | 175/? [02:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 175, loss 5.263216018676758\n",
      "Epoch 2: |          | 176/? [02:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 176, loss 4.330622673034668\n",
      "Epoch 2: |          | 177/? [02:23<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 177, loss 4.33417272567749\n",
      "Epoch 2: |          | 178/? [02:24<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 178, loss 4.205082893371582\n",
      "Epoch 2: |          | 179/? [02:24<00:00,  1.24it/s, v_num=30]   TRRAINING: Batch 179, loss 4.93587064743042\n",
      "Epoch 2: |          | 180/? [02:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 180, loss 4.401132106781006\n",
      "Epoch 2: |          | 181/? [02:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 181, loss 4.186363220214844\n",
      "Epoch 2: |          | 182/? [02:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 182, loss 4.673793792724609\n",
      "Epoch 2: |          | 183/? [02:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 183, loss 4.095768928527832\n",
      "Epoch 2: |          | 184/? [02:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 184, loss 4.250579357147217\n",
      "Epoch 2: |          | 185/? [02:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 185, loss 4.974881172180176\n",
      "Epoch 2: |          | 186/? [02:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 186, loss 4.364701271057129\n",
      "Epoch 2: |          | 187/? [02:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 187, loss 4.921913146972656\n",
      "Epoch 2: |          | 188/? [02:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 188, loss 4.317060470581055\n",
      "Epoch 2: |          | 189/? [02:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 189, loss 4.971662998199463\n",
      "Epoch 2: |          | 190/? [02:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 190, loss 4.412484645843506\n",
      "Epoch 2: |          | 191/? [02:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 191, loss 5.286177158355713\n",
      "Epoch 2: |          | 192/? [02:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 192, loss 5.115265369415283\n",
      "Epoch 2: |          | 193/? [02:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 193, loss 4.25040864944458\n",
      "Epoch 2: |          | 194/? [02:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 194, loss 4.2023797035217285\n",
      "Epoch 2: |          | 195/? [02:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 195, loss 4.886111259460449\n",
      "Epoch 2: |          | 196/? [02:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 196, loss 4.854043006896973\n",
      "Epoch 2: |          | 197/? [02:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 197, loss 4.620461940765381\n",
      "Epoch 2: |          | 198/? [02:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 198, loss 4.0154008865356445\n",
      "Epoch 2: |          | 199/? [02:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 199, loss 4.807424068450928\n",
      "Epoch 2: |          | 200/? [02:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 200, loss 4.439289093017578\n",
      "Epoch 2: |          | 201/? [02:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 201, loss 4.76971435546875\n",
      "Epoch 2: |          | 202/? [02:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 202, loss 4.741186618804932\n",
      "Epoch 2: |          | 203/? [02:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 203, loss 4.520558834075928\n",
      "Epoch 2: |          | 204/? [02:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 204, loss 4.561129570007324\n",
      "Epoch 2: |          | 205/? [02:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 205, loss 4.2749786376953125\n",
      "Epoch 2: |          | 206/? [02:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 206, loss 4.18891716003418\n",
      "Epoch 2: |          | 207/? [02:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 207, loss 4.6745381355285645\n",
      "Epoch 2: |          | 208/? [02:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 208, loss 4.665066719055176\n",
      "Epoch 2: |          | 209/? [02:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 209, loss 4.370115756988525\n",
      "Epoch 2: |          | 210/? [02:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 210, loss 5.121580123901367\n",
      "Epoch 2: |          | 211/? [02:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 211, loss 4.414605140686035\n",
      "Epoch 2: |          | 212/? [02:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 212, loss 4.622854709625244\n",
      "Epoch 2: |          | 213/? [02:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 213, loss 4.430209159851074\n",
      "Epoch 2: |          | 214/? [02:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 214, loss 4.358290195465088\n",
      "Epoch 2: |          | 215/? [02:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 215, loss 4.024899482727051\n",
      "Epoch 2: |          | 216/? [02:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 216, loss 4.774760723114014\n",
      "Epoch 2: |          | 217/? [02:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 217, loss 4.607054233551025\n",
      "Epoch 2: |          | 218/? [02:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 218, loss 4.689048767089844\n",
      "Epoch 2: |          | 219/? [02:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 219, loss 4.551026344299316\n",
      "Epoch 2: |          | 220/? [02:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 220, loss 4.695590496063232\n",
      "Epoch 2: |          | 221/? [02:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 221, loss 4.408060550689697\n",
      "Epoch 2: |          | 222/? [03:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 222, loss 3.6200454235076904\n",
      "Epoch 2: |          | 223/? [03:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 223, loss 5.052246570587158\n",
      "Epoch 2: |          | 224/? [03:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 224, loss 4.895744323730469\n",
      "Epoch 2: |          | 225/? [03:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 225, loss 4.640399932861328\n",
      "Epoch 2: |          | 226/? [03:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 226, loss 4.39577579498291\n",
      "Epoch 2: |          | 227/? [03:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 227, loss 4.81016206741333\n",
      "Epoch 2: |          | 228/? [03:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 228, loss 4.455559730529785\n",
      "Epoch 2: |          | 229/? [03:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 229, loss 4.666607856750488\n",
      "Epoch 2: |          | 230/? [03:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 230, loss 4.497917652130127\n",
      "Epoch 2: |          | 231/? [03:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 231, loss 4.427418231964111\n",
      "Epoch 2: |          | 232/? [03:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 232, loss 4.211977005004883\n",
      "Epoch 2: |          | 233/? [03:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 233, loss 4.978363513946533\n",
      "Epoch 2: |          | 234/? [03:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 234, loss 4.995537757873535\n",
      "Epoch 2: |          | 235/? [03:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 235, loss 5.0507330894470215\n",
      "Epoch 2: |          | 236/? [03:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 236, loss 4.439165115356445\n",
      "Epoch 2: |          | 237/? [03:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 237, loss 4.62630558013916\n",
      "Epoch 2: |          | 238/? [03:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 238, loss 4.831177711486816\n",
      "Epoch 2: |          | 239/? [03:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 239, loss 4.437307834625244\n",
      "Epoch 2: |          | 240/? [03:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 240, loss 3.835427761077881\n",
      "Epoch 2: |          | 241/? [03:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 241, loss 4.600482940673828\n",
      "Epoch 2: |          | 242/? [03:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 242, loss 4.909316539764404\n",
      "Epoch 2: |          | 243/? [03:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 243, loss 3.646761417388916\n",
      "Epoch 2: |          | 244/? [03:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 244, loss 4.255731105804443\n",
      "Epoch 2: |          | 245/? [03:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 245, loss 4.495575904846191\n",
      "Epoch 2: |          | 246/? [03:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 246, loss 4.728767395019531\n",
      "Epoch 2: |          | 247/? [03:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 247, loss 4.7634687423706055\n",
      "Epoch 2: |          | 248/? [03:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 248, loss 4.194686412811279\n",
      "Epoch 2: |          | 249/? [03:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 249, loss 3.951749801635742\n",
      "Epoch 2: |          | 250/? [03:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 250, loss 4.587643623352051\n",
      "Epoch 2: |          | 251/? [03:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 251, loss 4.640061378479004\n",
      "Epoch 2: |          | 252/? [03:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 252, loss 4.389632701873779\n",
      "Epoch 2: |          | 253/? [03:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 253, loss 5.208773612976074\n",
      "Epoch 2: |          | 254/? [03:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 254, loss 4.964699745178223\n",
      "Epoch 2: |          | 255/? [03:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 255, loss 4.465659141540527\n",
      "Epoch 2: |          | 256/? [03:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 256, loss 6.215980052947998\n",
      "Epoch 2: |          | 257/? [03:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 257, loss 4.3406147956848145\n",
      "Epoch 2: |          | 258/? [03:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 258, loss 4.424077033996582\n",
      "Epoch 2: |          | 259/? [03:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 259, loss 4.264260292053223\n",
      "Epoch 2: |          | 260/? [03:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 260, loss 4.237942695617676\n",
      "Epoch 2: |          | 261/? [03:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 261, loss 4.31070613861084\n",
      "Epoch 2: |          | 262/? [03:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 262, loss 4.792584419250488\n",
      "Epoch 2: |          | 263/? [03:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 263, loss 4.503348350524902\n",
      "Epoch 2: |          | 264/? [03:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 264, loss 4.565436840057373\n",
      "Epoch 2: |          | 265/? [03:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 265, loss 3.9780001640319824\n",
      "Epoch 2: |          | 266/? [03:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 266, loss 4.426403999328613\n",
      "Epoch 2: |          | 267/? [03:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 267, loss 4.077788829803467\n",
      "Epoch 2: |          | 268/? [03:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 268, loss 4.424227714538574\n",
      "Epoch 2: |          | 269/? [03:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 269, loss 4.8029584884643555\n",
      "Epoch 2: |          | 270/? [03:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 270, loss 4.393006324768066\n",
      "Epoch 2: |          | 271/? [03:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 271, loss 4.959635257720947\n",
      "Epoch 2: |          | 272/? [03:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 272, loss 4.9517903327941895\n",
      "Epoch 2: |          | 273/? [03:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 273, loss 4.088226318359375\n",
      "Epoch 2: |          | 274/? [03:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 274, loss 5.1898274421691895\n",
      "Epoch 2: |          | 275/? [03:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 275, loss 4.652399063110352\n",
      "Epoch 2: |          | 276/? [03:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 276, loss 3.898918867111206\n",
      "Epoch 2: |          | 277/? [03:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 277, loss 4.548549175262451\n",
      "Epoch 2: |          | 278/? [03:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 278, loss 3.557654857635498\n",
      "Epoch 2: |          | 279/? [03:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 279, loss 4.3857855796813965\n",
      "Epoch 2: |          | 280/? [03:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 280, loss 3.948702335357666\n",
      "Epoch 2: |          | 281/? [03:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 281, loss 4.782548904418945\n",
      "Epoch 2: |          | 282/? [03:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 282, loss 4.371591091156006\n",
      "Epoch 2: |          | 283/? [03:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 283, loss 4.40632963180542\n",
      "Epoch 2: |          | 284/? [03:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 284, loss 4.290068626403809\n",
      "Epoch 2: |          | 285/? [03:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 285, loss 3.7125611305236816\n",
      "Epoch 2: |          | 286/? [03:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 286, loss 4.368340969085693\n",
      "Epoch 2: |          | 287/? [03:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 287, loss 4.181271553039551\n",
      "Epoch 2: |          | 288/? [03:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 288, loss 4.176549434661865\n",
      "Epoch 2: |          | 289/? [03:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 289, loss 4.111645698547363\n",
      "Epoch 2: |          | 290/? [03:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 290, loss 3.5623717308044434\n",
      "Epoch 2: |          | 291/? [03:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 291, loss 4.572251319885254\n",
      "Epoch 2: |          | 292/? [03:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 292, loss 4.188313961029053\n",
      "Epoch 2: |          | 293/? [03:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 293, loss 4.556075096130371\n",
      "Epoch 2: |          | 294/? [03:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 294, loss 4.362885475158691\n",
      "Epoch 2: |          | 295/? [03:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 295, loss 4.7622480392456055\n",
      "Epoch 2: |          | 296/? [04:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 296, loss 4.158090591430664\n",
      "Epoch 2: |          | 297/? [04:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 297, loss 4.9013895988464355\n",
      "Epoch 2: |          | 298/? [04:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 298, loss 4.656980514526367\n",
      "Epoch 2: |          | 299/? [04:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 299, loss 5.407241344451904\n",
      "Epoch 2: |          | 300/? [04:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 300, loss 4.605298042297363\n",
      "Epoch 2: |          | 301/? [04:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 301, loss 4.283200740814209\n",
      "Epoch 2: |          | 302/? [04:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 302, loss 4.764626979827881\n",
      "Epoch 2: |          | 303/? [04:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 303, loss 4.487150192260742\n",
      "Epoch 2: |          | 304/? [04:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 304, loss 4.754307746887207\n",
      "Epoch 2: |          | 305/? [04:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 305, loss 4.815555572509766\n",
      "Epoch 2: |          | 306/? [04:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 306, loss 4.461570739746094\n",
      "Epoch 2: |          | 307/? [04:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 307, loss 4.732754230499268\n",
      "Epoch 2: |          | 308/? [04:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 308, loss 4.833549499511719\n",
      "Epoch 2: |          | 309/? [04:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 309, loss 4.535525321960449\n",
      "Epoch 2: |          | 310/? [04:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 310, loss 4.950203895568848\n",
      "Epoch 2: |          | 311/? [04:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 311, loss 4.462357521057129\n",
      "Epoch 2: |          | 312/? [04:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 312, loss 4.625957012176514\n",
      "Epoch 2: |          | 313/? [04:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 313, loss 4.165919780731201\n",
      "Epoch 2: |          | 314/? [04:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 314, loss 4.604881763458252\n",
      "Epoch 2: |          | 315/? [04:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 315, loss 4.186646461486816\n",
      "Epoch 2: |          | 316/? [04:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 316, loss 4.930795192718506\n",
      "Epoch 2: |          | 317/? [04:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 317, loss 4.629573822021484\n",
      "Epoch 2: |          | 318/? [04:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 318, loss 4.815855026245117\n",
      "Epoch 2: |          | 319/? [04:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 319, loss 3.952096939086914\n",
      "Epoch 2: |          | 320/? [04:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 320, loss 4.435787677764893\n",
      "Epoch 2: |          | 321/? [04:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 321, loss 4.191056728363037\n",
      "Epoch 2: |          | 322/? [04:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 322, loss 4.948119163513184\n",
      "Epoch 2: |          | 323/? [04:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 323, loss 4.97885274887085\n",
      "Epoch 2: |          | 324/? [04:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 324, loss 4.508734703063965\n",
      "Epoch 2: |          | 325/? [04:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 325, loss 5.062620162963867\n",
      "Epoch 2: |          | 326/? [04:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 326, loss 4.609364986419678\n",
      "Epoch 2: |          | 327/? [04:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 327, loss 4.3855061531066895\n",
      "Epoch 2: |          | 328/? [04:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 328, loss 4.078038215637207\n",
      "Epoch 2: |          | 329/? [04:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 329, loss 4.726165294647217\n",
      "Epoch 2: |          | 330/? [04:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 330, loss 5.173545837402344\n",
      "Epoch 2: |          | 331/? [04:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 331, loss 3.3265411853790283\n",
      "Epoch 2: |          | 332/? [04:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 332, loss 4.581508636474609\n",
      "Epoch 2: |          | 333/? [04:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 333, loss 4.3453755378723145\n",
      "Epoch 2: |          | 334/? [04:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 334, loss 5.3500237464904785\n",
      "Epoch 2: |          | 335/? [04:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 335, loss 5.119296073913574\n",
      "Epoch 2: |          | 336/? [04:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 336, loss 4.9583845138549805\n",
      "Epoch 2: |          | 337/? [04:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 337, loss 5.665812969207764\n",
      "Epoch 2: |          | 338/? [04:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 338, loss 5.206238746643066\n",
      "Epoch 2: |          | 339/? [04:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 339, loss 4.22479248046875\n",
      "Epoch 2: |          | 340/? [04:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 340, loss 4.507321357727051\n",
      "Epoch 2: |          | 341/? [04:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 341, loss 3.878594160079956\n",
      "Epoch 2: |          | 342/? [04:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 342, loss 4.640991687774658\n",
      "Epoch 2: |          | 343/? [04:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 343, loss 4.328685760498047\n",
      "Epoch 2: |          | 344/? [04:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 344, loss 5.1551690101623535\n",
      "Epoch 2: |          | 345/? [04:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 345, loss 4.4219489097595215\n",
      "Epoch 2: |          | 346/? [04:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 346, loss 4.65585470199585\n",
      "Epoch 2: |          | 347/? [04:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 347, loss 4.299497127532959\n",
      "Epoch 2: |          | 348/? [04:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 348, loss 3.6182894706726074\n",
      "Epoch 2: |          | 349/? [04:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 349, loss 3.447638750076294\n",
      "Epoch 2: |          | 350/? [04:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 350, loss 4.981598854064941\n",
      "Epoch 2: |          | 351/? [04:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 351, loss 4.937216758728027\n",
      "Epoch 2: |          | 352/? [04:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 352, loss 4.150646209716797\n",
      "Epoch 2: |          | 353/? [04:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 353, loss 3.8776230812072754\n",
      "Epoch 2: |          | 354/? [04:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 354, loss 4.286032676696777\n",
      "Epoch 2: |          | 355/? [04:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 355, loss 4.634095668792725\n",
      "Epoch 2: |          | 356/? [04:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 356, loss 4.622751712799072\n",
      "Epoch 2: |          | 357/? [04:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 357, loss 4.128531455993652\n",
      "Epoch 2: |          | 358/? [04:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 358, loss 4.057478904724121\n",
      "Epoch 2: |          | 359/? [04:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 359, loss 4.790379047393799\n",
      "Epoch 2: |          | 360/? [04:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 360, loss 4.259182929992676\n",
      "Epoch 2: |          | 361/? [04:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 361, loss 4.391777992248535\n",
      "Epoch 2: |          | 362/? [04:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 362, loss 4.219573020935059\n",
      "Epoch 2: |          | 363/? [04:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 363, loss 4.103982448577881\n",
      "Epoch 2: |          | 364/? [04:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 364, loss 4.734800815582275\n",
      "Epoch 2: |          | 365/? [04:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 365, loss 4.844142436981201\n",
      "Epoch 2: |          | 366/? [04:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 366, loss 4.509415626525879\n",
      "Epoch 2: |          | 367/? [04:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 367, loss 4.592817783355713\n",
      "Epoch 2: |          | 368/? [04:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 368, loss 4.00816011428833\n",
      "Epoch 2: |          | 369/? [05:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 369, loss 4.418532371520996\n",
      "Epoch 2: |          | 370/? [05:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 370, loss 3.9582247734069824\n",
      "Epoch 2: |          | 371/? [05:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 371, loss 5.029376029968262\n",
      "Epoch 2: |          | 372/? [05:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 372, loss 4.524940013885498\n",
      "Epoch 2: |          | 373/? [05:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 373, loss 4.570646286010742\n",
      "Epoch 2: |          | 374/? [05:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 374, loss 4.290521144866943\n",
      "Epoch 2: |          | 375/? [05:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 375, loss 5.034594535827637\n",
      "Epoch 2: |          | 376/? [05:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 376, loss 4.381420135498047\n",
      "Epoch 2: |          | 377/? [05:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 377, loss 4.592035293579102\n",
      "Epoch 2: |          | 378/? [05:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 378, loss 4.738546371459961\n",
      "Epoch 2: |          | 379/? [05:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 379, loss 4.560692310333252\n",
      "Epoch 2: |          | 380/? [05:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 380, loss 4.686428546905518\n",
      "Epoch 2: |          | 381/? [05:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 381, loss 4.6357011795043945\n",
      "Epoch 2: |          | 382/? [05:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 382, loss 4.376296043395996\n",
      "Epoch 2: |          | 383/? [05:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 383, loss 4.362521171569824\n",
      "Epoch 2: |          | 384/? [05:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 384, loss 4.898293495178223\n",
      "Epoch 2: |          | 385/? [05:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 385, loss 4.416312217712402\n",
      "Epoch 2: |          | 386/? [05:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 386, loss 3.297889232635498\n",
      "Epoch 2: |          | 387/? [05:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 387, loss 4.279922962188721\n",
      "Epoch 2: |          | 388/? [05:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 388, loss 4.711339473724365\n",
      "Epoch 2: |          | 389/? [05:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 389, loss 4.961292266845703\n",
      "Epoch 2: |          | 390/? [05:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 390, loss 4.258599281311035\n",
      "Epoch 2: |          | 391/? [05:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 391, loss 4.811354637145996\n",
      "Epoch 2: |          | 392/? [05:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 392, loss 4.816555500030518\n",
      "Epoch 2: |          | 393/? [05:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 393, loss 4.879553318023682\n",
      "Epoch 2: |          | 394/? [05:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 394, loss 4.483330726623535\n",
      "Epoch 2: |          | 395/? [05:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 395, loss 4.7676310539245605\n",
      "Epoch 2: |          | 396/? [05:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 396, loss 4.679590225219727\n",
      "Epoch 2: |          | 397/? [05:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 397, loss 4.490239143371582\n",
      "Epoch 2: |          | 398/? [05:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 398, loss 4.300844669342041\n",
      "Epoch 2: |          | 399/? [05:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 399, loss 4.449645519256592\n",
      "Epoch 2: |          | 400/? [05:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 400, loss 4.486536979675293\n",
      "Epoch 2: |          | 401/? [05:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 401, loss 4.3413896560668945\n",
      "Epoch 2: |          | 402/? [05:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 402, loss 4.91035795211792\n",
      "Epoch 2: |          | 403/? [05:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 403, loss 4.623239517211914\n",
      "Epoch 2: |          | 404/? [05:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 404, loss 4.111416816711426\n",
      "Epoch 2: |          | 405/? [05:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 405, loss 4.095678806304932\n",
      "Epoch 2: |          | 406/? [05:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 406, loss 4.498227119445801\n",
      "Epoch 2: |          | 407/? [05:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 407, loss 4.4532880783081055\n",
      "Epoch 2: |          | 408/? [05:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 408, loss 4.823332786560059\n",
      "Epoch 2: |          | 409/? [05:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 409, loss 4.76671028137207\n",
      "Epoch 2: |          | 410/? [05:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 410, loss 4.47084903717041\n",
      "Epoch 2: |          | 411/? [05:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 411, loss 4.344537258148193\n",
      "Epoch 2: |          | 412/? [05:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 412, loss 3.749533176422119\n",
      "Epoch 2: |          | 413/? [05:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 413, loss 4.615007400512695\n",
      "Epoch 2: |          | 414/? [05:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 414, loss 4.094887733459473\n",
      "Epoch 2: |          | 415/? [05:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 415, loss 4.571146488189697\n",
      "Epoch 2: |          | 416/? [05:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 416, loss 5.002002716064453\n",
      "Epoch 2: |          | 417/? [05:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 417, loss 5.227466583251953\n",
      "Epoch 2: |          | 418/? [05:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 418, loss 4.765158653259277\n",
      "Epoch 2: |          | 419/? [05:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 419, loss 4.758762359619141\n",
      "Epoch 2: |          | 420/? [05:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 420, loss 4.575108528137207\n",
      "Epoch 2: |          | 421/? [05:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 421, loss 5.143080711364746\n",
      "Epoch 2: |          | 422/? [05:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 422, loss 4.631577491760254\n",
      "Epoch 2: |          | 423/? [05:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 423, loss 4.131890296936035\n",
      "Epoch 2: |          | 424/? [05:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 424, loss 4.915322780609131\n",
      "Epoch 2: |          | 425/? [05:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 425, loss 4.78201961517334\n",
      "Epoch 2: |          | 426/? [05:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 426, loss 4.276838779449463\n",
      "Epoch 2: |          | 427/? [05:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 427, loss 4.2501420974731445\n",
      "Epoch 2: |          | 428/? [05:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 428, loss 5.179549217224121\n",
      "Epoch 2: |          | 429/? [05:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 429, loss 3.9486794471740723\n",
      "Epoch 2: |          | 430/? [05:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 430, loss 4.674962520599365\n",
      "Epoch 2: |          | 431/? [05:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 431, loss 4.534832000732422\n",
      "Epoch 2: |          | 432/? [05:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 432, loss 4.619936943054199\n",
      "Epoch 2: |          | 433/? [05:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 433, loss 4.486020565032959\n",
      "Epoch 2: |          | 434/? [05:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 434, loss 4.488710880279541\n",
      "Epoch 2: |          | 435/? [05:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 435, loss 4.140641689300537\n",
      "Epoch 2: |          | 436/? [05:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 436, loss 4.628448486328125\n",
      "Epoch 2: |          | 437/? [05:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 437, loss 4.709170341491699\n",
      "Epoch 2: |          | 438/? [05:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 438, loss 4.3112945556640625\n",
      "Epoch 2: |          | 439/? [05:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 439, loss 4.394024848937988\n",
      "Epoch 2: |          | 440/? [05:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 440, loss 4.355922222137451\n",
      "Epoch 2: |          | 441/? [05:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 441, loss 4.674620151519775\n",
      "Epoch 2: |          | 442/? [05:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 442, loss 4.407631874084473\n",
      "Epoch 2: |          | 443/? [06:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 443, loss 4.691817760467529\n",
      "Epoch 2: |          | 444/? [06:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 444, loss 4.515000343322754\n",
      "Epoch 2: |          | 445/? [06:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 445, loss 5.4224371910095215\n",
      "Epoch 2: |          | 446/? [06:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 446, loss 4.489706516265869\n",
      "Epoch 2: |          | 447/? [06:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 447, loss 5.057900905609131\n",
      "Epoch 2: |          | 448/? [06:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 448, loss 4.285081386566162\n",
      "Epoch 2: |          | 449/? [06:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 449, loss 4.453601360321045\n",
      "Epoch 2: |          | 450/? [06:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 450, loss 4.839022636413574\n",
      "Epoch 2: |          | 451/? [06:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 451, loss 4.463783264160156\n",
      "Epoch 2: |          | 452/? [06:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 452, loss 4.1868109703063965\n",
      "Epoch 2: |          | 453/? [06:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 453, loss 4.927539825439453\n",
      "Epoch 2: |          | 454/? [06:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 454, loss 4.2715559005737305\n",
      "Epoch 2: |          | 455/? [06:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 455, loss 4.713264465332031\n",
      "Epoch 2: |          | 456/? [06:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 456, loss 4.095236778259277\n",
      "Epoch 2: |          | 457/? [06:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 457, loss 4.440588474273682\n",
      "Epoch 2: |          | 458/? [06:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 458, loss 4.8677473068237305\n",
      "Epoch 2: |          | 459/? [06:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 459, loss 4.86423397064209\n",
      "Epoch 2: |          | 460/? [06:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 460, loss 4.617873191833496\n",
      "Epoch 2: |          | 461/? [06:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 461, loss 4.628050327301025\n",
      "Epoch 2: |          | 462/? [06:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 462, loss 4.621148586273193\n",
      "Epoch 2: |          | 463/? [06:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 463, loss 4.509267807006836\n",
      "Epoch 2: |          | 464/? [06:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 464, loss 3.9805305004119873\n",
      "Epoch 2: |          | 465/? [06:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 465, loss 4.256134033203125\n",
      "Epoch 2: |          | 466/? [06:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 466, loss 4.709420204162598\n",
      "Epoch 2: |          | 467/? [06:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 467, loss 4.5269670486450195\n",
      "Epoch 2: |          | 468/? [06:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 468, loss 4.433261871337891\n",
      "Epoch 2: |          | 469/? [06:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 469, loss 4.6319475173950195\n",
      "Epoch 2: |          | 470/? [06:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 470, loss 3.8206353187561035\n",
      "Epoch 2: |          | 471/? [06:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 471, loss 4.636050701141357\n",
      "Epoch 2: |          | 472/? [06:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 472, loss 4.246940612792969\n",
      "Epoch 2: |          | 473/? [06:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 473, loss 4.293417453765869\n",
      "Epoch 2: |          | 474/? [06:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 474, loss 3.825241804122925\n",
      "Epoch 2: |          | 475/? [06:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 475, loss 5.226011753082275\n",
      "Epoch 2: |          | 476/? [06:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 476, loss 4.149971008300781\n",
      "Epoch 2: |          | 477/? [06:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 477, loss 3.6448988914489746\n",
      "Epoch 2: |          | 478/? [06:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 478, loss 3.834336042404175\n",
      "Epoch 2: |          | 479/? [06:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 479, loss 4.457177639007568\n",
      "Epoch 2: |          | 480/? [06:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 480, loss 4.499078273773193\n",
      "Epoch 2: |          | 481/? [06:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 481, loss 4.095799922943115\n",
      "Epoch 2: |          | 482/? [06:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 482, loss 4.334993839263916\n",
      "Epoch 2: |          | 483/? [06:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 483, loss 4.013810634613037\n",
      "Epoch 2: |          | 484/? [06:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 484, loss 4.926085472106934\n",
      "Epoch 2: |          | 485/? [06:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 485, loss 4.656758785247803\n",
      "Epoch 2: |          | 486/? [06:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 486, loss 4.516064167022705\n",
      "Epoch 2: |          | 487/? [06:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 487, loss 4.679396629333496\n",
      "Epoch 2: |          | 488/? [06:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 488, loss 4.337126731872559\n",
      "Epoch 2: |          | 489/? [06:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 489, loss 3.8556580543518066\n",
      "Epoch 2: |          | 490/? [06:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 490, loss 4.647256374359131\n",
      "Epoch 2: |          | 491/? [06:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 491, loss 4.325520992279053\n",
      "Epoch 2: |          | 492/? [06:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 492, loss 3.772247791290283\n",
      "Epoch 2: |          | 493/? [06:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 493, loss 4.833666801452637\n",
      "Epoch 2: |          | 494/? [06:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 494, loss 4.698792457580566\n",
      "Epoch 2: |          | 495/? [06:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 495, loss 4.691769123077393\n",
      "Epoch 2: |          | 496/? [06:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 496, loss 4.311455726623535\n",
      "Epoch 2: |          | 497/? [06:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 497, loss 4.783782005310059\n",
      "Epoch 2: |          | 498/? [06:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 498, loss 4.502179145812988\n",
      "Epoch 2: |          | 499/? [06:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 499, loss 4.494411468505859\n",
      "Epoch 2: |          | 500/? [06:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 500, loss 4.4398884773254395\n",
      "Epoch 2: |          | 501/? [06:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 501, loss 4.083912372589111\n",
      "Epoch 2: |          | 502/? [06:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 502, loss 4.51787805557251\n",
      "Epoch 2: |          | 503/? [06:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 503, loss 4.560905456542969\n",
      "Epoch 2: |          | 504/? [06:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 504, loss 4.375341415405273\n",
      "Epoch 2: |          | 505/? [06:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 505, loss 3.6970324516296387\n",
      "Epoch 2: |          | 506/? [06:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 506, loss 4.413679599761963\n",
      "Epoch 2: |          | 507/? [06:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 507, loss 4.464235305786133\n",
      "Epoch 2: |          | 508/? [06:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 508, loss 4.7978315353393555\n",
      "Epoch 2: |          | 509/? [06:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 509, loss 4.069476127624512\n",
      "Epoch 2: |          | 510/? [06:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 510, loss 4.591651439666748\n",
      "Epoch 2: |          | 511/? [06:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 511, loss 4.535586357116699\n",
      "Epoch 2: |          | 512/? [06:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 512, loss 3.962430953979492\n",
      "Epoch 2: |          | 513/? [06:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 513, loss 4.2148661613464355\n",
      "Epoch 2: |          | 514/? [07:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 514, loss 4.284550666809082\n",
      "Epoch 2: |          | 515/? [07:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 515, loss 3.9856910705566406\n",
      "Epoch 2: |          | 516/? [07:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 516, loss 4.28494930267334\n",
      "Epoch 2: |          | 517/? [07:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 517, loss 4.6397705078125\n",
      "Epoch 2: |          | 518/? [07:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 518, loss 4.156437873840332\n",
      "Epoch 2: |          | 519/? [07:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 519, loss 4.618089199066162\n",
      "Epoch 2: |          | 520/? [07:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 520, loss 4.382495880126953\n",
      "Epoch 2: |          | 521/? [07:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 521, loss 4.399141788482666\n",
      "Epoch 2: |          | 522/? [07:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 522, loss 4.998495578765869\n",
      "Epoch 2: |          | 523/? [07:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 523, loss 5.061896324157715\n",
      "Epoch 2: |          | 524/? [07:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 524, loss 4.8089141845703125\n",
      "Epoch 2: |          | 525/? [07:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 525, loss 4.390560150146484\n",
      "Epoch 2: |          | 526/? [07:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 526, loss 4.203894138336182\n",
      "Epoch 2: |          | 527/? [07:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 527, loss 4.735710620880127\n",
      "Epoch 2: |          | 528/? [07:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 528, loss 4.645991802215576\n",
      "Epoch 2: |          | 529/? [07:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 529, loss 4.1914262771606445\n",
      "Epoch 2: |          | 530/? [07:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 530, loss 4.7850022315979\n",
      "Epoch 2: |          | 531/? [07:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 531, loss 4.231056213378906\n",
      "Epoch 2: |          | 532/? [07:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 532, loss 4.538478374481201\n",
      "Epoch 2: |          | 533/? [07:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 533, loss 4.222569465637207\n",
      "Epoch 2: |          | 534/? [07:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 534, loss 4.03439474105835\n",
      "Epoch 2: |          | 535/? [07:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 535, loss 4.850787162780762\n",
      "Epoch 2: |          | 536/? [07:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 536, loss 4.894257545471191\n",
      "Epoch 2: |          | 537/? [07:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 537, loss 4.541601181030273\n",
      "Epoch 2: |          | 538/? [07:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 538, loss 4.1848530769348145\n",
      "Epoch 2: |          | 539/? [07:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 539, loss 4.273633003234863\n",
      "Epoch 2: |          | 540/? [07:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 540, loss 4.817141532897949\n",
      "Epoch 2: |          | 541/? [07:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 541, loss 4.448973655700684\n",
      "Epoch 2: |          | 542/? [07:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 542, loss 4.271280765533447\n",
      "Epoch 2: |          | 543/? [07:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 543, loss 4.631264686584473\n",
      "Epoch 2: |          | 544/? [07:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 544, loss 4.47098445892334\n",
      "Epoch 2: |          | 545/? [07:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 545, loss 3.6830573081970215\n",
      "Epoch 2: |          | 546/? [07:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 546, loss 4.59799337387085\n",
      "Epoch 2: |          | 547/? [07:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 547, loss 4.97615909576416\n",
      "Epoch 2: |          | 548/? [07:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 548, loss 4.72251558303833\n",
      "Epoch 2: |          | 549/? [07:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 549, loss 4.5330586433410645\n",
      "Epoch 2: |          | 550/? [07:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 550, loss 4.932405948638916\n",
      "Epoch 2: |          | 551/? [07:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 551, loss 4.5320634841918945\n",
      "Epoch 2: |          | 552/? [07:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 552, loss 4.674238681793213\n",
      "Epoch 2: |          | 553/? [07:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 553, loss 3.9391162395477295\n",
      "Epoch 2: |          | 554/? [07:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 554, loss 4.576733112335205\n",
      "Epoch 2: |          | 555/? [07:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 555, loss 4.964998245239258\n",
      "Epoch 2: |          | 556/? [07:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 556, loss 4.55471658706665\n",
      "Epoch 2: |          | 557/? [07:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 557, loss 4.152128219604492\n",
      "Epoch 2: |          | 558/? [07:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 558, loss 4.236374378204346\n",
      "Epoch 2: |          | 559/? [07:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 559, loss 4.279675483703613\n",
      "Epoch 2: |          | 560/? [07:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 560, loss 3.7374866008758545\n",
      "Epoch 2: |          | 561/? [07:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 561, loss 3.766324520111084\n",
      "Epoch 2: |          | 562/? [07:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 562, loss 4.676403999328613\n",
      "Epoch 2: |          | 563/? [07:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 563, loss 3.7707266807556152\n",
      "Epoch 2: |          | 564/? [07:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 564, loss 4.382755279541016\n",
      "Epoch 2: |          | 565/? [07:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 565, loss 4.8238019943237305\n",
      "Epoch 2: |          | 566/? [07:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 566, loss 4.7715654373168945\n",
      "Epoch 2: |          | 567/? [07:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 567, loss 4.867461204528809\n",
      "Epoch 2: |          | 568/? [07:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 568, loss 3.986125946044922\n",
      "Epoch 2: |          | 569/? [07:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 569, loss 4.585489749908447\n",
      "Epoch 2: |          | 570/? [07:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 570, loss 4.596409797668457\n",
      "Epoch 2: |          | 571/? [07:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 571, loss 4.160475254058838\n",
      "Epoch 2: |          | 572/? [07:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 572, loss 5.12814474105835\n",
      "Epoch 2: |          | 573/? [07:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 573, loss 3.4359092712402344\n",
      "Epoch 2: |          | 574/? [07:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 574, loss 4.718239784240723\n",
      "Epoch 2: |          | 575/? [07:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 575, loss 4.060450553894043\n",
      "Epoch 2: |          | 576/? [07:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 576, loss 4.30476188659668\n",
      "Epoch 2: |          | 577/? [07:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 577, loss 4.530643939971924\n",
      "Epoch 2: |          | 578/? [07:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 578, loss 4.767199993133545\n",
      "Epoch 2: |          | 579/? [07:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 579, loss 3.777855396270752\n",
      "Epoch 2: |          | 580/? [07:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 580, loss 4.58837366104126\n",
      "Epoch 2: |          | 581/? [07:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 581, loss 4.6578593254089355\n",
      "Epoch 2: |          | 582/? [07:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 582, loss 4.638960361480713\n",
      "Epoch 2: |          | 583/? [07:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 583, loss 4.367095947265625\n",
      "Epoch 2: |          | 584/? [07:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 584, loss 4.58425235748291\n",
      "Epoch 2: |          | 585/? [07:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 585, loss 4.634692192077637\n",
      "Epoch 2: |          | 586/? [07:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 586, loss 4.677576065063477\n",
      "Epoch 2: |          | 587/? [07:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 587, loss 4.559047222137451\n",
      "Epoch 2: |          | 588/? [08:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 588, loss 4.710460186004639\n",
      "Epoch 2: |          | 589/? [08:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 589, loss 4.090723991394043\n",
      "Epoch 2: |          | 590/? [08:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 590, loss 4.743832588195801\n",
      "Epoch 2: |          | 591/? [08:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 591, loss 4.558869361877441\n",
      "Epoch 2: |          | 592/? [08:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 592, loss 4.24462890625\n",
      "Epoch 2: |          | 593/? [08:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 593, loss 4.4559149742126465\n",
      "Epoch 2: |          | 594/? [08:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 594, loss 5.326890468597412\n",
      "Epoch 2: |          | 595/? [08:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 595, loss 3.920320987701416\n",
      "Epoch 2: |          | 596/? [08:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 596, loss 4.035429954528809\n",
      "Epoch 2: |          | 597/? [08:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 597, loss 4.3852081298828125\n",
      "Epoch 2: |          | 598/? [08:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 598, loss 4.816025733947754\n",
      "Epoch 2: |          | 599/? [08:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 599, loss 4.451879978179932\n",
      "Epoch 2: |          | 600/? [08:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 600, loss 4.182247638702393\n",
      "Epoch 2: |          | 601/? [08:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 601, loss 4.511958122253418\n",
      "Epoch 2: |          | 602/? [08:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 602, loss 4.018702983856201\n",
      "Epoch 2: |          | 603/? [08:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 603, loss 4.281559467315674\n",
      "Epoch 2: |          | 604/? [08:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 604, loss 6.70769739151001\n",
      "Epoch 2: |          | 605/? [08:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 605, loss 3.960280656814575\n",
      "Epoch 2: |          | 606/? [08:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 606, loss 4.224739074707031\n",
      "Epoch 2: |          | 607/? [08:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 607, loss 4.612576484680176\n",
      "Epoch 2: |          | 608/? [08:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 608, loss 4.331864356994629\n",
      "Epoch 2: |          | 609/? [08:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 609, loss 4.227362632751465\n",
      "Epoch 2: |          | 610/? [08:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 610, loss 4.345958709716797\n",
      "Epoch 2: |          | 611/? [08:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 611, loss 4.454733848571777\n",
      "Epoch 2: |          | 612/? [08:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 612, loss 4.201680660247803\n",
      "Epoch 2: |          | 613/? [08:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 613, loss 4.575993061065674\n",
      "Epoch 2: |          | 614/? [08:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 614, loss 4.270111083984375\n",
      "Epoch 2: |          | 615/? [08:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 615, loss 4.80480432510376\n",
      "Epoch 2: |          | 616/? [08:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 616, loss 5.109897613525391\n",
      "Epoch 2: |          | 617/? [08:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 617, loss 3.734790086746216\n",
      "Epoch 2: |          | 618/? [08:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 618, loss 4.619994163513184\n",
      "Epoch 2: |          | 619/? [08:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 619, loss 4.346477508544922\n",
      "Epoch 2: |          | 620/? [08:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 620, loss 4.749462127685547\n",
      "Epoch 2: |          | 621/? [08:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 621, loss 4.083158493041992\n",
      "Epoch 2: |          | 622/? [08:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 622, loss 3.8796515464782715\n",
      "Epoch 2: |          | 623/? [08:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 623, loss 3.5366921424865723\n",
      "Epoch 2: |          | 624/? [08:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 624, loss 3.195986270904541\n",
      "Epoch 2: |          | 625/? [08:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 625, loss 4.8611650466918945\n",
      "Epoch 2: |          | 626/? [08:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 626, loss 4.36897087097168\n",
      "Epoch 2: |          | 627/? [08:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 627, loss 4.350794792175293\n",
      "Epoch 2: |          | 628/? [08:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 628, loss 4.200555324554443\n",
      "Epoch 2: |          | 629/? [08:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 629, loss 4.719990253448486\n",
      "Epoch 2: |          | 630/? [08:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 630, loss 4.427773475646973\n",
      "Epoch 2: |          | 631/? [08:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 631, loss 4.625476837158203\n",
      "Epoch 2: |          | 632/? [08:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 632, loss 3.6878960132598877\n",
      "Epoch 2: |          | 633/? [08:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 633, loss 4.673558712005615\n",
      "Epoch 2: |          | 634/? [08:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 634, loss 4.19661283493042\n",
      "Epoch 2: |          | 635/? [08:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 635, loss 3.9857239723205566\n",
      "Epoch 2: |          | 636/? [08:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 636, loss 4.448065280914307\n",
      "Epoch 2: |          | 637/? [08:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 637, loss 4.246041297912598\n",
      "Epoch 2: |          | 638/? [08:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 638, loss 4.4943766593933105\n",
      "Epoch 2: |          | 639/? [08:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 639, loss 4.212534427642822\n",
      "Epoch 2: |          | 640/? [08:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 640, loss 4.85609769821167\n",
      "Epoch 2: |          | 641/? [08:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 641, loss 3.9752249717712402\n",
      "Epoch 2: |          | 642/? [08:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 642, loss 4.62111759185791\n",
      "Epoch 2: |          | 643/? [08:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 643, loss 4.579133033752441\n",
      "Epoch 2: |          | 644/? [08:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 644, loss 4.407227516174316\n",
      "Epoch 2: |          | 645/? [08:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 645, loss 4.238173007965088\n",
      "Epoch 2: |          | 646/? [08:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 646, loss 4.193134784698486\n",
      "Epoch 2: |          | 647/? [08:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 647, loss 4.8801469802856445\n",
      "Epoch 2: |          | 648/? [08:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 648, loss 4.289505958557129\n",
      "Epoch 2: |          | 649/? [08:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 649, loss 4.303301811218262\n",
      "Epoch 2: |          | 650/? [08:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 650, loss 4.790659427642822\n",
      "Epoch 2: |          | 651/? [08:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 651, loss 4.959981441497803\n",
      "Epoch 2: |          | 652/? [08:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 652, loss 4.296483993530273\n",
      "Epoch 2: |          | 653/? [08:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 653, loss 4.420036315917969\n",
      "Epoch 2: |          | 654/? [08:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 654, loss 4.70538854598999\n",
      "Epoch 2: |          | 655/? [08:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 655, loss 4.3896942138671875\n",
      "Epoch 2: |          | 656/? [08:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 656, loss 3.9543004035949707\n",
      "Epoch 2: |          | 657/? [08:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 657, loss 6.535938262939453\n",
      "Epoch 2: |          | 658/? [08:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 658, loss 4.140908241271973\n",
      "Epoch 2: |          | 659/? [08:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 659, loss 4.409199237823486\n",
      "Epoch 2: |          | 660/? [08:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 660, loss 4.823992729187012\n",
      "Epoch 2: |          | 661/? [08:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 661, loss 4.734177112579346\n",
      "Epoch 2: |          | 662/? [08:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 662, loss 4.561568737030029\n",
      "Epoch 2: |          | 663/? [09:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 663, loss 4.218672752380371\n",
      "Epoch 2: |          | 664/? [09:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 664, loss 4.209817886352539\n",
      "Epoch 2: |          | 665/? [09:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 665, loss 4.563469886779785\n",
      "Epoch 2: |          | 666/? [09:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 666, loss 4.305357933044434\n",
      "Epoch 2: |          | 667/? [09:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 667, loss 5.22171688079834\n",
      "Epoch 2: |          | 668/? [09:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 668, loss 3.8981480598449707\n",
      "Epoch 2: |          | 669/? [09:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 669, loss 4.173775672912598\n",
      "Epoch 2: |          | 670/? [09:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 670, loss 4.857714653015137\n",
      "Epoch 2: |          | 671/? [09:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 671, loss 4.630724906921387\n",
      "Epoch 2: |          | 672/? [09:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 672, loss 4.575408935546875\n",
      "Epoch 2: |          | 673/? [09:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 673, loss 4.42633581161499\n",
      "Epoch 2: |          | 674/? [09:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 674, loss 3.035245656967163\n",
      "Epoch 2: |          | 675/? [09:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 675, loss 1.6358537673950195\n",
      "Epoch 2: |          | 676/? [09:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 676, loss 1.2637232542037964\n",
      "Epoch 2: |          | 677/? [09:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 677, loss 0.9639301300048828\n",
      "Epoch 2: |          | 678/? [09:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 678, loss 2.164577007293701\n",
      "Epoch 2: |          | 679/? [09:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 679, loss 3.8600826263427734\n",
      "Epoch 2: |          | 680/? [09:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 680, loss 4.433370113372803\n",
      "Epoch 2: |          | 681/? [09:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 681, loss 3.687622547149658\n",
      "Epoch 2: |          | 682/? [09:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 682, loss 4.251235485076904\n",
      "Epoch 2: |          | 683/? [09:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 683, loss 3.8940651416778564\n",
      "Epoch 2: |          | 684/? [09:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 684, loss 5.065318584442139\n",
      "Epoch 2: |          | 685/? [09:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 685, loss 4.572211265563965\n",
      "Epoch 2: |          | 686/? [09:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 686, loss 4.104100704193115\n",
      "Epoch 2: |          | 687/? [09:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 687, loss 4.79947566986084\n",
      "Epoch 2: |          | 688/? [09:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 688, loss 4.4619903564453125\n",
      "Epoch 2: |          | 689/? [09:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 689, loss 4.455248832702637\n",
      "Epoch 2: |          | 690/? [09:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 690, loss 4.902318000793457\n",
      "Epoch 2: |          | 691/? [09:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 691, loss 4.4872002601623535\n",
      "Epoch 2: |          | 692/? [09:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 692, loss 4.401429176330566\n",
      "Epoch 2: |          | 693/? [09:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 693, loss 4.921289443969727\n",
      "Epoch 2: |          | 694/? [09:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 694, loss 4.223105430603027\n",
      "Epoch 2: |          | 695/? [09:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 695, loss 4.929169654846191\n",
      "Epoch 2: |          | 696/? [09:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 696, loss 3.9795143604278564\n",
      "Epoch 2: |          | 697/? [09:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 697, loss 4.398554801940918\n",
      "Epoch 2: |          | 698/? [09:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 698, loss 3.568126678466797\n",
      "Epoch 2: |          | 699/? [09:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 699, loss 4.508799076080322\n",
      "Epoch 2: |          | 700/? [09:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 700, loss 4.700532913208008\n",
      "Epoch 2: |          | 701/? [09:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 701, loss 4.196974754333496\n",
      "Epoch 2: |          | 702/? [09:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 702, loss 4.441477298736572\n",
      "Epoch 2: |          | 703/? [09:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 703, loss 4.591711521148682\n",
      "Epoch 2: |          | 704/? [09:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 704, loss 4.490348815917969\n",
      "Epoch 2: |          | 705/? [09:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 705, loss 4.071773052215576\n",
      "Epoch 2: |          | 706/? [09:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 706, loss 4.1761322021484375\n",
      "Epoch 2: |          | 707/? [09:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 707, loss 4.621573448181152\n",
      "Epoch 2: |          | 708/? [09:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 708, loss 4.403050422668457\n",
      "Epoch 2: |          | 709/? [09:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 709, loss 4.298174858093262\n",
      "Epoch 2: |          | 710/? [09:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 710, loss 4.91168212890625\n",
      "Epoch 2: |          | 711/? [09:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 711, loss 5.027066707611084\n",
      "Epoch 2: |          | 712/? [09:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 712, loss 4.72515869140625\n",
      "Epoch 2: |          | 713/? [09:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 713, loss 4.688664436340332\n",
      "Epoch 2: |          | 714/? [09:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 714, loss 4.846906661987305\n",
      "Epoch 2: |          | 715/? [09:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 715, loss 3.645331621170044\n",
      "Epoch 2: |          | 716/? [09:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 716, loss 4.477445125579834\n",
      "Epoch 2: |          | 717/? [09:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 717, loss 4.260159969329834\n",
      "Epoch 2: |          | 718/? [09:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 718, loss 3.820518970489502\n",
      "Epoch 2: |          | 719/? [09:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 719, loss 4.3648271560668945\n",
      "Epoch 2: |          | 720/? [09:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 720, loss 4.083136558532715\n",
      "Epoch 2: |          | 721/? [09:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 721, loss 4.781586647033691\n",
      "Epoch 2: |          | 722/? [09:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 722, loss 3.9124064445495605\n",
      "Epoch 2: |          | 723/? [09:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 723, loss 4.528537273406982\n",
      "Epoch 2: |          | 724/? [09:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 724, loss 4.360150337219238\n",
      "Epoch 2: |          | 725/? [09:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 725, loss 4.079581260681152\n",
      "Epoch 2: |          | 726/? [09:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 726, loss 4.2863054275512695\n",
      "Epoch 2: |          | 727/? [09:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 727, loss 3.9936232566833496\n",
      "Epoch 2: |          | 728/? [09:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 728, loss 3.8510994911193848\n",
      "Epoch 2: |          | 729/? [09:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 729, loss 4.259659767150879\n",
      "Epoch 2: |          | 730/? [09:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 730, loss 4.258431911468506\n",
      "Epoch 2: |          | 731/? [09:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 731, loss 4.4493184089660645\n",
      "Epoch 2: |          | 732/? [09:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 732, loss 4.75757360458374\n",
      "Epoch 2: |          | 733/? [09:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 733, loss 4.333432674407959\n",
      "Epoch 2: |          | 734/? [09:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 734, loss 4.659360408782959\n",
      "Epoch 2: |          | 735/? [09:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 735, loss 4.552091121673584\n",
      "Epoch 2: |          | 736/? [09:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 736, loss 4.0401177406311035\n",
      "Epoch 2: |          | 737/? [10:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 737, loss 4.809189319610596\n",
      "Epoch 2: |          | 738/? [10:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 738, loss 3.952437162399292\n",
      "Epoch 2: |          | 739/? [10:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 739, loss 4.571160793304443\n",
      "Epoch 2: |          | 740/? [10:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 740, loss 4.102984428405762\n",
      "Epoch 2: |          | 741/? [10:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 741, loss 4.370789527893066\n",
      "Epoch 2: |          | 742/? [10:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 742, loss 4.830830097198486\n",
      "Epoch 2: |          | 743/? [10:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 743, loss 4.605315208435059\n",
      "Epoch 2: |          | 744/? [10:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 744, loss 4.493262767791748\n",
      "Epoch 2: |          | 745/? [10:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 745, loss 4.144171714782715\n",
      "Epoch 2: |          | 746/? [10:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 746, loss 4.455517768859863\n",
      "Epoch 2: |          | 747/? [10:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 747, loss 4.237547874450684\n",
      "Epoch 2: |          | 748/? [10:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 748, loss 3.56744647026062\n",
      "Epoch 2: |          | 749/? [10:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 749, loss 4.431197166442871\n",
      "Epoch 2: |          | 750/? [10:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 750, loss 4.786519527435303\n",
      "Epoch 2: |          | 751/? [10:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 751, loss 3.1498703956604004\n",
      "Epoch 2: |          | 752/? [10:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 752, loss 4.623710632324219\n",
      "Epoch 2: |          | 753/? [10:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 753, loss 3.821532726287842\n",
      "Epoch 2: |          | 754/? [10:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 754, loss 4.294997215270996\n",
      "Epoch 2: |          | 755/? [10:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 755, loss 3.9818339347839355\n",
      "Epoch 2: |          | 756/? [10:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 756, loss 4.413401126861572\n",
      "Epoch 2: |          | 757/? [10:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 757, loss 4.406280994415283\n",
      "Epoch 2: |          | 758/? [10:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 758, loss 4.11729097366333\n",
      "Epoch 2: |          | 759/? [10:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 759, loss 4.189357757568359\n",
      "Epoch 2: |          | 760/? [10:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 760, loss 4.5787577629089355\n",
      "Epoch 2: |          | 761/? [10:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 761, loss 4.665388584136963\n",
      "Epoch 2: |          | 762/? [10:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 762, loss 4.330563545227051\n",
      "Epoch 2: |          | 763/? [10:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 763, loss 4.6535139083862305\n",
      "Epoch 2: |          | 764/? [10:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 764, loss 4.819727420806885\n",
      "Epoch 2: |          | 765/? [10:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 765, loss 4.513450622558594\n",
      "Epoch 2: |          | 766/? [10:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 766, loss 4.906792640686035\n",
      "Epoch 2: |          | 767/? [10:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 767, loss 5.048356056213379\n",
      "Epoch 2: |          | 768/? [10:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 768, loss 4.484667778015137\n",
      "Epoch 2: |          | 769/? [10:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 769, loss 3.5942368507385254\n",
      "Epoch 2: |          | 770/? [10:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 770, loss 4.221123218536377\n",
      "Epoch 2: |          | 771/? [10:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 771, loss 5.005631923675537\n",
      "Epoch 2: |          | 772/? [10:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 772, loss 4.730783462524414\n",
      "Epoch 2: |          | 773/? [10:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 773, loss 4.335242748260498\n",
      "Epoch 2: |          | 774/? [10:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 774, loss 4.4759602546691895\n",
      "Epoch 2: |          | 775/? [10:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 775, loss 4.963290214538574\n",
      "Epoch 2: |          | 776/? [10:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 776, loss 4.404471397399902\n",
      "Epoch 2: |          | 777/? [10:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 777, loss 4.4002532958984375\n",
      "Epoch 2: |          | 778/? [10:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 778, loss 4.724416732788086\n",
      "Epoch 2: |          | 779/? [10:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 779, loss 5.137530326843262\n",
      "Epoch 2: |          | 780/? [10:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 780, loss 3.962002992630005\n",
      "Epoch 2: |          | 781/? [10:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 781, loss 4.162117958068848\n",
      "Epoch 2: |          | 782/? [10:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 782, loss 4.57987117767334\n",
      "Epoch 2: |          | 783/? [10:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 783, loss 4.651595115661621\n",
      "Epoch 2: |          | 784/? [10:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 784, loss 4.171738147735596\n",
      "Epoch 2: |          | 785/? [10:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 785, loss 4.055541515350342\n",
      "Epoch 2: |          | 786/? [10:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 786, loss 4.919277667999268\n",
      "Epoch 2: |          | 787/? [10:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 787, loss 4.8985676765441895\n",
      "Epoch 2: |          | 788/? [10:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 788, loss 3.4409282207489014\n",
      "Epoch 2: |          | 789/? [10:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 789, loss 4.387438774108887\n",
      "Epoch 2: |          | 790/? [10:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 790, loss 5.174227714538574\n",
      "Epoch 2: |          | 791/? [10:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 791, loss 4.934741973876953\n",
      "Epoch 2: |          | 792/? [10:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 792, loss 4.070839881896973\n",
      "Epoch 2: |          | 793/? [10:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 793, loss 4.499429702758789\n",
      "Epoch 2: |          | 794/? [10:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 794, loss 4.8914794921875\n",
      "Epoch 2: |          | 795/? [10:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 795, loss 4.395723342895508\n",
      "Epoch 2: |          | 796/? [10:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 796, loss 4.7914533615112305\n",
      "Epoch 2: |          | 797/? [10:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 797, loss 3.691661834716797\n",
      "Epoch 2: |          | 798/? [10:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 798, loss 3.8425755500793457\n",
      "Epoch 2: |          | 799/? [10:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 799, loss 4.791311264038086\n",
      "Epoch 2: |          | 800/? [10:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 800, loss 4.603840351104736\n",
      "Epoch 2: |          | 801/? [10:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 801, loss 4.02126407623291\n",
      "Epoch 2: |          | 802/? [10:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 802, loss 4.533551216125488\n",
      "Epoch 2: |          | 803/? [10:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 803, loss 4.2928595542907715\n",
      "Epoch 2: |          | 804/? [10:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 804, loss 4.563114643096924\n",
      "Epoch 2: |          | 805/? [10:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 805, loss 4.834061145782471\n",
      "Epoch 2: |          | 806/? [10:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 806, loss 5.051163196563721\n",
      "Epoch 2: |          | 807/? [10:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 807, loss 4.527397155761719\n",
      "Epoch 2: |          | 808/? [10:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 808, loss 3.9924023151397705\n",
      "Epoch 2: |          | 809/? [10:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 809, loss 4.598082065582275\n",
      "Epoch 2: |          | 810/? [11:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 810, loss 4.4491729736328125\n",
      "Epoch 2: |          | 811/? [11:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 811, loss 4.753840923309326\n",
      "Epoch 2: |          | 812/? [11:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 812, loss 5.3794426918029785\n",
      "Epoch 2: |          | 813/? [11:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 813, loss 4.9907121658325195\n",
      "Epoch 2: |          | 814/? [11:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 814, loss 3.9750053882598877\n",
      "Epoch 2: |          | 815/? [11:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 815, loss 4.79177713394165\n",
      "Epoch 2: |          | 816/? [11:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 816, loss 4.586292743682861\n",
      "Epoch 2: |          | 817/? [11:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 817, loss 3.899010181427002\n",
      "Epoch 2: |          | 818/? [11:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 818, loss 4.957894802093506\n",
      "Epoch 2: |          | 819/? [11:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 819, loss 4.632663726806641\n",
      "Epoch 2: |          | 820/? [11:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 820, loss 4.502224922180176\n",
      "Epoch 2: |          | 821/? [11:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 821, loss 4.417752742767334\n",
      "Epoch 2: |          | 822/? [11:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 822, loss 4.024325370788574\n",
      "Epoch 2: |          | 823/? [11:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 823, loss 3.993745803833008\n",
      "Epoch 2: |          | 824/? [11:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 824, loss 4.529107093811035\n",
      "Epoch 2: |          | 825/? [11:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 825, loss 4.0997467041015625\n",
      "Epoch 2: |          | 826/? [11:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 826, loss 4.579052448272705\n",
      "Epoch 2: |          | 827/? [11:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 827, loss 4.226889610290527\n",
      "Epoch 2: |          | 828/? [11:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 828, loss 4.745163917541504\n",
      "Epoch 2: |          | 829/? [11:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 829, loss 4.426365375518799\n",
      "Epoch 2: |          | 830/? [11:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 830, loss 5.043007850646973\n",
      "Epoch 2: |          | 831/? [11:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 831, loss 2.819826602935791\n",
      "Epoch 2: |          | 832/? [11:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 832, loss 4.39858341217041\n",
      "Epoch 2: |          | 833/? [11:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 833, loss 4.234043598175049\n",
      "Epoch 2: |          | 834/? [11:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 834, loss 5.018388748168945\n",
      "Epoch 2: |          | 835/? [11:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 835, loss 4.490772247314453\n",
      "Epoch 2: |          | 836/? [11:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 836, loss 5.05906867980957\n",
      "Epoch 2: |          | 837/? [11:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 837, loss 4.507546424865723\n",
      "Epoch 2: |          | 838/? [11:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 838, loss 3.7710251808166504\n",
      "Epoch 2: |          | 839/? [11:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 839, loss 4.09022855758667\n",
      "Epoch 2: |          | 840/? [11:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 840, loss 4.705288887023926\n",
      "Epoch 2: |          | 841/? [11:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 841, loss 4.761481285095215\n",
      "Epoch 2: |          | 842/? [11:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 842, loss 4.454251766204834\n",
      "Epoch 2: |          | 843/? [11:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 843, loss 4.800902843475342\n",
      "Epoch 2: |          | 844/? [11:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 844, loss 4.094729423522949\n",
      "Epoch 2: |          | 845/? [11:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 845, loss 4.527878284454346\n",
      "Epoch 2: |          | 846/? [11:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 846, loss 5.094372272491455\n",
      "Epoch 2: |          | 847/? [11:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 847, loss 4.606853008270264\n",
      "Epoch 2: |          | 848/? [11:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 848, loss 4.048624515533447\n",
      "Epoch 2: |          | 849/? [11:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 849, loss 4.21746826171875\n",
      "Epoch 2: |          | 850/? [11:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 850, loss 4.280813217163086\n",
      "Epoch 2: |          | 851/? [11:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 851, loss 4.661196708679199\n",
      "Epoch 2: |          | 852/? [11:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 852, loss 4.646389961242676\n",
      "Epoch 2: |          | 853/? [11:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 853, loss 4.625950813293457\n",
      "Epoch 2: |          | 854/? [11:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 854, loss 3.751471996307373\n",
      "Epoch 2: |          | 855/? [11:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 855, loss 4.140458583831787\n",
      "Epoch 2: |          | 856/? [11:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 856, loss 4.069674491882324\n",
      "Epoch 2: |          | 857/? [11:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 857, loss 4.647123336791992\n",
      "Epoch 2: |          | 858/? [11:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 858, loss 4.570950984954834\n",
      "Epoch 2: |          | 859/? [11:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 859, loss 4.592087745666504\n",
      "Epoch 2: |          | 860/? [11:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 860, loss 4.9327921867370605\n",
      "Epoch 2: |          | 861/? [11:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 861, loss 4.14523458480835\n",
      "Epoch 2: |          | 862/? [11:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 862, loss 4.611239910125732\n",
      "Epoch 2: |          | 863/? [11:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 863, loss 3.788952350616455\n",
      "Epoch 2: |          | 864/? [11:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 864, loss 4.564606666564941\n",
      "Epoch 2: |          | 865/? [11:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 865, loss 4.4751152992248535\n",
      "Epoch 2: |          | 866/? [11:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 866, loss 3.4533066749572754\n",
      "Epoch 2: |          | 867/? [11:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 867, loss 3.6893386840820312\n",
      "Epoch 2: |          | 868/? [11:48<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 868, loss 4.605677127838135\n",
      "Epoch 2: |          | 869/? [11:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 869, loss 4.566771507263184\n",
      "Epoch 2: |          | 870/? [11:49<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 870, loss 4.106677532196045\n",
      "Epoch 2: |          | 871/? [11:50<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 871, loss 4.587710857391357\n",
      "Epoch 2: |          | 872/? [11:51<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 872, loss 4.428383827209473\n",
      "Epoch 2: |          | 873/? [11:52<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 873, loss 4.332455635070801\n",
      "Epoch 2: |          | 874/? [11:53<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 874, loss 3.8598618507385254\n",
      "Epoch 2: |          | 875/? [11:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 875, loss 4.579363822937012\n",
      "Epoch 2: |          | 876/? [11:54<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 876, loss 4.279341220855713\n",
      "Epoch 2: |          | 877/? [11:55<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 877, loss 4.5028252601623535\n",
      "Epoch 2: |          | 878/? [11:56<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 878, loss 3.9398014545440674\n",
      "Epoch 2: |          | 879/? [11:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 879, loss 4.0146284103393555\n",
      "Epoch 2: |          | 880/? [11:57<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 880, loss 5.127402305603027\n",
      "Epoch 2: |          | 881/? [11:58<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 881, loss 4.5320000648498535\n",
      "Epoch 2: |          | 882/? [11:59<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 882, loss 4.284545421600342\n",
      "Epoch 2: |          | 883/? [12:00<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 883, loss 4.463622093200684\n",
      "Epoch 2: |          | 884/? [12:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 884, loss 4.5408196449279785\n",
      "Epoch 2: |          | 885/? [12:01<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 885, loss 4.223658561706543\n",
      "Epoch 2: |          | 886/? [12:02<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 886, loss 4.888521671295166\n",
      "Epoch 2: |          | 887/? [12:03<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 887, loss 4.944293022155762\n",
      "Epoch 2: |          | 888/? [12:04<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 888, loss 4.6196794509887695\n",
      "Epoch 2: |          | 889/? [12:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 889, loss 4.294835090637207\n",
      "Epoch 2: |          | 890/? [12:05<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 890, loss 4.598631858825684\n",
      "Epoch 2: |          | 891/? [12:06<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 891, loss 4.0846123695373535\n",
      "Epoch 2: |          | 892/? [12:07<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 892, loss 4.868863105773926\n",
      "Epoch 2: |          | 893/? [12:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 893, loss 4.287568092346191\n",
      "Epoch 2: |          | 894/? [12:08<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 894, loss 3.8945841789245605\n",
      "Epoch 2: |          | 895/? [12:09<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 895, loss 4.978113174438477\n",
      "Epoch 2: |          | 896/? [12:10<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 896, loss 4.568072319030762\n",
      "Epoch 2: |          | 897/? [12:11<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 897, loss 4.605358123779297\n",
      "Epoch 2: |          | 898/? [12:12<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 898, loss 4.5809478759765625\n",
      "Epoch 2: |          | 899/? [12:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 899, loss 4.326085567474365\n",
      "Epoch 2: |          | 900/? [12:13<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 900, loss 4.232880592346191\n",
      "Epoch 2: |          | 901/? [12:14<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 901, loss 4.693329811096191\n",
      "Epoch 2: |          | 902/? [12:15<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 902, loss 4.7586164474487305\n",
      "Epoch 2: |          | 903/? [12:16<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 903, loss 4.007021427154541\n",
      "Epoch 2: |          | 904/? [12:17<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 904, loss 4.5045952796936035\n",
      "Epoch 2: |          | 905/? [12:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 905, loss 4.776658535003662\n",
      "Epoch 2: |          | 906/? [12:18<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 906, loss 4.417719841003418\n",
      "Epoch 2: |          | 907/? [12:19<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 907, loss 4.53530216217041\n",
      "Epoch 2: |          | 908/? [12:20<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 908, loss 4.6155500411987305\n",
      "Epoch 2: |          | 909/? [12:21<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 909, loss 4.605968952178955\n",
      "Epoch 2: |          | 910/? [12:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 910, loss 4.4242777824401855\n",
      "Epoch 2: |          | 911/? [12:22<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 911, loss 4.386551856994629\n",
      "Epoch 2: |          | 912/? [12:23<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 912, loss 4.3407158851623535\n",
      "Epoch 2: |          | 913/? [12:24<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 913, loss 4.330967903137207\n",
      "Epoch 2: |          | 914/? [12:25<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 914, loss 4.680700302124023\n",
      "Epoch 2: |          | 915/? [12:26<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 915, loss 4.572794437408447\n",
      "Epoch 2: |          | 916/? [12:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 916, loss 4.349478721618652\n",
      "Epoch 2: |          | 917/? [12:27<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 917, loss 4.41964864730835\n",
      "Epoch 2: |          | 918/? [12:28<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 918, loss 4.278158664703369\n",
      "Epoch 2: |          | 919/? [12:29<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 919, loss 4.227461814880371\n",
      "Epoch 2: |          | 920/? [12:30<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 920, loss 4.409715175628662\n",
      "Epoch 2: |          | 921/? [12:31<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 921, loss 4.294075012207031\n",
      "Epoch 2: |          | 922/? [12:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 922, loss 4.447096347808838\n",
      "Epoch 2: |          | 923/? [12:32<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 923, loss 4.2764763832092285\n",
      "Epoch 2: |          | 924/? [12:33<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 924, loss 4.27332878112793\n",
      "Epoch 2: |          | 925/? [12:34<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 925, loss 4.546482086181641\n",
      "Epoch 2: |          | 926/? [12:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 926, loss 4.3880295753479\n",
      "Epoch 2: |          | 927/? [12:35<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 927, loss 4.564465522766113\n",
      "Epoch 2: |          | 928/? [12:36<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 928, loss 4.035576820373535\n",
      "Epoch 2: |          | 929/? [12:37<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 929, loss 4.168304920196533\n",
      "Epoch 2: |          | 930/? [12:38<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 930, loss 4.075303077697754\n",
      "Epoch 2: |          | 931/? [12:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 931, loss 3.747689723968506\n",
      "Epoch 2: |          | 932/? [12:39<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 932, loss 4.485644817352295\n",
      "Epoch 2: |          | 933/? [12:40<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 933, loss 4.282693386077881\n",
      "Epoch 2: |          | 934/? [12:41<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 934, loss 4.921939373016357\n",
      "Epoch 2: |          | 935/? [12:42<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 935, loss 5.123711109161377\n",
      "Epoch 2: |          | 936/? [12:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 936, loss 4.3880486488342285\n",
      "Epoch 2: |          | 937/? [12:43<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 937, loss 4.769442558288574\n",
      "Epoch 2: |          | 938/? [12:44<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 938, loss 4.255112171173096\n",
      "Epoch 2: |          | 939/? [12:45<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 939, loss 4.574154376983643\n",
      "Epoch 2: |          | 940/? [12:46<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 940, loss 4.864278316497803\n",
      "Epoch 2: |          | 941/? [12:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 941, loss 4.1789350509643555\n",
      "Epoch 2: |          | 942/? [12:47<00:00,  1.23it/s, v_num=30]   TRRAINING: Batch 942, loss 3.733365297317505\n",
      "Epoch 2: |          | 943/? [12:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 943, loss 4.68642520904541\n",
      "Epoch 2: |          | 944/? [12:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 944, loss 3.7067553997039795\n",
      "Epoch 2: |          | 945/? [12:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 945, loss 4.334767818450928\n",
      "Epoch 2: |          | 946/? [12:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 946, loss 4.402420997619629\n",
      "Epoch 2: |          | 947/? [12:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 947, loss 4.246555805206299\n",
      "Epoch 2: |          | 948/? [12:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 948, loss 4.437251567840576\n",
      "Epoch 2: |          | 949/? [12:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 949, loss 4.298688888549805\n",
      "Epoch 2: |          | 950/? [12:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 950, loss 4.134184837341309\n",
      "Epoch 2: |          | 951/? [12:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 951, loss 4.937200546264648\n",
      "Epoch 2: |          | 952/? [13:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 952, loss 4.82043981552124\n",
      "Epoch 2: |          | 953/? [13:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 953, loss 5.292845726013184\n",
      "Epoch 2: |          | 954/? [13:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 954, loss 4.3445940017700195\n",
      "Epoch 2: |          | 955/? [13:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 955, loss 5.05203914642334\n",
      "Epoch 2: |          | 956/? [13:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 956, loss 4.2938456535339355\n",
      "Epoch 2: |          | 957/? [13:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 957, loss 4.626304626464844\n",
      "Epoch 2: |          | 958/? [13:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 958, loss 4.91198205947876\n",
      "Epoch 2: |          | 959/? [13:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 959, loss 4.772803783416748\n",
      "Epoch 2: |          | 960/? [13:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 960, loss 4.864718437194824\n",
      "Epoch 2: |          | 961/? [13:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 961, loss 4.918527126312256\n",
      "Epoch 2: |          | 962/? [13:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 962, loss 4.570265769958496\n",
      "Epoch 2: |          | 963/? [13:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 963, loss 4.140061855316162\n",
      "Epoch 2: |          | 964/? [13:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 964, loss 4.667131423950195\n",
      "Epoch 2: |          | 965/? [13:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 965, loss 4.205775737762451\n",
      "Epoch 2: |          | 966/? [13:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 966, loss 4.09200382232666\n",
      "Epoch 2: |          | 967/? [13:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 967, loss 4.314774036407471\n",
      "Epoch 2: |          | 968/? [13:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 968, loss 4.330063819885254\n",
      "Epoch 2: |          | 969/? [13:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 969, loss 4.044844627380371\n",
      "Epoch 2: |          | 970/? [13:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 970, loss 4.5836262702941895\n",
      "Epoch 2: |          | 971/? [13:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 971, loss 5.013850688934326\n",
      "Epoch 2: |          | 972/? [13:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 972, loss 4.2494001388549805\n",
      "Epoch 2: |          | 973/? [13:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 973, loss 4.613975524902344\n",
      "Epoch 2: |          | 974/? [13:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 974, loss 4.450564861297607\n",
      "Epoch 2: |          | 975/? [13:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 975, loss 4.433413505554199\n",
      "Epoch 2: |          | 976/? [13:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 976, loss 4.533459186553955\n",
      "Epoch 2: |          | 977/? [13:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 977, loss 5.140044212341309\n",
      "Epoch 2: |          | 978/? [13:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 978, loss 4.790122985839844\n",
      "Epoch 2: |          | 979/? [13:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 979, loss 4.891391277313232\n",
      "Epoch 2: |          | 980/? [13:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 980, loss 4.092726707458496\n",
      "Epoch 2: |          | 981/? [13:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 981, loss 3.7603137493133545\n",
      "Epoch 2: |          | 982/? [13:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 982, loss 4.478353977203369\n",
      "Epoch 2: |          | 983/? [13:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 983, loss 4.837655067443848\n",
      "Epoch 2: |          | 984/? [13:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 984, loss 3.8602232933044434\n",
      "Epoch 2: |          | 985/? [13:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 985, loss 4.216193199157715\n",
      "Epoch 2: |          | 986/? [13:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 986, loss 4.154016017913818\n",
      "Epoch 2: |          | 987/? [13:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 987, loss 3.839447498321533\n",
      "Epoch 2: |          | 988/? [13:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 988, loss 4.742610931396484\n",
      "Epoch 2: |          | 989/? [13:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 989, loss 4.455794334411621\n",
      "Epoch 2: |          | 990/? [13:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 990, loss 3.5673251152038574\n",
      "Epoch 2: |          | 991/? [13:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 991, loss 4.3866987228393555\n",
      "Epoch 2: |          | 992/? [13:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 992, loss 5.134442329406738\n",
      "Epoch 2: |          | 993/? [13:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 993, loss 4.172678470611572\n",
      "Epoch 2: |          | 994/? [13:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 994, loss 4.279767036437988\n",
      "Epoch 2: |          | 995/? [13:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 995, loss 4.7619309425354\n",
      "Epoch 2: |          | 996/? [13:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 996, loss 4.715147018432617\n",
      "Epoch 2: |          | 997/? [13:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 997, loss 4.261139869689941\n",
      "Epoch 2: |          | 998/? [13:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 998, loss 4.517467975616455\n",
      "Epoch 2: |          | 999/? [13:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 999, loss 4.724480152130127\n",
      "Epoch 2: |          | 1000/? [13:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1000, loss 4.1850175857543945\n",
      "Epoch 2: |          | 1001/? [13:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1001, loss 4.723954200744629\n",
      "Epoch 2: |          | 1002/? [13:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1002, loss 4.79163122177124\n",
      "Epoch 2: |          | 1003/? [13:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1003, loss 4.809558868408203\n",
      "Epoch 2: |          | 1004/? [13:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1004, loss 3.677978992462158\n",
      "Epoch 2: |          | 1005/? [13:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1005, loss 4.334734916687012\n",
      "Epoch 2: |          | 1006/? [13:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1006, loss 4.755046844482422\n",
      "Epoch 2: |          | 1007/? [13:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1007, loss 4.341577053070068\n",
      "Epoch 2: |          | 1008/? [13:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1008, loss 4.342255592346191\n",
      "Epoch 2: |          | 1009/? [13:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1009, loss 4.762541770935059\n",
      "Epoch 2: |          | 1010/? [13:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1010, loss 3.800233840942383\n",
      "Epoch 2: |          | 1011/? [13:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1011, loss 4.443285942077637\n",
      "Epoch 2: |          | 1012/? [13:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1012, loss 4.145289421081543\n",
      "Epoch 2: |          | 1013/? [13:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1013, loss 4.467404365539551\n",
      "Epoch 2: |          | 1014/? [13:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1014, loss 4.776540279388428\n",
      "Epoch 2: |          | 1015/? [13:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1015, loss 4.478420257568359\n",
      "Epoch 2: |          | 1016/? [13:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1016, loss 4.357174396514893\n",
      "Epoch 2: |          | 1017/? [13:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1017, loss 3.9390511512756348\n",
      "Epoch 2: |          | 1018/? [13:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1018, loss 4.421221733093262\n",
      "Epoch 2: |          | 1019/? [13:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1019, loss 4.400906085968018\n",
      "Epoch 2: |          | 1020/? [13:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1020, loss 3.9304909706115723\n",
      "Epoch 2: |          | 1021/? [13:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1021, loss 4.188998699188232\n",
      "Epoch 2: |          | 1022/? [13:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1022, loss 3.915163040161133\n",
      "Epoch 2: |          | 1023/? [13:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1023, loss 3.622560501098633\n",
      "Epoch 2: |          | 1024/? [13:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1024, loss 4.262360095977783\n",
      "Epoch 2: |          | 1025/? [13:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1025, loss 4.20009708404541\n",
      "Epoch 2: |          | 1026/? [14:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1026, loss 3.13057279586792\n",
      "Epoch 2: |          | 1027/? [14:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1027, loss 4.467022895812988\n",
      "Epoch 2: |          | 1028/? [14:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1028, loss 4.257374286651611\n",
      "Epoch 2: |          | 1029/? [14:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1029, loss 4.244447708129883\n",
      "Epoch 2: |          | 1030/? [14:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1030, loss 3.9079957008361816\n",
      "Epoch 2: |          | 1031/? [14:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1031, loss 4.017116069793701\n",
      "Epoch 2: |          | 1032/? [14:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1032, loss 4.711625099182129\n",
      "Epoch 2: |          | 1033/? [14:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1033, loss 4.8017683029174805\n",
      "Epoch 2: |          | 1034/? [14:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1034, loss 4.184367656707764\n",
      "Epoch 2: |          | 1035/? [14:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1035, loss 4.2040300369262695\n",
      "Epoch 2: |          | 1036/? [14:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1036, loss 4.031723499298096\n",
      "Epoch 2: |          | 1037/? [14:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1037, loss 4.7747392654418945\n",
      "Epoch 2: |          | 1038/? [14:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1038, loss 4.936650276184082\n",
      "Epoch 2: |          | 1039/? [14:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1039, loss 5.095387935638428\n",
      "Epoch 2: |          | 1040/? [14:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1040, loss 4.465928077697754\n",
      "Epoch 2: |          | 1041/? [14:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1041, loss 4.857898235321045\n",
      "Epoch 2: |          | 1042/? [14:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1042, loss 4.397836685180664\n",
      "Epoch 2: |          | 1043/? [14:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1043, loss 4.674907684326172\n",
      "Epoch 2: |          | 1044/? [14:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1044, loss 4.2392377853393555\n",
      "Epoch 2: |          | 1045/? [14:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1045, loss 3.858961820602417\n",
      "Epoch 2: |          | 1046/? [14:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1046, loss 3.6640639305114746\n",
      "Epoch 2: |          | 1047/? [14:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1047, loss 4.961001873016357\n",
      "Epoch 2: |          | 1048/? [14:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1048, loss 4.1679368019104\n",
      "Epoch 2: |          | 1049/? [14:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1049, loss 4.418993949890137\n",
      "Epoch 2: |          | 1050/? [14:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1050, loss 3.994265079498291\n",
      "Epoch 2: |          | 1051/? [14:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1051, loss 4.143997669219971\n",
      "Epoch 2: |          | 1052/? [14:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1052, loss 4.735806465148926\n",
      "Epoch 2: |          | 1053/? [14:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1053, loss 4.786839485168457\n",
      "Epoch 2: |          | 1054/? [14:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1054, loss 4.171355247497559\n",
      "Epoch 2: |          | 1055/? [14:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1055, loss 3.9085183143615723\n",
      "Epoch 2: |          | 1056/? [14:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1056, loss 3.9087443351745605\n",
      "Epoch 2: |          | 1057/? [14:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1057, loss 4.600366592407227\n",
      "Epoch 2: |          | 1058/? [14:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1058, loss 4.137546062469482\n",
      "Epoch 2: |          | 1059/? [14:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1059, loss 4.848003387451172\n",
      "Epoch 2: |          | 1060/? [14:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1060, loss 4.668332099914551\n",
      "Epoch 2: |          | 1061/? [14:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1061, loss 3.2469780445098877\n",
      "Epoch 2: |          | 1062/? [14:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1062, loss 4.446969032287598\n",
      "Epoch 2: |          | 1063/? [14:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1063, loss 4.380881309509277\n",
      "Epoch 2: |          | 1064/? [14:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1064, loss 4.584475040435791\n",
      "Epoch 2: |          | 1065/? [14:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1065, loss 3.237177610397339\n",
      "Epoch 2: |          | 1066/? [14:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1066, loss 4.510804176330566\n",
      "Epoch 2: |          | 1067/? [14:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1067, loss 3.875188112258911\n",
      "Epoch 2: |          | 1068/? [14:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1068, loss 4.049231052398682\n",
      "Epoch 2: |          | 1069/? [14:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1069, loss 4.597393035888672\n",
      "Epoch 2: |          | 1070/? [14:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1070, loss 4.2818498611450195\n",
      "Epoch 2: |          | 1071/? [14:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1071, loss 4.604799747467041\n",
      "Epoch 2: |          | 1072/? [14:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1072, loss 4.627185821533203\n",
      "Epoch 2: |          | 1073/? [14:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1073, loss 4.978011131286621\n",
      "Epoch 2: |          | 1074/? [14:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1074, loss 4.2042412757873535\n",
      "Epoch 2: |          | 1075/? [14:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1075, loss 4.023932456970215\n",
      "Epoch 2: |          | 1076/? [14:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1076, loss 4.79034948348999\n",
      "Epoch 2: |          | 1077/? [14:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1077, loss 4.193473815917969\n",
      "Epoch 2: |          | 1078/? [14:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1078, loss 4.3629655838012695\n",
      "Epoch 2: |          | 1079/? [14:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1079, loss 5.126368045806885\n",
      "Epoch 2: |          | 1080/? [14:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1080, loss 4.477622032165527\n",
      "Epoch 2: |          | 1081/? [14:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1081, loss 4.687211036682129\n",
      "Epoch 2: |          | 1082/? [14:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1082, loss 4.061249256134033\n",
      "Epoch 2: |          | 1083/? [14:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1083, loss 3.8953704833984375\n",
      "Epoch 2: |          | 1084/? [14:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1084, loss 3.5648117065429688\n",
      "Epoch 2: |          | 1085/? [14:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1085, loss 4.224950790405273\n",
      "Epoch 2: |          | 1086/? [14:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1086, loss 4.538503170013428\n",
      "Epoch 2: |          | 1087/? [14:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1087, loss 5.052302360534668\n",
      "Epoch 2: |          | 1088/? [14:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1088, loss 4.640836238861084\n",
      "Epoch 2: |          | 1089/? [14:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1089, loss 4.818598747253418\n",
      "Epoch 2: |          | 1090/? [14:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1090, loss 4.5408735275268555\n",
      "Epoch 2: |          | 1091/? [14:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1091, loss 4.221827507019043\n",
      "Epoch 2: |          | 1092/? [14:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1092, loss 4.466854095458984\n",
      "Epoch 2: |          | 1093/? [14:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1093, loss 3.9649784564971924\n",
      "Epoch 2: |          | 1094/? [14:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1094, loss 4.4535393714904785\n",
      "Epoch 2: |          | 1095/? [14:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1095, loss 4.527199745178223\n",
      "Epoch 2: |          | 1096/? [14:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1096, loss 4.731586456298828\n",
      "Epoch 2: |          | 1097/? [14:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1097, loss 4.311712265014648\n",
      "Epoch 2: |          | 1098/? [14:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1098, loss 3.497816801071167\n",
      "Epoch 2: |          | 1099/? [14:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1099, loss 4.267792224884033\n",
      "Epoch 2: |          | 1100/? [15:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1100, loss 4.538542747497559\n",
      "Epoch 2: |          | 1101/? [15:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1101, loss 4.030454158782959\n",
      "Epoch 2: |          | 1102/? [15:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1102, loss 4.9384002685546875\n",
      "Epoch 2: |          | 1103/? [15:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1103, loss 5.632528305053711\n",
      "Epoch 2: |          | 1104/? [15:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1104, loss 4.691783905029297\n",
      "Epoch 2: |          | 1105/? [15:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1105, loss 4.75162410736084\n",
      "Epoch 2: |          | 1106/? [15:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1106, loss 4.242321014404297\n",
      "Epoch 2: |          | 1107/? [15:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1107, loss 4.365851879119873\n",
      "Epoch 2: |          | 1108/? [15:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1108, loss 4.316320419311523\n",
      "Epoch 2: |          | 1109/? [15:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1109, loss 3.8427109718322754\n",
      "Epoch 2: |          | 1110/? [15:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1110, loss 5.0507917404174805\n",
      "Epoch 2: |          | 1111/? [15:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1111, loss 4.613520622253418\n",
      "Epoch 2: |          | 1112/? [15:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1112, loss 4.507161617279053\n",
      "Epoch 2: |          | 1113/? [15:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1113, loss 4.244783878326416\n",
      "Epoch 2: |          | 1114/? [15:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1114, loss 3.703382968902588\n",
      "Epoch 2: |          | 1115/? [15:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1115, loss 3.3762710094451904\n",
      "Epoch 2: |          | 1116/? [15:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1116, loss 3.9050261974334717\n",
      "Epoch 2: |          | 1117/? [15:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1117, loss 4.165495872497559\n",
      "Epoch 2: |          | 1118/? [15:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1118, loss 4.136251926422119\n",
      "Epoch 2: |          | 1119/? [15:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1119, loss 4.891613960266113\n",
      "Epoch 2: |          | 1120/? [15:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1120, loss 4.353375434875488\n",
      "Epoch 2: |          | 1121/? [15:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1121, loss 4.672318458557129\n",
      "Epoch 2: |          | 1122/? [15:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1122, loss 4.384955406188965\n",
      "Epoch 2: |          | 1123/? [15:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1123, loss 4.386785984039307\n",
      "Epoch 2: |          | 1124/? [15:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1124, loss 4.768587112426758\n",
      "Epoch 2: |          | 1125/? [15:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1125, loss 3.9977822303771973\n",
      "Epoch 2: |          | 1126/? [15:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1126, loss 3.950408935546875\n",
      "Epoch 2: |          | 1127/? [15:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1127, loss 4.277937889099121\n",
      "Epoch 2: |          | 1128/? [15:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1128, loss 4.310084819793701\n",
      "Epoch 2: |          | 1129/? [15:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1129, loss 4.442377090454102\n",
      "Epoch 2: |          | 1130/? [15:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1130, loss 4.60522985458374\n",
      "Epoch 2: |          | 1131/? [15:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1131, loss 4.687418460845947\n",
      "Epoch 2: |          | 1132/? [15:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1132, loss 3.3480541706085205\n",
      "Epoch 2: |          | 1133/? [15:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1133, loss 4.4352617263793945\n",
      "Epoch 2: |          | 1134/? [15:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1134, loss 4.044012546539307\n",
      "Epoch 2: |          | 1135/? [15:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1135, loss 4.704894065856934\n",
      "Epoch 2: |          | 1136/? [15:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1136, loss 4.356593132019043\n",
      "Epoch 2: |          | 1137/? [15:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1137, loss 4.404431343078613\n",
      "Epoch 2: |          | 1138/? [15:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1138, loss 4.9269328117370605\n",
      "Epoch 2: |          | 1139/? [15:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1139, loss 5.241255760192871\n",
      "Epoch 2: |          | 1140/? [15:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1140, loss 4.510462760925293\n",
      "Epoch 2: |          | 1141/? [15:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1141, loss 4.690966606140137\n",
      "Epoch 2: |          | 1142/? [15:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1142, loss 4.832748889923096\n",
      "Epoch 2: |          | 1143/? [15:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1143, loss 4.872243404388428\n",
      "Epoch 2: |          | 1144/? [15:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1144, loss 4.3192548751831055\n",
      "Epoch 2: |          | 1145/? [15:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1145, loss 4.350101470947266\n",
      "Epoch 2: |          | 1146/? [15:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1146, loss 4.225607872009277\n",
      "Epoch 2: |          | 1147/? [15:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1147, loss 3.819004535675049\n",
      "Epoch 2: |          | 1148/? [15:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1148, loss 4.113039970397949\n",
      "Epoch 2: |          | 1149/? [15:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1149, loss 5.5246901512146\n",
      "Epoch 2: |          | 1150/? [15:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1150, loss 4.5547966957092285\n",
      "Epoch 2: |          | 1151/? [15:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1151, loss 4.9468302726745605\n",
      "Epoch 2: |          | 1152/? [15:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1152, loss 4.083247184753418\n",
      "Epoch 2: |          | 1153/? [15:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1153, loss 4.44687557220459\n",
      "Epoch 2: |          | 1154/? [15:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1154, loss 4.073721408843994\n",
      "Epoch 2: |          | 1155/? [15:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1155, loss 4.356499671936035\n",
      "Epoch 2: |          | 1156/? [15:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1156, loss 4.486588001251221\n",
      "Epoch 2: |          | 1157/? [15:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1157, loss 4.6258649826049805\n",
      "Epoch 2: |          | 1158/? [15:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1158, loss 4.986428260803223\n",
      "Epoch 2: |          | 1159/? [15:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1159, loss 3.50518536567688\n",
      "Epoch 2: |          | 1160/? [15:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1160, loss 4.798113822937012\n",
      "Epoch 2: |          | 1161/? [15:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1161, loss 4.7130818367004395\n",
      "Epoch 2: |          | 1162/? [15:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1162, loss 4.596673488616943\n",
      "Epoch 2: |          | 1163/? [15:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1163, loss 5.105883598327637\n",
      "Epoch 2: |          | 1164/? [15:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1164, loss 4.956894874572754\n",
      "Epoch 2: |          | 1165/? [15:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1165, loss 3.9621474742889404\n",
      "Epoch 2: |          | 1166/? [15:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1166, loss 4.567103862762451\n",
      "Epoch 2: |          | 1167/? [15:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1167, loss 4.776033401489258\n",
      "Epoch 2: |          | 1168/? [15:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1168, loss 5.135178089141846\n",
      "Epoch 2: |          | 1169/? [15:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1169, loss 4.069634914398193\n",
      "Epoch 2: |          | 1170/? [15:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1170, loss 4.658931255340576\n",
      "Epoch 2: |          | 1171/? [15:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1171, loss 4.120938301086426\n",
      "Epoch 2: |          | 1172/? [15:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1172, loss 3.8941731452941895\n",
      "Epoch 2: |          | 1173/? [15:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1173, loss 4.565265655517578\n",
      "Epoch 2: |          | 1174/? [16:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1174, loss 4.047958850860596\n",
      "Epoch 2: |          | 1175/? [16:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1175, loss 4.686649322509766\n",
      "Epoch 2: |          | 1176/? [16:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1176, loss 4.7267680168151855\n",
      "Epoch 2: |          | 1177/? [16:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1177, loss 4.819380760192871\n",
      "Epoch 2: |          | 1178/? [16:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1178, loss 4.204218864440918\n",
      "Epoch 2: |          | 1179/? [16:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1179, loss 4.764049530029297\n",
      "Epoch 2: |          | 1180/? [16:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1180, loss 4.584117889404297\n",
      "Epoch 2: |          | 1181/? [16:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1181, loss 4.6019792556762695\n",
      "Epoch 2: |          | 1182/? [16:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1182, loss 4.426174163818359\n",
      "Epoch 2: |          | 1183/? [16:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1183, loss 4.094653129577637\n",
      "Epoch 2: |          | 1184/? [16:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1184, loss 4.346691608428955\n",
      "Epoch 2: |          | 1185/? [16:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1185, loss 4.279214859008789\n",
      "Epoch 2: |          | 1186/? [16:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1186, loss 4.507895469665527\n",
      "Epoch 2: |          | 1187/? [16:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1187, loss 4.291683673858643\n",
      "Epoch 2: |          | 1188/? [16:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1188, loss 4.738692760467529\n",
      "Epoch 2: |          | 1189/? [16:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1189, loss 4.766136169433594\n",
      "Epoch 2: |          | 1190/? [16:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1190, loss 4.315426826477051\n",
      "Epoch 2: |          | 1191/? [16:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1191, loss 4.317371845245361\n",
      "Epoch 2: |          | 1192/? [16:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1192, loss 4.663752555847168\n",
      "Epoch 2: |          | 1193/? [16:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1193, loss 4.12540864944458\n",
      "Epoch 2: |          | 1194/? [16:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1194, loss 3.6899940967559814\n",
      "Epoch 2: |          | 1195/? [16:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1195, loss 4.4267497062683105\n",
      "Epoch 2: |          | 1196/? [16:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1196, loss 4.586811542510986\n",
      "Epoch 2: |          | 1197/? [16:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1197, loss 4.451412200927734\n",
      "Epoch 2: |          | 1198/? [16:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1198, loss 4.501149654388428\n",
      "Epoch 2: |          | 1199/? [16:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1199, loss 4.71053409576416\n",
      "Epoch 2: |          | 1200/? [16:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1200, loss 3.891496181488037\n",
      "Epoch 2: |          | 1201/? [16:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1201, loss 4.622511863708496\n",
      "Epoch 2: |          | 1202/? [16:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1202, loss 4.178099155426025\n",
      "Epoch 2: |          | 1203/? [16:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1203, loss 4.233218193054199\n",
      "Epoch 2: |          | 1204/? [16:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1204, loss 3.6705939769744873\n",
      "Epoch 2: |          | 1205/? [16:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1205, loss 4.343374252319336\n",
      "Epoch 2: |          | 1206/? [16:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1206, loss 4.351314544677734\n",
      "Epoch 2: |          | 1207/? [16:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1207, loss 4.720580101013184\n",
      "Epoch 2: |          | 1208/? [16:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1208, loss 4.874154090881348\n",
      "Epoch 2: |          | 1209/? [16:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1209, loss 4.410487174987793\n",
      "Epoch 2: |          | 1210/? [16:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1210, loss 4.771679878234863\n",
      "Epoch 2: |          | 1211/? [16:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1211, loss 4.748750686645508\n",
      "Epoch 2: |          | 1212/? [16:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1212, loss 4.505726337432861\n",
      "Epoch 2: |          | 1213/? [16:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1213, loss 4.216721057891846\n",
      "Epoch 2: |          | 1214/? [16:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1214, loss 4.921067237854004\n",
      "Epoch 2: |          | 1215/? [16:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1215, loss 4.157082557678223\n",
      "Epoch 2: |          | 1216/? [16:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1216, loss 4.487854957580566\n",
      "Epoch 2: |          | 1217/? [16:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1217, loss 4.572502613067627\n",
      "Epoch 2: |          | 1218/? [16:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1218, loss 4.63492488861084\n",
      "Epoch 2: |          | 1219/? [16:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1219, loss 4.1793904304504395\n",
      "Epoch 2: |          | 1220/? [16:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1220, loss 4.987340450286865\n",
      "Epoch 2: |          | 1221/? [16:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1221, loss 4.664188861846924\n",
      "Epoch 2: |          | 1222/? [16:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1222, loss 3.4937801361083984\n",
      "Epoch 2: |          | 1223/? [16:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1223, loss 3.641552448272705\n",
      "Epoch 2: |          | 1224/? [16:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1224, loss 4.07878303527832\n",
      "Epoch 2: |          | 1225/? [16:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1225, loss 4.784263610839844\n",
      "Epoch 2: |          | 1226/? [16:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1226, loss 4.803679466247559\n",
      "Epoch 2: |          | 1227/? [16:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1227, loss 4.355397701263428\n",
      "Epoch 2: |          | 1228/? [16:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1228, loss 4.370891094207764\n",
      "Epoch 2: |          | 1229/? [16:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1229, loss 3.8300914764404297\n",
      "Epoch 2: |          | 1230/? [16:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1230, loss 4.533679008483887\n",
      "Epoch 2: |          | 1231/? [16:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1231, loss 4.539102077484131\n",
      "Epoch 2: |          | 1232/? [16:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1232, loss 4.714097023010254\n",
      "Epoch 2: |          | 1233/? [16:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1233, loss 4.563258647918701\n",
      "Epoch 2: |          | 1234/? [16:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1234, loss 3.4329700469970703\n",
      "Epoch 2: |          | 1235/? [16:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1235, loss 4.577902317047119\n",
      "Epoch 2: |          | 1236/? [16:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1236, loss 3.995121479034424\n",
      "Epoch 2: |          | 1237/? [16:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1237, loss 4.388911724090576\n",
      "Epoch 2: |          | 1238/? [16:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1238, loss 4.319396018981934\n",
      "Epoch 2: |          | 1239/? [16:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1239, loss 4.2449541091918945\n",
      "Epoch 2: |          | 1240/? [16:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1240, loss 4.906197547912598\n",
      "Epoch 2: |          | 1241/? [16:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1241, loss 4.364250183105469\n",
      "Epoch 2: |          | 1242/? [16:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1242, loss 4.277544975280762\n",
      "Epoch 2: |          | 1243/? [16:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1243, loss 4.02184534072876\n",
      "Epoch 2: |          | 1244/? [16:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1244, loss 4.213362693786621\n",
      "Epoch 2: |          | 1245/? [17:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1245, loss 3.7293381690979004\n",
      "Epoch 2: |          | 1246/? [17:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1246, loss 4.564471244812012\n",
      "Epoch 2: |          | 1247/? [17:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1247, loss 4.600107669830322\n",
      "Epoch 2: |          | 1248/? [17:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1248, loss 4.026019096374512\n",
      "Epoch 2: |          | 1249/? [17:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1249, loss 4.234807014465332\n",
      "Epoch 2: |          | 1250/? [17:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1250, loss 4.407292366027832\n",
      "Epoch 2: |          | 1251/? [17:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1251, loss 4.135793209075928\n",
      "Epoch 2: |          | 1252/? [17:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1252, loss 4.912292003631592\n",
      "Epoch 2: |          | 1253/? [17:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1253, loss 4.26681661605835\n",
      "Epoch 2: |          | 1254/? [17:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1254, loss 3.560028076171875\n",
      "Epoch 2: |          | 1255/? [17:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1255, loss 5.002249717712402\n",
      "Epoch 2: |          | 1256/? [17:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1256, loss 4.004047870635986\n",
      "Epoch 2: |          | 1257/? [17:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1257, loss 4.044042110443115\n",
      "Epoch 2: |          | 1258/? [17:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1258, loss 4.760647773742676\n",
      "Epoch 2: |          | 1259/? [17:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1259, loss 4.5112833976745605\n",
      "Epoch 2: |          | 1260/? [17:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1260, loss 4.878036975860596\n",
      "Epoch 2: |          | 1261/? [17:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1261, loss 4.290363311767578\n",
      "Epoch 2: |          | 1262/? [17:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1262, loss 4.149469375610352\n",
      "Epoch 2: |          | 1263/? [17:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1263, loss 4.640280723571777\n",
      "Epoch 2: |          | 1264/? [17:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1264, loss 4.871429443359375\n",
      "Epoch 2: |          | 1265/? [17:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1265, loss 4.634360313415527\n",
      "Epoch 2: |          | 1266/? [17:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1266, loss 4.270561218261719\n",
      "Epoch 2: |          | 1267/? [17:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1267, loss 4.398251533508301\n",
      "Epoch 2: |          | 1268/? [17:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1268, loss 4.248644828796387\n",
      "Epoch 2: |          | 1269/? [17:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1269, loss 3.7577311992645264\n",
      "Epoch 2: |          | 1270/? [17:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1270, loss 4.170784950256348\n",
      "Epoch 2: |          | 1271/? [17:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1271, loss 4.355884552001953\n",
      "Epoch 2: |          | 1272/? [17:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1272, loss 3.8515350818634033\n",
      "Epoch 2: |          | 1273/? [17:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1273, loss 4.726720333099365\n",
      "Epoch 2: |          | 1274/? [17:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1274, loss 3.4820923805236816\n",
      "Epoch 2: |          | 1275/? [17:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1275, loss 4.014825820922852\n",
      "Epoch 2: |          | 1276/? [17:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1276, loss 4.42282772064209\n",
      "Epoch 2: |          | 1277/? [17:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1277, loss 4.059835910797119\n",
      "Epoch 2: |          | 1278/? [17:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1278, loss 4.0652666091918945\n",
      "Epoch 2: |          | 1279/? [17:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1279, loss 4.547149658203125\n",
      "Epoch 2: |          | 1280/? [17:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1280, loss 3.6901488304138184\n",
      "Epoch 2: |          | 1281/? [17:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1281, loss 4.129701614379883\n",
      "Epoch 2: |          | 1282/? [17:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1282, loss 3.862825393676758\n",
      "Epoch 2: |          | 1283/? [17:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1283, loss 4.686942100524902\n",
      "Epoch 2: |          | 1284/? [17:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1284, loss 3.6724300384521484\n",
      "Epoch 2: |          | 1285/? [17:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1285, loss 4.863766670227051\n",
      "Epoch 2: |          | 1286/? [17:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1286, loss 3.2397594451904297\n",
      "Epoch 2: |          | 1287/? [17:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1287, loss 4.696196556091309\n",
      "Epoch 2: |          | 1288/? [17:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1288, loss 4.468972682952881\n",
      "Epoch 2: |          | 1289/? [17:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1289, loss 3.4769020080566406\n",
      "Epoch 2: |          | 1290/? [17:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1290, loss 4.500475883483887\n",
      "Epoch 2: |          | 1291/? [17:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1291, loss 5.3660688400268555\n",
      "Epoch 2: |          | 1292/? [17:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1292, loss 4.780681133270264\n",
      "Epoch 2: |          | 1293/? [17:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1293, loss 4.133805274963379\n",
      "Epoch 2: |          | 1294/? [17:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1294, loss 4.358592987060547\n",
      "Epoch 2: |          | 1295/? [17:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1295, loss 4.410742282867432\n",
      "Epoch 2: |          | 1296/? [17:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1296, loss 3.589632749557495\n",
      "Epoch 2: |          | 1297/? [17:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1297, loss 4.6541032791137695\n",
      "Epoch 2: |          | 1298/? [17:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1298, loss 4.36708402633667\n",
      "Epoch 2: |          | 1299/? [17:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1299, loss 3.371147632598877\n",
      "Epoch 2: |          | 1300/? [17:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1300, loss 4.381132125854492\n",
      "Epoch 2: |          | 1301/? [17:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1301, loss 4.124114990234375\n",
      "Epoch 2: |          | 1302/? [17:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1302, loss 4.294500350952148\n",
      "Epoch 2: |          | 1303/? [17:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1303, loss 4.069482326507568\n",
      "Epoch 2: |          | 1304/? [17:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1304, loss 4.9352827072143555\n",
      "Epoch 2: |          | 1305/? [17:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1305, loss 3.677931547164917\n",
      "Epoch 2: |          | 1306/? [17:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1306, loss 4.340729713439941\n",
      "Epoch 2: |          | 1307/? [17:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1307, loss 3.854860782623291\n",
      "Epoch 2: |          | 1308/? [17:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1308, loss 3.8306992053985596\n",
      "Epoch 2: |          | 1309/? [17:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1309, loss 3.974950075149536\n",
      "Epoch 2: |          | 1310/? [17:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1310, loss 4.482733249664307\n",
      "Epoch 2: |          | 1311/? [17:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1311, loss 3.8899753093719482\n",
      "Epoch 2: |          | 1312/? [17:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1312, loss 3.639007091522217\n",
      "Epoch 2: |          | 1313/? [17:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1313, loss 4.853976726531982\n",
      "Epoch 2: |          | 1314/? [17:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1314, loss 4.008820533752441\n",
      "Epoch 2: |          | 1315/? [17:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1315, loss 4.82772159576416\n",
      "Epoch 2: |          | 1316/? [17:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1316, loss 4.540806293487549\n",
      "Epoch 2: |          | 1317/? [18:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1317, loss 4.152449607849121\n",
      "Epoch 2: |          | 1318/? [18:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1318, loss 4.371630668640137\n",
      "Epoch 2: |          | 1319/? [18:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1319, loss 4.563085079193115\n",
      "Epoch 2: |          | 1320/? [18:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1320, loss 4.107637882232666\n",
      "Epoch 2: |          | 1321/? [18:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1321, loss 4.524421691894531\n",
      "Epoch 2: |          | 1322/? [18:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1322, loss 4.587114334106445\n",
      "Epoch 2: |          | 1323/? [18:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1323, loss 3.978394031524658\n",
      "Epoch 2: |          | 1324/? [18:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1324, loss 4.914609432220459\n",
      "Epoch 2: |          | 1325/? [18:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1325, loss 5.007903575897217\n",
      "Epoch 2: |          | 1326/? [18:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1326, loss 4.43018913269043\n",
      "Epoch 2: |          | 1327/? [18:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1327, loss 4.280578136444092\n",
      "Epoch 2: |          | 1328/? [18:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1328, loss 3.957720994949341\n",
      "Epoch 2: |          | 1329/? [18:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1329, loss 4.6814680099487305\n",
      "Epoch 2: |          | 1330/? [18:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1330, loss 4.388702869415283\n",
      "Epoch 2: |          | 1331/? [18:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1331, loss 4.4093732833862305\n",
      "Epoch 2: |          | 1332/? [18:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1332, loss 4.224194526672363\n",
      "Epoch 2: |          | 1333/? [18:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1333, loss 4.113650798797607\n",
      "Epoch 2: |          | 1334/? [18:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1334, loss 4.25785493850708\n",
      "Epoch 2: |          | 1335/? [18:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1335, loss 4.240940570831299\n",
      "Epoch 2: |          | 1336/? [18:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1336, loss 3.799853563308716\n",
      "Epoch 2: |          | 1337/? [18:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1337, loss 4.51798677444458\n",
      "Epoch 2: |          | 1338/? [18:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1338, loss 3.6210780143737793\n",
      "Epoch 2: |          | 1339/? [18:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1339, loss 4.287733554840088\n",
      "Epoch 2: |          | 1340/? [18:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1340, loss 3.6666557788848877\n",
      "Epoch 2: |          | 1341/? [18:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1341, loss 4.541486740112305\n",
      "Epoch 2: |          | 1342/? [18:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1342, loss 4.761551856994629\n",
      "Epoch 2: |          | 1343/? [18:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1343, loss 4.208862781524658\n",
      "Epoch 2: |          | 1344/? [18:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1344, loss 4.320961952209473\n",
      "Epoch 2: |          | 1345/? [18:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1345, loss 4.4408063888549805\n",
      "Epoch 2: |          | 1346/? [18:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1346, loss 5.653810501098633\n",
      "Epoch 2: |          | 1347/? [18:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1347, loss 4.734902858734131\n",
      "Epoch 2: |          | 1348/? [18:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1348, loss 4.956812858581543\n",
      "Epoch 2: |          | 1349/? [18:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1349, loss 4.586859703063965\n",
      "Epoch 2: |          | 1350/? [18:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1350, loss 4.900384902954102\n",
      "Epoch 2: |          | 1351/? [18:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1351, loss 4.668387413024902\n",
      "Epoch 2: |          | 1352/? [18:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1352, loss 3.718170642852783\n",
      "Epoch 2: |          | 1353/? [18:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1353, loss 4.018923759460449\n",
      "Epoch 2: |          | 1354/? [18:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1354, loss 4.614772796630859\n",
      "Epoch 2: |          | 1355/? [18:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1355, loss 4.781466484069824\n",
      "Epoch 2: |          | 1356/? [18:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1356, loss 4.4244256019592285\n",
      "Epoch 2: |          | 1357/? [18:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1357, loss 4.157501697540283\n",
      "Epoch 2: |          | 1358/? [18:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1358, loss 4.388779163360596\n",
      "Epoch 2: |          | 1359/? [18:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1359, loss 4.241225719451904\n",
      "Epoch 2: |          | 1360/? [18:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1360, loss 4.473511219024658\n",
      "Epoch 2: |          | 1361/? [18:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1361, loss 4.372436046600342\n",
      "Epoch 2: |          | 1362/? [18:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1362, loss 4.25175142288208\n",
      "Epoch 2: |          | 1363/? [18:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1363, loss 3.6269774436950684\n",
      "Epoch 2: |          | 1364/? [18:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1364, loss 4.229775905609131\n",
      "Epoch 2: |          | 1365/? [18:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1365, loss 3.8403849601745605\n",
      "Epoch 2: |          | 1366/? [18:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1366, loss 4.585841655731201\n",
      "Epoch 2: |          | 1367/? [18:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1367, loss 3.8786416053771973\n",
      "Epoch 2: |          | 1368/? [18:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1368, loss 3.6262316703796387\n",
      "Epoch 2: |          | 1369/? [18:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1369, loss 4.359516143798828\n",
      "Epoch 2: |          | 1370/? [18:43<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1370, loss 3.846867799758911\n",
      "Epoch 2: |          | 1371/? [18:44<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1371, loss 4.870732307434082\n",
      "Epoch 2: |          | 1372/? [18:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1372, loss 4.223106861114502\n",
      "Epoch 2: |          | 1373/? [18:45<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1373, loss 4.77026891708374\n",
      "Epoch 2: |          | 1374/? [18:46<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1374, loss 3.7681782245635986\n",
      "Epoch 2: |          | 1375/? [18:47<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1375, loss 4.418498992919922\n",
      "Epoch 2: |          | 1376/? [18:48<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1376, loss 4.369946002960205\n",
      "Epoch 2: |          | 1377/? [18:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1377, loss 4.259655952453613\n",
      "Epoch 2: |          | 1378/? [18:49<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1378, loss 4.50070333480835\n",
      "Epoch 2: |          | 1379/? [18:50<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1379, loss 4.3777360916137695\n",
      "Epoch 2: |          | 1380/? [18:51<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1380, loss 4.461213111877441\n",
      "Epoch 2: |          | 1381/? [18:52<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1381, loss 4.613335609436035\n",
      "Epoch 2: |          | 1382/? [18:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1382, loss 4.066102027893066\n",
      "Epoch 2: |          | 1383/? [18:53<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1383, loss 4.365595817565918\n",
      "Epoch 2: |          | 1384/? [18:54<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1384, loss 4.3653483390808105\n",
      "Epoch 2: |          | 1385/? [18:55<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1385, loss 4.285647392272949\n",
      "Epoch 2: |          | 1386/? [18:56<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1386, loss 4.294188499450684\n",
      "Epoch 2: |          | 1387/? [18:57<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1387, loss 4.319847583770752\n",
      "Epoch 2: |          | 1388/? [18:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1388, loss 3.6803641319274902\n",
      "Epoch 2: |          | 1389/? [18:58<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1389, loss 4.579895496368408\n",
      "Epoch 2: |          | 1390/? [18:59<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1390, loss 4.849148750305176\n",
      "Epoch 2: |          | 1391/? [19:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1391, loss 4.489930152893066\n",
      "Epoch 2: |          | 1392/? [19:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1392, loss 3.9283077716827393\n",
      "Epoch 2: |          | 1393/? [19:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1393, loss 4.216729164123535\n",
      "Epoch 2: |          | 1394/? [19:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1394, loss 3.8748068809509277\n",
      "Epoch 2: |          | 1395/? [19:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1395, loss 4.4854416847229\n",
      "Epoch 2: |          | 1396/? [19:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1396, loss 4.416601181030273\n",
      "Epoch 2: |          | 1397/? [19:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1397, loss 3.543832302093506\n",
      "Epoch 2: |          | 1398/? [19:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1398, loss 4.770864486694336\n",
      "Epoch 2: |          | 1399/? [19:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1399, loss 4.833308219909668\n",
      "Epoch 2: |          | 1400/? [19:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1400, loss 3.842766523361206\n",
      "Epoch 2: |          | 1401/? [19:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1401, loss 4.797503471374512\n",
      "Epoch 2: |          | 1402/? [19:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1402, loss 4.299987316131592\n",
      "Epoch 2: |          | 1403/? [19:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1403, loss 4.487840175628662\n",
      "Epoch 2: |          | 1404/? [19:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1404, loss 4.460677146911621\n",
      "Epoch 2: |          | 1405/? [19:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1405, loss 4.852019309997559\n",
      "Epoch 2: |          | 1406/? [19:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1406, loss 4.72577428817749\n",
      "Epoch 2: |          | 1407/? [19:13<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1407, loss 4.885588645935059\n",
      "Epoch 2: |          | 1408/? [19:14<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1408, loss 4.044234752655029\n",
      "Epoch 2: |          | 1409/? [19:15<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1409, loss 4.122881889343262\n",
      "Epoch 2: |          | 1410/? [19:16<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1410, loss 4.245197296142578\n",
      "Epoch 2: |          | 1411/? [19:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1411, loss 4.602222442626953\n",
      "Epoch 2: |          | 1412/? [19:17<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1412, loss 4.061360836029053\n",
      "Epoch 2: |          | 1413/? [19:18<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1413, loss 3.902007579803467\n",
      "Epoch 2: |          | 1414/? [19:19<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1414, loss 4.036028861999512\n",
      "Epoch 2: |          | 1415/? [19:20<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1415, loss 4.385667324066162\n",
      "Epoch 2: |          | 1416/? [19:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1416, loss 4.788912296295166\n",
      "Epoch 2: |          | 1417/? [19:21<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1417, loss 4.3446197509765625\n",
      "Epoch 2: |          | 1418/? [19:22<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1418, loss 4.566930770874023\n",
      "Epoch 2: |          | 1419/? [19:23<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1419, loss 4.237236976623535\n",
      "Epoch 2: |          | 1420/? [19:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1420, loss 4.166474342346191\n",
      "Epoch 2: |          | 1421/? [19:24<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1421, loss 3.7473671436309814\n",
      "Epoch 2: |          | 1422/? [19:25<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1422, loss 4.6538872718811035\n",
      "Epoch 2: |          | 1423/? [19:26<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1423, loss 4.681572437286377\n",
      "Epoch 2: |          | 1424/? [19:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1424, loss 4.195863723754883\n",
      "Epoch 2: |          | 1425/? [19:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1425, loss 4.507157325744629\n",
      "Epoch 2: |          | 1426/? [19:29<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1426, loss 4.002800941467285\n",
      "Epoch 2: |          | 1427/? [19:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1427, loss 4.6503801345825195\n",
      "Epoch 2: |          | 1428/? [19:30<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1428, loss 4.625974178314209\n",
      "Epoch 2: |          | 1429/? [19:31<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1429, loss 4.540524959564209\n",
      "Epoch 2: |          | 1430/? [19:32<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1430, loss 4.653471946716309\n",
      "Epoch 2: |          | 1431/? [19:33<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1431, loss 4.394657135009766\n",
      "Epoch 2: |          | 1432/? [19:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1432, loss 4.371960639953613\n",
      "Epoch 2: |          | 1433/? [19:34<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1433, loss 4.306950569152832\n",
      "Epoch 2: |          | 1434/? [19:35<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1434, loss 4.456673622131348\n",
      "Epoch 2: |          | 1435/? [19:36<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1435, loss 4.011685848236084\n",
      "Epoch 2: |          | 1436/? [19:37<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1436, loss 4.34838342666626\n",
      "Epoch 2: |          | 1437/? [19:38<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1437, loss 3.635817766189575\n",
      "Epoch 2: |          | 1438/? [19:39<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1438, loss 5.39127254486084\n",
      "Epoch 2: |          | 1439/? [19:40<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1439, loss 4.452869892120361\n",
      "Epoch 2: |          | 1440/? [19:41<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1440, loss 4.5240702629089355\n",
      "Epoch 2: |          | 1441/? [19:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1441, loss 4.881832122802734\n",
      "Epoch 2: |          | 1442/? [19:42<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 1442, loss 4.891711711883545\n",
      "Epoch 2: |          | 1443/? [19:49<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1443, loss 3.946157932281494\n",
      "Epoch 2: |          | 1444/? [19:50<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1444, loss 4.067008018493652\n",
      "Epoch 2: |          | 1445/? [19:50<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1445, loss 4.654345989227295\n",
      "Epoch 2: |          | 1446/? [19:51<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1446, loss 4.1532979011535645\n",
      "Epoch 2: |          | 1447/? [19:52<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1447, loss 4.32985782623291\n",
      "Epoch 2: |          | 1448/? [19:53<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1448, loss 4.124926567077637\n",
      "Epoch 2: |          | 1449/? [19:53<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1449, loss 4.430748462677002\n",
      "Epoch 2: |          | 1450/? [19:54<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1450, loss 4.524994373321533\n",
      "Epoch 2: |          | 1451/? [19:55<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1451, loss 4.879958152770996\n",
      "Epoch 2: |          | 1452/? [19:56<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1452, loss 4.421557903289795\n",
      "Epoch 2: |          | 1453/? [19:56<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1453, loss 3.6759250164031982\n",
      "Epoch 2: |          | 1454/? [19:57<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1454, loss 4.284952163696289\n",
      "Epoch 2: |          | 1455/? [19:58<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1455, loss 4.472413539886475\n",
      "Epoch 2: |          | 1456/? [19:59<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1456, loss 3.9523110389709473\n",
      "Epoch 2: |          | 1457/? [20:00<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1457, loss 4.151336669921875\n",
      "Epoch 2: |          | 1458/? [20:00<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1458, loss 4.264910697937012\n",
      "Epoch 2: |          | 1459/? [20:01<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1459, loss 4.56845235824585\n",
      "Epoch 2: |          | 1460/? [20:02<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1460, loss 4.392141819000244\n",
      "Epoch 2: |          | 1461/? [20:03<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1461, loss 4.446527004241943\n",
      "Epoch 2: |          | 1462/? [20:04<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1462, loss 4.750514984130859\n",
      "Epoch 2: |          | 1463/? [20:04<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1463, loss 4.653408527374268\n",
      "Epoch 2: |          | 1464/? [20:05<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1464, loss 3.9383156299591064\n",
      "Epoch 2: |          | 1465/? [20:06<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1465, loss 4.322937488555908\n",
      "Epoch 2: |          | 1466/? [20:07<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1466, loss 3.947291612625122\n",
      "Epoch 2: |          | 1467/? [20:08<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1467, loss 4.701404094696045\n",
      "Epoch 2: |          | 1468/? [20:09<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1468, loss 4.264496803283691\n",
      "Epoch 2: |          | 1469/? [20:09<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1469, loss 3.868248462677002\n",
      "Epoch 2: |          | 1470/? [20:10<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1470, loss 4.511791229248047\n",
      "Epoch 2: |          | 1471/? [20:11<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1471, loss 4.685269355773926\n",
      "Epoch 2: |          | 1472/? [20:12<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1472, loss 4.423272132873535\n",
      "Epoch 2: |          | 1473/? [20:13<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1473, loss 4.195530891418457\n",
      "Epoch 2: |          | 1474/? [20:14<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1474, loss 4.130814552307129\n",
      "Epoch 2: |          | 1475/? [20:15<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1475, loss 3.7644333839416504\n",
      "Epoch 2: |          | 1476/? [20:15<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1476, loss 4.343255519866943\n",
      "Epoch 2: |          | 1477/? [20:16<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1477, loss 4.385036468505859\n",
      "Epoch 2: |          | 1478/? [20:17<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1478, loss 4.227222442626953\n",
      "Epoch 2: |          | 1479/? [20:18<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1479, loss 4.8397932052612305\n",
      "Epoch 2: |          | 1480/? [20:19<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1480, loss 4.572273254394531\n",
      "Epoch 2: |          | 1481/? [20:20<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1481, loss 4.2797532081604\n",
      "Epoch 2: |          | 1482/? [20:21<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1482, loss 4.394524097442627\n",
      "Epoch 2: |          | 1483/? [20:21<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1483, loss 3.9776089191436768\n",
      "Epoch 2: |          | 1484/? [20:22<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1484, loss 4.187402248382568\n",
      "Epoch 2: |          | 1485/? [20:23<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1485, loss 4.443330764770508\n",
      "Epoch 2: |          | 1486/? [20:24<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1486, loss 4.357519626617432\n",
      "Epoch 2: |          | 1487/? [20:25<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1487, loss 3.8299880027770996\n",
      "Epoch 2: |          | 1488/? [20:25<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1488, loss 4.527907848358154\n",
      "Epoch 2: |          | 1489/? [20:26<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1489, loss 4.453064441680908\n",
      "Epoch 2: |          | 1490/? [20:27<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1490, loss 4.3082427978515625\n",
      "Epoch 2: |          | 1491/? [20:28<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1491, loss 3.198601007461548\n",
      "Epoch 2: |          | 1492/? [20:29<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1492, loss 3.880479097366333\n",
      "Epoch 2: |          | 1493/? [20:29<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1493, loss 3.66182017326355\n",
      "Epoch 2: |          | 1494/? [20:30<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1494, loss 4.290759086608887\n",
      "Epoch 2: |          | 1495/? [20:31<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1495, loss 4.188368797302246\n",
      "Epoch 2: |          | 1496/? [20:32<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1496, loss 4.473134517669678\n",
      "Epoch 2: |          | 1497/? [20:33<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1497, loss 3.664829969406128\n",
      "Epoch 2: |          | 1498/? [20:33<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1498, loss 4.060378551483154\n",
      "Epoch 2: |          | 1499/? [20:34<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1499, loss 4.669186115264893\n",
      "Epoch 2: |          | 1500/? [20:35<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1500, loss 4.543169021606445\n",
      "Epoch 2: |          | 1501/? [20:36<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1501, loss 4.366800785064697\n",
      "Epoch 2: |          | 1502/? [20:37<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1502, loss 4.476832389831543\n",
      "Epoch 2: |          | 1503/? [20:38<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1503, loss 4.129905700683594\n",
      "Epoch 2: |          | 1504/? [20:38<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1504, loss 4.798554420471191\n",
      "Epoch 2: |          | 1505/? [20:39<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1505, loss 4.6969194412231445\n",
      "Epoch 2: |          | 1506/? [20:40<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1506, loss 4.276986122131348\n",
      "Epoch 2: |          | 1507/? [20:41<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1507, loss 4.130878448486328\n",
      "Epoch 2: |          | 1508/? [20:42<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1508, loss 4.309214115142822\n",
      "Epoch 2: |          | 1509/? [20:42<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1509, loss 4.220850944519043\n",
      "Epoch 2: |          | 1510/? [20:43<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1510, loss 4.540842533111572\n",
      "Epoch 2: |          | 1511/? [20:44<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1511, loss 4.047645568847656\n",
      "Epoch 2: |          | 1512/? [20:45<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1512, loss 4.815706253051758\n",
      "Epoch 2: |          | 1513/? [20:45<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1513, loss 4.8663506507873535\n",
      "Epoch 2: |          | 1514/? [20:46<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1514, loss 3.939671039581299\n",
      "Epoch 2: |          | 1515/? [20:47<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1515, loss 4.928125858306885\n",
      "Epoch 2: |          | 1516/? [20:48<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1516, loss 4.699163913726807\n",
      "Epoch 2: |          | 1517/? [20:49<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1517, loss 4.183969974517822\n",
      "Epoch 2: |          | 1518/? [20:50<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1518, loss 3.980288028717041\n",
      "Epoch 2: |          | 1519/? [20:50<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1519, loss 4.529593467712402\n",
      "Epoch 2: |          | 1520/? [20:51<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1520, loss 4.741353511810303\n",
      "Epoch 2: |          | 1521/? [20:52<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1521, loss 4.302702903747559\n",
      "Epoch 2: |          | 1522/? [20:53<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1522, loss 4.012894630432129\n",
      "Epoch 2: |          | 1523/? [20:53<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1523, loss 4.392690181732178\n",
      "Epoch 2: |          | 1524/? [20:54<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1524, loss 4.303393840789795\n",
      "Epoch 2: |          | 1525/? [20:55<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1525, loss 4.124954700469971\n",
      "Epoch 2: |          | 1526/? [20:56<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1526, loss 4.570723533630371\n",
      "Epoch 2: |          | 1527/? [20:57<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 1527, loss 4.5816826820373535\n",
      "Epoch 2: |          | 1528/? [20:57<00:00,  1.21it/s, v_num=30]ERROR: Input has inproper shape\n",
      "Epoch 2: |          | 1529/? [20:57<00:00,  1.22it/s, v_num=30]   VALIDATION: Batch 0, loss 4.864780426025391\n",
      "   VALIDATION: Batch 1, loss 3.777040958404541\n",
      "   VALIDATION: Batch 2, loss 5.0117292404174805\n",
      "   VALIDATION: Batch 3, loss 4.684230327606201\n",
      "   VALIDATION: Batch 4, loss 4.281224727630615\n",
      "   VALIDATION: Batch 5, loss 3.891690492630005\n",
      "   VALIDATION: Batch 6, loss 4.209717273712158\n",
      "   VALIDATION: Batch 7, loss 4.851587772369385\n",
      "   VALIDATION: Batch 8, loss 4.775978088378906\n",
      "   VALIDATION: Batch 9, loss 4.811257362365723\n",
      "   VALIDATION: Batch 10, loss 4.597514629364014\n",
      "   VALIDATION: Batch 11, loss 4.164225101470947\n",
      "   VALIDATION: Batch 12, loss 4.534858226776123\n",
      "   VALIDATION: Batch 13, loss 4.951809883117676\n",
      "   VALIDATION: Batch 14, loss 4.400264739990234\n",
      "   VALIDATION: Batch 15, loss 4.238429069519043\n",
      "   VALIDATION: Batch 16, loss 4.793056011199951\n",
      "   VALIDATION: Batch 17, loss 4.451437473297119\n",
      "   VALIDATION: Batch 18, loss 3.7220470905303955\n",
      "   VALIDATION: Batch 19, loss 4.727627754211426\n",
      "   VALIDATION: Batch 20, loss 4.9248576164245605\n",
      "   VALIDATION: Batch 21, loss 5.12468147277832\n",
      "   VALIDATION: Batch 22, loss 4.808050155639648\n",
      "   VALIDATION: Batch 23, loss 4.304357051849365\n",
      "   VALIDATION: Batch 24, loss 4.188529968261719\n",
      "   VALIDATION: Batch 25, loss 4.62860631942749\n",
      "   VALIDATION: Batch 26, loss 4.835744857788086\n",
      "   VALIDATION: Batch 27, loss 4.743190765380859\n",
      "   VALIDATION: Batch 28, loss 4.4283061027526855\n",
      "   VALIDATION: Batch 29, loss 4.735573768615723\n",
      "   VALIDATION: Batch 30, loss 4.23370885848999\n",
      "   VALIDATION: Batch 31, loss 4.590415954589844\n",
      "   VALIDATION: Batch 32, loss 5.163102149963379\n",
      "   VALIDATION: Batch 33, loss 3.303022861480713\n",
      "   VALIDATION: Batch 34, loss 4.531968116760254\n",
      "   VALIDATION: Batch 35, loss 4.748242378234863\n",
      "   VALIDATION: Batch 36, loss 4.216334819793701\n",
      "   VALIDATION: Batch 37, loss 4.067098140716553\n",
      "   VALIDATION: Batch 38, loss 4.137002468109131\n",
      "   VALIDATION: Batch 39, loss 4.544477939605713\n",
      "   VALIDATION: Batch 40, loss 4.6198344230651855\n",
      "   VALIDATION: Batch 41, loss 3.4928061962127686\n",
      "   VALIDATION: Batch 42, loss 4.659964561462402\n",
      "   VALIDATION: Batch 43, loss 4.78109884262085\n",
      "   VALIDATION: Batch 44, loss 4.325690269470215\n",
      "   VALIDATION: Batch 45, loss 4.836331367492676\n",
      "   VALIDATION: Batch 46, loss 3.940608501434326\n",
      "   VALIDATION: Batch 47, loss 4.948326110839844\n",
      "   VALIDATION: Batch 48, loss 5.015505313873291\n",
      "   VALIDATION: Batch 49, loss 4.669928550720215\n",
      "   VALIDATION: Batch 50, loss 4.622058391571045\n",
      "   VALIDATION: Batch 51, loss 5.093502521514893\n",
      "   VALIDATION: Batch 52, loss 4.190707206726074\n",
      "   VALIDATION: Batch 53, loss 4.13956356048584\n",
      "   VALIDATION: Batch 54, loss 4.199165344238281\n",
      "   VALIDATION: Batch 55, loss 5.082755088806152\n",
      "   VALIDATION: Batch 56, loss 4.395072937011719\n",
      "   VALIDATION: Batch 57, loss 5.6883416175842285\n",
      "   VALIDATION: Batch 58, loss 4.4702911376953125\n",
      "   VALIDATION: Batch 59, loss 4.146646022796631\n",
      "   VALIDATION: Batch 60, loss 3.6370067596435547\n",
      "   VALIDATION: Batch 61, loss 4.521389484405518\n",
      "   VALIDATION: Batch 62, loss 4.533890724182129\n",
      "   VALIDATION: Batch 63, loss 5.009932518005371\n",
      "   VALIDATION: Batch 64, loss 4.827635765075684\n",
      "   VALIDATION: Batch 65, loss 3.9454352855682373\n",
      "   VALIDATION: Batch 66, loss 4.84844446182251\n",
      "   VALIDATION: Batch 67, loss 4.27998161315918\n",
      "   VALIDATION: Batch 68, loss 4.460714340209961\n",
      "   VALIDATION: Batch 69, loss 4.74215030670166\n",
      "   VALIDATION: Batch 70, loss 4.8626017570495605\n",
      "   VALIDATION: Batch 71, loss 4.35218620300293\n",
      "   VALIDATION: Batch 72, loss 5.268442153930664\n",
      "   VALIDATION: Batch 73, loss 4.016021728515625\n",
      "   VALIDATION: Batch 74, loss 4.693113803863525\n",
      "   VALIDATION: Batch 75, loss 4.7570977210998535\n",
      "   VALIDATION: Batch 76, loss 4.625897407531738\n",
      "   VALIDATION: Batch 77, loss 4.8075480461120605\n",
      "   VALIDATION: Batch 78, loss 4.634012699127197\n",
      "   VALIDATION: Batch 79, loss 4.5375566482543945\n",
      "   VALIDATION: Batch 80, loss 4.693007946014404\n",
      "   VALIDATION: Batch 81, loss 4.442025184631348\n",
      "   VALIDATION: Batch 82, loss 4.774260520935059\n",
      "   VALIDATION: Batch 83, loss 4.094326972961426\n",
      "   VALIDATION: Batch 84, loss 4.7686991691589355\n",
      "   VALIDATION: Batch 85, loss 4.555881023406982\n",
      "   VALIDATION: Batch 86, loss 4.483712673187256\n",
      "   VALIDATION: Batch 87, loss 4.339285850524902\n",
      "   VALIDATION: Batch 88, loss 3.922365665435791\n",
      "   VALIDATION: Batch 89, loss 4.240110874176025\n",
      "   VALIDATION: Batch 90, loss 4.520724773406982\n",
      "   VALIDATION: Batch 91, loss 4.715727806091309\n",
      "   VALIDATION: Batch 92, loss 4.481961727142334\n",
      "   VALIDATION: Batch 93, loss 4.986469268798828\n",
      "   VALIDATION: Batch 94, loss 4.500385284423828\n",
      "   VALIDATION: Batch 95, loss 3.932060718536377\n",
      "   VALIDATION: Batch 96, loss 4.41422700881958\n",
      "   VALIDATION: Batch 97, loss 4.174242973327637\n",
      "   VALIDATION: Batch 98, loss 4.730624198913574\n",
      "   VALIDATION: Batch 99, loss 4.797443389892578\n",
      "   VALIDATION: Batch 100, loss 5.151719093322754\n",
      "   VALIDATION: Batch 101, loss 3.767005443572998\n",
      "   VALIDATION: Batch 102, loss 5.182657718658447\n",
      "   VALIDATION: Batch 103, loss 5.134173393249512\n",
      "   VALIDATION: Batch 104, loss 4.069637775421143\n",
      "   VALIDATION: Batch 105, loss 4.64528751373291\n",
      "   VALIDATION: Batch 106, loss 4.344834327697754\n",
      "   VALIDATION: Batch 107, loss 4.519248962402344\n",
      "   VALIDATION: Batch 108, loss 4.245396614074707\n",
      "   VALIDATION: Batch 109, loss 4.8439788818359375\n",
      "   VALIDATION: Batch 110, loss 4.630524158477783\n",
      "   VALIDATION: Batch 111, loss 4.9095234870910645\n",
      "   VALIDATION: Batch 112, loss 5.623807430267334\n",
      "   VALIDATION: Batch 113, loss 5.034787654876709\n",
      "   VALIDATION: Batch 114, loss 4.785604953765869\n",
      "   VALIDATION: Batch 115, loss 4.269799709320068\n",
      "   VALIDATION: Batch 116, loss 4.089685916900635\n",
      "   VALIDATION: Batch 117, loss 4.797640800476074\n",
      "   VALIDATION: Batch 118, loss 4.950953960418701\n",
      "   VALIDATION: Batch 119, loss 4.073493480682373\n",
      "   VALIDATION: Batch 120, loss 3.660533905029297\n",
      "   VALIDATION: Batch 121, loss 4.074923515319824\n",
      "   VALIDATION: Batch 122, loss 4.478233814239502\n",
      "   VALIDATION: Batch 123, loss 4.551784038543701\n",
      "   VALIDATION: Batch 124, loss 3.7918827533721924\n",
      "   VALIDATION: Batch 125, loss 4.4894256591796875\n",
      "   VALIDATION: Batch 126, loss 4.684377670288086\n",
      "   VALIDATION: Batch 127, loss 4.511749744415283\n",
      "   VALIDATION: Batch 128, loss 4.579821586608887\n",
      "   VALIDATION: Batch 129, loss 4.28236198425293\n",
      "   VALIDATION: Batch 130, loss 3.8392281532287598\n",
      "   VALIDATION: Batch 131, loss 3.836416244506836\n",
      "   VALIDATION: Batch 132, loss 4.4857048988342285\n",
      "   VALIDATION: Batch 133, loss 4.719484329223633\n",
      "   VALIDATION: Batch 134, loss 4.622947692871094\n",
      "   VALIDATION: Batch 135, loss 4.866465091705322\n",
      "   VALIDATION: Batch 136, loss 4.945986747741699\n",
      "   VALIDATION: Batch 137, loss 4.7735466957092285\n",
      "   VALIDATION: Batch 138, loss 4.514064788818359\n",
      "   VALIDATION: Batch 139, loss 4.929389476776123\n",
      "   VALIDATION: Batch 140, loss 3.9282310009002686\n",
      "   VALIDATION: Batch 141, loss 4.862882614135742\n",
      "   VALIDATION: Batch 142, loss 3.631132125854492\n",
      "   VALIDATION: Batch 143, loss 4.4851765632629395\n",
      "   VALIDATION: Batch 144, loss 4.73292350769043\n",
      "   VALIDATION: Batch 145, loss 4.494085788726807\n",
      "   VALIDATION: Batch 146, loss 4.315371513366699\n",
      "   VALIDATION: Batch 147, loss 4.7029266357421875\n",
      "   VALIDATION: Batch 148, loss 4.786602020263672\n",
      "   VALIDATION: Batch 149, loss 5.232746601104736\n",
      "   VALIDATION: Batch 150, loss 4.917841911315918\n",
      "   VALIDATION: Batch 151, loss 5.073705196380615\n",
      "   VALIDATION: Batch 152, loss 4.5145368576049805\n",
      "   VALIDATION: Batch 153, loss 4.700173377990723\n",
      "   VALIDATION: Batch 154, loss 4.582225799560547\n",
      "   VALIDATION: Batch 155, loss 4.276623725891113\n",
      "   VALIDATION: Batch 156, loss 4.9982709884643555\n",
      "   VALIDATION: Batch 157, loss 4.716536521911621\n",
      "   VALIDATION: Batch 158, loss 3.982849597930908\n",
      "   VALIDATION: Batch 159, loss 4.484323978424072\n",
      "   VALIDATION: Batch 160, loss 4.862886905670166\n",
      "   VALIDATION: Batch 161, loss 5.127534866333008\n",
      "   VALIDATION: Batch 162, loss 4.553730010986328\n",
      "   VALIDATION: Batch 163, loss 3.994873046875\n",
      "   VALIDATION: Batch 164, loss 4.4596052169799805\n",
      "   VALIDATION: Batch 165, loss 4.981485843658447\n",
      "   VALIDATION: Batch 166, loss 4.415640830993652\n",
      "   VALIDATION: Batch 167, loss 4.853494167327881\n",
      "   VALIDATION: Batch 168, loss 3.679630994796753\n",
      "   VALIDATION: Batch 169, loss 4.3263654708862305\n",
      "   VALIDATION: Batch 170, loss 4.573513507843018\n",
      "   VALIDATION: Batch 171, loss 4.651535987854004\n",
      "   VALIDATION: Batch 172, loss 4.523402214050293\n",
      "   VALIDATION: Batch 173, loss 4.463033676147461\n",
      "   VALIDATION: Batch 174, loss 4.873976707458496\n",
      "   VALIDATION: Batch 175, loss 4.608189105987549\n",
      "   VALIDATION: Batch 176, loss 4.366999626159668\n",
      "   VALIDATION: Batch 177, loss 4.622946739196777\n",
      "   VALIDATION: Batch 178, loss 5.419628620147705\n",
      "   VALIDATION: Batch 179, loss 4.6752119064331055\n",
      "   VALIDATION: Batch 180, loss 4.264665603637695\n",
      "   VALIDATION: Batch 181, loss 4.400505065917969\n",
      "   VALIDATION: Batch 182, loss 4.623992919921875\n",
      "   VALIDATION: Batch 183, loss 3.657932758331299\n",
      "   VALIDATION: Batch 184, loss 3.3928184509277344\n",
      "   VALIDATION: Batch 185, loss 4.195838451385498\n",
      "   VALIDATION: Batch 186, loss 4.182883262634277\n",
      "   VALIDATION: Batch 187, loss 4.419220924377441\n",
      "   VALIDATION: Batch 188, loss 4.773631572723389\n",
      "   VALIDATION: Batch 189, loss 4.1012396812438965\n",
      "   VALIDATION: Batch 190, loss 4.103795528411865\n",
      "   VALIDATION: Batch 191, loss 4.652135372161865\n",
      "   VALIDATION: Batch 192, loss 5.114530086517334\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=30]              TRRAINING: Batch 0, loss 4.525240898132324\n",
      "Epoch 3: |          | 1/? [00:01<00:00,  0.91it/s, v_num=30]   TRRAINING: Batch 1, loss 4.047834873199463\n",
      "Epoch 3: |          | 2/? [00:01<00:00,  1.03it/s, v_num=30]   TRRAINING: Batch 2, loss 4.1692657470703125\n",
      "Epoch 3: |          | 3/? [00:02<00:00,  1.07it/s, v_num=30]   TRRAINING: Batch 3, loss 3.8061611652374268\n",
      "Epoch 3: |          | 4/? [00:03<00:00,  1.10it/s, v_num=30]   TRRAINING: Batch 4, loss 4.230955600738525\n",
      "Epoch 3: |          | 5/? [00:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 5, loss 5.082545757293701\n",
      "Epoch 3: |          | 6/? [00:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 6, loss 4.750918388366699\n",
      "Epoch 3: |          | 7/? [00:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 7, loss 3.9528281688690186\n",
      "Epoch 3: |          | 8/? [00:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 8, loss 4.103287220001221\n",
      "Epoch 3: |          | 9/? [00:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 9, loss 4.341010093688965\n",
      "Epoch 3: |          | 10/? [00:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 10, loss 4.617856979370117\n",
      "Epoch 3: |          | 11/? [00:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 11, loss 4.506041049957275\n",
      "Epoch 3: |          | 12/? [00:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 12, loss 5.578624248504639\n",
      "Epoch 3: |          | 13/? [00:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 13, loss 4.38387393951416\n",
      "Epoch 3: |          | 14/? [00:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 14, loss 4.6271162033081055\n",
      "Epoch 3: |          | 15/? [00:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 15, loss 3.859464645385742\n",
      "Epoch 3: |          | 16/? [00:13<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 16, loss 3.621997833251953\n",
      "Epoch 3: |          | 17/? [00:14<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 17, loss 4.903706073760986\n",
      "Epoch 3: |          | 18/? [00:14<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 18, loss 4.352217674255371\n",
      "Epoch 3: |          | 19/? [00:15<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 19, loss 4.1574602127075195\n",
      "Epoch 3: |          | 20/? [00:16<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 20, loss 4.472362041473389\n",
      "Epoch 3: |          | 21/? [00:17<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 21, loss 4.552934646606445\n",
      "Epoch 3: |          | 22/? [00:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 22, loss 4.41530704498291\n",
      "Epoch 3: |          | 23/? [00:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 23, loss 3.777623414993286\n",
      "Epoch 3: |          | 24/? [00:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 24, loss 4.426141738891602\n",
      "Epoch 3: |          | 25/? [00:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 25, loss 4.288893699645996\n",
      "Epoch 3: |          | 26/? [00:21<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 26, loss 4.069691181182861\n",
      "Epoch 3: |          | 27/? [00:22<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 27, loss 3.962200880050659\n",
      "Epoch 3: |          | 28/? [00:23<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 28, loss 4.860576152801514\n",
      "Epoch 3: |          | 29/? [00:24<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 29, loss 4.366045951843262\n",
      "Epoch 3: |          | 30/? [00:24<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 30, loss 4.272860527038574\n",
      "Epoch 3: |          | 31/? [00:25<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 31, loss 4.914186954498291\n",
      "Epoch 3: |          | 32/? [00:26<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 32, loss 4.409040451049805\n",
      "Epoch 3: |          | 33/? [00:27<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 33, loss 4.226801872253418\n",
      "Epoch 3: |          | 34/? [00:27<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 34, loss 4.169740200042725\n",
      "Epoch 3: |          | 35/? [00:28<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 35, loss 3.5769646167755127\n",
      "Epoch 3: |          | 36/? [00:29<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 36, loss 4.518105506896973\n",
      "Epoch 3: |          | 37/? [00:30<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 37, loss 4.616562843322754\n",
      "Epoch 3: |          | 38/? [00:31<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 38, loss 4.922593593597412\n",
      "Epoch 3: |          | 39/? [00:32<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 39, loss 4.8459954261779785\n",
      "Epoch 3: |          | 40/? [00:32<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 40, loss 4.2418622970581055\n",
      "Epoch 3: |          | 41/? [00:33<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 41, loss 4.228882789611816\n",
      "Epoch 3: |          | 42/? [00:34<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 42, loss 4.046720027923584\n",
      "Epoch 3: |          | 43/? [00:35<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 43, loss 4.204309463500977\n",
      "Epoch 3: |          | 44/? [00:36<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 44, loss 3.511805772781372\n",
      "Epoch 3: |          | 45/? [00:37<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 45, loss 3.345019578933716\n",
      "Epoch 3: |          | 46/? [00:38<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 46, loss 4.941376686096191\n",
      "Epoch 3: |          | 47/? [00:38<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 47, loss 4.076623439788818\n",
      "Epoch 3: |          | 48/? [00:39<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 48, loss 3.7991080284118652\n",
      "Epoch 3: |          | 49/? [00:40<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 49, loss 4.296664237976074\n",
      "Epoch 3: |          | 50/? [00:41<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 50, loss 4.175349712371826\n",
      "Epoch 3: |          | 51/? [00:42<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 51, loss 4.206183433532715\n",
      "Epoch 3: |          | 52/? [00:42<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 52, loss 4.810051918029785\n",
      "Epoch 3: |          | 53/? [00:43<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 53, loss 4.442626953125\n",
      "Epoch 3: |          | 54/? [00:44<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 54, loss 4.288926124572754\n",
      "Epoch 3: |          | 55/? [00:45<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 55, loss 4.35379695892334\n",
      "Epoch 3: |          | 56/? [00:46<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 56, loss 4.444509029388428\n",
      "Epoch 3: |          | 57/? [00:47<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 57, loss 4.392507553100586\n",
      "Epoch 3: |          | 58/? [00:48<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 58, loss 5.659839153289795\n",
      "Epoch 3: |          | 59/? [00:48<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 59, loss 4.506023406982422\n",
      "Epoch 3: |          | 60/? [00:49<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 60, loss 4.632260799407959\n",
      "Epoch 3: |          | 61/? [00:50<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 61, loss 4.677797794342041\n",
      "Epoch 3: |          | 62/? [00:51<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 62, loss 4.221433639526367\n",
      "Epoch 3: |          | 63/? [00:52<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 63, loss 4.4405059814453125\n",
      "Epoch 3: |          | 64/? [00:52<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 64, loss 4.248019695281982\n",
      "Epoch 3: |          | 65/? [00:53<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 65, loss 4.319188594818115\n",
      "Epoch 3: |          | 66/? [00:54<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 66, loss 3.6756482124328613\n",
      "Epoch 3: |          | 67/? [00:55<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 67, loss 4.405215740203857\n",
      "Epoch 3: |          | 68/? [00:56<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 68, loss 4.536761283874512\n",
      "Epoch 3: |          | 69/? [00:56<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 69, loss 4.293755531311035\n",
      "Epoch 3: |          | 70/? [00:57<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 70, loss 3.9900124073028564\n",
      "Epoch 3: |          | 71/? [00:58<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 71, loss 3.9631800651550293\n",
      "Epoch 3: |          | 72/? [00:59<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 72, loss 4.377679824829102\n",
      "Epoch 3: |          | 73/? [01:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 73, loss 4.457056045532227\n",
      "Epoch 3: |          | 74/? [01:00<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 74, loss 4.067126274108887\n",
      "Epoch 3: |          | 75/? [01:01<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 75, loss 4.2438130378723145\n",
      "Epoch 3: |          | 76/? [01:02<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 76, loss 4.2544074058532715\n",
      "Epoch 3: |          | 77/? [01:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 77, loss 4.302631378173828\n",
      "Epoch 3: |          | 78/? [01:03<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 78, loss 3.986750841140747\n",
      "Epoch 3: |          | 79/? [01:04<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 79, loss 4.261445045471191\n",
      "Epoch 3: |          | 80/? [01:05<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 80, loss 4.1687774658203125\n",
      "Epoch 3: |          | 81/? [01:06<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 81, loss 3.664976119995117\n",
      "Epoch 3: |          | 82/? [01:07<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 82, loss 4.572453498840332\n",
      "Epoch 3: |          | 83/? [01:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 83, loss 3.8605504035949707\n",
      "Epoch 3: |          | 84/? [01:08<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 84, loss 3.729762315750122\n",
      "Epoch 3: |          | 85/? [01:09<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 85, loss 3.6739296913146973\n",
      "Epoch 3: |          | 86/? [01:10<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 86, loss 3.7509918212890625\n",
      "Epoch 3: |          | 87/? [01:11<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 87, loss 3.9190826416015625\n",
      "Epoch 3: |          | 88/? [01:12<00:00,  1.22it/s, v_num=30]   TRRAINING: Batch 88, loss 4.793658256530762\n",
      "Epoch 3: |          | 89/? [01:13<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 89, loss 4.5757551193237305\n",
      "Epoch 3: |          | 90/? [01:14<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 90, loss 4.412719249725342\n",
      "Epoch 3: |          | 91/? [01:15<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 91, loss 4.217863082885742\n",
      "Epoch 3: |          | 92/? [01:15<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 92, loss 4.600301265716553\n",
      "Epoch 3: |          | 93/? [01:16<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 93, loss 4.7366437911987305\n",
      "Epoch 3: |          | 94/? [01:17<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 94, loss 4.598901271820068\n",
      "Epoch 3: |          | 95/? [01:18<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 95, loss 4.898631572723389\n",
      "Epoch 3: |          | 96/? [01:19<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 96, loss 4.060636043548584\n",
      "Epoch 3: |          | 97/? [01:20<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 97, loss 3.9311814308166504\n",
      "Epoch 3: |          | 98/? [01:20<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 98, loss 4.422362327575684\n",
      "Epoch 3: |          | 99/? [01:21<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 99, loss 4.606008529663086\n",
      "Epoch 3: |          | 100/? [01:22<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 100, loss 4.6253767013549805\n",
      "Epoch 3: |          | 101/? [01:23<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 101, loss 4.254279136657715\n",
      "Epoch 3: |          | 102/? [01:24<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 102, loss 4.242603778839111\n",
      "Epoch 3: |          | 103/? [01:25<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 103, loss 4.003373146057129\n",
      "Epoch 3: |          | 104/? [01:26<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 104, loss 4.397818088531494\n",
      "Epoch 3: |          | 105/? [01:26<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 105, loss 4.272704124450684\n",
      "Epoch 3: |          | 106/? [01:27<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 106, loss 4.340285778045654\n",
      "Epoch 3: |          | 107/? [01:28<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 107, loss 4.4864068031311035\n",
      "Epoch 3: |          | 108/? [01:29<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 108, loss 4.388510704040527\n",
      "Epoch 3: |          | 109/? [01:30<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 109, loss 4.019608497619629\n",
      "Epoch 3: |          | 110/? [01:30<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 110, loss 4.379486083984375\n",
      "Epoch 3: |          | 111/? [01:31<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 111, loss 4.991665840148926\n",
      "Epoch 3: |          | 112/? [01:32<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 112, loss 3.7361607551574707\n",
      "Epoch 3: |          | 113/? [01:33<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 113, loss 3.286905288696289\n",
      "Epoch 3: |          | 114/? [01:34<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 114, loss 4.548255920410156\n",
      "Epoch 3: |          | 115/? [01:35<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 115, loss 4.736780166625977\n",
      "Epoch 3: |          | 116/? [01:36<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 116, loss 3.925102710723877\n",
      "Epoch 3: |          | 117/? [01:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 117, loss 3.906017780303955\n",
      "Epoch 3: |          | 118/? [01:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 118, loss 4.58164119720459\n",
      "Epoch 3: |          | 119/? [01:38<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 119, loss 4.8167572021484375\n",
      "Epoch 3: |          | 120/? [01:39<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 120, loss 4.56341028213501\n",
      "Epoch 3: |          | 121/? [01:40<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 121, loss 4.303603172302246\n",
      "Epoch 3: |          | 122/? [01:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 122, loss 3.729076862335205\n",
      "Epoch 3: |          | 123/? [01:42<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 123, loss 4.25729513168335\n",
      "Epoch 3: |          | 124/? [01:42<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 124, loss 4.416327953338623\n",
      "Epoch 3: |          | 125/? [01:43<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 125, loss 4.1051764488220215\n",
      "Epoch 3: |          | 126/? [01:44<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 126, loss 4.597630023956299\n",
      "Epoch 3: |          | 127/? [01:45<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 127, loss 4.641470909118652\n",
      "Epoch 3: |          | 128/? [01:46<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 128, loss 3.6898162364959717\n",
      "Epoch 3: |          | 129/? [01:46<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 129, loss 4.408336162567139\n",
      "Epoch 3: |          | 130/? [01:47<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 130, loss 3.383080005645752\n",
      "Epoch 3: |          | 131/? [01:48<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 131, loss 4.320263385772705\n",
      "Epoch 3: |          | 132/? [01:49<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 132, loss 4.308575630187988\n",
      "Epoch 3: |          | 133/? [01:50<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 133, loss 4.337886333465576\n",
      "Epoch 3: |          | 134/? [01:51<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 134, loss 4.410186290740967\n",
      "Epoch 3: |          | 135/? [01:51<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 135, loss 4.571828842163086\n",
      "Epoch 3: |          | 136/? [01:52<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 136, loss 4.532547950744629\n",
      "Epoch 3: |          | 137/? [01:53<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 137, loss 3.450183153152466\n",
      "Epoch 3: |          | 138/? [01:54<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 138, loss 4.175690650939941\n",
      "Epoch 3: |          | 139/? [01:55<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 139, loss 4.651406764984131\n",
      "Epoch 3: |          | 140/? [01:55<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 140, loss 3.7262187004089355\n",
      "Epoch 3: |          | 141/? [01:56<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 141, loss 3.921112060546875\n",
      "Epoch 3: |          | 142/? [01:57<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 142, loss 5.49603796005249\n",
      "Epoch 3: |          | 143/? [01:58<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 143, loss 5.121817588806152\n",
      "Epoch 3: |          | 144/? [01:59<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 144, loss 4.265815258026123\n",
      "Epoch 3: |          | 145/? [02:00<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 145, loss 3.792656660079956\n",
      "Epoch 3: |          | 146/? [02:00<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 146, loss 3.9573428630828857\n",
      "Epoch 3: |          | 147/? [02:01<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 147, loss 4.268589973449707\n",
      "Epoch 3: |          | 148/? [02:02<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 148, loss 3.9675559997558594\n",
      "Epoch 3: |          | 149/? [02:03<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 149, loss 3.5265166759490967\n",
      "Epoch 3: |          | 150/? [02:03<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 150, loss 4.4007391929626465\n",
      "Epoch 3: |          | 151/? [02:04<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 151, loss 4.48618221282959\n",
      "Epoch 3: |          | 152/? [02:05<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 152, loss 4.5357255935668945\n",
      "Epoch 3: |          | 153/? [02:06<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 153, loss 3.5741629600524902\n",
      "Epoch 3: |          | 154/? [02:07<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 154, loss 4.811144828796387\n",
      "Epoch 3: |          | 155/? [02:08<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 155, loss 4.349221706390381\n",
      "Epoch 3: |          | 156/? [02:08<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 156, loss 3.6314125061035156\n",
      "Epoch 3: |          | 157/? [02:09<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 157, loss 4.410778999328613\n",
      "Epoch 3: |          | 158/? [02:10<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 158, loss 4.476155757904053\n",
      "Epoch 3: |          | 159/? [02:11<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 159, loss 4.179323673248291\n",
      "Epoch 3: |          | 160/? [02:12<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 160, loss 3.928123950958252\n",
      "Epoch 3: |          | 161/? [02:13<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 161, loss 4.392294883728027\n",
      "Epoch 3: |          | 162/? [02:14<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 162, loss 4.4719438552856445\n",
      "Epoch 3: |          | 163/? [02:14<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 163, loss 3.4330756664276123\n",
      "Epoch 3: |          | 164/? [02:15<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 164, loss 3.9122366905212402\n",
      "Epoch 3: |          | 165/? [02:16<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 165, loss 4.807812690734863\n",
      "Epoch 3: |          | 166/? [02:17<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 166, loss 4.461200714111328\n",
      "Epoch 3: |          | 167/? [02:18<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 167, loss 4.5692243576049805\n",
      "Epoch 3: |          | 168/? [02:19<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 168, loss 4.113354682922363\n",
      "Epoch 3: |          | 169/? [02:19<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 169, loss 3.6962192058563232\n",
      "Epoch 3: |          | 170/? [02:20<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 170, loss 4.022742748260498\n",
      "Epoch 3: |          | 171/? [02:21<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 171, loss 4.409259796142578\n",
      "Epoch 3: |          | 172/? [02:22<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 172, loss 4.122725963592529\n",
      "Epoch 3: |          | 173/? [02:23<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 173, loss 4.8954973220825195\n",
      "Epoch 3: |          | 174/? [02:24<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 174, loss 4.706611156463623\n",
      "Epoch 3: |          | 175/? [02:24<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 175, loss 4.98328971862793\n",
      "Epoch 3: |          | 176/? [02:25<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 176, loss 4.0976362228393555\n",
      "Epoch 3: |          | 177/? [02:26<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 177, loss 4.077883720397949\n",
      "Epoch 3: |          | 178/? [02:27<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 178, loss 3.956660747528076\n",
      "Epoch 3: |          | 179/? [02:28<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 179, loss 4.699300765991211\n",
      "Epoch 3: |          | 180/? [02:29<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 180, loss 4.1813063621521\n",
      "Epoch 3: |          | 181/? [02:30<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 181, loss 3.9754467010498047\n",
      "Epoch 3: |          | 182/? [02:30<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 182, loss 4.436960697174072\n",
      "Epoch 3: |          | 183/? [02:31<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 183, loss 3.8723607063293457\n",
      "Epoch 3: |          | 184/? [02:32<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 184, loss 4.045297622680664\n",
      "Epoch 3: |          | 185/? [02:33<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 185, loss 4.6663498878479\n",
      "Epoch 3: |          | 186/? [02:34<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 186, loss 4.1002702713012695\n",
      "Epoch 3: |          | 187/? [02:35<00:00,  1.21it/s, v_num=30]   TRRAINING: Batch 187, loss 4.674813270568848\n",
      "Epoch 3: |          | 188/? [02:36<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 188, loss 4.052035331726074\n",
      "Epoch 3: |          | 189/? [02:36<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 189, loss 4.72267484664917\n",
      "Epoch 3: |          | 190/? [02:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 190, loss 4.19199275970459\n",
      "Epoch 3: |          | 191/? [02:38<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 191, loss 5.007128715515137\n",
      "Epoch 3: |          | 192/? [02:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 192, loss 4.800742149353027\n",
      "Epoch 3: |          | 193/? [02:40<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 193, loss 4.0270867347717285\n",
      "Epoch 3: |          | 194/? [02:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 194, loss 3.9496421813964844\n",
      "Epoch 3: |          | 195/? [02:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 195, loss 4.652713298797607\n",
      "Epoch 3: |          | 196/? [02:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 196, loss 4.583397388458252\n",
      "Epoch 3: |          | 197/? [02:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 197, loss 4.359997272491455\n",
      "Epoch 3: |          | 198/? [02:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 198, loss 3.7255301475524902\n",
      "Epoch 3: |          | 199/? [02:45<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 199, loss 4.577276229858398\n",
      "Epoch 3: |          | 200/? [02:46<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 200, loss 4.16355037689209\n",
      "Epoch 3: |          | 201/? [02:47<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 201, loss 4.450560569763184\n",
      "Epoch 3: |          | 202/? [02:48<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 202, loss 4.5066118240356445\n",
      "Epoch 3: |          | 203/? [02:48<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 203, loss 4.218167304992676\n",
      "Epoch 3: |          | 204/? [02:49<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 204, loss 4.273317813873291\n",
      "Epoch 3: |          | 205/? [02:50<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 205, loss 4.0638813972473145\n",
      "Epoch 3: |          | 206/? [02:51<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 206, loss 3.9443321228027344\n",
      "Epoch 3: |          | 207/? [02:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 207, loss 4.440645694732666\n",
      "Epoch 3: |          | 208/? [02:53<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 208, loss 4.406531810760498\n",
      "Epoch 3: |          | 209/? [02:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 209, loss 4.123517990112305\n",
      "Epoch 3: |          | 210/? [02:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 210, loss 4.853672981262207\n",
      "Epoch 3: |          | 211/? [02:55<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 211, loss 4.174910545349121\n",
      "Epoch 3: |          | 212/? [02:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 212, loss 4.393470764160156\n",
      "Epoch 3: |          | 213/? [02:57<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 213, loss 4.216060638427734\n",
      "Epoch 3: |          | 214/? [02:58<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 214, loss 4.114681720733643\n",
      "Epoch 3: |          | 215/? [02:59<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 215, loss 3.77563214302063\n",
      "Epoch 3: |          | 216/? [03:00<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 216, loss 4.5039496421813965\n",
      "Epoch 3: |          | 217/? [03:01<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 217, loss 4.386769771575928\n",
      "Epoch 3: |          | 218/? [03:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 218, loss 4.412561893463135\n",
      "Epoch 3: |          | 219/? [03:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 219, loss 4.326332092285156\n",
      "Epoch 3: |          | 220/? [03:03<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 220, loss 4.4269700050354\n",
      "Epoch 3: |          | 221/? [03:04<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 221, loss 4.200774192810059\n",
      "Epoch 3: |          | 222/? [03:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 222, loss 3.4110279083251953\n",
      "Epoch 3: |          | 223/? [03:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 223, loss 4.7605438232421875\n",
      "Epoch 3: |          | 224/? [03:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 224, loss 4.617308616638184\n",
      "Epoch 3: |          | 225/? [03:07<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 225, loss 4.379915237426758\n",
      "Epoch 3: |          | 226/? [03:08<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 226, loss 4.167846202850342\n",
      "Epoch 3: |          | 227/? [03:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 227, loss 4.562930583953857\n",
      "Epoch 3: |          | 228/? [03:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 228, loss 4.24288272857666\n",
      "Epoch 3: |          | 229/? [03:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 229, loss 4.404470443725586\n",
      "Epoch 3: |          | 230/? [03:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 230, loss 4.258942604064941\n",
      "Epoch 3: |          | 231/? [03:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 231, loss 4.2054877281188965\n",
      "Epoch 3: |          | 232/? [03:13<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 232, loss 3.9606361389160156\n",
      "Epoch 3: |          | 233/? [03:14<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 233, loss 4.711291313171387\n",
      "Epoch 3: |          | 234/? [03:15<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 234, loss 4.715948581695557\n",
      "Epoch 3: |          | 235/? [03:16<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 235, loss 4.768553256988525\n",
      "Epoch 3: |          | 236/? [03:17<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 236, loss 4.1636505126953125\n",
      "Epoch 3: |          | 237/? [03:17<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 237, loss 4.354088306427002\n",
      "Epoch 3: |          | 238/? [03:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 238, loss 4.589979648590088\n",
      "Epoch 3: |          | 239/? [03:19<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 239, loss 4.184828758239746\n",
      "Epoch 3: |          | 240/? [03:20<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 240, loss 3.6321182250976562\n",
      "Epoch 3: |          | 241/? [03:21<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 241, loss 4.340567111968994\n",
      "Epoch 3: |          | 242/? [03:22<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 242, loss 4.68031120300293\n",
      "Epoch 3: |          | 243/? [03:22<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 243, loss 3.454677104949951\n",
      "Epoch 3: |          | 244/? [03:23<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 244, loss 4.009481906890869\n",
      "Epoch 3: |          | 245/? [03:24<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 245, loss 4.236170768737793\n",
      "Epoch 3: |          | 246/? [03:25<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 246, loss 4.433381080627441\n",
      "Epoch 3: |          | 247/? [03:26<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 247, loss 4.497926235198975\n",
      "Epoch 3: |          | 248/? [03:27<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 248, loss 3.9602551460266113\n",
      "Epoch 3: |          | 249/? [03:27<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 249, loss 3.744511842727661\n",
      "Epoch 3: |          | 250/? [03:28<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 250, loss 4.36812686920166\n",
      "Epoch 3: |          | 251/? [03:29<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 251, loss 4.409536361694336\n",
      "Epoch 3: |          | 252/? [03:30<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 252, loss 4.169609069824219\n",
      "Epoch 3: |          | 253/? [03:31<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 253, loss 4.996224403381348\n",
      "Epoch 3: |          | 254/? [03:32<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 254, loss 4.686953544616699\n",
      "Epoch 3: |          | 255/? [03:32<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 255, loss 4.224820137023926\n",
      "Epoch 3: |          | 256/? [03:33<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 256, loss 5.626683235168457\n",
      "Epoch 3: |          | 257/? [03:34<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 257, loss 4.112374305725098\n",
      "Epoch 3: |          | 258/? [03:35<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 258, loss 4.199725151062012\n",
      "Epoch 3: |          | 259/? [03:35<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 259, loss 4.045299053192139\n",
      "Epoch 3: |          | 260/? [03:36<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 260, loss 4.005951881408691\n",
      "Epoch 3: |          | 261/? [03:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 261, loss 3.977552890777588\n",
      "Epoch 3: |          | 262/? [03:38<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 262, loss 4.5541157722473145\n",
      "Epoch 3: |          | 263/? [03:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 263, loss 4.239447593688965\n",
      "Epoch 3: |          | 264/? [03:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 264, loss 4.33485221862793\n",
      "Epoch 3: |          | 265/? [03:40<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 265, loss 3.759462356567383\n",
      "Epoch 3: |          | 266/? [03:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 266, loss 4.200839042663574\n",
      "Epoch 3: |          | 267/? [03:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 267, loss 3.8381011486053467\n",
      "Epoch 3: |          | 268/? [03:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 268, loss 4.1894731521606445\n",
      "Epoch 3: |          | 269/? [03:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 269, loss 4.564761161804199\n",
      "Epoch 3: |          | 270/? [03:45<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 270, loss 4.1693830490112305\n",
      "Epoch 3: |          | 271/? [03:45<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 271, loss 4.6548967361450195\n",
      "Epoch 3: |          | 272/? [03:46<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 272, loss 4.721092700958252\n",
      "Epoch 3: |          | 273/? [03:47<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 273, loss 3.8695006370544434\n",
      "Epoch 3: |          | 274/? [03:48<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 274, loss 4.911014556884766\n",
      "Epoch 3: |          | 275/? [03:49<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 275, loss 4.421116352081299\n",
      "Epoch 3: |          | 276/? [03:49<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 276, loss 3.674318790435791\n",
      "Epoch 3: |          | 277/? [03:50<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 277, loss 4.3454484939575195\n",
      "Epoch 3: |          | 278/? [03:51<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 278, loss 3.366227626800537\n",
      "Epoch 3: |          | 279/? [03:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 279, loss 4.118875980377197\n",
      "Epoch 3: |          | 280/? [03:53<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 280, loss 3.7410244941711426\n",
      "Epoch 3: |          | 281/? [03:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 281, loss 4.51438045501709\n",
      "Epoch 3: |          | 282/? [03:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 282, loss 4.1286540031433105\n",
      "Epoch 3: |          | 283/? [03:55<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 283, loss 4.140379905700684\n",
      "Epoch 3: |          | 284/? [03:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 284, loss 4.051677227020264\n",
      "Epoch 3: |          | 285/? [03:57<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 285, loss 3.5237700939178467\n",
      "Epoch 3: |          | 286/? [03:58<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 286, loss 4.116433620452881\n",
      "Epoch 3: |          | 287/? [03:59<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 287, loss 3.921740770339966\n",
      "Epoch 3: |          | 288/? [04:00<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 288, loss 3.916658878326416\n",
      "Epoch 3: |          | 289/? [04:01<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 289, loss 3.797334671020508\n",
      "Epoch 3: |          | 290/? [04:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 290, loss 3.1768546104431152\n",
      "Epoch 3: |          | 291/? [04:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 291, loss 4.323423862457275\n",
      "Epoch 3: |          | 292/? [04:03<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 292, loss 3.9761815071105957\n",
      "Epoch 3: |          | 293/? [04:04<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 293, loss 4.291816711425781\n",
      "Epoch 3: |          | 294/? [04:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 294, loss 4.141780376434326\n",
      "Epoch 3: |          | 295/? [04:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 295, loss 4.513688564300537\n",
      "Epoch 3: |          | 296/? [04:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 296, loss 3.918635845184326\n",
      "Epoch 3: |          | 297/? [04:07<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 297, loss 4.651924133300781\n",
      "Epoch 3: |          | 298/? [04:08<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 298, loss 4.432135581970215\n",
      "Epoch 3: |          | 299/? [04:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 299, loss 5.026727199554443\n",
      "Epoch 3: |          | 300/? [04:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 300, loss 4.308431625366211\n",
      "Epoch 3: |          | 301/? [04:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 301, loss 4.0355730056762695\n",
      "Epoch 3: |          | 302/? [04:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 302, loss 4.529312610626221\n",
      "Epoch 3: |          | 303/? [04:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 303, loss 4.269618034362793\n",
      "Epoch 3: |          | 304/? [04:13<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 304, loss 4.5048933029174805\n",
      "Epoch 3: |          | 305/? [04:14<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 305, loss 4.562808990478516\n",
      "Epoch 3: |          | 306/? [04:15<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 306, loss 4.211026191711426\n",
      "Epoch 3: |          | 307/? [04:16<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 307, loss 4.4626874923706055\n",
      "Epoch 3: |          | 308/? [04:16<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 308, loss 4.582037448883057\n",
      "Epoch 3: |          | 309/? [04:17<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 309, loss 4.259825706481934\n",
      "Epoch 3: |          | 310/? [04:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 310, loss 4.683629512786865\n",
      "Epoch 3: |          | 311/? [04:19<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 311, loss 4.212096691131592\n",
      "Epoch 3: |          | 312/? [04:20<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 312, loss 4.290211200714111\n",
      "Epoch 3: |          | 313/? [04:21<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 313, loss 3.970623731613159\n",
      "Epoch 3: |          | 314/? [04:22<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 314, loss 4.359538555145264\n",
      "Epoch 3: |          | 315/? [04:22<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 315, loss 3.983640670776367\n",
      "Epoch 3: |          | 316/? [04:23<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 316, loss 4.647377014160156\n",
      "Epoch 3: |          | 317/? [04:24<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 317, loss 4.380623817443848\n",
      "Epoch 3: |          | 318/? [04:25<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 318, loss 4.556991100311279\n",
      "Epoch 3: |          | 319/? [04:26<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 319, loss 3.7277235984802246\n",
      "Epoch 3: |          | 320/? [04:27<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 320, loss 4.244105339050293\n",
      "Epoch 3: |          | 321/? [04:28<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 321, loss 3.9860172271728516\n",
      "Epoch 3: |          | 322/? [04:29<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 322, loss 4.714676856994629\n",
      "Epoch 3: |          | 323/? [04:30<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 323, loss 4.694026470184326\n",
      "Epoch 3: |          | 324/? [04:30<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 324, loss 4.297835826873779\n",
      "Epoch 3: |          | 325/? [04:31<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 325, loss 4.783305644989014\n",
      "Epoch 3: |          | 326/? [04:32<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 326, loss 4.3182291984558105\n",
      "Epoch 3: |          | 327/? [04:33<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 327, loss 4.108732223510742\n",
      "Epoch 3: |          | 328/? [04:34<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 328, loss 3.863391399383545\n",
      "Epoch 3: |          | 329/? [04:35<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 329, loss 4.47703742980957\n",
      "Epoch 3: |          | 330/? [04:35<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 330, loss 4.9576826095581055\n",
      "Epoch 3: |          | 331/? [04:36<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 331, loss 3.1328673362731934\n",
      "Epoch 3: |          | 332/? [04:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 332, loss 4.339777946472168\n",
      "Epoch 3: |          | 333/? [04:38<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 333, loss 4.132899284362793\n",
      "Epoch 3: |          | 334/? [04:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 334, loss 4.94770622253418\n",
      "Epoch 3: |          | 335/? [04:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 335, loss 4.80460262298584\n",
      "Epoch 3: |          | 336/? [04:40<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 336, loss 4.699348449707031\n",
      "Epoch 3: |          | 337/? [04:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 337, loss 5.374576091766357\n",
      "Epoch 3: |          | 338/? [04:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 338, loss 4.963830947875977\n",
      "Epoch 3: |          | 339/? [04:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 339, loss 3.9868369102478027\n",
      "Epoch 3: |          | 340/? [04:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 340, loss 4.060101509094238\n",
      "Epoch 3: |          | 341/? [04:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 341, loss 3.670896530151367\n",
      "Epoch 3: |          | 342/? [04:45<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 342, loss 4.398728370666504\n",
      "Epoch 3: |          | 343/? [04:46<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 343, loss 4.080365180969238\n",
      "Epoch 3: |          | 344/? [04:47<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 344, loss 4.948206424713135\n",
      "Epoch 3: |          | 345/? [04:48<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 345, loss 4.135557174682617\n",
      "Epoch 3: |          | 346/? [04:48<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 346, loss 4.3787665367126465\n",
      "Epoch 3: |          | 347/? [04:49<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 347, loss 4.093425273895264\n",
      "Epoch 3: |          | 348/? [04:50<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 348, loss 3.4372973442077637\n",
      "Epoch 3: |          | 349/? [04:51<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 349, loss 3.275745391845703\n",
      "Epoch 3: |          | 350/? [04:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 350, loss 4.724925994873047\n",
      "Epoch 3: |          | 351/? [04:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 351, loss 4.693716526031494\n",
      "Epoch 3: |          | 352/? [04:53<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 352, loss 3.9042344093322754\n",
      "Epoch 3: |          | 353/? [04:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 353, loss 3.6721012592315674\n",
      "Epoch 3: |          | 354/? [04:55<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 354, loss 4.073432922363281\n",
      "Epoch 3: |          | 355/? [04:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 355, loss 4.4043869972229\n",
      "Epoch 3: |          | 356/? [04:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 356, loss 4.397680759429932\n",
      "Epoch 3: |          | 357/? [04:57<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 357, loss 3.916102647781372\n",
      "Epoch 3: |          | 358/? [04:58<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 358, loss 3.8350131511688232\n",
      "Epoch 3: |          | 359/? [04:59<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 359, loss 4.509647369384766\n",
      "Epoch 3: |          | 360/? [05:00<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 360, loss 4.028071880340576\n",
      "Epoch 3: |          | 361/? [05:01<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 361, loss 4.181811332702637\n",
      "Epoch 3: |          | 362/? [05:01<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 362, loss 3.9698803424835205\n",
      "Epoch 3: |          | 363/? [05:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 363, loss 3.8766300678253174\n",
      "Epoch 3: |          | 364/? [05:03<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 364, loss 4.495610237121582\n",
      "Epoch 3: |          | 365/? [05:04<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 365, loss 4.5722503662109375\n",
      "Epoch 3: |          | 366/? [05:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 366, loss 4.276745796203613\n",
      "Epoch 3: |          | 367/? [05:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 367, loss 4.314126014709473\n",
      "Epoch 3: |          | 368/? [05:07<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 368, loss 3.7897109985351562\n",
      "Epoch 3: |          | 369/? [05:07<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 369, loss 4.117956638336182\n",
      "Epoch 3: |          | 370/? [05:08<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 370, loss 3.759592056274414\n",
      "Epoch 3: |          | 371/? [05:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 371, loss 4.77661657333374\n",
      "Epoch 3: |          | 372/? [05:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 372, loss 4.200109004974365\n",
      "Epoch 3: |          | 373/? [05:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 373, loss 4.365360260009766\n",
      "Epoch 3: |          | 374/? [05:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 374, loss 4.04833459854126\n",
      "Epoch 3: |          | 375/? [05:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 375, loss 4.799870491027832\n",
      "Epoch 3: |          | 376/? [05:13<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 376, loss 4.1607561111450195\n",
      "Epoch 3: |          | 377/? [05:14<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 377, loss 4.364687919616699\n",
      "Epoch 3: |          | 378/? [05:15<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 378, loss 4.494321823120117\n",
      "Epoch 3: |          | 379/? [05:16<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 379, loss 4.301530838012695\n",
      "Epoch 3: |          | 380/? [05:17<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 380, loss 4.421814441680908\n",
      "Epoch 3: |          | 381/? [05:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 381, loss 4.4125261306762695\n",
      "Epoch 3: |          | 382/? [05:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 382, loss 4.100455284118652\n",
      "Epoch 3: |          | 383/? [05:19<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 383, loss 4.157074928283691\n",
      "Epoch 3: |          | 384/? [05:20<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 384, loss 4.65463924407959\n",
      "Epoch 3: |          | 385/? [05:21<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 385, loss 4.204955101013184\n",
      "Epoch 3: |          | 386/? [05:21<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 386, loss 3.1188111305236816\n",
      "Epoch 3: |          | 387/? [05:22<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 387, loss 4.05698299407959\n",
      "Epoch 3: |          | 388/? [05:23<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 388, loss 4.271639347076416\n",
      "Epoch 3: |          | 389/? [05:24<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 389, loss 4.686153411865234\n",
      "Epoch 3: |          | 390/? [05:25<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 390, loss 4.02761173248291\n",
      "Epoch 3: |          | 391/? [05:25<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 391, loss 4.546769618988037\n",
      "Epoch 3: |          | 392/? [05:26<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 392, loss 4.588392734527588\n",
      "Epoch 3: |          | 393/? [05:27<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 393, loss 4.632227897644043\n",
      "Epoch 3: |          | 394/? [05:28<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 394, loss 4.262753963470459\n",
      "Epoch 3: |          | 395/? [05:29<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 395, loss 4.5149054527282715\n",
      "Epoch 3: |          | 396/? [05:30<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 396, loss 4.428973197937012\n",
      "Epoch 3: |          | 397/? [05:31<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 397, loss 4.231074810028076\n",
      "Epoch 3: |          | 398/? [05:32<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 398, loss 4.076216697692871\n",
      "Epoch 3: |          | 399/? [05:32<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 399, loss 4.196443557739258\n",
      "Epoch 3: |          | 400/? [05:33<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 400, loss 4.20473051071167\n",
      "Epoch 3: |          | 401/? [05:34<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 401, loss 4.099179267883301\n",
      "Epoch 3: |          | 402/? [05:35<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 402, loss 4.599618911743164\n",
      "Epoch 3: |          | 403/? [05:36<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 403, loss 4.380064487457275\n",
      "Epoch 3: |          | 404/? [05:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 404, loss 3.9003288745880127\n",
      "Epoch 3: |          | 405/? [05:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 405, loss 3.889932155609131\n",
      "Epoch 3: |          | 406/? [05:38<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 406, loss 4.275455951690674\n",
      "Epoch 3: |          | 407/? [05:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 407, loss 4.2053937911987305\n",
      "Epoch 3: |          | 408/? [05:40<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 408, loss 4.597222805023193\n",
      "Epoch 3: |          | 409/? [05:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 409, loss 4.525551795959473\n",
      "Epoch 3: |          | 410/? [05:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 410, loss 4.207364559173584\n",
      "Epoch 3: |          | 411/? [05:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 411, loss 4.130134582519531\n",
      "Epoch 3: |          | 412/? [05:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 412, loss 3.560882568359375\n",
      "Epoch 3: |          | 413/? [05:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 413, loss 4.384269714355469\n",
      "Epoch 3: |          | 414/? [05:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 414, loss 3.8814380168914795\n",
      "Epoch 3: |          | 415/? [05:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 415, loss 4.358270168304443\n",
      "Epoch 3: |          | 416/? [05:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 416, loss 4.79335355758667\n",
      "Epoch 3: |          | 417/? [05:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 417, loss 4.947434425354004\n",
      "Epoch 3: |          | 418/? [05:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 418, loss 4.496647834777832\n",
      "Epoch 3: |          | 419/? [05:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 419, loss 4.411593437194824\n",
      "Epoch 3: |          | 420/? [05:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 420, loss 4.348410606384277\n",
      "Epoch 3: |          | 421/? [05:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 421, loss 4.910040855407715\n",
      "Epoch 3: |          | 422/? [05:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 422, loss 4.385412216186523\n",
      "Epoch 3: |          | 423/? [05:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 423, loss 3.922426223754883\n",
      "Epoch 3: |          | 424/? [05:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 424, loss 4.6213507652282715\n",
      "Epoch 3: |          | 425/? [05:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 425, loss 4.406944751739502\n",
      "Epoch 3: |          | 426/? [06:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 426, loss 3.9981846809387207\n",
      "Epoch 3: |          | 427/? [06:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 427, loss 4.027261734008789\n",
      "Epoch 3: |          | 428/? [06:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 428, loss 4.8943023681640625\n",
      "Epoch 3: |          | 429/? [06:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 429, loss 3.712430953979492\n",
      "Epoch 3: |          | 430/? [06:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 430, loss 4.387534141540527\n",
      "Epoch 3: |          | 431/? [06:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 431, loss 4.309234619140625\n",
      "Epoch 3: |          | 432/? [06:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 432, loss 4.392576694488525\n",
      "Epoch 3: |          | 433/? [06:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 433, loss 4.2674102783203125\n",
      "Epoch 3: |          | 434/? [06:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 434, loss 4.250572204589844\n",
      "Epoch 3: |          | 435/? [06:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 435, loss 3.891037702560425\n",
      "Epoch 3: |          | 436/? [06:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 436, loss 4.3473029136657715\n",
      "Epoch 3: |          | 437/? [06:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 437, loss 4.4788594245910645\n",
      "Epoch 3: |          | 438/? [06:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 438, loss 4.0885329246521\n",
      "Epoch 3: |          | 439/? [06:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 439, loss 4.053994178771973\n",
      "Epoch 3: |          | 440/? [06:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 440, loss 4.020762920379639\n",
      "Epoch 3: |          | 441/? [06:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 441, loss 4.381407260894775\n",
      "Epoch 3: |          | 442/? [06:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 442, loss 4.188164710998535\n",
      "Epoch 3: |          | 443/? [06:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 443, loss 4.403368949890137\n",
      "Epoch 3: |          | 444/? [06:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 444, loss 4.288506031036377\n",
      "Epoch 3: |          | 445/? [06:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 445, loss 5.203243255615234\n",
      "Epoch 3: |          | 446/? [06:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 446, loss 4.23873233795166\n",
      "Epoch 3: |          | 447/? [06:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 447, loss 4.804230690002441\n",
      "Epoch 3: |          | 448/? [06:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 448, loss 3.9754481315612793\n",
      "Epoch 3: |          | 449/? [06:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 449, loss 4.231917381286621\n",
      "Epoch 3: |          | 450/? [06:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 450, loss 4.588686943054199\n",
      "Epoch 3: |          | 451/? [06:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 451, loss 4.229071140289307\n",
      "Epoch 3: |          | 452/? [06:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 452, loss 3.962815761566162\n",
      "Epoch 3: |          | 453/? [06:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 453, loss 4.709907054901123\n",
      "Epoch 3: |          | 454/? [06:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 454, loss 4.054055213928223\n",
      "Epoch 3: |          | 455/? [06:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 455, loss 4.482403755187988\n",
      "Epoch 3: |          | 456/? [06:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 456, loss 3.793816089630127\n",
      "Epoch 3: |          | 457/? [06:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 457, loss 4.221157550811768\n",
      "Epoch 3: |          | 458/? [06:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 458, loss 4.6167893409729\n",
      "Epoch 3: |          | 459/? [06:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 459, loss 4.624560356140137\n",
      "Epoch 3: |          | 460/? [06:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 460, loss 4.376242160797119\n",
      "Epoch 3: |          | 461/? [06:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 461, loss 4.3701581954956055\n",
      "Epoch 3: |          | 462/? [06:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 462, loss 4.3874831199646\n",
      "Epoch 3: |          | 463/? [06:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 463, loss 4.278932094573975\n",
      "Epoch 3: |          | 464/? [06:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 464, loss 3.7467143535614014\n",
      "Epoch 3: |          | 465/? [06:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 465, loss 4.034432411193848\n",
      "Epoch 3: |          | 466/? [06:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 466, loss 4.466434001922607\n",
      "Epoch 3: |          | 467/? [06:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 467, loss 4.3351054191589355\n",
      "Epoch 3: |          | 468/? [06:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 468, loss 4.195281028747559\n",
      "Epoch 3: |          | 469/? [06:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 469, loss 4.3848161697387695\n",
      "Epoch 3: |          | 470/? [06:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 470, loss 3.651398181915283\n",
      "Epoch 3: |          | 471/? [06:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 471, loss 4.457798004150391\n",
      "Epoch 3: |          | 472/? [06:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 472, loss 4.014378070831299\n",
      "Epoch 3: |          | 473/? [06:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 473, loss 4.074751853942871\n",
      "Epoch 3: |          | 474/? [06:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 474, loss 3.6366686820983887\n",
      "Epoch 3: |          | 475/? [06:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 475, loss 4.9915571212768555\n",
      "Epoch 3: |          | 476/? [06:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 476, loss 3.941239833831787\n",
      "Epoch 3: |          | 477/? [06:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 477, loss 3.45622181892395\n",
      "Epoch 3: |          | 478/? [06:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 478, loss 3.666421413421631\n",
      "Epoch 3: |          | 479/? [06:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 479, loss 4.2523322105407715\n",
      "Epoch 3: |          | 480/? [06:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 480, loss 4.218153953552246\n",
      "Epoch 3: |          | 481/? [06:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 481, loss 3.7981133460998535\n",
      "Epoch 3: |          | 482/? [06:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 482, loss 4.057349681854248\n",
      "Epoch 3: |          | 483/? [06:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 483, loss 3.769510269165039\n",
      "Epoch 3: |          | 484/? [06:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 484, loss 4.68304443359375\n",
      "Epoch 3: |          | 485/? [06:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 485, loss 4.449280738830566\n",
      "Epoch 3: |          | 486/? [06:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 486, loss 4.243598937988281\n",
      "Epoch 3: |          | 487/? [06:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 487, loss 4.470999717712402\n",
      "Epoch 3: |          | 488/? [06:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 488, loss 4.149474143981934\n",
      "Epoch 3: |          | 489/? [06:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 489, loss 3.6626479625701904\n",
      "Epoch 3: |          | 490/? [06:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 490, loss 4.373185157775879\n",
      "Epoch 3: |          | 491/? [06:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 491, loss 4.117334842681885\n",
      "Epoch 3: |          | 492/? [06:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 492, loss 3.445446729660034\n",
      "Epoch 3: |          | 493/? [06:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 493, loss 4.581900596618652\n",
      "Epoch 3: |          | 494/? [06:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 494, loss 4.428154945373535\n",
      "Epoch 3: |          | 495/? [06:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 495, loss 4.432916164398193\n",
      "Epoch 3: |          | 496/? [06:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 496, loss 4.055711269378662\n",
      "Epoch 3: |          | 497/? [06:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 497, loss 4.549929618835449\n",
      "Epoch 3: |          | 498/? [06:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 498, loss 4.245085716247559\n",
      "Epoch 3: |          | 499/? [06:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 499, loss 4.275710105895996\n",
      "Epoch 3: |          | 500/? [07:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 500, loss 4.147800445556641\n",
      "Epoch 3: |          | 501/? [07:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 501, loss 3.868337631225586\n",
      "Epoch 3: |          | 502/? [07:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 502, loss 4.300475597381592\n",
      "Epoch 3: |          | 503/? [07:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 503, loss 4.291220188140869\n",
      "Epoch 3: |          | 504/? [07:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 504, loss 4.116692543029785\n",
      "Epoch 3: |          | 505/? [07:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 505, loss 3.5262176990509033\n",
      "Epoch 3: |          | 506/? [07:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 506, loss 4.21929407119751\n",
      "Epoch 3: |          | 507/? [07:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 507, loss 4.250069618225098\n",
      "Epoch 3: |          | 508/? [07:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 508, loss 4.575882911682129\n",
      "Epoch 3: |          | 509/? [07:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 509, loss 3.871065139770508\n",
      "Epoch 3: |          | 510/? [07:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 510, loss 4.380746364593506\n",
      "Epoch 3: |          | 511/? [07:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 511, loss 4.266697883605957\n",
      "Epoch 3: |          | 512/? [07:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 512, loss 3.688549041748047\n",
      "Epoch 3: |          | 513/? [07:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 513, loss 3.967020034790039\n",
      "Epoch 3: |          | 514/? [07:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 514, loss 4.077397346496582\n",
      "Epoch 3: |          | 515/? [07:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 515, loss 3.7335898876190186\n",
      "Epoch 3: |          | 516/? [07:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 516, loss 4.048247337341309\n",
      "Epoch 3: |          | 517/? [07:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 517, loss 4.3685102462768555\n",
      "Epoch 3: |          | 518/? [07:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 518, loss 3.8797218799591064\n",
      "Epoch 3: |          | 519/? [07:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 519, loss 4.324583053588867\n",
      "Epoch 3: |          | 520/? [07:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 520, loss 4.1627373695373535\n",
      "Epoch 3: |          | 521/? [07:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 521, loss 4.1432108879089355\n",
      "Epoch 3: |          | 522/? [07:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 522, loss 4.716854572296143\n",
      "Epoch 3: |          | 523/? [07:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 523, loss 4.806358337402344\n",
      "Epoch 3: |          | 524/? [07:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 524, loss 4.560245513916016\n",
      "Epoch 3: |          | 525/? [07:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 525, loss 4.148073196411133\n",
      "Epoch 3: |          | 526/? [07:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 526, loss 3.9494540691375732\n",
      "Epoch 3: |          | 527/? [07:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 527, loss 4.544014930725098\n",
      "Epoch 3: |          | 528/? [07:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 528, loss 4.394283294677734\n",
      "Epoch 3: |          | 529/? [07:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 529, loss 3.9516263008117676\n",
      "Epoch 3: |          | 530/? [07:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 530, loss 4.540746688842773\n",
      "Epoch 3: |          | 531/? [07:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 531, loss 4.02856969833374\n",
      "Epoch 3: |          | 532/? [07:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 532, loss 4.326944351196289\n",
      "Epoch 3: |          | 533/? [07:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 533, loss 3.973140239715576\n",
      "Epoch 3: |          | 534/? [07:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 534, loss 3.723407030105591\n",
      "Epoch 3: |          | 535/? [07:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 535, loss 4.392472743988037\n",
      "Epoch 3: |          | 536/? [07:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 536, loss 4.63251256942749\n",
      "Epoch 3: |          | 537/? [07:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 537, loss 4.328380584716797\n",
      "Epoch 3: |          | 538/? [07:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 538, loss 3.958354949951172\n",
      "Epoch 3: |          | 539/? [07:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 539, loss 4.072319984436035\n",
      "Epoch 3: |          | 540/? [07:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 540, loss 4.563851833343506\n",
      "Epoch 3: |          | 541/? [07:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 541, loss 4.247225284576416\n",
      "Epoch 3: |          | 542/? [07:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 542, loss 4.008120059967041\n",
      "Epoch 3: |          | 543/? [07:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 543, loss 4.386087417602539\n",
      "Epoch 3: |          | 544/? [07:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 544, loss 4.264075756072998\n",
      "Epoch 3: |          | 545/? [07:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 545, loss 3.490870952606201\n",
      "Epoch 3: |          | 546/? [07:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 546, loss 4.356224536895752\n",
      "Epoch 3: |          | 547/? [07:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 547, loss 4.777104377746582\n",
      "Epoch 3: |          | 548/? [07:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 548, loss 4.482469081878662\n",
      "Epoch 3: |          | 549/? [07:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 549, loss 4.33165979385376\n",
      "Epoch 3: |          | 550/? [07:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 550, loss 4.682154655456543\n",
      "Epoch 3: |          | 551/? [07:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 551, loss 4.305763244628906\n",
      "Epoch 3: |          | 552/? [07:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 552, loss 4.392510414123535\n",
      "Epoch 3: |          | 553/? [07:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 553, loss 3.74360728263855\n",
      "Epoch 3: |          | 554/? [07:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 554, loss 4.362343788146973\n",
      "Epoch 3: |          | 555/? [07:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 555, loss 4.693352699279785\n",
      "Epoch 3: |          | 556/? [07:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 556, loss 4.33783483505249\n",
      "Epoch 3: |          | 557/? [07:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 557, loss 3.9077250957489014\n",
      "Epoch 3: |          | 558/? [07:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 558, loss 4.038461685180664\n",
      "Epoch 3: |          | 559/? [07:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 559, loss 4.065540313720703\n",
      "Epoch 3: |          | 560/? [07:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 560, loss 3.5538864135742188\n",
      "Epoch 3: |          | 561/? [07:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 561, loss 3.4877219200134277\n",
      "Epoch 3: |          | 562/? [07:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 562, loss 4.476206302642822\n",
      "Epoch 3: |          | 563/? [07:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 563, loss 3.585282802581787\n",
      "Epoch 3: |          | 564/? [07:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 564, loss 4.100076675415039\n",
      "Epoch 3: |          | 565/? [07:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 565, loss 4.487550735473633\n",
      "Epoch 3: |          | 566/? [07:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 566, loss 4.542162895202637\n",
      "Epoch 3: |          | 567/? [07:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 567, loss 4.629181861877441\n",
      "Epoch 3: |          | 568/? [07:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 568, loss 3.743739366531372\n",
      "Epoch 3: |          | 569/? [07:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 569, loss 4.3307600021362305\n",
      "Epoch 3: |          | 570/? [07:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 570, loss 4.380659103393555\n",
      "Epoch 3: |          | 571/? [07:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 571, loss 3.948413372039795\n",
      "Epoch 3: |          | 572/? [08:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 572, loss 4.883677005767822\n",
      "Epoch 3: |          | 573/? [08:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 573, loss 3.255446672439575\n",
      "Epoch 3: |          | 574/? [08:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 574, loss 4.496184349060059\n",
      "Epoch 3: |          | 575/? [08:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 575, loss 3.837725877761841\n",
      "Epoch 3: |          | 576/? [08:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 576, loss 4.0702409744262695\n",
      "Epoch 3: |          | 577/? [08:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 577, loss 4.287789821624756\n",
      "Epoch 3: |          | 578/? [08:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 578, loss 4.53476619720459\n",
      "Epoch 3: |          | 579/? [08:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 579, loss 3.60026216506958\n",
      "Epoch 3: |          | 580/? [08:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 580, loss 4.3665666580200195\n",
      "Epoch 3: |          | 581/? [08:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 581, loss 4.385504722595215\n",
      "Epoch 3: |          | 582/? [08:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 582, loss 4.407975673675537\n",
      "Epoch 3: |          | 583/? [08:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 583, loss 4.149534225463867\n",
      "Epoch 3: |          | 584/? [08:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 584, loss 4.384441375732422\n",
      "Epoch 3: |          | 585/? [08:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 585, loss 4.360381126403809\n",
      "Epoch 3: |          | 586/? [08:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 586, loss 4.458216667175293\n",
      "Epoch 3: |          | 587/? [08:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 587, loss 4.361312389373779\n",
      "Epoch 3: |          | 588/? [08:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 588, loss 4.420840740203857\n",
      "Epoch 3: |          | 589/? [08:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 589, loss 3.8442535400390625\n",
      "Epoch 3: |          | 590/? [08:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 590, loss 4.471233367919922\n",
      "Epoch 3: |          | 591/? [08:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 591, loss 4.2983927726745605\n",
      "Epoch 3: |          | 592/? [08:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 592, loss 3.977665662765503\n",
      "Epoch 3: |          | 593/? [08:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 593, loss 4.242138385772705\n",
      "Epoch 3: |          | 594/? [08:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 594, loss 5.081608772277832\n",
      "Epoch 3: |          | 595/? [08:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 595, loss 3.727485179901123\n",
      "Epoch 3: |          | 596/? [08:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 596, loss 3.8307483196258545\n",
      "Epoch 3: |          | 597/? [08:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 597, loss 4.1293463706970215\n",
      "Epoch 3: |          | 598/? [08:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 598, loss 4.582489967346191\n",
      "Epoch 3: |          | 599/? [08:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 599, loss 4.219907760620117\n",
      "Epoch 3: |          | 600/? [08:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 600, loss 3.97037935256958\n",
      "Epoch 3: |          | 601/? [08:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 601, loss 4.302394866943359\n",
      "Epoch 3: |          | 602/? [08:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 602, loss 3.8077170848846436\n",
      "Epoch 3: |          | 603/? [08:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 603, loss 3.9002761840820312\n",
      "Epoch 3: |          | 604/? [08:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 604, loss 6.132842540740967\n",
      "Epoch 3: |          | 605/? [08:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 605, loss 3.7284023761749268\n",
      "Epoch 3: |          | 606/? [08:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 606, loss 4.009035110473633\n",
      "Epoch 3: |          | 607/? [08:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 607, loss 4.350935935974121\n",
      "Epoch 3: |          | 608/? [08:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 608, loss 4.117347717285156\n",
      "Epoch 3: |          | 609/? [08:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 609, loss 4.0033159255981445\n",
      "Epoch 3: |          | 610/? [08:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 610, loss 4.12259578704834\n",
      "Epoch 3: |          | 611/? [08:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 611, loss 4.23414421081543\n",
      "Epoch 3: |          | 612/? [08:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 612, loss 3.9640700817108154\n",
      "Epoch 3: |          | 613/? [08:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 613, loss 4.321831226348877\n",
      "Epoch 3: |          | 614/? [08:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 614, loss 4.0606842041015625\n",
      "Epoch 3: |          | 615/? [08:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 615, loss 4.583302021026611\n",
      "Epoch 3: |          | 616/? [08:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 616, loss 4.85660982131958\n",
      "Epoch 3: |          | 617/? [08:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 617, loss 3.404226303100586\n",
      "Epoch 3: |          | 618/? [08:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 618, loss 4.359728813171387\n",
      "Epoch 3: |          | 619/? [08:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 619, loss 4.037903785705566\n",
      "Epoch 3: |          | 620/? [08:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 620, loss 4.473149299621582\n",
      "Epoch 3: |          | 621/? [08:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 621, loss 3.8738338947296143\n",
      "Epoch 3: |          | 622/? [08:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 622, loss 3.6601226329803467\n",
      "Epoch 3: |          | 623/? [08:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 623, loss 3.3726627826690674\n",
      "Epoch 3: |          | 624/? [08:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 624, loss 3.0490639209747314\n",
      "Epoch 3: |          | 625/? [08:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 625, loss 4.642317771911621\n",
      "Epoch 3: |          | 626/? [08:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 626, loss 4.140581130981445\n",
      "Epoch 3: |          | 627/? [08:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 627, loss 4.079825401306152\n",
      "Epoch 3: |          | 628/? [08:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 628, loss 4.019904136657715\n",
      "Epoch 3: |          | 629/? [08:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 629, loss 4.459982395172119\n",
      "Epoch 3: |          | 630/? [08:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 630, loss 4.20485782623291\n",
      "Epoch 3: |          | 631/? [08:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 631, loss 4.363653182983398\n",
      "Epoch 3: |          | 632/? [08:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 632, loss 3.5398316383361816\n",
      "Epoch 3: |          | 633/? [08:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 633, loss 4.429506301879883\n",
      "Epoch 3: |          | 634/? [08:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 634, loss 3.9371140003204346\n",
      "Epoch 3: |          | 635/? [08:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 635, loss 3.765556812286377\n",
      "Epoch 3: |          | 636/? [08:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 636, loss 4.21995735168457\n",
      "Epoch 3: |          | 637/? [08:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 637, loss 4.02994441986084\n",
      "Epoch 3: |          | 638/? [08:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 638, loss 4.286259174346924\n",
      "Epoch 3: |          | 639/? [08:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 639, loss 4.003451824188232\n",
      "Epoch 3: |          | 640/? [08:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 640, loss 4.6107940673828125\n",
      "Epoch 3: |          | 641/? [08:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 641, loss 3.660113573074341\n",
      "Epoch 3: |          | 642/? [08:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 642, loss 4.4034318923950195\n",
      "Epoch 3: |          | 643/? [08:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 643, loss 4.3446125984191895\n",
      "Epoch 3: |          | 644/? [08:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 644, loss 4.19044828414917\n",
      "Epoch 3: |          | 645/? [09:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 645, loss 3.9749526977539062\n",
      "Epoch 3: |          | 646/? [09:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 646, loss 3.9714951515197754\n",
      "Epoch 3: |          | 647/? [09:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 647, loss 4.595408916473389\n",
      "Epoch 3: |          | 648/? [09:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 648, loss 3.9688944816589355\n",
      "Epoch 3: |          | 649/? [09:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 649, loss 3.8184814453125\n",
      "Epoch 3: |          | 650/? [09:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 650, loss 4.535641670227051\n",
      "Epoch 3: |          | 651/? [09:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 651, loss 4.654830455780029\n",
      "Epoch 3: |          | 652/? [09:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 652, loss 4.080447196960449\n",
      "Epoch 3: |          | 653/? [09:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 653, loss 4.19171142578125\n",
      "Epoch 3: |          | 654/? [09:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 654, loss 4.427475929260254\n",
      "Epoch 3: |          | 655/? [09:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 655, loss 4.145254135131836\n",
      "Epoch 3: |          | 656/? [09:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 656, loss 3.736466884613037\n",
      "Epoch 3: |          | 657/? [09:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 657, loss 6.173055648803711\n",
      "Epoch 3: |          | 658/? [09:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 658, loss 3.8305745124816895\n",
      "Epoch 3: |          | 659/? [09:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 659, loss 4.182377815246582\n",
      "Epoch 3: |          | 660/? [09:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 660, loss 4.573429107666016\n",
      "Epoch 3: |          | 661/? [09:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 661, loss 4.497792720794678\n",
      "Epoch 3: |          | 662/? [09:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 662, loss 4.331502914428711\n",
      "Epoch 3: |          | 663/? [09:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 663, loss 4.030586242675781\n",
      "Epoch 3: |          | 664/? [09:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 664, loss 3.9797909259796143\n",
      "Epoch 3: |          | 665/? [09:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 665, loss 4.321353912353516\n",
      "Epoch 3: |          | 666/? [09:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 666, loss 4.074709415435791\n",
      "Epoch 3: |          | 667/? [09:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 667, loss 4.965937614440918\n",
      "Epoch 3: |          | 668/? [09:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 668, loss 3.6702189445495605\n",
      "Epoch 3: |          | 669/? [09:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 669, loss 3.9463768005371094\n",
      "Epoch 3: |          | 670/? [09:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 670, loss 4.6266374588012695\n",
      "Epoch 3: |          | 671/? [09:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 671, loss 4.429820537567139\n",
      "Epoch 3: |          | 672/? [09:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 672, loss 4.346513271331787\n",
      "Epoch 3: |          | 673/? [09:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 673, loss 4.185030460357666\n",
      "Epoch 3: |          | 674/? [09:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 674, loss 2.733572244644165\n",
      "Epoch 3: |          | 675/? [09:24<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 675, loss 1.1703213453292847\n",
      "Epoch 3: |          | 676/? [09:25<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 676, loss 1.019426703453064\n",
      "Epoch 3: |          | 677/? [09:25<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 677, loss 0.7885050177574158\n",
      "Epoch 3: |          | 678/? [09:26<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 678, loss 1.9308608770370483\n",
      "Epoch 3: |          | 679/? [09:27<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 679, loss 3.621304750442505\n",
      "Epoch 3: |          | 680/? [09:28<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 680, loss 4.162538051605225\n",
      "Epoch 3: |          | 681/? [09:29<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 681, loss 3.567683458328247\n",
      "Epoch 3: |          | 682/? [09:29<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 682, loss 3.9762046337127686\n",
      "Epoch 3: |          | 683/? [09:30<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 683, loss 3.624835252761841\n",
      "Epoch 3: |          | 684/? [09:31<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 684, loss 4.788722038269043\n",
      "Epoch 3: |          | 685/? [09:32<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 685, loss 4.337231159210205\n",
      "Epoch 3: |          | 686/? [09:33<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 686, loss 3.914868116378784\n",
      "Epoch 3: |          | 687/? [09:33<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 687, loss 4.550239086151123\n",
      "Epoch 3: |          | 688/? [09:34<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 688, loss 4.095332145690918\n",
      "Epoch 3: |          | 689/? [09:35<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 689, loss 4.165675163269043\n",
      "Epoch 3: |          | 690/? [09:36<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 690, loss 4.672976016998291\n",
      "Epoch 3: |          | 691/? [09:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 691, loss 4.1950554847717285\n",
      "Epoch 3: |          | 692/? [09:38<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 692, loss 4.180027961730957\n",
      "Epoch 3: |          | 693/? [09:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 693, loss 4.713065147399902\n",
      "Epoch 3: |          | 694/? [09:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 694, loss 4.022573471069336\n",
      "Epoch 3: |          | 695/? [09:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 695, loss 4.67315673828125\n",
      "Epoch 3: |          | 696/? [09:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 696, loss 3.8484065532684326\n",
      "Epoch 3: |          | 697/? [09:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 697, loss 4.133424282073975\n",
      "Epoch 3: |          | 698/? [09:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 698, loss 3.4159507751464844\n",
      "Epoch 3: |          | 699/? [09:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 699, loss 4.305771350860596\n",
      "Epoch 3: |          | 700/? [09:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 700, loss 4.431431770324707\n",
      "Epoch 3: |          | 701/? [09:45<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 701, loss 3.9891228675842285\n",
      "Epoch 3: |          | 702/? [09:46<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 702, loss 4.242798805236816\n",
      "Epoch 3: |          | 703/? [09:47<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 703, loss 4.375028133392334\n",
      "Epoch 3: |          | 704/? [09:48<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 704, loss 4.264074325561523\n",
      "Epoch 3: |          | 705/? [09:49<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 705, loss 3.851611375808716\n",
      "Epoch 3: |          | 706/? [09:50<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 706, loss 3.965251922607422\n",
      "Epoch 3: |          | 707/? [09:50<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 707, loss 4.391554832458496\n",
      "Epoch 3: |          | 708/? [09:51<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 708, loss 4.171712875366211\n",
      "Epoch 3: |          | 709/? [09:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 709, loss 4.053885459899902\n",
      "Epoch 3: |          | 710/? [09:53<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 710, loss 4.67830753326416\n",
      "Epoch 3: |          | 711/? [09:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 711, loss 4.760042190551758\n",
      "Epoch 3: |          | 712/? [09:55<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 712, loss 4.450234889984131\n",
      "Epoch 3: |          | 713/? [09:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 713, loss 4.47548770904541\n",
      "Epoch 3: |          | 714/? [09:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 714, loss 4.592465400695801\n",
      "Epoch 3: |          | 715/? [09:57<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 715, loss 3.4443671703338623\n",
      "Epoch 3: |          | 716/? [09:58<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 716, loss 4.239605903625488\n",
      "Epoch 3: |          | 717/? [09:59<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 717, loss 4.057338714599609\n",
      "Epoch 3: |          | 718/? [10:00<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 718, loss 3.590172529220581\n",
      "Epoch 3: |          | 719/? [10:01<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 719, loss 4.136780738830566\n",
      "Epoch 3: |          | 720/? [10:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 720, loss 3.8280253410339355\n",
      "Epoch 3: |          | 721/? [10:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 721, loss 4.537539005279541\n",
      "Epoch 3: |          | 722/? [10:03<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 722, loss 3.735029697418213\n",
      "Epoch 3: |          | 723/? [10:04<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 723, loss 4.310049533843994\n",
      "Epoch 3: |          | 724/? [10:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 724, loss 4.036559104919434\n",
      "Epoch 3: |          | 725/? [10:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 725, loss 3.833350658416748\n",
      "Epoch 3: |          | 726/? [10:07<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 726, loss 4.067094326019287\n",
      "Epoch 3: |          | 727/? [10:08<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 727, loss 3.8051726818084717\n",
      "Epoch 3: |          | 728/? [10:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 728, loss 3.6535065174102783\n",
      "Epoch 3: |          | 729/? [10:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 729, loss 4.079893589019775\n",
      "Epoch 3: |          | 730/? [10:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 730, loss 4.067040920257568\n",
      "Epoch 3: |          | 731/? [10:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 731, loss 4.245482444763184\n",
      "Epoch 3: |          | 732/? [10:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 732, loss 4.5188093185424805\n",
      "Epoch 3: |          | 733/? [10:13<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 733, loss 4.138614177703857\n",
      "Epoch 3: |          | 734/? [10:13<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 734, loss 4.400196552276611\n",
      "Epoch 3: |          | 735/? [10:14<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 735, loss 4.305930137634277\n",
      "Epoch 3: |          | 736/? [10:15<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 736, loss 3.828343152999878\n",
      "Epoch 3: |          | 737/? [10:16<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 737, loss 4.587080955505371\n",
      "Epoch 3: |          | 738/? [10:17<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 738, loss 3.7737255096435547\n",
      "Epoch 3: |          | 739/? [10:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 739, loss 4.341916084289551\n",
      "Epoch 3: |          | 740/? [10:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 740, loss 3.9170334339141846\n",
      "Epoch 3: |          | 741/? [10:19<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 741, loss 4.139168739318848\n",
      "Epoch 3: |          | 742/? [10:20<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 742, loss 4.557284832000732\n",
      "Epoch 3: |          | 743/? [10:21<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 743, loss 4.378967761993408\n",
      "Epoch 3: |          | 744/? [10:22<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 744, loss 4.296466827392578\n",
      "Epoch 3: |          | 745/? [10:23<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 745, loss 3.9276199340820312\n",
      "Epoch 3: |          | 746/? [10:23<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 746, loss 4.235801696777344\n",
      "Epoch 3: |          | 747/? [10:24<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 747, loss 4.0015974044799805\n",
      "Epoch 3: |          | 748/? [10:25<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 748, loss 3.1320300102233887\n",
      "Epoch 3: |          | 749/? [10:26<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 749, loss 4.158328533172607\n",
      "Epoch 3: |          | 750/? [10:27<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 750, loss 4.482576370239258\n",
      "Epoch 3: |          | 751/? [10:27<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 751, loss 2.819831609725952\n",
      "Epoch 3: |          | 752/? [10:28<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 752, loss 4.3650288581848145\n",
      "Epoch 3: |          | 753/? [10:29<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 753, loss 3.5220584869384766\n",
      "Epoch 3: |          | 754/? [10:30<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 754, loss 3.9932472705841064\n",
      "Epoch 3: |          | 755/? [10:31<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 755, loss 3.7662177085876465\n",
      "Epoch 3: |          | 756/? [10:32<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 756, loss 4.158934593200684\n",
      "Epoch 3: |          | 757/? [10:33<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 757, loss 4.190690994262695\n",
      "Epoch 3: |          | 758/? [10:33<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 758, loss 3.9161198139190674\n",
      "Epoch 3: |          | 759/? [10:34<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 759, loss 3.9201266765594482\n",
      "Epoch 3: |          | 760/? [10:35<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 760, loss 4.367026329040527\n",
      "Epoch 3: |          | 761/? [10:36<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 761, loss 4.439135551452637\n",
      "Epoch 3: |          | 762/? [10:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 762, loss 4.1167521476745605\n",
      "Epoch 3: |          | 763/? [10:38<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 763, loss 4.368015289306641\n",
      "Epoch 3: |          | 764/? [10:38<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 764, loss 4.552277565002441\n",
      "Epoch 3: |          | 765/? [10:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 765, loss 4.283949851989746\n",
      "Epoch 3: |          | 766/? [10:40<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 766, loss 4.650008678436279\n",
      "Epoch 3: |          | 767/? [10:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 767, loss 4.77787971496582\n",
      "Epoch 3: |          | 768/? [10:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 768, loss 4.262846946716309\n",
      "Epoch 3: |          | 769/? [10:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 769, loss 3.4080615043640137\n",
      "Epoch 3: |          | 770/? [10:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 770, loss 3.998746871948242\n",
      "Epoch 3: |          | 771/? [10:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 771, loss 4.764081001281738\n",
      "Epoch 3: |          | 772/? [10:45<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 772, loss 4.49105978012085\n",
      "Epoch 3: |          | 773/? [10:46<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 773, loss 4.117049217224121\n",
      "Epoch 3: |          | 774/? [10:47<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 774, loss 4.266033172607422\n",
      "Epoch 3: |          | 775/? [10:48<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 775, loss 4.73049259185791\n",
      "Epoch 3: |          | 776/? [10:49<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 776, loss 4.177203178405762\n",
      "Epoch 3: |          | 777/? [10:49<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 777, loss 4.1387529373168945\n",
      "Epoch 3: |          | 778/? [10:50<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 778, loss 4.447693347930908\n",
      "Epoch 3: |          | 779/? [10:51<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 779, loss 4.8782854080200195\n",
      "Epoch 3: |          | 780/? [10:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 780, loss 3.792236328125\n",
      "Epoch 3: |          | 781/? [10:53<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 781, loss 3.9416656494140625\n",
      "Epoch 3: |          | 782/? [10:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 782, loss 4.344513893127441\n",
      "Epoch 3: |          | 783/? [10:55<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 783, loss 4.416908264160156\n",
      "Epoch 3: |          | 784/? [10:55<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 784, loss 3.9618937969207764\n",
      "Epoch 3: |          | 785/? [10:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 785, loss 3.8182456493377686\n",
      "Epoch 3: |          | 786/? [10:57<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 786, loss 4.67496395111084\n",
      "Epoch 3: |          | 787/? [10:58<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 787, loss 4.644751071929932\n",
      "Epoch 3: |          | 788/? [10:59<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 788, loss 3.046038866043091\n",
      "Epoch 3: |          | 789/? [11:00<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 789, loss 4.1307854652404785\n",
      "Epoch 3: |          | 790/? [11:01<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 790, loss 4.9638190269470215\n",
      "Epoch 3: |          | 791/? [11:01<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 791, loss 4.639974117279053\n",
      "Epoch 3: |          | 792/? [11:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 792, loss 3.7834019660949707\n",
      "Epoch 3: |          | 793/? [11:03<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 793, loss 4.274491310119629\n",
      "Epoch 3: |          | 794/? [11:04<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 794, loss 4.640570640563965\n",
      "Epoch 3: |          | 795/? [11:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 795, loss 4.1671857833862305\n",
      "Epoch 3: |          | 796/? [11:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 796, loss 4.561749458312988\n",
      "Epoch 3: |          | 797/? [11:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 797, loss 3.5087599754333496\n",
      "Epoch 3: |          | 798/? [11:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 798, loss 3.6139864921569824\n",
      "Epoch 3: |          | 799/? [11:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 799, loss 4.559453010559082\n",
      "Epoch 3: |          | 800/? [11:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 800, loss 4.411168098449707\n",
      "Epoch 3: |          | 801/? [11:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 801, loss 3.8644721508026123\n",
      "Epoch 3: |          | 802/? [11:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 802, loss 4.300793647766113\n",
      "Epoch 3: |          | 803/? [11:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 803, loss 4.115393161773682\n",
      "Epoch 3: |          | 804/? [11:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 804, loss 4.3084821701049805\n",
      "Epoch 3: |          | 805/? [11:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 805, loss 4.58557653427124\n",
      "Epoch 3: |          | 806/? [11:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 806, loss 4.835605144500732\n",
      "Epoch 3: |          | 807/? [11:15<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 807, loss 4.297552108764648\n",
      "Epoch 3: |          | 808/? [11:16<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 808, loss 3.80208158493042\n",
      "Epoch 3: |          | 809/? [11:16<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 809, loss 4.3968119621276855\n",
      "Epoch 3: |          | 810/? [11:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 810, loss 4.201245307922363\n",
      "Epoch 3: |          | 811/? [11:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 811, loss 4.487814903259277\n",
      "Epoch 3: |          | 812/? [11:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 812, loss 5.116015434265137\n",
      "Epoch 3: |          | 813/? [11:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 813, loss 4.777115821838379\n",
      "Epoch 3: |          | 814/? [11:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 814, loss 3.786283493041992\n",
      "Epoch 3: |          | 815/? [11:21<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 815, loss 4.547541618347168\n",
      "Epoch 3: |          | 816/? [11:22<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 816, loss 4.368117809295654\n",
      "Epoch 3: |          | 817/? [11:23<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 817, loss 3.6868178844451904\n",
      "Epoch 3: |          | 818/? [11:24<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 818, loss 4.716089725494385\n",
      "Epoch 3: |          | 819/? [11:25<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 819, loss 4.393863201141357\n",
      "Epoch 3: |          | 820/? [11:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 820, loss 4.241542816162109\n",
      "Epoch 3: |          | 821/? [11:26<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 821, loss 4.185658931732178\n",
      "Epoch 3: |          | 822/? [11:27<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 822, loss 3.812093734741211\n",
      "Epoch 3: |          | 823/? [11:28<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 823, loss 3.8307647705078125\n",
      "Epoch 3: |          | 824/? [11:29<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 824, loss 4.312507629394531\n",
      "Epoch 3: |          | 825/? [11:30<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 825, loss 3.8725624084472656\n",
      "Epoch 3: |          | 826/? [11:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 826, loss 4.382858753204346\n",
      "Epoch 3: |          | 827/? [11:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 827, loss 4.015640735626221\n",
      "Epoch 3: |          | 828/? [11:32<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 828, loss 4.522311210632324\n",
      "Epoch 3: |          | 829/? [11:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 829, loss 4.187465667724609\n",
      "Epoch 3: |          | 830/? [11:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 830, loss 4.805967330932617\n",
      "Epoch 3: |          | 831/? [11:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 831, loss 2.639868974685669\n",
      "Epoch 3: |          | 832/? [11:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 832, loss 4.163641452789307\n",
      "Epoch 3: |          | 833/? [11:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 833, loss 4.037685394287109\n",
      "Epoch 3: |          | 834/? [11:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 834, loss 4.806700706481934\n",
      "Epoch 3: |          | 835/? [11:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 835, loss 4.202328681945801\n",
      "Epoch 3: |          | 836/? [11:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 836, loss 4.812562465667725\n",
      "Epoch 3: |          | 837/? [11:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 837, loss 4.262933731079102\n",
      "Epoch 3: |          | 838/? [11:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 838, loss 3.6154937744140625\n",
      "Epoch 3: |          | 839/? [11:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 839, loss 3.8977761268615723\n",
      "Epoch 3: |          | 840/? [11:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 840, loss 4.4750237464904785\n",
      "Epoch 3: |          | 841/? [11:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 841, loss 4.529120445251465\n",
      "Epoch 3: |          | 842/? [11:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 842, loss 4.215536594390869\n",
      "Epoch 3: |          | 843/? [11:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 843, loss 4.577634334564209\n",
      "Epoch 3: |          | 844/? [11:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 844, loss 3.8714823722839355\n",
      "Epoch 3: |          | 845/? [11:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 845, loss 4.286402702331543\n",
      "Epoch 3: |          | 846/? [11:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 846, loss 4.822802543640137\n",
      "Epoch 3: |          | 847/? [11:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 847, loss 4.354634761810303\n",
      "Epoch 3: |          | 848/? [11:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 848, loss 3.850742816925049\n",
      "Epoch 3: |          | 849/? [11:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 849, loss 3.976684093475342\n",
      "Epoch 3: |          | 850/? [11:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 850, loss 4.076216697692871\n",
      "Epoch 3: |          | 851/? [11:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 851, loss 4.422667503356934\n",
      "Epoch 3: |          | 852/? [11:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 852, loss 4.4588704109191895\n",
      "Epoch 3: |          | 853/? [11:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 853, loss 4.373897075653076\n",
      "Epoch 3: |          | 854/? [11:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 854, loss 3.578611373901367\n",
      "Epoch 3: |          | 855/? [11:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 855, loss 3.9201807975769043\n",
      "Epoch 3: |          | 856/? [11:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 856, loss 3.8647704124450684\n",
      "Epoch 3: |          | 857/? [11:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 857, loss 4.414939880371094\n",
      "Epoch 3: |          | 858/? [11:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 858, loss 4.338308811187744\n",
      "Epoch 3: |          | 859/? [11:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 859, loss 4.351893424987793\n",
      "Epoch 3: |          | 860/? [12:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 860, loss 4.720151424407959\n",
      "Epoch 3: |          | 861/? [12:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 861, loss 3.9584522247314453\n",
      "Epoch 3: |          | 862/? [12:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 862, loss 4.376213550567627\n",
      "Epoch 3: |          | 863/? [12:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 863, loss 3.6281425952911377\n",
      "Epoch 3: |          | 864/? [12:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 864, loss 4.317955017089844\n",
      "Epoch 3: |          | 865/? [12:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 865, loss 4.247162818908691\n",
      "Epoch 3: |          | 866/? [12:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 866, loss 3.2428410053253174\n",
      "Epoch 3: |          | 867/? [12:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 867, loss 3.4711506366729736\n",
      "Epoch 3: |          | 868/? [12:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 868, loss 4.375422954559326\n",
      "Epoch 3: |          | 869/? [12:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 869, loss 4.368861198425293\n",
      "Epoch 3: |          | 870/? [12:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 870, loss 3.9117648601531982\n",
      "Epoch 3: |          | 871/? [12:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 871, loss 4.357775688171387\n",
      "Epoch 3: |          | 872/? [12:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 872, loss 4.1686787605285645\n",
      "Epoch 3: |          | 873/? [12:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 873, loss 4.13705587387085\n",
      "Epoch 3: |          | 874/? [12:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 874, loss 3.6527352333068848\n",
      "Epoch 3: |          | 875/? [12:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 875, loss 4.353584289550781\n",
      "Epoch 3: |          | 876/? [12:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 876, loss 3.9970412254333496\n",
      "Epoch 3: |          | 877/? [12:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 877, loss 4.298424243927002\n",
      "Epoch 3: |          | 878/? [12:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 878, loss 3.7259299755096436\n",
      "Epoch 3: |          | 879/? [12:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 879, loss 3.7927372455596924\n",
      "Epoch 3: |          | 880/? [12:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 880, loss 4.924691200256348\n",
      "Epoch 3: |          | 881/? [12:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 881, loss 4.308460235595703\n",
      "Epoch 3: |          | 882/? [12:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 882, loss 4.078581809997559\n",
      "Epoch 3: |          | 883/? [12:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 883, loss 4.22044038772583\n",
      "Epoch 3: |          | 884/? [12:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 884, loss 4.298370361328125\n",
      "Epoch 3: |          | 885/? [12:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 885, loss 3.999404191970825\n",
      "Epoch 3: |          | 886/? [12:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 886, loss 4.6626410484313965\n",
      "Epoch 3: |          | 887/? [12:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 887, loss 4.722911834716797\n",
      "Epoch 3: |          | 888/? [12:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 888, loss 4.435238838195801\n",
      "Epoch 3: |          | 889/? [12:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 889, loss 4.024303436279297\n",
      "Epoch 3: |          | 890/? [12:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 890, loss 4.298720359802246\n",
      "Epoch 3: |          | 891/? [12:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 891, loss 3.9167027473449707\n",
      "Epoch 3: |          | 892/? [12:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 892, loss 4.640591621398926\n",
      "Epoch 3: |          | 893/? [12:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 893, loss 4.068210601806641\n",
      "Epoch 3: |          | 894/? [12:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 894, loss 3.584454298019409\n",
      "Epoch 3: |          | 895/? [12:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 895, loss 4.753174781799316\n",
      "Epoch 3: |          | 896/? [12:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 896, loss 4.3160481452941895\n",
      "Epoch 3: |          | 897/? [12:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 897, loss 4.366806507110596\n",
      "Epoch 3: |          | 898/? [12:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 898, loss 4.3493852615356445\n",
      "Epoch 3: |          | 899/? [12:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 899, loss 4.08587646484375\n",
      "Epoch 3: |          | 900/? [12:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 900, loss 4.018315315246582\n",
      "Epoch 3: |          | 901/? [12:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 901, loss 4.43676233291626\n",
      "Epoch 3: |          | 902/? [12:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 902, loss 4.514902591705322\n",
      "Epoch 3: |          | 903/? [12:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 903, loss 3.7906806468963623\n",
      "Epoch 3: |          | 904/? [12:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 904, loss 4.28680419921875\n",
      "Epoch 3: |          | 905/? [12:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 905, loss 4.531407356262207\n",
      "Epoch 3: |          | 906/? [12:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 906, loss 4.200576305389404\n",
      "Epoch 3: |          | 907/? [12:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 907, loss 4.31381893157959\n",
      "Epoch 3: |          | 908/? [12:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 908, loss 4.401579856872559\n",
      "Epoch 3: |          | 909/? [12:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 909, loss 4.377070426940918\n",
      "Epoch 3: |          | 910/? [12:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 910, loss 4.115358829498291\n",
      "Epoch 3: |          | 911/? [12:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 911, loss 4.164727687835693\n",
      "Epoch 3: |          | 912/? [12:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 912, loss 4.129792213439941\n",
      "Epoch 3: |          | 913/? [12:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 913, loss 4.125374794006348\n",
      "Epoch 3: |          | 914/? [12:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 914, loss 4.43169641494751\n",
      "Epoch 3: |          | 915/? [12:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 915, loss 4.314377784729004\n",
      "Epoch 3: |          | 916/? [12:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 916, loss 4.160947322845459\n",
      "Epoch 3: |          | 917/? [12:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 917, loss 4.1625776290893555\n",
      "Epoch 3: |          | 918/? [12:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 918, loss 4.05894660949707\n",
      "Epoch 3: |          | 919/? [12:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 919, loss 4.022389888763428\n",
      "Epoch 3: |          | 920/? [12:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 920, loss 4.198779582977295\n",
      "Epoch 3: |          | 921/? [12:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 921, loss 4.045810222625732\n",
      "Epoch 3: |          | 922/? [12:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 922, loss 4.216120719909668\n",
      "Epoch 3: |          | 923/? [12:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 923, loss 4.043093204498291\n",
      "Epoch 3: |          | 924/? [12:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 924, loss 4.038168907165527\n",
      "Epoch 3: |          | 925/? [13:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 925, loss 4.358590126037598\n",
      "Epoch 3: |          | 926/? [13:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 926, loss 4.156510829925537\n",
      "Epoch 3: |          | 927/? [13:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 927, loss 4.367093563079834\n",
      "Epoch 3: |          | 928/? [13:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 928, loss 3.8381359577178955\n",
      "Epoch 3: |          | 929/? [13:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 929, loss 3.9639153480529785\n",
      "Epoch 3: |          | 930/? [13:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 930, loss 3.883924961090088\n",
      "Epoch 3: |          | 931/? [13:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 931, loss 3.566493272781372\n",
      "Epoch 3: |          | 932/? [13:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 932, loss 4.281888484954834\n",
      "Epoch 3: |          | 933/? [13:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 933, loss 4.0519890785217285\n",
      "Epoch 3: |          | 934/? [13:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 934, loss 4.666142463684082\n",
      "Epoch 3: |          | 935/? [13:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 935, loss 4.910464286804199\n",
      "Epoch 3: |          | 936/? [13:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 936, loss 4.163140296936035\n",
      "Epoch 3: |          | 937/? [13:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 937, loss 4.364348411560059\n",
      "Epoch 3: |          | 938/? [13:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 938, loss 4.0540361404418945\n",
      "Epoch 3: |          | 939/? [13:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 939, loss 4.345298767089844\n",
      "Epoch 3: |          | 940/? [13:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 940, loss 4.584336757659912\n",
      "Epoch 3: |          | 941/? [13:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 941, loss 3.996145725250244\n",
      "Epoch 3: |          | 942/? [13:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 942, loss 3.5181877613067627\n",
      "Epoch 3: |          | 943/? [13:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 943, loss 4.451435089111328\n",
      "Epoch 3: |          | 944/? [13:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 944, loss 3.4155967235565186\n",
      "Epoch 3: |          | 945/? [13:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 945, loss 4.123106956481934\n",
      "Epoch 3: |          | 946/? [13:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 946, loss 4.148904800415039\n",
      "Epoch 3: |          | 947/? [13:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 947, loss 4.0136847496032715\n",
      "Epoch 3: |          | 948/? [13:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 948, loss 4.241112232208252\n",
      "Epoch 3: |          | 949/? [13:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 949, loss 4.103318691253662\n",
      "Epoch 3: |          | 950/? [13:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 950, loss 3.890773296356201\n",
      "Epoch 3: |          | 951/? [13:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 951, loss 4.583244323730469\n",
      "Epoch 3: |          | 952/? [13:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 952, loss 4.5531134605407715\n",
      "Epoch 3: |          | 953/? [13:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 953, loss 5.068721771240234\n",
      "Epoch 3: |          | 954/? [13:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 954, loss 4.051785945892334\n",
      "Epoch 3: |          | 955/? [13:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 955, loss 4.7515363693237305\n",
      "Epoch 3: |          | 956/? [13:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 956, loss 4.0792412757873535\n",
      "Epoch 3: |          | 957/? [13:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 957, loss 4.363259792327881\n",
      "Epoch 3: |          | 958/? [13:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 958, loss 4.590559482574463\n",
      "Epoch 3: |          | 959/? [13:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 959, loss 4.2774224281311035\n",
      "Epoch 3: |          | 960/? [13:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 960, loss 4.540310859680176\n",
      "Epoch 3: |          | 961/? [13:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 961, loss 4.696653366088867\n",
      "Epoch 3: |          | 962/? [13:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 962, loss 4.27040433883667\n",
      "Epoch 3: |          | 963/? [13:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 963, loss 3.9650015830993652\n",
      "Epoch 3: |          | 964/? [13:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 964, loss 4.4412946701049805\n",
      "Epoch 3: |          | 965/? [13:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 965, loss 3.9382431507110596\n",
      "Epoch 3: |          | 966/? [13:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 966, loss 3.8419899940490723\n",
      "Epoch 3: |          | 967/? [13:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 967, loss 4.072121620178223\n",
      "Epoch 3: |          | 968/? [13:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 968, loss 4.059823036193848\n",
      "Epoch 3: |          | 969/? [13:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 969, loss 3.842787504196167\n",
      "Epoch 3: |          | 970/? [13:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 970, loss 4.3443169593811035\n",
      "Epoch 3: |          | 971/? [13:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 971, loss 4.6910014152526855\n",
      "Epoch 3: |          | 972/? [13:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 972, loss 3.994579315185547\n",
      "Epoch 3: |          | 973/? [13:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 973, loss 4.2674713134765625\n",
      "Epoch 3: |          | 974/? [13:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 974, loss 4.213056564331055\n",
      "Epoch 3: |          | 975/? [13:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 975, loss 4.211658954620361\n",
      "Epoch 3: |          | 976/? [13:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 976, loss 4.299731254577637\n",
      "Epoch 3: |          | 977/? [13:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 977, loss 4.8911848068237305\n",
      "Epoch 3: |          | 978/? [13:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 978, loss 4.463559627532959\n",
      "Epoch 3: |          | 979/? [13:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 979, loss 4.623749732971191\n",
      "Epoch 3: |          | 980/? [13:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 980, loss 3.8108315467834473\n",
      "Epoch 3: |          | 981/? [13:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 981, loss 3.544184446334839\n",
      "Epoch 3: |          | 982/? [13:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 982, loss 4.240782260894775\n",
      "Epoch 3: |          | 983/? [13:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 983, loss 4.6322832107543945\n",
      "Epoch 3: |          | 984/? [13:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 984, loss 3.6735634803771973\n",
      "Epoch 3: |          | 985/? [13:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 985, loss 3.9711899757385254\n",
      "Epoch 3: |          | 986/? [13:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 986, loss 3.9533283710479736\n",
      "Epoch 3: |          | 987/? [13:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 987, loss 3.5261967182159424\n",
      "Epoch 3: |          | 988/? [13:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 988, loss 4.536080360412598\n",
      "Epoch 3: |          | 989/? [13:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 989, loss 4.206138610839844\n",
      "Epoch 3: |          | 990/? [13:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 990, loss 3.405041456222534\n",
      "Epoch 3: |          | 991/? [13:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 991, loss 4.201189994812012\n",
      "Epoch 3: |          | 992/? [13:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 992, loss 4.922909259796143\n",
      "Epoch 3: |          | 993/? [13:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 993, loss 4.0005950927734375\n",
      "Epoch 3: |          | 994/? [13:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 994, loss 4.035661220550537\n",
      "Epoch 3: |          | 995/? [13:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 995, loss 4.491334915161133\n",
      "Epoch 3: |          | 996/? [13:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 996, loss 4.459730625152588\n",
      "Epoch 3: |          | 997/? [13:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 997, loss 4.0562825202941895\n",
      "Epoch 3: |          | 998/? [14:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 998, loss 4.305656433105469\n",
      "Epoch 3: |          | 999/? [14:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 999, loss 4.420107841491699\n",
      "Epoch 3: |          | 1000/? [14:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1000, loss 3.8327109813690186\n",
      "Epoch 3: |          | 1001/? [14:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1001, loss 4.5009379386901855\n",
      "Epoch 3: |          | 1002/? [14:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1002, loss 4.488479137420654\n",
      "Epoch 3: |          | 1003/? [14:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1003, loss 4.611016750335693\n",
      "Epoch 3: |          | 1004/? [14:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1004, loss 3.4860260486602783\n",
      "Epoch 3: |          | 1005/? [14:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1005, loss 4.125042915344238\n",
      "Epoch 3: |          | 1006/? [14:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1006, loss 4.50832462310791\n",
      "Epoch 3: |          | 1007/? [14:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1007, loss 4.092557907104492\n",
      "Epoch 3: |          | 1008/? [14:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1008, loss 4.164963722229004\n",
      "Epoch 3: |          | 1009/? [14:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1009, loss 4.509116172790527\n",
      "Epoch 3: |          | 1010/? [14:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1010, loss 3.5778260231018066\n",
      "Epoch 3: |          | 1011/? [14:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1011, loss 4.190852642059326\n",
      "Epoch 3: |          | 1012/? [14:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1012, loss 3.934325695037842\n",
      "Epoch 3: |          | 1013/? [14:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1013, loss 4.2626142501831055\n",
      "Epoch 3: |          | 1014/? [14:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1014, loss 4.574564456939697\n",
      "Epoch 3: |          | 1015/? [14:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1015, loss 4.242944717407227\n",
      "Epoch 3: |          | 1016/? [14:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1016, loss 4.078796863555908\n",
      "Epoch 3: |          | 1017/? [14:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1017, loss 3.606945514678955\n",
      "Epoch 3: |          | 1018/? [14:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1018, loss 4.15604305267334\n",
      "Epoch 3: |          | 1019/? [14:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1019, loss 4.1815571784973145\n",
      "Epoch 3: |          | 1020/? [14:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1020, loss 3.7197728157043457\n",
      "Epoch 3: |          | 1021/? [14:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1021, loss 3.9806277751922607\n",
      "Epoch 3: |          | 1022/? [14:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1022, loss 3.739077091217041\n",
      "Epoch 3: |          | 1023/? [14:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1023, loss 3.455991268157959\n",
      "Epoch 3: |          | 1024/? [14:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1024, loss 4.020416736602783\n",
      "Epoch 3: |          | 1025/? [14:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1025, loss 3.9412848949432373\n",
      "Epoch 3: |          | 1026/? [14:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1026, loss 2.9566073417663574\n",
      "Epoch 3: |          | 1027/? [14:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1027, loss 4.244992256164551\n",
      "Epoch 3: |          | 1028/? [14:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1028, loss 4.063505172729492\n",
      "Epoch 3: |          | 1029/? [14:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1029, loss 4.00681734085083\n",
      "Epoch 3: |          | 1030/? [14:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1030, loss 3.7512309551239014\n",
      "Epoch 3: |          | 1031/? [14:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1031, loss 3.8265883922576904\n",
      "Epoch 3: |          | 1032/? [14:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1032, loss 4.397186279296875\n",
      "Epoch 3: |          | 1033/? [14:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1033, loss 4.569842338562012\n",
      "Epoch 3: |          | 1034/? [14:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1034, loss 3.951096296310425\n",
      "Epoch 3: |          | 1035/? [14:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1035, loss 3.966115951538086\n",
      "Epoch 3: |          | 1036/? [14:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1036, loss 3.847230911254883\n",
      "Epoch 3: |          | 1037/? [14:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1037, loss 4.51338529586792\n",
      "Epoch 3: |          | 1038/? [14:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1038, loss 4.674830913543701\n",
      "Epoch 3: |          | 1039/? [14:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1039, loss 4.916194915771484\n",
      "Epoch 3: |          | 1040/? [14:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1040, loss 4.255915641784668\n",
      "Epoch 3: |          | 1041/? [14:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1041, loss 4.594018936157227\n",
      "Epoch 3: |          | 1042/? [14:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1042, loss 4.1305251121521\n",
      "Epoch 3: |          | 1043/? [14:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1043, loss 4.488277435302734\n",
      "Epoch 3: |          | 1044/? [14:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1044, loss 4.05232572555542\n",
      "Epoch 3: |          | 1045/? [14:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1045, loss 3.67358660697937\n",
      "Epoch 3: |          | 1046/? [14:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1046, loss 3.474668502807617\n",
      "Epoch 3: |          | 1047/? [14:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1047, loss 4.705645561218262\n",
      "Epoch 3: |          | 1048/? [14:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1048, loss 4.000848293304443\n",
      "Epoch 3: |          | 1049/? [14:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1049, loss 4.260188579559326\n",
      "Epoch 3: |          | 1050/? [14:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1050, loss 3.819174289703369\n",
      "Epoch 3: |          | 1051/? [14:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1051, loss 3.888476610183716\n",
      "Epoch 3: |          | 1052/? [14:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1052, loss 4.4908013343811035\n",
      "Epoch 3: |          | 1053/? [14:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1053, loss 4.572450637817383\n",
      "Epoch 3: |          | 1054/? [14:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1054, loss 3.9944660663604736\n",
      "Epoch 3: |          | 1055/? [14:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1055, loss 3.69230318069458\n",
      "Epoch 3: |          | 1056/? [14:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1056, loss 3.6509692668914795\n",
      "Epoch 3: |          | 1057/? [14:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1057, loss 4.3657121658325195\n",
      "Epoch 3: |          | 1058/? [14:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1058, loss 3.8970985412597656\n",
      "Epoch 3: |          | 1059/? [14:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1059, loss 4.5792670249938965\n",
      "Epoch 3: |          | 1060/? [14:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1060, loss 4.419973373413086\n",
      "Epoch 3: |          | 1061/? [14:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1061, loss 3.0469822883605957\n",
      "Epoch 3: |          | 1062/? [14:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1062, loss 4.046879768371582\n",
      "Epoch 3: |          | 1063/? [14:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1063, loss 4.138336181640625\n",
      "Epoch 3: |          | 1064/? [14:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1064, loss 4.335697174072266\n",
      "Epoch 3: |          | 1065/? [14:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1065, loss 3.0005130767822266\n",
      "Epoch 3: |          | 1066/? [14:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1066, loss 4.287178039550781\n",
      "Epoch 3: |          | 1067/? [14:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1067, loss 3.7312958240509033\n",
      "Epoch 3: |          | 1068/? [14:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1068, loss 3.8509724140167236\n",
      "Epoch 3: |          | 1069/? [15:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1069, loss 4.293007850646973\n",
      "Epoch 3: |          | 1070/? [15:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1070, loss 4.0452141761779785\n",
      "Epoch 3: |          | 1071/? [15:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1071, loss 4.399531364440918\n",
      "Epoch 3: |          | 1072/? [15:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1072, loss 4.429394721984863\n",
      "Epoch 3: |          | 1073/? [15:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1073, loss 4.694599628448486\n",
      "Epoch 3: |          | 1074/? [15:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1074, loss 3.95662260055542\n",
      "Epoch 3: |          | 1075/? [15:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1075, loss 3.7817845344543457\n",
      "Epoch 3: |          | 1076/? [15:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1076, loss 4.486225128173828\n",
      "Epoch 3: |          | 1077/? [15:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1077, loss 3.9232144355773926\n",
      "Epoch 3: |          | 1078/? [15:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1078, loss 4.145997047424316\n",
      "Epoch 3: |          | 1079/? [15:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1079, loss 4.696763038635254\n",
      "Epoch 3: |          | 1080/? [15:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1080, loss 4.166337966918945\n",
      "Epoch 3: |          | 1081/? [15:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1081, loss 4.391127109527588\n",
      "Epoch 3: |          | 1082/? [15:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1082, loss 3.9073033332824707\n",
      "Epoch 3: |          | 1083/? [15:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1083, loss 3.603286027908325\n",
      "Epoch 3: |          | 1084/? [15:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1084, loss 3.343594789505005\n",
      "Epoch 3: |          | 1085/? [15:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1085, loss 4.010165214538574\n",
      "Epoch 3: |          | 1086/? [15:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1086, loss 4.310250759124756\n",
      "Epoch 3: |          | 1087/? [15:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1087, loss 4.825370788574219\n",
      "Epoch 3: |          | 1088/? [15:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1088, loss 4.426966190338135\n",
      "Epoch 3: |          | 1089/? [15:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1089, loss 4.495234966278076\n",
      "Epoch 3: |          | 1090/? [15:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1090, loss 4.192159652709961\n",
      "Epoch 3: |          | 1091/? [15:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1091, loss 3.9297237396240234\n",
      "Epoch 3: |          | 1092/? [15:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1092, loss 4.236212253570557\n",
      "Epoch 3: |          | 1093/? [15:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1093, loss 3.7150988578796387\n",
      "Epoch 3: |          | 1094/? [15:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1094, loss 4.224761486053467\n",
      "Epoch 3: |          | 1095/? [15:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1095, loss 4.279202938079834\n",
      "Epoch 3: |          | 1096/? [15:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1096, loss 4.506436347961426\n",
      "Epoch 3: |          | 1097/? [15:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1097, loss 4.081234455108643\n",
      "Epoch 3: |          | 1098/? [15:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1098, loss 3.333308696746826\n",
      "Epoch 3: |          | 1099/? [15:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1099, loss 4.052423477172852\n",
      "Epoch 3: |          | 1100/? [15:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1100, loss 4.283174514770508\n",
      "Epoch 3: |          | 1101/? [15:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1101, loss 3.834383487701416\n",
      "Epoch 3: |          | 1102/? [15:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1102, loss 4.700325965881348\n",
      "Epoch 3: |          | 1103/? [15:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1103, loss 5.253807544708252\n",
      "Epoch 3: |          | 1104/? [15:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1104, loss 4.431947231292725\n",
      "Epoch 3: |          | 1105/? [15:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1105, loss 4.5467424392700195\n",
      "Epoch 3: |          | 1106/? [15:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1106, loss 4.050112724304199\n",
      "Epoch 3: |          | 1107/? [15:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1107, loss 4.145127296447754\n",
      "Epoch 3: |          | 1108/? [15:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1108, loss 4.133694648742676\n",
      "Epoch 3: |          | 1109/? [15:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1109, loss 3.698484420776367\n",
      "Epoch 3: |          | 1110/? [15:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1110, loss 4.763487815856934\n",
      "Epoch 3: |          | 1111/? [15:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1111, loss 4.410208702087402\n",
      "Epoch 3: |          | 1112/? [15:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1112, loss 4.277237415313721\n",
      "Epoch 3: |          | 1113/? [15:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1113, loss 4.060919284820557\n",
      "Epoch 3: |          | 1114/? [15:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1114, loss 3.4762606620788574\n",
      "Epoch 3: |          | 1115/? [15:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1115, loss 3.2043120861053467\n",
      "Epoch 3: |          | 1116/? [15:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1116, loss 3.6485512256622314\n",
      "Epoch 3: |          | 1117/? [15:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1117, loss 3.926191806793213\n",
      "Epoch 3: |          | 1118/? [15:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1118, loss 3.945892333984375\n",
      "Epoch 3: |          | 1119/? [15:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1119, loss 4.625103950500488\n",
      "Epoch 3: |          | 1120/? [15:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1120, loss 4.1226677894592285\n",
      "Epoch 3: |          | 1121/? [15:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1121, loss 4.402716636657715\n",
      "Epoch 3: |          | 1122/? [15:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1122, loss 4.043030738830566\n",
      "Epoch 3: |          | 1123/? [15:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1123, loss 4.172555923461914\n",
      "Epoch 3: |          | 1124/? [15:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1124, loss 4.553189754486084\n",
      "Epoch 3: |          | 1125/? [15:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1125, loss 3.806605577468872\n",
      "Epoch 3: |          | 1126/? [15:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1126, loss 3.7358779907226562\n",
      "Epoch 3: |          | 1127/? [15:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1127, loss 4.034677505493164\n",
      "Epoch 3: |          | 1128/? [15:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1128, loss 4.093074321746826\n",
      "Epoch 3: |          | 1129/? [15:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1129, loss 4.209560394287109\n",
      "Epoch 3: |          | 1130/? [15:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1130, loss 4.409907817840576\n",
      "Epoch 3: |          | 1131/? [15:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1131, loss 4.49406623840332\n",
      "Epoch 3: |          | 1132/? [15:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1132, loss 3.199800729751587\n",
      "Epoch 3: |          | 1133/? [15:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1133, loss 4.166335105895996\n",
      "Epoch 3: |          | 1134/? [15:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1134, loss 3.8638038635253906\n",
      "Epoch 3: |          | 1135/? [15:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1135, loss 4.501163482666016\n",
      "Epoch 3: |          | 1136/? [15:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1136, loss 4.145018577575684\n",
      "Epoch 3: |          | 1137/? [15:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1137, loss 4.1856842041015625\n",
      "Epoch 3: |          | 1138/? [15:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1138, loss 4.694437503814697\n",
      "Epoch 3: |          | 1139/? [15:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1139, loss 4.7371015548706055\n",
      "Epoch 3: |          | 1140/? [15:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1140, loss 4.1389055252075195\n",
      "Epoch 3: |          | 1141/? [16:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1141, loss 4.465003967285156\n",
      "Epoch 3: |          | 1142/? [16:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1142, loss 4.622544288635254\n",
      "Epoch 3: |          | 1143/? [16:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1143, loss 4.625971794128418\n",
      "Epoch 3: |          | 1144/? [16:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1144, loss 4.05330228805542\n",
      "Epoch 3: |          | 1145/? [16:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1145, loss 4.136176586151123\n",
      "Epoch 3: |          | 1146/? [16:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1146, loss 3.8334126472473145\n",
      "Epoch 3: |          | 1147/? [16:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1147, loss 3.6493606567382812\n",
      "Epoch 3: |          | 1148/? [16:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1148, loss 3.9549052715301514\n",
      "Epoch 3: |          | 1149/? [16:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1149, loss 4.952320575714111\n",
      "Epoch 3: |          | 1150/? [16:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1150, loss 4.332472324371338\n",
      "Epoch 3: |          | 1151/? [16:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1151, loss 4.68433952331543\n",
      "Epoch 3: |          | 1152/? [16:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1152, loss 3.8560149669647217\n",
      "Epoch 3: |          | 1153/? [16:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1153, loss 4.206773281097412\n",
      "Epoch 3: |          | 1154/? [16:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1154, loss 3.85986065864563\n",
      "Epoch 3: |          | 1155/? [16:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1155, loss 4.122291564941406\n",
      "Epoch 3: |          | 1156/? [16:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1156, loss 4.1822614669799805\n",
      "Epoch 3: |          | 1157/? [16:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1157, loss 4.404156684875488\n",
      "Epoch 3: |          | 1158/? [16:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1158, loss 4.6614789962768555\n",
      "Epoch 3: |          | 1159/? [16:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1159, loss 3.321476459503174\n",
      "Epoch 3: |          | 1160/? [16:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1160, loss 4.546305179595947\n",
      "Epoch 3: |          | 1161/? [16:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1161, loss 4.450078010559082\n",
      "Epoch 3: |          | 1162/? [16:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1162, loss 4.342013835906982\n",
      "Epoch 3: |          | 1163/? [16:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1163, loss 4.894848823547363\n",
      "Epoch 3: |          | 1164/? [16:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1164, loss 4.702664375305176\n",
      "Epoch 3: |          | 1165/? [16:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1165, loss 3.7864136695861816\n",
      "Epoch 3: |          | 1166/? [16:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1166, loss 4.341498374938965\n",
      "Epoch 3: |          | 1167/? [16:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1167, loss 4.467648506164551\n",
      "Epoch 3: |          | 1168/? [16:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1168, loss 4.875938892364502\n",
      "Epoch 3: |          | 1169/? [16:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1169, loss 3.87080717086792\n",
      "Epoch 3: |          | 1170/? [16:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1170, loss 4.444520473480225\n",
      "Epoch 3: |          | 1171/? [16:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1171, loss 3.837362766265869\n",
      "Epoch 3: |          | 1172/? [16:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1172, loss 3.7218048572540283\n",
      "Epoch 3: |          | 1173/? [16:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1173, loss 4.3065667152404785\n",
      "Epoch 3: |          | 1174/? [16:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1174, loss 3.8149001598358154\n",
      "Epoch 3: |          | 1175/? [16:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1175, loss 4.409607887268066\n",
      "Epoch 3: |          | 1176/? [16:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1176, loss 4.496129035949707\n",
      "Epoch 3: |          | 1177/? [16:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1177, loss 4.6174211502075195\n",
      "Epoch 3: |          | 1178/? [16:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1178, loss 4.004603862762451\n",
      "Epoch 3: |          | 1179/? [16:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1179, loss 4.5494585037231445\n",
      "Epoch 3: |          | 1180/? [16:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1180, loss 4.376288414001465\n",
      "Epoch 3: |          | 1181/? [16:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1181, loss 4.351993560791016\n",
      "Epoch 3: |          | 1182/? [16:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1182, loss 4.164486885070801\n",
      "Epoch 3: |          | 1183/? [16:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1183, loss 3.8870596885681152\n",
      "Epoch 3: |          | 1184/? [16:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1184, loss 4.1998796463012695\n",
      "Epoch 3: |          | 1185/? [16:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1185, loss 4.028563499450684\n",
      "Epoch 3: |          | 1186/? [16:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1186, loss 4.2828240394592285\n",
      "Epoch 3: |          | 1187/? [16:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1187, loss 4.080944061279297\n",
      "Epoch 3: |          | 1188/? [16:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1188, loss 4.510372161865234\n",
      "Epoch 3: |          | 1189/? [16:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1189, loss 4.552795886993408\n",
      "Epoch 3: |          | 1190/? [16:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1190, loss 4.140216827392578\n",
      "Epoch 3: |          | 1191/? [16:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1191, loss 4.147756099700928\n",
      "Epoch 3: |          | 1192/? [16:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1192, loss 4.448132038116455\n",
      "Epoch 3: |          | 1193/? [16:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1193, loss 3.9357573986053467\n",
      "Epoch 3: |          | 1194/? [16:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1194, loss 3.5511016845703125\n",
      "Epoch 3: |          | 1195/? [16:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1195, loss 4.2002153396606445\n",
      "Epoch 3: |          | 1196/? [16:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1196, loss 4.3893232345581055\n",
      "Epoch 3: |          | 1197/? [16:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1197, loss 4.187529563903809\n",
      "Epoch 3: |          | 1198/? [16:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1198, loss 4.27304220199585\n",
      "Epoch 3: |          | 1199/? [16:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1199, loss 4.513700008392334\n",
      "Epoch 3: |          | 1200/? [16:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1200, loss 3.704124927520752\n",
      "Epoch 3: |          | 1201/? [16:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1201, loss 4.384769916534424\n",
      "Epoch 3: |          | 1202/? [16:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1202, loss 4.000178813934326\n",
      "Epoch 3: |          | 1203/? [16:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1203, loss 4.019062519073486\n",
      "Epoch 3: |          | 1204/? [16:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1204, loss 3.5039565563201904\n",
      "Epoch 3: |          | 1205/? [16:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1205, loss 4.135019302368164\n",
      "Epoch 3: |          | 1206/? [16:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1206, loss 4.146132469177246\n",
      "Epoch 3: |          | 1207/? [16:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1207, loss 4.460176467895508\n",
      "Epoch 3: |          | 1208/? [16:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1208, loss 4.6489386558532715\n",
      "Epoch 3: |          | 1209/? [16:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1209, loss 4.200783729553223\n",
      "Epoch 3: |          | 1210/? [16:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1210, loss 4.51652717590332\n",
      "Epoch 3: |          | 1211/? [17:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1211, loss 4.510169506072998\n",
      "Epoch 3: |          | 1212/? [17:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1212, loss 4.265029430389404\n",
      "Epoch 3: |          | 1213/? [17:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1213, loss 3.9949429035186768\n",
      "Epoch 3: |          | 1214/? [17:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1214, loss 4.626555442810059\n",
      "Epoch 3: |          | 1215/? [17:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1215, loss 3.975647449493408\n",
      "Epoch 3: |          | 1216/? [17:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1216, loss 4.183774948120117\n",
      "Epoch 3: |          | 1217/? [17:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1217, loss 4.310534954071045\n",
      "Epoch 3: |          | 1218/? [17:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1218, loss 4.37709903717041\n",
      "Epoch 3: |          | 1219/? [17:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1219, loss 3.9732208251953125\n",
      "Epoch 3: |          | 1220/? [17:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1220, loss 4.761202812194824\n",
      "Epoch 3: |          | 1221/? [17:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1221, loss 4.310706615447998\n",
      "Epoch 3: |          | 1222/? [17:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1222, loss 3.3112735748291016\n",
      "Epoch 3: |          | 1223/? [17:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1223, loss 3.4951701164245605\n",
      "Epoch 3: |          | 1224/? [17:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1224, loss 3.8812954425811768\n",
      "Epoch 3: |          | 1225/? [17:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1225, loss 4.579995632171631\n",
      "Epoch 3: |          | 1226/? [17:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1226, loss 4.579686164855957\n",
      "Epoch 3: |          | 1227/? [17:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1227, loss 4.162754058837891\n",
      "Epoch 3: |          | 1228/? [17:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1228, loss 4.075900554656982\n",
      "Epoch 3: |          | 1229/? [17:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1229, loss 3.643122911453247\n",
      "Epoch 3: |          | 1230/? [17:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1230, loss 4.323709964752197\n",
      "Epoch 3: |          | 1231/? [17:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1231, loss 4.345122337341309\n",
      "Epoch 3: |          | 1232/? [17:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1232, loss 4.536723613739014\n",
      "Epoch 3: |          | 1233/? [17:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1233, loss 4.324056625366211\n",
      "Epoch 3: |          | 1234/? [17:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1234, loss 3.282762050628662\n",
      "Epoch 3: |          | 1235/? [17:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1235, loss 4.383150100708008\n",
      "Epoch 3: |          | 1236/? [17:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1236, loss 3.7923247814178467\n",
      "Epoch 3: |          | 1237/? [17:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1237, loss 4.150346755981445\n",
      "Epoch 3: |          | 1238/? [17:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1238, loss 4.126833915710449\n",
      "Epoch 3: |          | 1239/? [17:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1239, loss 4.017270088195801\n",
      "Epoch 3: |          | 1240/? [17:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1240, loss 4.677010536193848\n",
      "Epoch 3: |          | 1241/? [17:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1241, loss 4.163835048675537\n",
      "Epoch 3: |          | 1242/? [17:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1242, loss 4.040343284606934\n",
      "Epoch 3: |          | 1243/? [17:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1243, loss 3.858246326446533\n",
      "Epoch 3: |          | 1244/? [17:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1244, loss 4.037367820739746\n",
      "Epoch 3: |          | 1245/? [17:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1245, loss 3.581059694290161\n",
      "Epoch 3: |          | 1246/? [17:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1246, loss 4.351816177368164\n",
      "Epoch 3: |          | 1247/? [17:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1247, loss 4.3815178871154785\n",
      "Epoch 3: |          | 1248/? [17:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1248, loss 3.835486888885498\n",
      "Epoch 3: |          | 1249/? [17:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1249, loss 4.046881675720215\n",
      "Epoch 3: |          | 1250/? [17:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1250, loss 4.222281455993652\n",
      "Epoch 3: |          | 1251/? [17:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1251, loss 3.913874864578247\n",
      "Epoch 3: |          | 1252/? [17:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1252, loss 4.707103729248047\n",
      "Epoch 3: |          | 1253/? [17:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1253, loss 4.060951232910156\n",
      "Epoch 3: |          | 1254/? [17:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1254, loss 3.355056047439575\n",
      "Epoch 3: |          | 1255/? [17:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1255, loss 4.791752815246582\n",
      "Epoch 3: |          | 1256/? [17:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1256, loss 3.795693874359131\n",
      "Epoch 3: |          | 1257/? [17:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1257, loss 3.8223185539245605\n",
      "Epoch 3: |          | 1258/? [17:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1258, loss 4.547078609466553\n",
      "Epoch 3: |          | 1259/? [17:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1259, loss 4.270951747894287\n",
      "Epoch 3: |          | 1260/? [17:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1260, loss 4.662732124328613\n",
      "Epoch 3: |          | 1261/? [17:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1261, loss 4.0383405685424805\n",
      "Epoch 3: |          | 1262/? [17:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1262, loss 3.9375438690185547\n",
      "Epoch 3: |          | 1263/? [17:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1263, loss 4.365897178649902\n",
      "Epoch 3: |          | 1264/? [17:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1264, loss 4.6378583908081055\n",
      "Epoch 3: |          | 1265/? [17:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1265, loss 4.418396949768066\n",
      "Epoch 3: |          | 1266/? [17:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1266, loss 4.086971759796143\n",
      "Epoch 3: |          | 1267/? [17:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1267, loss 4.200236797332764\n",
      "Epoch 3: |          | 1268/? [17:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1268, loss 4.069023132324219\n",
      "Epoch 3: |          | 1269/? [17:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1269, loss 3.5819993019104004\n",
      "Epoch 3: |          | 1270/? [17:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1270, loss 3.9524829387664795\n",
      "Epoch 3: |          | 1271/? [17:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1271, loss 4.158553123474121\n",
      "Epoch 3: |          | 1272/? [17:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1272, loss 3.666416883468628\n",
      "Epoch 3: |          | 1273/? [17:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1273, loss 4.468425273895264\n",
      "Epoch 3: |          | 1274/? [17:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1274, loss 3.281170606613159\n",
      "Epoch 3: |          | 1275/? [17:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1275, loss 3.842825412750244\n",
      "Epoch 3: |          | 1276/? [17:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1276, loss 4.194134712219238\n",
      "Epoch 3: |          | 1277/? [17:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1277, loss 3.874004364013672\n",
      "Epoch 3: |          | 1278/? [17:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1278, loss 3.64829683303833\n",
      "Epoch 3: |          | 1279/? [17:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1279, loss 4.2979326248168945\n",
      "Epoch 3: |          | 1280/? [17:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1280, loss 3.486168622970581\n",
      "Epoch 3: |          | 1281/? [18:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1281, loss 3.9468295574188232\n",
      "Epoch 3: |          | 1282/? [18:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1282, loss 3.6757073402404785\n",
      "Epoch 3: |          | 1283/? [18:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1283, loss 4.450355052947998\n",
      "Epoch 3: |          | 1284/? [18:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1284, loss 3.501382827758789\n",
      "Epoch 3: |          | 1285/? [18:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1285, loss 4.637114524841309\n",
      "Epoch 3: |          | 1286/? [18:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1286, loss 3.05794358253479\n",
      "Epoch 3: |          | 1287/? [18:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1287, loss 4.478570461273193\n",
      "Epoch 3: |          | 1288/? [18:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1288, loss 4.284090995788574\n",
      "Epoch 3: |          | 1289/? [18:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1289, loss 3.3557121753692627\n",
      "Epoch 3: |          | 1290/? [18:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1290, loss 4.233340263366699\n",
      "Epoch 3: |          | 1291/? [18:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1291, loss 5.140103340148926\n",
      "Epoch 3: |          | 1292/? [18:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1292, loss 4.4649739265441895\n",
      "Epoch 3: |          | 1293/? [18:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1293, loss 3.9113945960998535\n",
      "Epoch 3: |          | 1294/? [18:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1294, loss 4.144981384277344\n",
      "Epoch 3: |          | 1295/? [18:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1295, loss 4.214458465576172\n",
      "Epoch 3: |          | 1296/? [18:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1296, loss 3.409879684448242\n",
      "Epoch 3: |          | 1297/? [18:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1297, loss 4.409631252288818\n",
      "Epoch 3: |          | 1298/? [18:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1298, loss 4.150130748748779\n",
      "Epoch 3: |          | 1299/? [18:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1299, loss 3.2074286937713623\n",
      "Epoch 3: |          | 1300/? [18:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1300, loss 4.180296897888184\n",
      "Epoch 3: |          | 1301/? [18:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1301, loss 3.870176315307617\n",
      "Epoch 3: |          | 1302/? [18:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1302, loss 4.0384440422058105\n",
      "Epoch 3: |          | 1303/? [18:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1303, loss 3.872347593307495\n",
      "Epoch 3: |          | 1304/? [18:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1304, loss 4.705651760101318\n",
      "Epoch 3: |          | 1305/? [18:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1305, loss 3.474353313446045\n",
      "Epoch 3: |          | 1306/? [18:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1306, loss 4.128590106964111\n",
      "Epoch 3: |          | 1307/? [18:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1307, loss 3.686464786529541\n",
      "Epoch 3: |          | 1308/? [18:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1308, loss 3.6431565284729004\n",
      "Epoch 3: |          | 1309/? [18:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1309, loss 3.7531471252441406\n",
      "Epoch 3: |          | 1310/? [18:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1310, loss 4.292145729064941\n",
      "Epoch 3: |          | 1311/? [18:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1311, loss 3.688884735107422\n",
      "Epoch 3: |          | 1312/? [18:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1312, loss 3.439182758331299\n",
      "Epoch 3: |          | 1313/? [18:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1313, loss 4.638538360595703\n",
      "Epoch 3: |          | 1314/? [18:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1314, loss 3.8155899047851562\n",
      "Epoch 3: |          | 1315/? [18:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1315, loss 4.610064506530762\n",
      "Epoch 3: |          | 1316/? [18:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1316, loss 4.353882312774658\n",
      "Epoch 3: |          | 1317/? [18:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1317, loss 3.9637973308563232\n",
      "Epoch 3: |          | 1318/? [18:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1318, loss 4.1675848960876465\n",
      "Epoch 3: |          | 1319/? [18:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1319, loss 4.354466438293457\n",
      "Epoch 3: |          | 1320/? [18:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1320, loss 3.9202728271484375\n",
      "Epoch 3: |          | 1321/? [18:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1321, loss 4.372347354888916\n",
      "Epoch 3: |          | 1322/? [18:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1322, loss 4.3060503005981445\n",
      "Epoch 3: |          | 1323/? [18:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1323, loss 3.7579548358917236\n",
      "Epoch 3: |          | 1324/? [18:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1324, loss 4.696854591369629\n",
      "Epoch 3: |          | 1325/? [18:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1325, loss 4.79249906539917\n",
      "Epoch 3: |          | 1326/? [18:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1326, loss 4.226806163787842\n",
      "Epoch 3: |          | 1327/? [18:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1327, loss 4.089383125305176\n",
      "Epoch 3: |          | 1328/? [18:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1328, loss 3.7801480293273926\n",
      "Epoch 3: |          | 1329/? [18:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1329, loss 4.436293601989746\n",
      "Epoch 3: |          | 1330/? [18:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1330, loss 4.160290241241455\n",
      "Epoch 3: |          | 1331/? [18:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1331, loss 4.212307453155518\n",
      "Epoch 3: |          | 1332/? [18:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1332, loss 3.9652678966522217\n",
      "Epoch 3: |          | 1333/? [18:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1333, loss 3.8988285064697266\n",
      "Epoch 3: |          | 1334/? [18:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1334, loss 4.052764892578125\n",
      "Epoch 3: |          | 1335/? [18:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1335, loss 4.013967514038086\n",
      "Epoch 3: |          | 1336/? [18:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1336, loss 3.5680594444274902\n",
      "Epoch 3: |          | 1337/? [18:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1337, loss 4.253608226776123\n",
      "Epoch 3: |          | 1338/? [18:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1338, loss 3.4443187713623047\n",
      "Epoch 3: |          | 1339/? [18:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1339, loss 4.099337100982666\n",
      "Epoch 3: |          | 1340/? [18:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1340, loss 3.5140175819396973\n",
      "Epoch 3: |          | 1341/? [18:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1341, loss 4.31603479385376\n",
      "Epoch 3: |          | 1342/? [18:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1342, loss 4.538494110107422\n",
      "Epoch 3: |          | 1343/? [18:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1343, loss 4.019335746765137\n",
      "Epoch 3: |          | 1344/? [18:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1344, loss 4.118579864501953\n",
      "Epoch 3: |          | 1345/? [18:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1345, loss 4.259711265563965\n",
      "Epoch 3: |          | 1346/? [18:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1346, loss 5.425405979156494\n",
      "Epoch 3: |          | 1347/? [18:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1347, loss 4.437929630279541\n",
      "Epoch 3: |          | 1348/? [18:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1348, loss 4.57539701461792\n",
      "Epoch 3: |          | 1349/? [18:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1349, loss 4.33561897277832\n",
      "Epoch 3: |          | 1350/? [18:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1350, loss 4.5320258140563965\n",
      "Epoch 3: |          | 1351/? [18:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1351, loss 4.408544063568115\n",
      "Epoch 3: |          | 1352/? [19:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1352, loss 3.5420806407928467\n",
      "Epoch 3: |          | 1353/? [19:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1353, loss 3.851940870285034\n",
      "Epoch 3: |          | 1354/? [19:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1354, loss 4.387723922729492\n",
      "Epoch 3: |          | 1355/? [19:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1355, loss 4.521087646484375\n",
      "Epoch 3: |          | 1356/? [19:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1356, loss 4.216647148132324\n",
      "Epoch 3: |          | 1357/? [19:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1357, loss 3.964712619781494\n",
      "Epoch 3: |          | 1358/? [19:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1358, loss 4.167088508605957\n",
      "Epoch 3: |          | 1359/? [19:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1359, loss 4.0346245765686035\n",
      "Epoch 3: |          | 1360/? [19:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1360, loss 4.242579460144043\n",
      "Epoch 3: |          | 1361/? [19:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1361, loss 4.164089202880859\n",
      "Epoch 3: |          | 1362/? [19:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1362, loss 4.025031089782715\n",
      "Epoch 3: |          | 1363/? [19:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1363, loss 3.445561647415161\n",
      "Epoch 3: |          | 1364/? [19:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1364, loss 3.988015651702881\n",
      "Epoch 3: |          | 1365/? [19:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1365, loss 3.642422914505005\n",
      "Epoch 3: |          | 1366/? [19:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1366, loss 4.37557315826416\n",
      "Epoch 3: |          | 1367/? [19:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1367, loss 3.667813777923584\n",
      "Epoch 3: |          | 1368/? [19:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1368, loss 3.437330961227417\n",
      "Epoch 3: |          | 1369/? [19:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1369, loss 4.159395217895508\n",
      "Epoch 3: |          | 1370/? [19:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1370, loss 3.6544761657714844\n",
      "Epoch 3: |          | 1371/? [19:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1371, loss 4.640385627746582\n",
      "Epoch 3: |          | 1372/? [19:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1372, loss 4.052891731262207\n",
      "Epoch 3: |          | 1373/? [19:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1373, loss 4.497039794921875\n",
      "Epoch 3: |          | 1374/? [19:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1374, loss 3.5702762603759766\n",
      "Epoch 3: |          | 1375/? [19:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1375, loss 4.208162307739258\n",
      "Epoch 3: |          | 1376/? [19:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1376, loss 4.094006061553955\n",
      "Epoch 3: |          | 1377/? [19:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1377, loss 4.057819843292236\n",
      "Epoch 3: |          | 1378/? [19:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1378, loss 4.206194877624512\n",
      "Epoch 3: |          | 1379/? [19:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1379, loss 4.110930919647217\n",
      "Epoch 3: |          | 1380/? [19:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1380, loss 4.2149553298950195\n",
      "Epoch 3: |          | 1381/? [19:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1381, loss 4.398715019226074\n",
      "Epoch 3: |          | 1382/? [19:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1382, loss 3.8652358055114746\n",
      "Epoch 3: |          | 1383/? [19:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1383, loss 4.131710052490234\n",
      "Epoch 3: |          | 1384/? [19:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1384, loss 4.14044713973999\n",
      "Epoch 3: |          | 1385/? [19:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1385, loss 4.064093589782715\n",
      "Epoch 3: |          | 1386/? [19:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1386, loss 4.142910957336426\n",
      "Epoch 3: |          | 1387/? [19:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1387, loss 4.107142448425293\n",
      "Epoch 3: |          | 1388/? [19:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1388, loss 3.5392518043518066\n",
      "Epoch 3: |          | 1389/? [19:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1389, loss 4.338629722595215\n",
      "Epoch 3: |          | 1390/? [19:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1390, loss 4.643357276916504\n",
      "Epoch 3: |          | 1391/? [19:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1391, loss 4.261196136474609\n",
      "Epoch 3: |          | 1392/? [19:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1392, loss 3.722059965133667\n",
      "Epoch 3: |          | 1393/? [19:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1393, loss 3.9590423107147217\n",
      "Epoch 3: |          | 1394/? [19:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1394, loss 3.5916073322296143\n",
      "Epoch 3: |          | 1395/? [19:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1395, loss 4.2690324783325195\n",
      "Epoch 3: |          | 1396/? [19:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 1396, loss 4.180691719055176\n",
      "Epoch 3: |          | 1397/? [19:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1397, loss 3.338608980178833\n",
      "Epoch 3: |          | 1398/? [19:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1398, loss 4.572659015655518\n",
      "Epoch 3: |          | 1399/? [19:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1399, loss 4.631579399108887\n",
      "Epoch 3: |          | 1400/? [19:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1400, loss 3.6589157581329346\n",
      "Epoch 3: |          | 1401/? [19:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1401, loss 4.513303756713867\n",
      "Epoch 3: |          | 1402/? [19:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1402, loss 4.086121559143066\n",
      "Epoch 3: |          | 1403/? [19:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1403, loss 4.295499801635742\n",
      "Epoch 3: |          | 1404/? [19:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1404, loss 4.266817092895508\n",
      "Epoch 3: |          | 1405/? [19:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1405, loss 4.62016487121582\n",
      "Epoch 3: |          | 1406/? [19:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1406, loss 4.514308929443359\n",
      "Epoch 3: |          | 1407/? [19:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1407, loss 4.635493278503418\n",
      "Epoch 3: |          | 1408/? [19:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1408, loss 3.80926775932312\n",
      "Epoch 3: |          | 1409/? [19:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1409, loss 3.864719867706299\n",
      "Epoch 3: |          | 1410/? [19:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1410, loss 3.962296962738037\n",
      "Epoch 3: |          | 1411/? [19:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1411, loss 4.304474830627441\n",
      "Epoch 3: |          | 1412/? [19:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1412, loss 3.8468127250671387\n",
      "Epoch 3: |          | 1413/? [19:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1413, loss 3.7312698364257812\n",
      "Epoch 3: |          | 1414/? [19:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1414, loss 3.876857280731201\n",
      "Epoch 3: |          | 1415/? [19:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1415, loss 4.167740345001221\n",
      "Epoch 3: |          | 1416/? [20:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1416, loss 4.560070037841797\n",
      "Epoch 3: |          | 1417/? [20:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1417, loss 4.166630744934082\n",
      "Epoch 3: |          | 1418/? [20:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1418, loss 4.338568210601807\n",
      "Epoch 3: |          | 1419/? [20:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1419, loss 4.041491508483887\n",
      "Epoch 3: |          | 1420/? [20:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1420, loss 3.9370598793029785\n",
      "Epoch 3: |          | 1421/? [20:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1421, loss 3.5955898761749268\n",
      "Epoch 3: |          | 1422/? [20:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1422, loss 4.404726028442383\n",
      "Epoch 3: |          | 1423/? [20:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1423, loss 4.44413423538208\n",
      "Epoch 3: |          | 1424/? [20:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1424, loss 4.007660388946533\n",
      "Epoch 3: |          | 1425/? [20:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1425, loss 4.29634952545166\n",
      "Epoch 3: |          | 1426/? [20:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1426, loss 3.790588855743408\n",
      "Epoch 3: |          | 1427/? [20:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1427, loss 4.441924095153809\n",
      "Epoch 3: |          | 1428/? [20:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1428, loss 4.41218900680542\n",
      "Epoch 3: |          | 1429/? [20:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1429, loss 4.325850486755371\n",
      "Epoch 3: |          | 1430/? [20:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1430, loss 4.403742790222168\n",
      "Epoch 3: |          | 1431/? [20:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1431, loss 4.166481971740723\n",
      "Epoch 3: |          | 1432/? [20:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1432, loss 4.172663688659668\n",
      "Epoch 3: |          | 1433/? [20:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1433, loss 4.097479820251465\n",
      "Epoch 3: |          | 1434/? [20:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1434, loss 4.226826190948486\n",
      "Epoch 3: |          | 1435/? [20:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1435, loss 3.8249919414520264\n",
      "Epoch 3: |          | 1436/? [20:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1436, loss 4.1573967933654785\n",
      "Epoch 3: |          | 1437/? [20:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1437, loss 3.4383883476257324\n",
      "Epoch 3: |          | 1438/? [20:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1438, loss 5.139449119567871\n",
      "Epoch 3: |          | 1439/? [20:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1439, loss 4.210773944854736\n",
      "Epoch 3: |          | 1440/? [20:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1440, loss 4.335741996765137\n",
      "Epoch 3: |          | 1441/? [20:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1441, loss 4.667718410491943\n",
      "Epoch 3: |          | 1442/? [20:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1442, loss 4.682928085327148\n",
      "Epoch 3: |          | 1443/? [20:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1443, loss 3.7641053199768066\n",
      "Epoch 3: |          | 1444/? [20:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1444, loss 3.864719867706299\n",
      "Epoch 3: |          | 1445/? [20:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1445, loss 4.428939342498779\n",
      "Epoch 3: |          | 1446/? [20:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1446, loss 3.9412848949432373\n",
      "Epoch 3: |          | 1447/? [20:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1447, loss 4.066359043121338\n",
      "Epoch 3: |          | 1448/? [20:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1448, loss 3.8982033729553223\n",
      "Epoch 3: |          | 1449/? [20:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1449, loss 4.174535751342773\n",
      "Epoch 3: |          | 1450/? [20:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1450, loss 4.263548374176025\n",
      "Epoch 3: |          | 1451/? [20:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1451, loss 4.59000825881958\n",
      "Epoch 3: |          | 1452/? [20:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1452, loss 4.186549186706543\n",
      "Epoch 3: |          | 1453/? [20:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1453, loss 3.5026869773864746\n",
      "Epoch 3: |          | 1454/? [20:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1454, loss 4.102569103240967\n",
      "Epoch 3: |          | 1455/? [20:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1455, loss 4.270261287689209\n",
      "Epoch 3: |          | 1456/? [20:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1456, loss 3.760267734527588\n",
      "Epoch 3: |          | 1457/? [20:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1457, loss 3.9416568279266357\n",
      "Epoch 3: |          | 1458/? [20:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1458, loss 4.101284027099609\n",
      "Epoch 3: |          | 1459/? [20:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1459, loss 4.3342814445495605\n",
      "Epoch 3: |          | 1460/? [20:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1460, loss 4.199306011199951\n",
      "Epoch 3: |          | 1461/? [20:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1461, loss 4.2425384521484375\n",
      "Epoch 3: |          | 1462/? [20:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1462, loss 4.509312152862549\n",
      "Epoch 3: |          | 1463/? [20:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1463, loss 4.436335563659668\n",
      "Epoch 3: |          | 1464/? [20:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1464, loss 3.6916351318359375\n",
      "Epoch 3: |          | 1465/? [20:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1465, loss 4.120380401611328\n",
      "Epoch 3: |          | 1466/? [20:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1466, loss 3.7549545764923096\n",
      "Epoch 3: |          | 1467/? [20:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1467, loss 4.469913959503174\n",
      "Epoch 3: |          | 1468/? [20:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1468, loss 4.0042853355407715\n",
      "Epoch 3: |          | 1469/? [20:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1469, loss 3.686650037765503\n",
      "Epoch 3: |          | 1470/? [20:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1470, loss 4.2566118240356445\n",
      "Epoch 3: |          | 1471/? [20:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1471, loss 4.487802982330322\n",
      "Epoch 3: |          | 1472/? [20:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1472, loss 4.231766700744629\n",
      "Epoch 3: |          | 1473/? [20:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1473, loss 4.009718894958496\n",
      "Epoch 3: |          | 1474/? [20:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1474, loss 3.932302474975586\n",
      "Epoch 3: |          | 1475/? [20:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1475, loss 3.5424110889434814\n",
      "Epoch 3: |          | 1476/? [20:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1476, loss 4.1359734535217285\n",
      "Epoch 3: |          | 1477/? [20:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1477, loss 4.1573028564453125\n",
      "Epoch 3: |          | 1478/? [20:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1478, loss 4.041958808898926\n",
      "Epoch 3: |          | 1479/? [20:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1479, loss 4.6098785400390625\n",
      "Epoch 3: |          | 1480/? [20:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1480, loss 4.324328899383545\n",
      "Epoch 3: |          | 1481/? [20:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1481, loss 4.046278476715088\n",
      "Epoch 3: |          | 1482/? [20:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1482, loss 4.1917595863342285\n",
      "Epoch 3: |          | 1483/? [20:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1483, loss 3.7810802459716797\n",
      "Epoch 3: |          | 1484/? [20:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1484, loss 3.9937214851379395\n",
      "Epoch 3: |          | 1485/? [20:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1485, loss 4.201390266418457\n",
      "Epoch 3: |          | 1486/? [21:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1486, loss 4.18019962310791\n",
      "Epoch 3: |          | 1487/? [21:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1487, loss 3.5993411540985107\n",
      "Epoch 3: |          | 1488/? [21:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1488, loss 4.3235578536987305\n",
      "Epoch 3: |          | 1489/? [21:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1489, loss 4.2330732345581055\n",
      "Epoch 3: |          | 1490/? [21:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1490, loss 4.101435661315918\n",
      "Epoch 3: |          | 1491/? [21:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1491, loss 3.038712978363037\n",
      "Epoch 3: |          | 1492/? [21:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1492, loss 3.6708617210388184\n",
      "Epoch 3: |          | 1493/? [21:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1493, loss 3.3373515605926514\n",
      "Epoch 3: |          | 1494/? [21:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1494, loss 4.050388813018799\n",
      "Epoch 3: |          | 1495/? [21:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1495, loss 3.9558491706848145\n",
      "Epoch 3: |          | 1496/? [21:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1496, loss 4.2663774490356445\n",
      "Epoch 3: |          | 1497/? [21:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1497, loss 3.498284101486206\n",
      "Epoch 3: |          | 1498/? [21:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1498, loss 3.846618175506592\n",
      "Epoch 3: |          | 1499/? [21:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1499, loss 4.417722225189209\n",
      "Epoch 3: |          | 1500/? [21:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1500, loss 4.300886631011963\n",
      "Epoch 3: |          | 1501/? [21:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1501, loss 4.169551372528076\n",
      "Epoch 3: |          | 1502/? [21:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1502, loss 4.242318153381348\n",
      "Epoch 3: |          | 1503/? [21:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1503, loss 3.9579567909240723\n",
      "Epoch 3: |          | 1504/? [21:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1504, loss 4.6270222663879395\n",
      "Epoch 3: |          | 1505/? [21:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1505, loss 4.469926357269287\n",
      "Epoch 3: |          | 1506/? [21:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1506, loss 4.057042598724365\n",
      "Epoch 3: |          | 1507/? [21:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1507, loss 3.9288928508758545\n",
      "Epoch 3: |          | 1508/? [21:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1508, loss 4.1010613441467285\n",
      "Epoch 3: |          | 1509/? [21:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1509, loss 4.034929275512695\n",
      "Epoch 3: |          | 1510/? [21:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1510, loss 4.333752632141113\n",
      "Epoch 3: |          | 1511/? [21:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1511, loss 3.8743157386779785\n",
      "Epoch 3: |          | 1512/? [21:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1512, loss 4.563424587249756\n",
      "Epoch 3: |          | 1513/? [21:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1513, loss 4.694001197814941\n",
      "Epoch 3: |          | 1514/? [21:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1514, loss 3.7765297889709473\n",
      "Epoch 3: |          | 1515/? [21:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1515, loss 4.675333023071289\n",
      "Epoch 3: |          | 1516/? [21:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1516, loss 4.529423713684082\n",
      "Epoch 3: |          | 1517/? [21:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1517, loss 4.005159854888916\n",
      "Epoch 3: |          | 1518/? [21:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1518, loss 3.7840399742126465\n",
      "Epoch 3: |          | 1519/? [21:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1519, loss 4.331477642059326\n",
      "Epoch 3: |          | 1520/? [21:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1520, loss 4.444375038146973\n",
      "Epoch 3: |          | 1521/? [21:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1521, loss 4.118850231170654\n",
      "Epoch 3: |          | 1522/? [21:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1522, loss 3.847937822341919\n",
      "Epoch 3: |          | 1523/? [21:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1523, loss 4.2095513343811035\n",
      "Epoch 3: |          | 1524/? [21:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1524, loss 4.090778350830078\n",
      "Epoch 3: |          | 1525/? [21:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1525, loss 3.8531241416931152\n",
      "Epoch 3: |          | 1526/? [21:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1526, loss 4.34755802154541\n",
      "Epoch 3: |          | 1527/? [21:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1527, loss 4.373002052307129\n",
      "Epoch 3: |          | 1528/? [21:35<00:00,  1.18it/s, v_num=30]ERROR: Input has inproper shape\n",
      "Epoch 3: |          | 1529/? [21:35<00:00,  1.18it/s, v_num=30]   VALIDATION: Batch 0, loss 4.714451789855957\n",
      "   VALIDATION: Batch 1, loss 3.6734671592712402\n",
      "   VALIDATION: Batch 2, loss 4.882163047790527\n",
      "   VALIDATION: Batch 3, loss 4.538853168487549\n",
      "   VALIDATION: Batch 4, loss 4.160639762878418\n",
      "   VALIDATION: Batch 5, loss 3.7897064685821533\n",
      "   VALIDATION: Batch 6, loss 4.120270729064941\n",
      "   VALIDATION: Batch 7, loss 4.7293596267700195\n",
      "   VALIDATION: Batch 8, loss 4.6090006828308105\n",
      "   VALIDATION: Batch 9, loss 4.714268684387207\n",
      "   VALIDATION: Batch 10, loss 4.4395341873168945\n",
      "   VALIDATION: Batch 11, loss 4.063447952270508\n",
      "   VALIDATION: Batch 12, loss 4.355286598205566\n",
      "   VALIDATION: Batch 13, loss 4.831219673156738\n",
      "   VALIDATION: Batch 14, loss 4.193415641784668\n",
      "   VALIDATION: Batch 15, loss 4.070143699645996\n",
      "   VALIDATION: Batch 16, loss 4.646485328674316\n",
      "   VALIDATION: Batch 17, loss 4.26749324798584\n",
      "   VALIDATION: Batch 18, loss 3.6296207904815674\n",
      "   VALIDATION: Batch 19, loss 4.565781116485596\n",
      "   VALIDATION: Batch 20, loss 4.819495677947998\n",
      "   VALIDATION: Batch 21, loss 5.019041061401367\n",
      "   VALIDATION: Batch 22, loss 4.6825737953186035\n",
      "   VALIDATION: Batch 23, loss 4.1935505867004395\n",
      "   VALIDATION: Batch 24, loss 4.077383518218994\n",
      "   VALIDATION: Batch 25, loss 4.498684883117676\n",
      "   VALIDATION: Batch 26, loss 4.71884298324585\n",
      "   VALIDATION: Batch 27, loss 4.614701271057129\n",
      "   VALIDATION: Batch 28, loss 4.356393814086914\n",
      "   VALIDATION: Batch 29, loss 4.590843200683594\n",
      "   VALIDATION: Batch 30, loss 4.1490068435668945\n",
      "   VALIDATION: Batch 31, loss 4.476422309875488\n",
      "   VALIDATION: Batch 32, loss 5.074554920196533\n",
      "   VALIDATION: Batch 33, loss 3.2246527671813965\n",
      "   VALIDATION: Batch 34, loss 4.430464267730713\n",
      "   VALIDATION: Batch 35, loss 4.632208824157715\n",
      "   VALIDATION: Batch 36, loss 4.016304969787598\n",
      "   VALIDATION: Batch 37, loss 3.9476211071014404\n",
      "   VALIDATION: Batch 38, loss 4.037814140319824\n",
      "   VALIDATION: Batch 39, loss 4.442466735839844\n",
      "   VALIDATION: Batch 40, loss 4.502991676330566\n",
      "   VALIDATION: Batch 41, loss 3.2891883850097656\n",
      "   VALIDATION: Batch 42, loss 4.510797023773193\n",
      "   VALIDATION: Batch 43, loss 4.640429496765137\n",
      "   VALIDATION: Batch 44, loss 4.226882457733154\n",
      "   VALIDATION: Batch 45, loss 4.682617664337158\n",
      "   VALIDATION: Batch 46, loss 3.8008906841278076\n",
      "   VALIDATION: Batch 47, loss 4.8006792068481445\n",
      "   VALIDATION: Batch 48, loss 4.899215221405029\n",
      "   VALIDATION: Batch 49, loss 4.552348613739014\n",
      "   VALIDATION: Batch 50, loss 4.477473258972168\n",
      "   VALIDATION: Batch 51, loss 4.963710308074951\n",
      "   VALIDATION: Batch 52, loss 4.117213249206543\n",
      "   VALIDATION: Batch 53, loss 4.030059337615967\n",
      "   VALIDATION: Batch 54, loss 4.0923590660095215\n",
      "   VALIDATION: Batch 55, loss 4.903691291809082\n",
      "   VALIDATION: Batch 56, loss 4.247303009033203\n",
      "   VALIDATION: Batch 57, loss 5.674984931945801\n",
      "   VALIDATION: Batch 58, loss 4.357465744018555\n",
      "   VALIDATION: Batch 59, loss 4.018914222717285\n",
      "   VALIDATION: Batch 60, loss 3.558985471725464\n",
      "   VALIDATION: Batch 61, loss 4.423085689544678\n",
      "   VALIDATION: Batch 62, loss 4.388060569763184\n",
      "   VALIDATION: Batch 63, loss 4.90515661239624\n",
      "   VALIDATION: Batch 64, loss 4.6907057762146\n",
      "   VALIDATION: Batch 65, loss 3.854264736175537\n",
      "   VALIDATION: Batch 66, loss 4.751753807067871\n",
      "   VALIDATION: Batch 67, loss 4.173027038574219\n",
      "   VALIDATION: Batch 68, loss 4.3594841957092285\n",
      "   VALIDATION: Batch 69, loss 4.60142707824707\n",
      "   VALIDATION: Batch 70, loss 4.729598522186279\n",
      "   VALIDATION: Batch 71, loss 4.2524919509887695\n",
      "   VALIDATION: Batch 72, loss 5.144223690032959\n",
      "   VALIDATION: Batch 73, loss 3.929032802581787\n",
      "   VALIDATION: Batch 74, loss 4.565999507904053\n",
      "   VALIDATION: Batch 75, loss 4.60125207901001\n",
      "   VALIDATION: Batch 76, loss 4.468019962310791\n",
      "   VALIDATION: Batch 77, loss 4.6807684898376465\n",
      "   VALIDATION: Batch 78, loss 4.513915538787842\n",
      "   VALIDATION: Batch 79, loss 4.4400739669799805\n",
      "   VALIDATION: Batch 80, loss 4.554898262023926\n",
      "   VALIDATION: Batch 81, loss 4.320037841796875\n",
      "   VALIDATION: Batch 82, loss 4.660969257354736\n",
      "   VALIDATION: Batch 83, loss 3.947216749191284\n",
      "   VALIDATION: Batch 84, loss 4.654601097106934\n",
      "   VALIDATION: Batch 85, loss 4.341059684753418\n",
      "   VALIDATION: Batch 86, loss 4.352736473083496\n",
      "   VALIDATION: Batch 87, loss 4.223228454589844\n",
      "   VALIDATION: Batch 88, loss 3.810769557952881\n",
      "   VALIDATION: Batch 89, loss 4.123872756958008\n",
      "   VALIDATION: Batch 90, loss 4.381671905517578\n",
      "   VALIDATION: Batch 91, loss 4.501690864562988\n",
      "   VALIDATION: Batch 92, loss 4.274188995361328\n",
      "   VALIDATION: Batch 93, loss 4.877293586730957\n",
      "   VALIDATION: Batch 94, loss 4.373629093170166\n",
      "   VALIDATION: Batch 95, loss 3.839604139328003\n",
      "   VALIDATION: Batch 96, loss 4.290776252746582\n",
      "   VALIDATION: Batch 97, loss 4.0367960929870605\n",
      "   VALIDATION: Batch 98, loss 4.643513202667236\n",
      "   VALIDATION: Batch 99, loss 4.688589572906494\n",
      "   VALIDATION: Batch 100, loss 5.041007041931152\n",
      "   VALIDATION: Batch 101, loss 3.647732973098755\n",
      "   VALIDATION: Batch 102, loss 5.068644046783447\n",
      "   VALIDATION: Batch 103, loss 4.994690895080566\n",
      "   VALIDATION: Batch 104, loss 3.9609527587890625\n",
      "   VALIDATION: Batch 105, loss 4.47707462310791\n",
      "   VALIDATION: Batch 106, loss 4.319485664367676\n",
      "   VALIDATION: Batch 107, loss 4.38833475112915\n",
      "   VALIDATION: Batch 108, loss 4.131846904754639\n",
      "   VALIDATION: Batch 109, loss 4.741481781005859\n",
      "   VALIDATION: Batch 110, loss 4.444695472717285\n",
      "   VALIDATION: Batch 111, loss 4.751647472381592\n",
      "   VALIDATION: Batch 112, loss 5.536717891693115\n",
      "   VALIDATION: Batch 113, loss 4.898777484893799\n",
      "   VALIDATION: Batch 114, loss 4.6818156242370605\n",
      "   VALIDATION: Batch 115, loss 4.1570940017700195\n",
      "   VALIDATION: Batch 116, loss 4.000908851623535\n",
      "   VALIDATION: Batch 117, loss 4.675053596496582\n",
      "   VALIDATION: Batch 118, loss 4.823781967163086\n",
      "   VALIDATION: Batch 119, loss 3.9693634510040283\n",
      "   VALIDATION: Batch 120, loss 3.6010117530822754\n",
      "   VALIDATION: Batch 121, loss 3.952491283416748\n",
      "   VALIDATION: Batch 122, loss 4.347690582275391\n",
      "   VALIDATION: Batch 123, loss 4.370688438415527\n",
      "   VALIDATION: Batch 124, loss 3.71014142036438\n",
      "   VALIDATION: Batch 125, loss 4.3525309562683105\n",
      "   VALIDATION: Batch 126, loss 4.5353827476501465\n",
      "   VALIDATION: Batch 127, loss 4.348989009857178\n",
      "   VALIDATION: Batch 128, loss 4.486198902130127\n",
      "   VALIDATION: Batch 129, loss 4.146726131439209\n",
      "   VALIDATION: Batch 130, loss 3.731247663497925\n",
      "   VALIDATION: Batch 131, loss 3.7461819648742676\n",
      "   VALIDATION: Batch 132, loss 4.391782283782959\n",
      "   VALIDATION: Batch 133, loss 4.598755359649658\n",
      "   VALIDATION: Batch 134, loss 4.473257541656494\n",
      "   VALIDATION: Batch 135, loss 4.726609230041504\n",
      "   VALIDATION: Batch 136, loss 4.834661483764648\n",
      "   VALIDATION: Batch 137, loss 4.64316463470459\n",
      "   VALIDATION: Batch 138, loss 4.36684513092041\n",
      "   VALIDATION: Batch 139, loss 4.765088081359863\n",
      "   VALIDATION: Batch 140, loss 3.843843460083008\n",
      "   VALIDATION: Batch 141, loss 4.784114837646484\n",
      "   VALIDATION: Batch 142, loss 3.5282135009765625\n",
      "   VALIDATION: Batch 143, loss 4.339168548583984\n",
      "   VALIDATION: Batch 144, loss 4.5799760818481445\n",
      "   VALIDATION: Batch 145, loss 4.377972602844238\n",
      "   VALIDATION: Batch 146, loss 4.188733100891113\n",
      "   VALIDATION: Batch 147, loss 4.548931121826172\n",
      "   VALIDATION: Batch 148, loss 4.6664838790893555\n",
      "   VALIDATION: Batch 149, loss 5.110384941101074\n",
      "   VALIDATION: Batch 150, loss 4.762173175811768\n",
      "   VALIDATION: Batch 151, loss 4.976914405822754\n",
      "   VALIDATION: Batch 152, loss 4.372439384460449\n",
      "   VALIDATION: Batch 153, loss 4.584017753601074\n",
      "   VALIDATION: Batch 154, loss 4.462316513061523\n",
      "   VALIDATION: Batch 155, loss 4.179452896118164\n",
      "   VALIDATION: Batch 156, loss 4.853518009185791\n",
      "   VALIDATION: Batch 157, loss 4.542787075042725\n",
      "   VALIDATION: Batch 158, loss 3.9114489555358887\n",
      "   VALIDATION: Batch 159, loss 4.396948337554932\n",
      "   VALIDATION: Batch 160, loss 4.750693321228027\n",
      "   VALIDATION: Batch 161, loss 5.005589008331299\n",
      "   VALIDATION: Batch 162, loss 4.442446231842041\n",
      "   VALIDATION: Batch 163, loss 3.8663628101348877\n",
      "   VALIDATION: Batch 164, loss 4.338379859924316\n",
      "   VALIDATION: Batch 165, loss 4.834928512573242\n",
      "   VALIDATION: Batch 166, loss 4.288193225860596\n",
      "   VALIDATION: Batch 167, loss 4.732399940490723\n",
      "   VALIDATION: Batch 168, loss 3.5653796195983887\n",
      "   VALIDATION: Batch 169, loss 4.213077068328857\n",
      "   VALIDATION: Batch 170, loss 4.428021430969238\n",
      "   VALIDATION: Batch 171, loss 4.577811241149902\n",
      "   VALIDATION: Batch 172, loss 4.400528907775879\n",
      "   VALIDATION: Batch 173, loss 4.309964656829834\n",
      "   VALIDATION: Batch 174, loss 4.72583532333374\n",
      "   VALIDATION: Batch 175, loss 4.484445095062256\n",
      "   VALIDATION: Batch 176, loss 4.270809173583984\n",
      "   VALIDATION: Batch 177, loss 4.407649517059326\n",
      "   VALIDATION: Batch 178, loss 5.29638671875\n",
      "   VALIDATION: Batch 179, loss 4.576067924499512\n",
      "   VALIDATION: Batch 180, loss 4.117125988006592\n",
      "   VALIDATION: Batch 181, loss 4.269444465637207\n",
      "   VALIDATION: Batch 182, loss 4.553106784820557\n",
      "   VALIDATION: Batch 183, loss 3.5756232738494873\n",
      "   VALIDATION: Batch 184, loss 3.309527635574341\n",
      "   VALIDATION: Batch 185, loss 4.097981929779053\n",
      "   VALIDATION: Batch 186, loss 4.048586368560791\n",
      "   VALIDATION: Batch 187, loss 4.259035587310791\n",
      "   VALIDATION: Batch 188, loss 4.63990592956543\n",
      "   VALIDATION: Batch 189, loss 3.9858603477478027\n",
      "   VALIDATION: Batch 190, loss 4.003528594970703\n",
      "   VALIDATION: Batch 191, loss 4.528794765472412\n",
      "   VALIDATION: Batch 192, loss 4.9190239906311035\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=30]              TRRAINING: Batch 0, loss 4.32148551940918\n",
      "Epoch 4: |          | 1/? [00:01<00:00,  0.89it/s, v_num=30]   TRRAINING: Batch 1, loss 3.880826950073242\n",
      "Epoch 4: |          | 2/? [00:01<00:00,  1.01it/s, v_num=30]   TRRAINING: Batch 2, loss 3.9917349815368652\n",
      "Epoch 4: |          | 3/? [00:02<00:00,  1.04it/s, v_num=30]   TRRAINING: Batch 3, loss 3.660346269607544\n",
      "Epoch 4: |          | 4/? [00:03<00:00,  1.07it/s, v_num=30]   TRRAINING: Batch 4, loss 4.057549476623535\n",
      "Epoch 4: |          | 5/? [00:04<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 5, loss 4.864340782165527\n",
      "Epoch 4: |          | 6/? [00:05<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 6, loss 4.447410583496094\n",
      "Epoch 4: |          | 7/? [00:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 7, loss 3.717819929122925\n",
      "Epoch 4: |          | 8/? [00:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 8, loss 3.843318223953247\n",
      "Epoch 4: |          | 9/? [00:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 9, loss 4.111942291259766\n",
      "Epoch 4: |          | 10/? [00:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 10, loss 4.363897800445557\n",
      "Epoch 4: |          | 11/? [00:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 11, loss 4.270520210266113\n",
      "Epoch 4: |          | 12/? [00:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 12, loss 5.105377674102783\n",
      "Epoch 4: |          | 13/? [00:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 13, loss 4.189348220825195\n",
      "Epoch 4: |          | 14/? [00:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 14, loss 4.4116926193237305\n",
      "Epoch 4: |          | 15/? [00:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 15, loss 3.6536049842834473\n",
      "Epoch 4: |          | 16/? [00:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 16, loss 3.452784299850464\n",
      "Epoch 4: |          | 17/? [00:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 17, loss 4.673656940460205\n",
      "Epoch 4: |          | 18/? [00:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 18, loss 4.14390754699707\n",
      "Epoch 4: |          | 19/? [00:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 19, loss 3.958188533782959\n",
      "Epoch 4: |          | 20/? [00:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 20, loss 4.254809379577637\n",
      "Epoch 4: |          | 21/? [00:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 21, loss 4.332012176513672\n",
      "Epoch 4: |          | 22/? [00:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 22, loss 4.211981296539307\n",
      "Epoch 4: |          | 23/? [00:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 23, loss 3.5765540599823\n",
      "Epoch 4: |          | 24/? [00:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 24, loss 4.247542381286621\n",
      "Epoch 4: |          | 25/? [00:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 25, loss 4.167903423309326\n",
      "Epoch 4: |          | 26/? [00:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 26, loss 3.8211982250213623\n",
      "Epoch 4: |          | 27/? [00:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 27, loss 3.752584457397461\n",
      "Epoch 4: |          | 28/? [00:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 28, loss 4.6575517654418945\n",
      "Epoch 4: |          | 29/? [00:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 29, loss 4.168349266052246\n",
      "Epoch 4: |          | 30/? [00:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 30, loss 4.058114528656006\n",
      "Epoch 4: |          | 31/? [00:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 31, loss 4.688730716705322\n",
      "Epoch 4: |          | 32/? [00:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 32, loss 4.197644233703613\n",
      "Epoch 4: |          | 33/? [00:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 33, loss 3.9961190223693848\n",
      "Epoch 4: |          | 34/? [00:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 34, loss 3.9857017993927\n",
      "Epoch 4: |          | 35/? [00:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 35, loss 3.4169929027557373\n",
      "Epoch 4: |          | 36/? [00:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 36, loss 4.261512756347656\n",
      "Epoch 4: |          | 37/? [00:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 37, loss 4.412014961242676\n",
      "Epoch 4: |          | 38/? [00:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 38, loss 4.710351467132568\n",
      "Epoch 4: |          | 39/? [00:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 39, loss 4.625211238861084\n",
      "Epoch 4: |          | 40/? [00:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 40, loss 4.017165184020996\n",
      "Epoch 4: |          | 41/? [00:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 41, loss 4.0504584312438965\n",
      "Epoch 4: |          | 42/? [00:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 42, loss 3.8513119220733643\n",
      "Epoch 4: |          | 43/? [00:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 43, loss 4.036756992340088\n",
      "Epoch 4: |          | 44/? [00:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 44, loss 3.262563705444336\n",
      "Epoch 4: |          | 45/? [00:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 45, loss 2.9389595985412598\n",
      "Epoch 4: |          | 46/? [00:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 46, loss 4.670623302459717\n",
      "Epoch 4: |          | 47/? [00:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 47, loss 3.850372314453125\n",
      "Epoch 4: |          | 48/? [00:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 48, loss 3.6214396953582764\n",
      "Epoch 4: |          | 49/? [00:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 49, loss 4.118178367614746\n",
      "Epoch 4: |          | 50/? [00:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 50, loss 3.9833221435546875\n",
      "Epoch 4: |          | 51/? [00:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 51, loss 3.970296859741211\n",
      "Epoch 4: |          | 52/? [00:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 52, loss 4.6204352378845215\n",
      "Epoch 4: |          | 53/? [00:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 53, loss 4.251401424407959\n",
      "Epoch 4: |          | 54/? [00:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 54, loss 4.098258018493652\n",
      "Epoch 4: |          | 55/? [00:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 55, loss 4.128766059875488\n",
      "Epoch 4: |          | 56/? [00:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 56, loss 4.258265972137451\n",
      "Epoch 4: |          | 57/? [00:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 57, loss 4.167935371398926\n",
      "Epoch 4: |          | 58/? [00:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 58, loss 5.432350158691406\n",
      "Epoch 4: |          | 59/? [00:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 59, loss 4.304110527038574\n",
      "Epoch 4: |          | 60/? [00:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 60, loss 4.4292707443237305\n",
      "Epoch 4: |          | 61/? [00:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 61, loss 4.437788486480713\n",
      "Epoch 4: |          | 62/? [00:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 62, loss 4.035309314727783\n",
      "Epoch 4: |          | 63/? [00:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 63, loss 4.244582653045654\n",
      "Epoch 4: |          | 64/? [00:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 64, loss 4.042568206787109\n",
      "Epoch 4: |          | 65/? [00:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 65, loss 4.102989673614502\n",
      "Epoch 4: |          | 66/? [00:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 66, loss 3.5062263011932373\n",
      "Epoch 4: |          | 67/? [00:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 67, loss 4.226905822753906\n",
      "Epoch 4: |          | 68/? [00:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 68, loss 4.288987159729004\n",
      "Epoch 4: |          | 69/? [00:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 69, loss 4.1193647384643555\n",
      "Epoch 4: |          | 70/? [00:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 70, loss 3.8136534690856934\n",
      "Epoch 4: |          | 71/? [00:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 71, loss 3.7752156257629395\n",
      "Epoch 4: |          | 72/? [01:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 72, loss 4.082566261291504\n",
      "Epoch 4: |          | 73/? [01:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 73, loss 4.223352432250977\n",
      "Epoch 4: |          | 74/? [01:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 74, loss 3.8922266960144043\n",
      "Epoch 4: |          | 75/? [01:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 75, loss 4.046436309814453\n",
      "Epoch 4: |          | 76/? [01:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 76, loss 4.039916038513184\n",
      "Epoch 4: |          | 77/? [01:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 77, loss 4.077271461486816\n",
      "Epoch 4: |          | 78/? [01:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 78, loss 3.834242582321167\n",
      "Epoch 4: |          | 79/? [01:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 79, loss 4.064218044281006\n",
      "Epoch 4: |          | 80/? [01:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 80, loss 3.969968795776367\n",
      "Epoch 4: |          | 81/? [01:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 81, loss 3.4633803367614746\n",
      "Epoch 4: |          | 82/? [01:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 82, loss 4.324307918548584\n",
      "Epoch 4: |          | 83/? [01:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 83, loss 3.7129273414611816\n",
      "Epoch 4: |          | 84/? [01:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 84, loss 3.559027910232544\n",
      "Epoch 4: |          | 85/? [01:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 85, loss 3.47794771194458\n",
      "Epoch 4: |          | 86/? [01:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 86, loss 3.589519500732422\n",
      "Epoch 4: |          | 87/? [01:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 87, loss 3.760155439376831\n",
      "Epoch 4: |          | 88/? [01:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 88, loss 4.514027118682861\n",
      "Epoch 4: |          | 89/? [01:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 89, loss 4.389003276824951\n",
      "Epoch 4: |          | 90/? [01:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 90, loss 4.185658931732178\n",
      "Epoch 4: |          | 91/? [01:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 91, loss 3.948845386505127\n",
      "Epoch 4: |          | 92/? [01:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 92, loss 4.371472358703613\n",
      "Epoch 4: |          | 93/? [01:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 93, loss 4.515529632568359\n",
      "Epoch 4: |          | 94/? [01:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 94, loss 4.403143882751465\n",
      "Epoch 4: |          | 95/? [01:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 95, loss 4.756762504577637\n",
      "Epoch 4: |          | 96/? [01:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 96, loss 3.876359224319458\n",
      "Epoch 4: |          | 97/? [01:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 97, loss 3.7376136779785156\n",
      "Epoch 4: |          | 98/? [01:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 98, loss 4.229765892028809\n",
      "Epoch 4: |          | 99/? [01:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 99, loss 4.411020755767822\n",
      "Epoch 4: |          | 100/? [01:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 100, loss 4.426301956176758\n",
      "Epoch 4: |          | 101/? [01:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 101, loss 4.050054550170898\n",
      "Epoch 4: |          | 102/? [01:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 102, loss 4.058669090270996\n",
      "Epoch 4: |          | 103/? [01:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 103, loss 3.8317313194274902\n",
      "Epoch 4: |          | 104/? [01:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 104, loss 4.204036235809326\n",
      "Epoch 4: |          | 105/? [01:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 105, loss 4.102973461151123\n",
      "Epoch 4: |          | 106/? [01:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 106, loss 4.200361251831055\n",
      "Epoch 4: |          | 107/? [01:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 107, loss 4.283371448516846\n",
      "Epoch 4: |          | 108/? [01:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 108, loss 4.216043949127197\n",
      "Epoch 4: |          | 109/? [01:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 109, loss 3.891599655151367\n",
      "Epoch 4: |          | 110/? [01:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 110, loss 4.172080039978027\n",
      "Epoch 4: |          | 111/? [01:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 111, loss 4.795564651489258\n",
      "Epoch 4: |          | 112/? [01:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 112, loss 3.4995059967041016\n",
      "Epoch 4: |          | 113/? [01:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 113, loss 3.0556743144989014\n",
      "Epoch 4: |          | 114/? [01:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 114, loss 4.358983039855957\n",
      "Epoch 4: |          | 115/? [01:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 115, loss 4.502634048461914\n",
      "Epoch 4: |          | 116/? [01:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 116, loss 3.6693191528320312\n",
      "Epoch 4: |          | 117/? [01:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 117, loss 3.676048994064331\n",
      "Epoch 4: |          | 118/? [01:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 118, loss 4.424088478088379\n",
      "Epoch 4: |          | 119/? [01:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 119, loss 4.621828556060791\n",
      "Epoch 4: |          | 120/? [01:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 120, loss 4.395291328430176\n",
      "Epoch 4: |          | 121/? [01:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 121, loss 4.131152153015137\n",
      "Epoch 4: |          | 122/? [01:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 122, loss 3.5798487663269043\n",
      "Epoch 4: |          | 123/? [01:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 123, loss 4.069252967834473\n",
      "Epoch 4: |          | 124/? [01:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 124, loss 4.244529724121094\n",
      "Epoch 4: |          | 125/? [01:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 125, loss 3.918452739715576\n",
      "Epoch 4: |          | 126/? [01:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 126, loss 4.380405426025391\n",
      "Epoch 4: |          | 127/? [01:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 127, loss 4.486702919006348\n",
      "Epoch 4: |          | 128/? [01:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 128, loss 3.5506770610809326\n",
      "Epoch 4: |          | 129/? [01:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 129, loss 4.2098188400268555\n",
      "Epoch 4: |          | 130/? [01:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 130, loss 3.2741928100585938\n",
      "Epoch 4: |          | 131/? [01:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 131, loss 4.137718200683594\n",
      "Epoch 4: |          | 132/? [01:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 132, loss 4.148380756378174\n",
      "Epoch 4: |          | 133/? [01:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 133, loss 4.110915660858154\n",
      "Epoch 4: |          | 134/? [01:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 134, loss 4.266157150268555\n",
      "Epoch 4: |          | 135/? [01:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 135, loss 4.328708648681641\n",
      "Epoch 4: |          | 136/? [01:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 136, loss 4.350655555725098\n",
      "Epoch 4: |          | 137/? [01:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 137, loss 3.3178112506866455\n",
      "Epoch 4: |          | 138/? [01:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 138, loss 3.982430934906006\n",
      "Epoch 4: |          | 139/? [01:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 139, loss 4.416110038757324\n",
      "Epoch 4: |          | 140/? [01:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 140, loss 3.5975658893585205\n",
      "Epoch 4: |          | 141/? [01:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 141, loss 3.732020616531372\n",
      "Epoch 4: |          | 142/? [01:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 142, loss 5.310415267944336\n",
      "Epoch 4: |          | 143/? [01:59<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 143, loss 4.915285110473633\n",
      "Epoch 4: |          | 144/? [02:00<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 144, loss 4.109827995300293\n",
      "Epoch 4: |          | 145/? [02:01<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 145, loss 3.598134994506836\n",
      "Epoch 4: |          | 146/? [02:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 146, loss 3.7464802265167236\n",
      "Epoch 4: |          | 147/? [02:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 147, loss 4.085635185241699\n",
      "Epoch 4: |          | 148/? [02:03<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 148, loss 3.811497211456299\n",
      "Epoch 4: |          | 149/? [02:04<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 149, loss 3.371924877166748\n",
      "Epoch 4: |          | 150/? [02:04<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 150, loss 4.228141784667969\n",
      "Epoch 4: |          | 151/? [02:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 151, loss 4.33099365234375\n",
      "Epoch 4: |          | 152/? [02:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 152, loss 4.337510585784912\n",
      "Epoch 4: |          | 153/? [02:07<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 153, loss 3.4056034088134766\n",
      "Epoch 4: |          | 154/? [02:08<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 154, loss 4.631890296936035\n",
      "Epoch 4: |          | 155/? [02:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 155, loss 4.148915767669678\n",
      "Epoch 4: |          | 156/? [02:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 156, loss 3.4963507652282715\n",
      "Epoch 4: |          | 157/? [02:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 157, loss 4.250757694244385\n",
      "Epoch 4: |          | 158/? [02:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 158, loss 4.308307647705078\n",
      "Epoch 4: |          | 159/? [02:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 159, loss 4.01407527923584\n",
      "Epoch 4: |          | 160/? [02:13<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 160, loss 3.7871804237365723\n",
      "Epoch 4: |          | 161/? [02:14<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 161, loss 4.223763942718506\n",
      "Epoch 4: |          | 162/? [02:15<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 162, loss 4.242208003997803\n",
      "Epoch 4: |          | 163/? [02:15<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 163, loss 3.303332567214966\n",
      "Epoch 4: |          | 164/? [02:16<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 164, loss 3.7614059448242188\n",
      "Epoch 4: |          | 165/? [02:17<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 165, loss 4.608475208282471\n",
      "Epoch 4: |          | 166/? [02:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 166, loss 4.278789520263672\n",
      "Epoch 4: |          | 167/? [02:19<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 167, loss 4.388370037078857\n",
      "Epoch 4: |          | 168/? [02:19<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 168, loss 3.920351028442383\n",
      "Epoch 4: |          | 169/? [02:20<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 169, loss 3.531646728515625\n",
      "Epoch 4: |          | 170/? [02:21<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 170, loss 3.814365863800049\n",
      "Epoch 4: |          | 171/? [02:22<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 171, loss 4.182620048522949\n",
      "Epoch 4: |          | 172/? [02:23<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 172, loss 3.958800792694092\n",
      "Epoch 4: |          | 173/? [02:24<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 173, loss 4.6433258056640625\n",
      "Epoch 4: |          | 174/? [02:24<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 174, loss 4.329900741577148\n",
      "Epoch 4: |          | 175/? [02:25<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 175, loss 4.754723072052002\n",
      "Epoch 4: |          | 176/? [02:26<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 176, loss 3.896221876144409\n",
      "Epoch 4: |          | 177/? [02:27<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 177, loss 3.9180099964141846\n",
      "Epoch 4: |          | 178/? [02:28<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 178, loss 3.779284954071045\n",
      "Epoch 4: |          | 179/? [02:28<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 179, loss 4.5166449546813965\n",
      "Epoch 4: |          | 180/? [02:29<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 180, loss 4.00637149810791\n",
      "Epoch 4: |          | 181/? [02:30<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 181, loss 3.8099989891052246\n",
      "Epoch 4: |          | 182/? [02:31<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 182, loss 4.228776454925537\n",
      "Epoch 4: |          | 183/? [02:32<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 183, loss 3.685657024383545\n",
      "Epoch 4: |          | 184/? [02:33<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 184, loss 3.8788743019104004\n",
      "Epoch 4: |          | 185/? [02:34<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 185, loss 4.4492506980896\n",
      "Epoch 4: |          | 186/? [02:34<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 186, loss 3.933401584625244\n",
      "Epoch 4: |          | 187/? [02:35<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 187, loss 4.475580215454102\n",
      "Epoch 4: |          | 188/? [02:36<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 188, loss 3.847525119781494\n",
      "Epoch 4: |          | 189/? [02:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 189, loss 4.499045372009277\n",
      "Epoch 4: |          | 190/? [02:38<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 190, loss 4.007856369018555\n",
      "Epoch 4: |          | 191/? [02:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 191, loss 4.783628463745117\n",
      "Epoch 4: |          | 192/? [02:40<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 192, loss 4.531540870666504\n",
      "Epoch 4: |          | 193/? [02:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 193, loss 3.8565285205841064\n",
      "Epoch 4: |          | 194/? [02:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 194, loss 3.7536518573760986\n",
      "Epoch 4: |          | 195/? [02:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 195, loss 4.4671549797058105\n",
      "Epoch 4: |          | 196/? [02:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 196, loss 4.374340534210205\n",
      "Epoch 4: |          | 197/? [02:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 197, loss 4.161941051483154\n",
      "Epoch 4: |          | 198/? [02:45<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 198, loss 3.510788679122925\n",
      "Epoch 4: |          | 199/? [02:46<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 199, loss 4.403524398803711\n",
      "Epoch 4: |          | 200/? [02:46<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 200, loss 3.917968273162842\n",
      "Epoch 4: |          | 201/? [02:47<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 201, loss 4.20911169052124\n",
      "Epoch 4: |          | 202/? [02:48<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 202, loss 4.30873966217041\n",
      "Epoch 4: |          | 203/? [02:49<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 203, loss 3.98608136177063\n",
      "Epoch 4: |          | 204/? [02:50<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 204, loss 4.030974864959717\n",
      "Epoch 4: |          | 205/? [02:51<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 205, loss 3.8761210441589355\n",
      "Epoch 4: |          | 206/? [02:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 206, loss 3.735748767852783\n",
      "Epoch 4: |          | 207/? [02:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 207, loss 4.248955726623535\n",
      "Epoch 4: |          | 208/? [02:53<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 208, loss 4.176872730255127\n",
      "Epoch 4: |          | 209/? [02:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 209, loss 3.9379324913024902\n",
      "Epoch 4: |          | 210/? [02:55<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 210, loss 4.643596172332764\n",
      "Epoch 4: |          | 211/? [02:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 211, loss 3.987717390060425\n",
      "Epoch 4: |          | 212/? [02:57<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 212, loss 4.224996089935303\n",
      "Epoch 4: |          | 213/? [02:58<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 213, loss 4.054945468902588\n",
      "Epoch 4: |          | 214/? [02:58<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 214, loss 3.9258360862731934\n",
      "Epoch 4: |          | 215/? [02:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 215, loss 3.6108272075653076\n",
      "Epoch 4: |          | 216/? [03:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 216, loss 4.29839563369751\n",
      "Epoch 4: |          | 217/? [03:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 217, loss 4.204702854156494\n",
      "Epoch 4: |          | 218/? [03:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 218, loss 4.202461242675781\n",
      "Epoch 4: |          | 219/? [03:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 219, loss 4.149643421173096\n",
      "Epoch 4: |          | 220/? [03:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 220, loss 4.235572338104248\n",
      "Epoch 4: |          | 221/? [03:04<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 221, loss 4.033500671386719\n",
      "Epoch 4: |          | 222/? [03:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 222, loss 3.242696762084961\n",
      "Epoch 4: |          | 223/? [03:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 223, loss 4.416572093963623\n",
      "Epoch 4: |          | 224/? [03:07<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 224, loss 4.394167900085449\n",
      "Epoch 4: |          | 225/? [03:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 225, loss 4.152012348175049\n",
      "Epoch 4: |          | 226/? [03:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 226, loss 4.001859188079834\n",
      "Epoch 4: |          | 227/? [03:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 227, loss 4.367031097412109\n",
      "Epoch 4: |          | 228/? [03:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 228, loss 4.074398517608643\n",
      "Epoch 4: |          | 229/? [03:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 229, loss 4.169888496398926\n",
      "Epoch 4: |          | 230/? [03:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 230, loss 4.063553810119629\n",
      "Epoch 4: |          | 231/? [03:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 231, loss 4.011584758758545\n",
      "Epoch 4: |          | 232/? [03:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 232, loss 3.7738564014434814\n",
      "Epoch 4: |          | 233/? [03:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 233, loss 4.497325897216797\n",
      "Epoch 4: |          | 234/? [03:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 234, loss 4.47646427154541\n",
      "Epoch 4: |          | 235/? [03:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 235, loss 4.522652626037598\n",
      "Epoch 4: |          | 236/? [03:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 236, loss 3.962885618209839\n",
      "Epoch 4: |          | 237/? [03:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 237, loss 4.138934135437012\n",
      "Epoch 4: |          | 238/? [03:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 238, loss 4.393673419952393\n",
      "Epoch 4: |          | 239/? [03:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 239, loss 3.9713871479034424\n",
      "Epoch 4: |          | 240/? [03:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 240, loss 3.480487823486328\n",
      "Epoch 4: |          | 241/? [03:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 241, loss 4.135682106018066\n",
      "Epoch 4: |          | 242/? [03:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 242, loss 4.494408130645752\n",
      "Epoch 4: |          | 243/? [03:23<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 243, loss 3.3032162189483643\n",
      "Epoch 4: |          | 244/? [03:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 244, loss 3.7946414947509766\n",
      "Epoch 4: |          | 245/? [03:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 245, loss 4.029653072357178\n",
      "Epoch 4: |          | 246/? [03:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 246, loss 4.205630302429199\n",
      "Epoch 4: |          | 247/? [03:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 247, loss 4.279885768890381\n",
      "Epoch 4: |          | 248/? [03:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 248, loss 3.769664764404297\n",
      "Epoch 4: |          | 249/? [03:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 249, loss 3.591808795928955\n",
      "Epoch 4: |          | 250/? [03:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 250, loss 4.203095436096191\n",
      "Epoch 4: |          | 251/? [03:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 251, loss 4.2031354904174805\n",
      "Epoch 4: |          | 252/? [03:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 252, loss 3.9858832359313965\n",
      "Epoch 4: |          | 253/? [03:31<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 253, loss 4.814360618591309\n",
      "Epoch 4: |          | 254/? [03:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 254, loss 4.473076820373535\n",
      "Epoch 4: |          | 255/? [03:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 255, loss 4.067249298095703\n",
      "Epoch 4: |          | 256/? [03:34<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 256, loss 5.185948371887207\n",
      "Epoch 4: |          | 257/? [03:34<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 257, loss 3.934081554412842\n",
      "Epoch 4: |          | 258/? [03:35<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 258, loss 4.034878253936768\n",
      "Epoch 4: |          | 259/? [03:36<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 259, loss 3.904282331466675\n",
      "Epoch 4: |          | 260/? [03:37<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 260, loss 3.8485748767852783\n",
      "Epoch 4: |          | 261/? [03:38<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 261, loss 3.727050304412842\n",
      "Epoch 4: |          | 262/? [03:38<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 262, loss 4.351180076599121\n",
      "Epoch 4: |          | 263/? [03:39<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 263, loss 4.02585506439209\n",
      "Epoch 4: |          | 264/? [03:40<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 264, loss 4.157099723815918\n",
      "Epoch 4: |          | 265/? [03:41<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 265, loss 3.5826897621154785\n",
      "Epoch 4: |          | 266/? [03:42<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 266, loss 4.035694122314453\n",
      "Epoch 4: |          | 267/? [03:43<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 267, loss 3.6503090858459473\n",
      "Epoch 4: |          | 268/? [03:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 268, loss 4.01305627822876\n",
      "Epoch 4: |          | 269/? [03:44<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 269, loss 4.384593963623047\n",
      "Epoch 4: |          | 270/? [03:45<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 270, loss 3.981208086013794\n",
      "Epoch 4: |          | 271/? [03:46<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 271, loss 4.364096641540527\n",
      "Epoch 4: |          | 272/? [03:47<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 272, loss 4.524394512176514\n",
      "Epoch 4: |          | 273/? [03:48<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 273, loss 3.7039947509765625\n",
      "Epoch 4: |          | 274/? [03:48<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 274, loss 4.632125377655029\n",
      "Epoch 4: |          | 275/? [03:49<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 275, loss 4.2551703453063965\n",
      "Epoch 4: |          | 276/? [03:50<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 276, loss 3.5374858379364014\n",
      "Epoch 4: |          | 277/? [03:51<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 277, loss 4.174952983856201\n",
      "Epoch 4: |          | 278/? [03:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 278, loss 3.2011172771453857\n",
      "Epoch 4: |          | 279/? [03:53<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 279, loss 3.8980541229248047\n",
      "Epoch 4: |          | 280/? [03:53<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 280, loss 3.5984363555908203\n",
      "Epoch 4: |          | 281/? [03:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 281, loss 4.305373668670654\n",
      "Epoch 4: |          | 282/? [03:55<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 282, loss 3.910001754760742\n",
      "Epoch 4: |          | 283/? [03:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 283, loss 3.9605565071105957\n",
      "Epoch 4: |          | 284/? [03:57<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 284, loss 3.8667023181915283\n",
      "Epoch 4: |          | 285/? [03:58<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 285, loss 3.3707973957061768\n",
      "Epoch 4: |          | 286/? [03:59<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 286, loss 3.9482016563415527\n",
      "Epoch 4: |          | 287/? [04:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 287, loss 3.7101948261260986\n",
      "Epoch 4: |          | 288/? [04:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 288, loss 3.724821090698242\n",
      "Epoch 4: |          | 289/? [04:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 289, loss 3.5307726860046387\n",
      "Epoch 4: |          | 290/? [04:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 290, loss 3.0135607719421387\n",
      "Epoch 4: |          | 291/? [04:03<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 291, loss 4.148101806640625\n",
      "Epoch 4: |          | 292/? [04:04<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 292, loss 3.7955539226531982\n",
      "Epoch 4: |          | 293/? [04:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 293, loss 4.10721492767334\n",
      "Epoch 4: |          | 294/? [04:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 294, loss 3.981964111328125\n",
      "Epoch 4: |          | 295/? [04:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 295, loss 4.31614351272583\n",
      "Epoch 4: |          | 296/? [04:07<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 296, loss 3.7562355995178223\n",
      "Epoch 4: |          | 297/? [04:08<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 297, loss 4.455836772918701\n",
      "Epoch 4: |          | 298/? [04:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 298, loss 4.2482452392578125\n",
      "Epoch 4: |          | 299/? [04:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 299, loss 4.712949752807617\n",
      "Epoch 4: |          | 300/? [04:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 300, loss 4.084479331970215\n",
      "Epoch 4: |          | 301/? [04:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 301, loss 3.868018627166748\n",
      "Epoch 4: |          | 302/? [04:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 302, loss 4.3170366287231445\n",
      "Epoch 4: |          | 303/? [04:13<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 303, loss 4.0817413330078125\n",
      "Epoch 4: |          | 304/? [04:14<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 304, loss 4.304138660430908\n",
      "Epoch 4: |          | 305/? [04:15<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 305, loss 4.336380481719971\n",
      "Epoch 4: |          | 306/? [04:16<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 306, loss 3.9984118938446045\n",
      "Epoch 4: |          | 307/? [04:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 307, loss 4.228681564331055\n",
      "Epoch 4: |          | 308/? [04:17<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 308, loss 4.398200988769531\n",
      "Epoch 4: |          | 309/? [04:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 309, loss 4.04518985748291\n",
      "Epoch 4: |          | 310/? [04:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 310, loss 4.472133159637451\n",
      "Epoch 4: |          | 311/? [04:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 311, loss 4.000004291534424\n",
      "Epoch 4: |          | 312/? [04:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 312, loss 4.055822849273682\n",
      "Epoch 4: |          | 313/? [04:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 313, loss 3.802935838699341\n",
      "Epoch 4: |          | 314/? [04:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 314, loss 4.168034553527832\n",
      "Epoch 4: |          | 315/? [04:23<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 315, loss 3.820014238357544\n",
      "Epoch 4: |          | 316/? [04:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 316, loss 4.407901287078857\n",
      "Epoch 4: |          | 317/? [04:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 317, loss 4.196982383728027\n",
      "Epoch 4: |          | 318/? [04:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 318, loss 4.340749740600586\n",
      "Epoch 4: |          | 319/? [04:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 319, loss 3.540301561355591\n",
      "Epoch 4: |          | 320/? [04:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 320, loss 4.066775321960449\n",
      "Epoch 4: |          | 321/? [04:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 321, loss 3.836688280105591\n",
      "Epoch 4: |          | 322/? [04:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 322, loss 4.499682903289795\n",
      "Epoch 4: |          | 323/? [04:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 323, loss 4.478795528411865\n",
      "Epoch 4: |          | 324/? [04:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 324, loss 4.127830505371094\n",
      "Epoch 4: |          | 325/? [04:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 325, loss 4.604499340057373\n",
      "Epoch 4: |          | 326/? [04:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 326, loss 4.135357856750488\n",
      "Epoch 4: |          | 327/? [04:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 327, loss 3.8935387134552\n",
      "Epoch 4: |          | 328/? [04:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 328, loss 3.676071882247925\n",
      "Epoch 4: |          | 329/? [04:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 329, loss 4.263857841491699\n",
      "Epoch 4: |          | 330/? [04:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 330, loss 4.765025615692139\n",
      "Epoch 4: |          | 331/? [04:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 331, loss 2.974820852279663\n",
      "Epoch 4: |          | 332/? [04:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 332, loss 4.113577842712402\n",
      "Epoch 4: |          | 333/? [04:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 333, loss 3.9384262561798096\n",
      "Epoch 4: |          | 334/? [04:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 334, loss 4.674135208129883\n",
      "Epoch 4: |          | 335/? [04:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 335, loss 4.569899559020996\n",
      "Epoch 4: |          | 336/? [04:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 336, loss 4.5116119384765625\n",
      "Epoch 4: |          | 337/? [04:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 337, loss 5.1265668869018555\n",
      "Epoch 4: |          | 338/? [04:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 338, loss 4.755521297454834\n",
      "Epoch 4: |          | 339/? [04:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 339, loss 3.8073318004608154\n",
      "Epoch 4: |          | 340/? [04:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 340, loss 3.7425777912139893\n",
      "Epoch 4: |          | 341/? [04:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 341, loss 3.517740249633789\n",
      "Epoch 4: |          | 342/? [04:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 342, loss 4.176149845123291\n",
      "Epoch 4: |          | 343/? [04:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 343, loss 3.8789963722229004\n",
      "Epoch 4: |          | 344/? [04:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 344, loss 4.7386250495910645\n",
      "Epoch 4: |          | 345/? [04:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 345, loss 3.926226854324341\n",
      "Epoch 4: |          | 346/? [04:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 346, loss 4.185373306274414\n",
      "Epoch 4: |          | 347/? [04:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 347, loss 3.9402785301208496\n",
      "Epoch 4: |          | 348/? [04:51<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 348, loss 3.3031020164489746\n",
      "Epoch 4: |          | 349/? [04:51<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 349, loss 3.151416301727295\n",
      "Epoch 4: |          | 350/? [04:52<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 350, loss 4.482149124145508\n",
      "Epoch 4: |          | 351/? [04:53<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 351, loss 4.501189231872559\n",
      "Epoch 4: |          | 352/? [04:54<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 352, loss 3.7366783618927\n",
      "Epoch 4: |          | 353/? [04:55<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 353, loss 3.497405529022217\n",
      "Epoch 4: |          | 354/? [04:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 354, loss 3.9110615253448486\n",
      "Epoch 4: |          | 355/? [04:56<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 355, loss 4.250741958618164\n",
      "Epoch 4: |          | 356/? [04:57<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 356, loss 4.232883930206299\n",
      "Epoch 4: |          | 357/? [04:58<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 357, loss 3.721081495285034\n",
      "Epoch 4: |          | 358/? [04:59<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 358, loss 3.6752891540527344\n",
      "Epoch 4: |          | 359/? [05:00<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 359, loss 4.305205821990967\n",
      "Epoch 4: |          | 360/? [05:01<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 360, loss 3.8719322681427\n",
      "Epoch 4: |          | 361/? [05:01<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 361, loss 3.9962058067321777\n",
      "Epoch 4: |          | 362/? [05:02<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 362, loss 3.7748985290527344\n",
      "Epoch 4: |          | 363/? [05:03<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 363, loss 3.704481601715088\n",
      "Epoch 4: |          | 364/? [05:04<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 364, loss 4.299896240234375\n",
      "Epoch 4: |          | 365/? [05:05<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 365, loss 4.373015403747559\n",
      "Epoch 4: |          | 366/? [05:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 366, loss 4.100009918212891\n",
      "Epoch 4: |          | 367/? [05:06<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 367, loss 4.10537576675415\n",
      "Epoch 4: |          | 368/? [05:07<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 368, loss 3.6276698112487793\n",
      "Epoch 4: |          | 369/? [05:08<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 369, loss 3.9463951587677\n",
      "Epoch 4: |          | 370/? [05:09<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 370, loss 3.600865125656128\n",
      "Epoch 4: |          | 371/? [05:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 371, loss 4.581393241882324\n",
      "Epoch 4: |          | 372/? [05:10<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 372, loss 3.9476699829101562\n",
      "Epoch 4: |          | 373/? [05:11<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 373, loss 4.199549198150635\n",
      "Epoch 4: |          | 374/? [05:12<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 374, loss 3.871255874633789\n",
      "Epoch 4: |          | 375/? [05:13<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 375, loss 4.576974391937256\n",
      "Epoch 4: |          | 376/? [05:14<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 376, loss 3.9635519981384277\n",
      "Epoch 4: |          | 377/? [05:15<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 377, loss 4.18393611907959\n",
      "Epoch 4: |          | 378/? [05:16<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 378, loss 4.302332878112793\n",
      "Epoch 4: |          | 379/? [05:17<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 379, loss 4.127270221710205\n",
      "Epoch 4: |          | 380/? [05:17<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 380, loss 4.192061424255371\n",
      "Epoch 4: |          | 381/? [05:18<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 381, loss 4.240782260894775\n",
      "Epoch 4: |          | 382/? [05:19<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 382, loss 3.9371867179870605\n",
      "Epoch 4: |          | 383/? [05:20<00:00,  1.20it/s, v_num=30]   TRRAINING: Batch 383, loss 4.017777442932129\n",
      "Epoch 4: |          | 384/? [05:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 384, loss 4.456147193908691\n",
      "Epoch 4: |          | 385/? [05:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 385, loss 4.0524702072143555\n",
      "Epoch 4: |          | 386/? [05:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 386, loss 3.004641056060791\n",
      "Epoch 4: |          | 387/? [05:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 387, loss 3.87804913520813\n",
      "Epoch 4: |          | 388/? [05:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 388, loss 3.8663742542266846\n",
      "Epoch 4: |          | 389/? [05:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 389, loss 4.445650577545166\n",
      "Epoch 4: |          | 390/? [05:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 390, loss 3.8663737773895264\n",
      "Epoch 4: |          | 391/? [05:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 391, loss 4.31796932220459\n",
      "Epoch 4: |          | 392/? [05:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 392, loss 4.401486396789551\n",
      "Epoch 4: |          | 393/? [05:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 393, loss 4.432638168334961\n",
      "Epoch 4: |          | 394/? [05:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 394, loss 4.064724922180176\n",
      "Epoch 4: |          | 395/? [05:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 395, loss 4.308197021484375\n",
      "Epoch 4: |          | 396/? [05:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 396, loss 4.216867923736572\n",
      "Epoch 4: |          | 397/? [05:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 397, loss 4.004490852355957\n",
      "Epoch 4: |          | 398/? [05:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 398, loss 3.8988258838653564\n",
      "Epoch 4: |          | 399/? [05:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 399, loss 3.996033191680908\n",
      "Epoch 4: |          | 400/? [05:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 400, loss 3.9732327461242676\n",
      "Epoch 4: |          | 401/? [05:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 401, loss 3.913206100463867\n",
      "Epoch 4: |          | 402/? [05:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 402, loss 4.3703742027282715\n",
      "Epoch 4: |          | 403/? [05:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 403, loss 4.2036662101745605\n",
      "Epoch 4: |          | 404/? [05:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 404, loss 3.7263927459716797\n",
      "Epoch 4: |          | 405/? [05:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 405, loss 3.7444756031036377\n",
      "Epoch 4: |          | 406/? [05:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 406, loss 4.114471912384033\n",
      "Epoch 4: |          | 407/? [05:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 407, loss 4.017543315887451\n",
      "Epoch 4: |          | 408/? [05:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 408, loss 4.410664081573486\n",
      "Epoch 4: |          | 409/? [05:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 409, loss 4.363133907318115\n",
      "Epoch 4: |          | 410/? [05:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 410, loss 4.043059349060059\n",
      "Epoch 4: |          | 411/? [05:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 411, loss 3.9424757957458496\n",
      "Epoch 4: |          | 412/? [05:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 412, loss 3.437898635864258\n",
      "Epoch 4: |          | 413/? [05:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 413, loss 4.196598052978516\n",
      "Epoch 4: |          | 414/? [05:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 414, loss 3.7242157459259033\n",
      "Epoch 4: |          | 415/? [05:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 415, loss 4.173408031463623\n",
      "Epoch 4: |          | 416/? [05:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 416, loss 4.613475322723389\n",
      "Epoch 4: |          | 417/? [05:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 417, loss 4.676999092102051\n",
      "Epoch 4: |          | 418/? [05:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 418, loss 4.2710795402526855\n",
      "Epoch 4: |          | 419/? [05:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 419, loss 4.122223854064941\n",
      "Epoch 4: |          | 420/? [05:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 420, loss 4.140076637268066\n",
      "Epoch 4: |          | 421/? [05:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 421, loss 4.702506065368652\n",
      "Epoch 4: |          | 422/? [05:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 422, loss 4.206908226013184\n",
      "Epoch 4: |          | 423/? [05:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 423, loss 3.768946886062622\n",
      "Epoch 4: |          | 424/? [05:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 424, loss 4.399209976196289\n",
      "Epoch 4: |          | 425/? [06:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 425, loss 4.142677307128906\n",
      "Epoch 4: |          | 426/? [06:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 426, loss 3.804602861404419\n",
      "Epoch 4: |          | 427/? [06:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 427, loss 3.865821361541748\n",
      "Epoch 4: |          | 428/? [06:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 428, loss 4.672029972076416\n",
      "Epoch 4: |          | 429/? [06:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 429, loss 3.5129265785217285\n",
      "Epoch 4: |          | 430/? [06:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 430, loss 4.168508052825928\n",
      "Epoch 4: |          | 431/? [06:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 431, loss 4.1014533042907715\n",
      "Epoch 4: |          | 432/? [06:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 432, loss 4.204644680023193\n",
      "Epoch 4: |          | 433/? [06:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 433, loss 4.123400688171387\n",
      "Epoch 4: |          | 434/? [06:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 434, loss 4.048480987548828\n",
      "Epoch 4: |          | 435/? [06:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 435, loss 3.6915974617004395\n",
      "Epoch 4: |          | 436/? [06:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 436, loss 4.133200645446777\n",
      "Epoch 4: |          | 437/? [06:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 437, loss 4.29331111907959\n",
      "Epoch 4: |          | 438/? [06:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 438, loss 3.9201290607452393\n",
      "Epoch 4: |          | 439/? [06:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 439, loss 3.834601640701294\n",
      "Epoch 4: |          | 440/? [06:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 440, loss 3.7745163440704346\n",
      "Epoch 4: |          | 441/? [06:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 441, loss 4.1673173904418945\n",
      "Epoch 4: |          | 442/? [06:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 442, loss 4.002873420715332\n",
      "Epoch 4: |          | 443/? [06:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 443, loss 4.159353256225586\n",
      "Epoch 4: |          | 444/? [06:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 444, loss 4.087541103363037\n",
      "Epoch 4: |          | 445/? [06:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 445, loss 5.025824546813965\n",
      "Epoch 4: |          | 446/? [06:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 446, loss 4.044460773468018\n",
      "Epoch 4: |          | 447/? [06:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 447, loss 4.606579780578613\n",
      "Epoch 4: |          | 448/? [06:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 448, loss 3.7283835411071777\n",
      "Epoch 4: |          | 449/? [06:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 449, loss 4.06979513168335\n",
      "Epoch 4: |          | 450/? [06:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 450, loss 4.390188694000244\n",
      "Epoch 4: |          | 451/? [06:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 451, loss 4.053500652313232\n",
      "Epoch 4: |          | 452/? [06:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 452, loss 3.7738423347473145\n",
      "Epoch 4: |          | 453/? [06:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 453, loss 4.511760711669922\n",
      "Epoch 4: |          | 454/? [06:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 454, loss 3.8947296142578125\n",
      "Epoch 4: |          | 455/? [06:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 455, loss 4.272345066070557\n",
      "Epoch 4: |          | 456/? [06:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 456, loss 3.5755233764648438\n",
      "Epoch 4: |          | 457/? [06:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 457, loss 4.025944232940674\n",
      "Epoch 4: |          | 458/? [06:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 458, loss 4.437371730804443\n",
      "Epoch 4: |          | 459/? [06:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 459, loss 4.4197845458984375\n",
      "Epoch 4: |          | 460/? [06:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 460, loss 4.179041385650635\n",
      "Epoch 4: |          | 461/? [06:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 461, loss 4.154858589172363\n",
      "Epoch 4: |          | 462/? [06:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 462, loss 4.209578990936279\n",
      "Epoch 4: |          | 463/? [06:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 463, loss 4.0751447677612305\n",
      "Epoch 4: |          | 464/? [06:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 464, loss 3.566053867340088\n",
      "Epoch 4: |          | 465/? [06:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 465, loss 3.8620879650115967\n",
      "Epoch 4: |          | 466/? [06:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 466, loss 4.301270484924316\n",
      "Epoch 4: |          | 467/? [06:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 467, loss 4.1034440994262695\n",
      "Epoch 4: |          | 468/? [06:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 468, loss 4.011768817901611\n",
      "Epoch 4: |          | 469/? [06:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 469, loss 4.175699234008789\n",
      "Epoch 4: |          | 470/? [06:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 470, loss 3.5173277854919434\n",
      "Epoch 4: |          | 471/? [06:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 471, loss 4.316960334777832\n",
      "Epoch 4: |          | 472/? [06:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 472, loss 3.8462576866149902\n",
      "Epoch 4: |          | 473/? [06:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 473, loss 3.8828110694885254\n",
      "Epoch 4: |          | 474/? [06:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 474, loss 3.488906145095825\n",
      "Epoch 4: |          | 475/? [06:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 475, loss 4.783793926239014\n",
      "Epoch 4: |          | 476/? [06:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 476, loss 3.7783961296081543\n",
      "Epoch 4: |          | 477/? [06:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 477, loss 3.3035576343536377\n",
      "Epoch 4: |          | 478/? [06:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 478, loss 3.514157772064209\n",
      "Epoch 4: |          | 479/? [06:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 479, loss 4.044577598571777\n",
      "Epoch 4: |          | 480/? [06:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 480, loss 4.021807670593262\n",
      "Epoch 4: |          | 481/? [06:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 481, loss 3.5689022541046143\n",
      "Epoch 4: |          | 482/? [06:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 482, loss 3.859663724899292\n",
      "Epoch 4: |          | 483/? [06:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 483, loss 3.550137996673584\n",
      "Epoch 4: |          | 484/? [06:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 484, loss 4.461167335510254\n",
      "Epoch 4: |          | 485/? [06:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 485, loss 4.275783538818359\n",
      "Epoch 4: |          | 486/? [06:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 486, loss 3.9425530433654785\n",
      "Epoch 4: |          | 487/? [06:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 487, loss 4.281350135803223\n",
      "Epoch 4: |          | 488/? [06:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 488, loss 3.999462127685547\n",
      "Epoch 4: |          | 489/? [06:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 489, loss 3.506624221801758\n",
      "Epoch 4: |          | 490/? [06:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 490, loss 4.1529998779296875\n",
      "Epoch 4: |          | 491/? [06:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 491, loss 3.944018840789795\n",
      "Epoch 4: |          | 492/? [06:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 492, loss 3.2887160778045654\n",
      "Epoch 4: |          | 493/? [06:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 493, loss 4.349693298339844\n",
      "Epoch 4: |          | 494/? [06:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 494, loss 4.194369316101074\n",
      "Epoch 4: |          | 495/? [06:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 495, loss 4.228701591491699\n",
      "Epoch 4: |          | 496/? [06:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 496, loss 3.8637726306915283\n",
      "Epoch 4: |          | 497/? [07:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 497, loss 4.393639087677002\n",
      "Epoch 4: |          | 498/? [07:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 498, loss 4.05049991607666\n",
      "Epoch 4: |          | 499/? [07:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 499, loss 4.12170934677124\n",
      "Epoch 4: |          | 500/? [07:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 500, loss 3.9165897369384766\n",
      "Epoch 4: |          | 501/? [07:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 501, loss 3.682657241821289\n",
      "Epoch 4: |          | 502/? [07:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 502, loss 4.114188194274902\n",
      "Epoch 4: |          | 503/? [07:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 503, loss 4.064213752746582\n",
      "Epoch 4: |          | 504/? [07:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 504, loss 3.9631073474884033\n",
      "Epoch 4: |          | 505/? [07:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 505, loss 3.399881362915039\n",
      "Epoch 4: |          | 506/? [07:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 506, loss 4.006014823913574\n",
      "Epoch 4: |          | 507/? [07:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 507, loss 4.063387393951416\n",
      "Epoch 4: |          | 508/? [07:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 508, loss 4.378499507904053\n",
      "Epoch 4: |          | 509/? [07:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 509, loss 3.7148971557617188\n",
      "Epoch 4: |          | 510/? [07:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 510, loss 4.2337141036987305\n",
      "Epoch 4: |          | 511/? [07:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 511, loss 4.107028484344482\n",
      "Epoch 4: |          | 512/? [07:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 512, loss 3.484389066696167\n",
      "Epoch 4: |          | 513/? [07:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 513, loss 3.7500693798065186\n",
      "Epoch 4: |          | 514/? [07:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 514, loss 3.9063773155212402\n",
      "Epoch 4: |          | 515/? [07:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 515, loss 3.5573534965515137\n",
      "Epoch 4: |          | 516/? [07:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 516, loss 3.869966506958008\n",
      "Epoch 4: |          | 517/? [07:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 517, loss 4.133881568908691\n",
      "Epoch 4: |          | 518/? [07:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 518, loss 3.6730995178222656\n",
      "Epoch 4: |          | 519/? [07:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 519, loss 4.122872352600098\n",
      "Epoch 4: |          | 520/? [07:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 520, loss 3.9653048515319824\n",
      "Epoch 4: |          | 521/? [07:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 521, loss 3.9698691368103027\n",
      "Epoch 4: |          | 522/? [07:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 522, loss 4.52102518081665\n",
      "Epoch 4: |          | 523/? [07:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 523, loss 4.603487014770508\n",
      "Epoch 4: |          | 524/? [07:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 524, loss 4.357814788818359\n",
      "Epoch 4: |          | 525/? [07:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 525, loss 3.9498775005340576\n",
      "Epoch 4: |          | 526/? [07:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 526, loss 3.744319438934326\n",
      "Epoch 4: |          | 527/? [07:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 527, loss 4.398975372314453\n",
      "Epoch 4: |          | 528/? [07:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 528, loss 4.192078590393066\n",
      "Epoch 4: |          | 529/? [07:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 529, loss 3.7698521614074707\n",
      "Epoch 4: |          | 530/? [07:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 530, loss 4.348576068878174\n",
      "Epoch 4: |          | 531/? [07:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 531, loss 3.8555126190185547\n",
      "Epoch 4: |          | 532/? [07:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 532, loss 4.126192569732666\n",
      "Epoch 4: |          | 533/? [07:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 533, loss 3.7754509449005127\n",
      "Epoch 4: |          | 534/? [07:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 534, loss 3.4764182567596436\n",
      "Epoch 4: |          | 535/? [07:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 535, loss 3.9996540546417236\n",
      "Epoch 4: |          | 536/? [07:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 536, loss 4.419591903686523\n",
      "Epoch 4: |          | 537/? [07:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 537, loss 4.163275718688965\n",
      "Epoch 4: |          | 538/? [07:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 538, loss 3.782172679901123\n",
      "Epoch 4: |          | 539/? [07:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 539, loss 3.898014783859253\n",
      "Epoch 4: |          | 540/? [07:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 540, loss 4.319429397583008\n",
      "Epoch 4: |          | 541/? [07:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 541, loss 4.054694652557373\n",
      "Epoch 4: |          | 542/? [07:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 542, loss 3.8139419555664062\n",
      "Epoch 4: |          | 543/? [07:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 543, loss 4.201026916503906\n",
      "Epoch 4: |          | 544/? [07:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 544, loss 4.100695610046387\n",
      "Epoch 4: |          | 545/? [07:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 545, loss 3.3612399101257324\n",
      "Epoch 4: |          | 546/? [07:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 546, loss 4.169014930725098\n",
      "Epoch 4: |          | 547/? [07:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 547, loss 4.608193874359131\n",
      "Epoch 4: |          | 548/? [07:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 548, loss 4.2685441970825195\n",
      "Epoch 4: |          | 549/? [07:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 549, loss 4.154813289642334\n",
      "Epoch 4: |          | 550/? [07:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 550, loss 4.470938682556152\n",
      "Epoch 4: |          | 551/? [07:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 551, loss 4.135947227478027\n",
      "Epoch 4: |          | 552/? [07:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 552, loss 4.163797855377197\n",
      "Epoch 4: |          | 553/? [07:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 553, loss 3.5829062461853027\n",
      "Epoch 4: |          | 554/? [07:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 554, loss 4.180150985717773\n",
      "Epoch 4: |          | 555/? [07:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 555, loss 4.466485500335693\n",
      "Epoch 4: |          | 556/? [07:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 556, loss 4.184857368469238\n",
      "Epoch 4: |          | 557/? [07:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 557, loss 3.709695816040039\n",
      "Epoch 4: |          | 558/? [07:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 558, loss 3.89697265625\n",
      "Epoch 4: |          | 559/? [07:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 559, loss 3.898408889770508\n",
      "Epoch 4: |          | 560/? [07:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 560, loss 3.419053316116333\n",
      "Epoch 4: |          | 561/? [07:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 561, loss 3.3769149780273438\n",
      "Epoch 4: |          | 562/? [07:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 562, loss 4.3131208419799805\n",
      "Epoch 4: |          | 563/? [07:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 563, loss 3.407702684402466\n",
      "Epoch 4: |          | 564/? [07:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 564, loss 3.9188313484191895\n",
      "Epoch 4: |          | 565/? [07:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 565, loss 4.285342693328857\n",
      "Epoch 4: |          | 566/? [07:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 566, loss 4.355391025543213\n",
      "Epoch 4: |          | 567/? [07:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 567, loss 4.449707984924316\n",
      "Epoch 4: |          | 568/? [07:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 568, loss 3.53332781791687\n",
      "Epoch 4: |          | 569/? [07:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 569, loss 4.111471652984619\n",
      "Epoch 4: |          | 570/? [08:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 570, loss 4.2175092697143555\n",
      "Epoch 4: |          | 571/? [08:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 571, loss 3.78608775138855\n",
      "Epoch 4: |          | 572/? [08:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 572, loss 4.718237400054932\n",
      "Epoch 4: |          | 573/? [08:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 573, loss 3.1079907417297363\n",
      "Epoch 4: |          | 574/? [08:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 574, loss 4.318482398986816\n",
      "Epoch 4: |          | 575/? [08:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 575, loss 3.6467647552490234\n",
      "Epoch 4: |          | 576/? [08:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 576, loss 3.862912654876709\n",
      "Epoch 4: |          | 577/? [08:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 577, loss 4.098527431488037\n",
      "Epoch 4: |          | 578/? [08:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 578, loss 4.3491716384887695\n",
      "Epoch 4: |          | 579/? [08:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 579, loss 3.4625420570373535\n",
      "Epoch 4: |          | 580/? [08:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 580, loss 4.172224998474121\n",
      "Epoch 4: |          | 581/? [08:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 581, loss 4.184020042419434\n",
      "Epoch 4: |          | 582/? [08:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 582, loss 4.223799705505371\n",
      "Epoch 4: |          | 583/? [08:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 583, loss 3.9842820167541504\n",
      "Epoch 4: |          | 584/? [08:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 584, loss 4.223340034484863\n",
      "Epoch 4: |          | 585/? [08:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 585, loss 4.164433479309082\n",
      "Epoch 4: |          | 586/? [08:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 586, loss 4.268496990203857\n",
      "Epoch 4: |          | 587/? [08:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 587, loss 4.192254066467285\n",
      "Epoch 4: |          | 588/? [08:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 588, loss 4.190658092498779\n",
      "Epoch 4: |          | 589/? [08:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 589, loss 3.6690139770507812\n",
      "Epoch 4: |          | 590/? [08:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 590, loss 4.237015724182129\n",
      "Epoch 4: |          | 591/? [08:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 591, loss 4.1115288734436035\n",
      "Epoch 4: |          | 592/? [08:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 592, loss 3.789088487625122\n",
      "Epoch 4: |          | 593/? [08:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 593, loss 4.07138729095459\n",
      "Epoch 4: |          | 594/? [08:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 594, loss 4.89260196685791\n",
      "Epoch 4: |          | 595/? [08:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 595, loss 3.598998546600342\n",
      "Epoch 4: |          | 596/? [08:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 596, loss 3.651535749435425\n",
      "Epoch 4: |          | 597/? [08:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 597, loss 3.9197018146514893\n",
      "Epoch 4: |          | 598/? [08:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 598, loss 4.401589393615723\n",
      "Epoch 4: |          | 599/? [08:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 599, loss 4.037441253662109\n",
      "Epoch 4: |          | 600/? [08:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 600, loss 3.808445453643799\n",
      "Epoch 4: |          | 601/? [08:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 601, loss 4.121006965637207\n",
      "Epoch 4: |          | 602/? [08:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 602, loss 3.6367106437683105\n",
      "Epoch 4: |          | 603/? [08:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 603, loss 3.732738494873047\n",
      "Epoch 4: |          | 604/? [08:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 604, loss 5.6282854080200195\n",
      "Epoch 4: |          | 605/? [08:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 605, loss 3.5857386589050293\n",
      "Epoch 4: |          | 606/? [08:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 606, loss 3.8229434490203857\n",
      "Epoch 4: |          | 607/? [08:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 607, loss 4.149455547332764\n",
      "Epoch 4: |          | 608/? [08:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 608, loss 3.930929660797119\n",
      "Epoch 4: |          | 609/? [08:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 609, loss 3.8505961894989014\n",
      "Epoch 4: |          | 610/? [08:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 610, loss 3.942396640777588\n",
      "Epoch 4: |          | 611/? [08:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 611, loss 4.059666633605957\n",
      "Epoch 4: |          | 612/? [08:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 612, loss 3.8017868995666504\n",
      "Epoch 4: |          | 613/? [08:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 613, loss 4.135594367980957\n",
      "Epoch 4: |          | 614/? [08:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 614, loss 3.904114246368408\n",
      "Epoch 4: |          | 615/? [08:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 615, loss 4.431017875671387\n",
      "Epoch 4: |          | 616/? [08:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 616, loss 4.656764030456543\n",
      "Epoch 4: |          | 617/? [08:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 617, loss 3.1847097873687744\n",
      "Epoch 4: |          | 618/? [08:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 618, loss 4.176868438720703\n",
      "Epoch 4: |          | 619/? [08:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 619, loss 3.7743782997131348\n",
      "Epoch 4: |          | 620/? [08:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 620, loss 4.257656097412109\n",
      "Epoch 4: |          | 621/? [08:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 621, loss 3.7165825366973877\n",
      "Epoch 4: |          | 622/? [08:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 622, loss 3.5085887908935547\n",
      "Epoch 4: |          | 623/? [08:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 623, loss 3.2484519481658936\n",
      "Epoch 4: |          | 624/? [08:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 624, loss 2.9431910514831543\n",
      "Epoch 4: |          | 625/? [08:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 625, loss 4.471776008605957\n",
      "Epoch 4: |          | 626/? [08:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 626, loss 3.959019184112549\n",
      "Epoch 4: |          | 627/? [08:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 627, loss 3.8907523155212402\n",
      "Epoch 4: |          | 628/? [08:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 628, loss 3.863321304321289\n",
      "Epoch 4: |          | 629/? [08:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 629, loss 4.277651786804199\n",
      "Epoch 4: |          | 630/? [08:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 630, loss 4.03177547454834\n",
      "Epoch 4: |          | 631/? [08:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 631, loss 4.158823490142822\n",
      "Epoch 4: |          | 632/? [08:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 632, loss 3.4119372367858887\n",
      "Epoch 4: |          | 633/? [08:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 633, loss 4.249337196350098\n",
      "Epoch 4: |          | 634/? [08:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 634, loss 3.788161516189575\n",
      "Epoch 4: |          | 635/? [08:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 635, loss 3.601940870285034\n",
      "Epoch 4: |          | 636/? [08:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 636, loss 4.041597366333008\n",
      "Epoch 4: |          | 637/? [08:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 637, loss 3.857022762298584\n",
      "Epoch 4: |          | 638/? [08:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 638, loss 4.096706867218018\n",
      "Epoch 4: |          | 639/? [08:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 639, loss 3.8352599143981934\n",
      "Epoch 4: |          | 640/? [08:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 640, loss 4.394103527069092\n",
      "Epoch 4: |          | 641/? [08:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 641, loss 3.4292190074920654\n",
      "Epoch 4: |          | 642/? [09:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 642, loss 4.246245384216309\n",
      "Epoch 4: |          | 643/? [09:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 643, loss 4.120067596435547\n",
      "Epoch 4: |          | 644/? [09:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 644, loss 4.032466888427734\n",
      "Epoch 4: |          | 645/? [09:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 645, loss 3.770214080810547\n",
      "Epoch 4: |          | 646/? [09:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 646, loss 3.8103389739990234\n",
      "Epoch 4: |          | 647/? [09:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 647, loss 4.389639377593994\n",
      "Epoch 4: |          | 648/? [09:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 648, loss 3.7752487659454346\n",
      "Epoch 4: |          | 649/? [09:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 649, loss 3.4649131298065186\n",
      "Epoch 4: |          | 650/? [09:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 650, loss 4.322113990783691\n",
      "Epoch 4: |          | 651/? [09:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 651, loss 4.452343463897705\n",
      "Epoch 4: |          | 652/? [09:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 652, loss 3.8896431922912598\n",
      "Epoch 4: |          | 653/? [09:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 653, loss 4.03537654876709\n",
      "Epoch 4: |          | 654/? [09:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 654, loss 4.190320014953613\n",
      "Epoch 4: |          | 655/? [09:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 655, loss 3.9385528564453125\n",
      "Epoch 4: |          | 656/? [09:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 656, loss 3.5934715270996094\n",
      "Epoch 4: |          | 657/? [09:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 657, loss 5.951273441314697\n",
      "Epoch 4: |          | 658/? [09:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 658, loss 3.5554232597351074\n",
      "Epoch 4: |          | 659/? [09:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 659, loss 3.9831321239471436\n",
      "Epoch 4: |          | 660/? [09:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 660, loss 4.382863998413086\n",
      "Epoch 4: |          | 661/? [09:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 661, loss 4.302984237670898\n",
      "Epoch 4: |          | 662/? [09:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 662, loss 4.140353202819824\n",
      "Epoch 4: |          | 663/? [09:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 663, loss 3.8757004737854004\n",
      "Epoch 4: |          | 664/? [09:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 664, loss 3.8278141021728516\n",
      "Epoch 4: |          | 665/? [09:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 665, loss 4.14337682723999\n",
      "Epoch 4: |          | 666/? [09:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 666, loss 3.9150402545928955\n",
      "Epoch 4: |          | 667/? [09:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 667, loss 4.760601997375488\n",
      "Epoch 4: |          | 668/? [09:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 668, loss 3.5042145252227783\n",
      "Epoch 4: |          | 669/? [09:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 669, loss 3.7579314708709717\n",
      "Epoch 4: |          | 670/? [09:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 670, loss 4.434080600738525\n",
      "Epoch 4: |          | 671/? [09:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 671, loss 4.165440082550049\n",
      "Epoch 4: |          | 672/? [09:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 672, loss 4.177295684814453\n",
      "Epoch 4: |          | 673/? [09:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 673, loss 4.021098613739014\n",
      "Epoch 4: |          | 674/? [09:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 674, loss 2.4726569652557373\n",
      "Epoch 4: |          | 675/? [09:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 675, loss 0.9764690399169922\n",
      "Epoch 4: |          | 676/? [09:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 676, loss 0.8321444392204285\n",
      "Epoch 4: |          | 677/? [09:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 677, loss 0.6830877065658569\n",
      "Epoch 4: |          | 678/? [09:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 678, loss 1.8180770874023438\n",
      "Epoch 4: |          | 679/? [09:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 679, loss 3.4381256103515625\n",
      "Epoch 4: |          | 680/? [09:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 680, loss 3.9616990089416504\n",
      "Epoch 4: |          | 681/? [09:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 681, loss 3.433450698852539\n",
      "Epoch 4: |          | 682/? [09:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 682, loss 3.813781261444092\n",
      "Epoch 4: |          | 683/? [09:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 683, loss 3.4596946239471436\n",
      "Epoch 4: |          | 684/? [09:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 684, loss 4.549869060516357\n",
      "Epoch 4: |          | 685/? [09:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 685, loss 4.147246837615967\n",
      "Epoch 4: |          | 686/? [09:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 686, loss 3.735825300216675\n",
      "Epoch 4: |          | 687/? [09:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 687, loss 4.316213130950928\n",
      "Epoch 4: |          | 688/? [09:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 688, loss 3.84978985786438\n",
      "Epoch 4: |          | 689/? [09:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 689, loss 3.918382167816162\n",
      "Epoch 4: |          | 690/? [09:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 690, loss 4.499200344085693\n",
      "Epoch 4: |          | 691/? [09:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 691, loss 4.005770206451416\n",
      "Epoch 4: |          | 692/? [09:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 692, loss 4.00299072265625\n",
      "Epoch 4: |          | 693/? [09:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 693, loss 4.530595302581787\n",
      "Epoch 4: |          | 694/? [09:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 694, loss 3.871366500854492\n",
      "Epoch 4: |          | 695/? [09:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 695, loss 4.469788551330566\n",
      "Epoch 4: |          | 696/? [09:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 696, loss 3.7183914184570312\n",
      "Epoch 4: |          | 697/? [09:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 697, loss 3.9529807567596436\n",
      "Epoch 4: |          | 698/? [09:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 698, loss 3.321892261505127\n",
      "Epoch 4: |          | 699/? [09:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 699, loss 4.117335796356201\n",
      "Epoch 4: |          | 700/? [09:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 700, loss 4.213460922241211\n",
      "Epoch 4: |          | 701/? [09:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 701, loss 3.8224120140075684\n",
      "Epoch 4: |          | 702/? [09:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 702, loss 4.084670066833496\n",
      "Epoch 4: |          | 703/? [09:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 703, loss 4.201523780822754\n",
      "Epoch 4: |          | 704/? [09:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 704, loss 4.054533958435059\n",
      "Epoch 4: |          | 705/? [09:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 705, loss 3.687483310699463\n",
      "Epoch 4: |          | 706/? [09:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 706, loss 3.7799949645996094\n",
      "Epoch 4: |          | 707/? [09:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 707, loss 4.232130527496338\n",
      "Epoch 4: |          | 708/? [09:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 708, loss 3.9832427501678467\n",
      "Epoch 4: |          | 709/? [09:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 709, loss 3.8625473976135254\n",
      "Epoch 4: |          | 710/? [09:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 710, loss 4.457781791687012\n",
      "Epoch 4: |          | 711/? [09:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 711, loss 4.510673522949219\n",
      "Epoch 4: |          | 712/? [09:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 712, loss 4.237974166870117\n",
      "Epoch 4: |          | 713/? [09:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 713, loss 4.289158821105957\n",
      "Epoch 4: |          | 714/? [09:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 714, loss 4.386993408203125\n",
      "Epoch 4: |          | 715/? [10:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 715, loss 3.3012871742248535\n",
      "Epoch 4: |          | 716/? [10:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 716, loss 4.059908390045166\n",
      "Epoch 4: |          | 717/? [10:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 717, loss 3.9071602821350098\n",
      "Epoch 4: |          | 718/? [10:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 718, loss 3.4245197772979736\n",
      "Epoch 4: |          | 719/? [10:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 719, loss 3.9481825828552246\n",
      "Epoch 4: |          | 720/? [10:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 720, loss 3.652738571166992\n",
      "Epoch 4: |          | 721/? [10:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 721, loss 4.3259124755859375\n",
      "Epoch 4: |          | 722/? [10:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 722, loss 3.598301649093628\n",
      "Epoch 4: |          | 723/? [10:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 723, loss 4.147265434265137\n",
      "Epoch 4: |          | 724/? [10:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 724, loss 3.7823376655578613\n",
      "Epoch 4: |          | 725/? [10:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 725, loss 3.667166233062744\n",
      "Epoch 4: |          | 726/? [10:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 726, loss 3.8817973136901855\n",
      "Epoch 4: |          | 727/? [10:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 727, loss 3.6457512378692627\n",
      "Epoch 4: |          | 728/? [10:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 728, loss 3.4869651794433594\n",
      "Epoch 4: |          | 729/? [10:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 729, loss 3.9392611980438232\n",
      "Epoch 4: |          | 730/? [10:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 730, loss 3.9099040031433105\n",
      "Epoch 4: |          | 731/? [10:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 731, loss 4.0640034675598145\n",
      "Epoch 4: |          | 732/? [10:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 732, loss 4.338552474975586\n",
      "Epoch 4: |          | 733/? [10:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 733, loss 3.989328384399414\n",
      "Epoch 4: |          | 734/? [10:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 734, loss 4.182385444641113\n",
      "Epoch 4: |          | 735/? [10:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 735, loss 4.1065168380737305\n",
      "Epoch 4: |          | 736/? [10:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 736, loss 3.6538727283477783\n",
      "Epoch 4: |          | 737/? [10:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 737, loss 4.406330108642578\n",
      "Epoch 4: |          | 738/? [10:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 738, loss 3.6037487983703613\n",
      "Epoch 4: |          | 739/? [10:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 739, loss 4.132210731506348\n",
      "Epoch 4: |          | 740/? [10:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 740, loss 3.7699551582336426\n",
      "Epoch 4: |          | 741/? [10:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 741, loss 3.9569766521453857\n",
      "Epoch 4: |          | 742/? [10:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 742, loss 4.353682041168213\n",
      "Epoch 4: |          | 743/? [10:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 743, loss 4.203103065490723\n",
      "Epoch 4: |          | 744/? [10:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 744, loss 4.132868766784668\n",
      "Epoch 4: |          | 745/? [10:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 745, loss 3.7665011882781982\n",
      "Epoch 4: |          | 746/? [10:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 746, loss 4.062684059143066\n",
      "Epoch 4: |          | 747/? [10:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 747, loss 3.822052001953125\n",
      "Epoch 4: |          | 748/? [10:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 748, loss 2.9070944786071777\n",
      "Epoch 4: |          | 749/? [10:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 749, loss 3.9339191913604736\n",
      "Epoch 4: |          | 750/? [10:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 750, loss 4.262359619140625\n",
      "Epoch 4: |          | 751/? [10:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 751, loss 2.6641292572021484\n",
      "Epoch 4: |          | 752/? [10:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 752, loss 4.159743785858154\n",
      "Epoch 4: |          | 753/? [10:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 753, loss 3.311358690261841\n",
      "Epoch 4: |          | 754/? [10:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 754, loss 3.7930169105529785\n",
      "Epoch 4: |          | 755/? [10:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 755, loss 3.5910847187042236\n",
      "Epoch 4: |          | 756/? [10:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 756, loss 3.954397201538086\n",
      "Epoch 4: |          | 757/? [10:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 757, loss 4.014899253845215\n",
      "Epoch 4: |          | 758/? [10:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 758, loss 3.756223678588867\n",
      "Epoch 4: |          | 759/? [10:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 759, loss 3.7454237937927246\n",
      "Epoch 4: |          | 760/? [10:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 760, loss 4.197686672210693\n",
      "Epoch 4: |          | 761/? [10:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 761, loss 4.251842021942139\n",
      "Epoch 4: |          | 762/? [10:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 762, loss 3.917020320892334\n",
      "Epoch 4: |          | 763/? [10:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 763, loss 4.12830924987793\n",
      "Epoch 4: |          | 764/? [10:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 764, loss 4.329819679260254\n",
      "Epoch 4: |          | 765/? [10:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 765, loss 4.1080427169799805\n",
      "Epoch 4: |          | 766/? [10:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 766, loss 4.480896949768066\n",
      "Epoch 4: |          | 767/? [10:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 767, loss 4.556233882904053\n",
      "Epoch 4: |          | 768/? [10:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 768, loss 4.084240436553955\n",
      "Epoch 4: |          | 769/? [10:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 769, loss 3.248046875\n",
      "Epoch 4: |          | 770/? [10:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 770, loss 3.8430354595184326\n",
      "Epoch 4: |          | 771/? [10:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 771, loss 4.5890631675720215\n",
      "Epoch 4: |          | 772/? [10:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 772, loss 4.289674758911133\n",
      "Epoch 4: |          | 773/? [10:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 773, loss 3.933877944946289\n",
      "Epoch 4: |          | 774/? [10:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 774, loss 4.116753578186035\n",
      "Epoch 4: |          | 775/? [10:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 775, loss 4.553959369659424\n",
      "Epoch 4: |          | 776/? [10:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 776, loss 3.991837978363037\n",
      "Epoch 4: |          | 777/? [10:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 777, loss 3.8839707374572754\n",
      "Epoch 4: |          | 778/? [10:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 778, loss 4.263584136962891\n",
      "Epoch 4: |          | 779/? [10:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 779, loss 4.709140777587891\n",
      "Epoch 4: |          | 780/? [10:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 780, loss 3.6500015258789062\n",
      "Epoch 4: |          | 781/? [10:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 781, loss 3.7781906127929688\n",
      "Epoch 4: |          | 782/? [10:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 782, loss 4.167768955230713\n",
      "Epoch 4: |          | 783/? [10:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 783, loss 4.222651958465576\n",
      "Epoch 4: |          | 784/? [10:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 784, loss 3.790825605392456\n",
      "Epoch 4: |          | 785/? [10:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 785, loss 3.625333786010742\n",
      "Epoch 4: |          | 786/? [11:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 786, loss 4.489816665649414\n",
      "Epoch 4: |          | 787/? [11:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 787, loss 4.447972297668457\n",
      "Epoch 4: |          | 788/? [11:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 788, loss 2.3153977394104004\n",
      "Epoch 4: |          | 789/? [11:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 789, loss 3.9235243797302246\n",
      "Epoch 4: |          | 790/? [11:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 790, loss 4.770175457000732\n",
      "Epoch 4: |          | 791/? [11:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 791, loss 4.4457268714904785\n",
      "Epoch 4: |          | 792/? [11:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 792, loss 3.6593315601348877\n",
      "Epoch 4: |          | 793/? [11:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 793, loss 4.116307258605957\n",
      "Epoch 4: |          | 794/? [11:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 794, loss 4.469873905181885\n",
      "Epoch 4: |          | 795/? [11:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 795, loss 3.971295118331909\n",
      "Epoch 4: |          | 796/? [11:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 796, loss 4.363587379455566\n",
      "Epoch 4: |          | 797/? [11:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 797, loss 3.3476357460021973\n",
      "Epoch 4: |          | 798/? [11:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 798, loss 3.447052001953125\n",
      "Epoch 4: |          | 799/? [11:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 799, loss 4.385204315185547\n",
      "Epoch 4: |          | 800/? [11:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 800, loss 4.2115583419799805\n",
      "Epoch 4: |          | 801/? [11:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 801, loss 3.728161573410034\n",
      "Epoch 4: |          | 802/? [11:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 802, loss 4.103451251983643\n",
      "Epoch 4: |          | 803/? [11:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 803, loss 3.9393935203552246\n",
      "Epoch 4: |          | 804/? [11:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 804, loss 4.088276386260986\n",
      "Epoch 4: |          | 805/? [11:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 805, loss 4.291292667388916\n",
      "Epoch 4: |          | 806/? [11:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 806, loss 4.65915584564209\n",
      "Epoch 4: |          | 807/? [11:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 807, loss 4.087662220001221\n",
      "Epoch 4: |          | 808/? [11:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 808, loss 3.6464359760284424\n",
      "Epoch 4: |          | 809/? [11:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 809, loss 4.225768089294434\n",
      "Epoch 4: |          | 810/? [11:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 810, loss 4.00081729888916\n",
      "Epoch 4: |          | 811/? [11:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 811, loss 4.257031440734863\n",
      "Epoch 4: |          | 812/? [11:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 812, loss 4.895878791809082\n",
      "Epoch 4: |          | 813/? [11:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 813, loss 4.604472637176514\n",
      "Epoch 4: |          | 814/? [11:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 814, loss 3.6229357719421387\n",
      "Epoch 4: |          | 815/? [11:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 815, loss 4.37032413482666\n",
      "Epoch 4: |          | 816/? [11:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 816, loss 4.195548057556152\n",
      "Epoch 4: |          | 817/? [11:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 817, loss 3.505995273590088\n",
      "Epoch 4: |          | 818/? [11:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 818, loss 4.521921157836914\n",
      "Epoch 4: |          | 819/? [11:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 819, loss 4.21321964263916\n",
      "Epoch 4: |          | 820/? [11:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 820, loss 4.033227920532227\n",
      "Epoch 4: |          | 821/? [11:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 821, loss 4.006479263305664\n",
      "Epoch 4: |          | 822/? [11:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 822, loss 3.6571342945098877\n",
      "Epoch 4: |          | 823/? [11:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 823, loss 3.698045015335083\n",
      "Epoch 4: |          | 824/? [11:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 824, loss 4.15844202041626\n",
      "Epoch 4: |          | 825/? [11:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 825, loss 3.6975631713867188\n",
      "Epoch 4: |          | 826/? [11:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 826, loss 4.232582092285156\n",
      "Epoch 4: |          | 827/? [11:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 827, loss 3.8508248329162598\n",
      "Epoch 4: |          | 828/? [11:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 828, loss 4.3417558670043945\n",
      "Epoch 4: |          | 829/? [11:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 829, loss 4.008862495422363\n",
      "Epoch 4: |          | 830/? [11:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 830, loss 4.6307692527771\n",
      "Epoch 4: |          | 831/? [11:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 831, loss 2.494188070297241\n",
      "Epoch 4: |          | 832/? [11:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 832, loss 3.9854793548583984\n",
      "Epoch 4: |          | 833/? [11:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 833, loss 3.8601622581481934\n",
      "Epoch 4: |          | 834/? [11:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 834, loss 4.627358913421631\n",
      "Epoch 4: |          | 835/? [11:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 835, loss 3.970057725906372\n",
      "Epoch 4: |          | 836/? [11:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 836, loss 4.637490272521973\n",
      "Epoch 4: |          | 837/? [11:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 837, loss 4.0768022537231445\n",
      "Epoch 4: |          | 838/? [11:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 838, loss 3.395434856414795\n",
      "Epoch 4: |          | 839/? [11:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 839, loss 3.7342820167541504\n",
      "Epoch 4: |          | 840/? [11:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 840, loss 4.2976579666137695\n",
      "Epoch 4: |          | 841/? [11:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 841, loss 4.3479719161987305\n",
      "Epoch 4: |          | 842/? [11:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 842, loss 4.027665138244629\n",
      "Epoch 4: |          | 843/? [11:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 843, loss 4.394529342651367\n",
      "Epoch 4: |          | 844/? [11:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 844, loss 3.7097861766815186\n",
      "Epoch 4: |          | 845/? [11:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 845, loss 4.131276607513428\n",
      "Epoch 4: |          | 846/? [11:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 846, loss 4.607072353363037\n",
      "Epoch 4: |          | 847/? [11:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 847, loss 4.158932685852051\n",
      "Epoch 4: |          | 848/? [11:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 848, loss 3.6788337230682373\n",
      "Epoch 4: |          | 849/? [11:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 849, loss 3.7909350395202637\n",
      "Epoch 4: |          | 850/? [11:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 850, loss 3.8951828479766846\n",
      "Epoch 4: |          | 851/? [11:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 851, loss 4.2197980880737305\n",
      "Epoch 4: |          | 852/? [11:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 852, loss 4.295735836029053\n",
      "Epoch 4: |          | 853/? [11:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 853, loss 4.164804935455322\n",
      "Epoch 4: |          | 854/? [11:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 854, loss 3.4484012126922607\n",
      "Epoch 4: |          | 855/? [11:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 855, loss 3.7374253273010254\n",
      "Epoch 4: |          | 856/? [12:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 856, loss 3.6814332008361816\n",
      "Epoch 4: |          | 857/? [12:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 857, loss 4.242825031280518\n",
      "Epoch 4: |          | 858/? [12:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 858, loss 4.152309894561768\n",
      "Epoch 4: |          | 859/? [12:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 859, loss 4.154674530029297\n",
      "Epoch 4: |          | 860/? [12:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 860, loss 4.536002159118652\n",
      "Epoch 4: |          | 861/? [12:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 861, loss 3.8021152019500732\n",
      "Epoch 4: |          | 862/? [12:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 862, loss 4.171677589416504\n",
      "Epoch 4: |          | 863/? [12:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 863, loss 3.5036873817443848\n",
      "Epoch 4: |          | 864/? [12:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 864, loss 4.115007400512695\n",
      "Epoch 4: |          | 865/? [12:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 865, loss 4.0906453132629395\n",
      "Epoch 4: |          | 866/? [12:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 866, loss 3.0935137271881104\n",
      "Epoch 4: |          | 867/? [12:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 867, loss 3.2895913124084473\n",
      "Epoch 4: |          | 868/? [12:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 868, loss 4.178110599517822\n",
      "Epoch 4: |          | 869/? [12:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 869, loss 4.199265480041504\n",
      "Epoch 4: |          | 870/? [12:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 870, loss 3.7864997386932373\n",
      "Epoch 4: |          | 871/? [12:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 871, loss 4.148703098297119\n",
      "Epoch 4: |          | 872/? [12:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 872, loss 3.9744906425476074\n",
      "Epoch 4: |          | 873/? [12:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 873, loss 3.961073398590088\n",
      "Epoch 4: |          | 874/? [12:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 874, loss 3.480501890182495\n",
      "Epoch 4: |          | 875/? [12:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 875, loss 4.195745468139648\n",
      "Epoch 4: |          | 876/? [12:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 876, loss 3.772059917449951\n",
      "Epoch 4: |          | 877/? [12:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 877, loss 4.132781505584717\n",
      "Epoch 4: |          | 878/? [12:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 878, loss 3.5645782947540283\n",
      "Epoch 4: |          | 879/? [12:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 879, loss 3.6080288887023926\n",
      "Epoch 4: |          | 880/? [12:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 880, loss 4.741818904876709\n",
      "Epoch 4: |          | 881/? [12:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 881, loss 4.119793891906738\n",
      "Epoch 4: |          | 882/? [12:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 882, loss 3.8989853858947754\n",
      "Epoch 4: |          | 883/? [12:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 883, loss 4.056442737579346\n",
      "Epoch 4: |          | 884/? [12:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 884, loss 4.102542877197266\n",
      "Epoch 4: |          | 885/? [12:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 885, loss 3.8558077812194824\n",
      "Epoch 4: |          | 886/? [12:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 886, loss 4.507680892944336\n",
      "Epoch 4: |          | 887/? [12:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 887, loss 4.528146266937256\n",
      "Epoch 4: |          | 888/? [12:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 888, loss 4.258791923522949\n",
      "Epoch 4: |          | 889/? [12:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 889, loss 3.8589630126953125\n",
      "Epoch 4: |          | 890/? [12:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 890, loss 4.097528457641602\n",
      "Epoch 4: |          | 891/? [12:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 891, loss 3.799315929412842\n",
      "Epoch 4: |          | 892/? [12:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 892, loss 4.470733165740967\n",
      "Epoch 4: |          | 893/? [12:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 893, loss 3.8895561695098877\n",
      "Epoch 4: |          | 894/? [12:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 894, loss 3.394439220428467\n",
      "Epoch 4: |          | 895/? [12:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 895, loss 4.554640293121338\n",
      "Epoch 4: |          | 896/? [12:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 896, loss 4.128972053527832\n",
      "Epoch 4: |          | 897/? [12:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 897, loss 4.163336277008057\n",
      "Epoch 4: |          | 898/? [12:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 898, loss 4.146204948425293\n",
      "Epoch 4: |          | 899/? [12:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 899, loss 3.9023537635803223\n",
      "Epoch 4: |          | 900/? [12:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 900, loss 3.841928482055664\n",
      "Epoch 4: |          | 901/? [12:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 901, loss 4.228360176086426\n",
      "Epoch 4: |          | 902/? [12:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 902, loss 4.344676971435547\n",
      "Epoch 4: |          | 903/? [12:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 903, loss 3.6426384449005127\n",
      "Epoch 4: |          | 904/? [12:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 904, loss 4.125454902648926\n",
      "Epoch 4: |          | 905/? [12:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 905, loss 4.319337368011475\n",
      "Epoch 4: |          | 906/? [12:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 906, loss 4.036837100982666\n",
      "Epoch 4: |          | 907/? [12:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 907, loss 4.120048522949219\n",
      "Epoch 4: |          | 908/? [12:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 908, loss 4.206080436706543\n",
      "Epoch 4: |          | 909/? [12:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 909, loss 4.1847991943359375\n",
      "Epoch 4: |          | 910/? [12:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 910, loss 3.970726490020752\n",
      "Epoch 4: |          | 911/? [12:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 911, loss 3.9953453540802\n",
      "Epoch 4: |          | 912/? [12:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 912, loss 3.968635082244873\n",
      "Epoch 4: |          | 913/? [12:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 913, loss 3.9791598320007324\n",
      "Epoch 4: |          | 914/? [12:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 914, loss 4.246613025665283\n",
      "Epoch 4: |          | 915/? [12:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 915, loss 4.090276718139648\n",
      "Epoch 4: |          | 916/? [12:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 916, loss 4.016009330749512\n",
      "Epoch 4: |          | 917/? [12:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 917, loss 3.992863893508911\n",
      "Epoch 4: |          | 918/? [12:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 918, loss 3.8754563331604004\n",
      "Epoch 4: |          | 919/? [12:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 919, loss 3.874922275543213\n",
      "Epoch 4: |          | 920/? [12:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 920, loss 4.046356201171875\n",
      "Epoch 4: |          | 921/? [13:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 921, loss 3.8706252574920654\n",
      "Epoch 4: |          | 922/? [13:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 922, loss 4.03475284576416\n",
      "Epoch 4: |          | 923/? [13:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 923, loss 3.8726534843444824\n",
      "Epoch 4: |          | 924/? [13:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 924, loss 3.8858776092529297\n",
      "Epoch 4: |          | 925/? [13:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 925, loss 4.188796043395996\n",
      "Epoch 4: |          | 926/? [13:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 926, loss 3.9615371227264404\n",
      "Epoch 4: |          | 927/? [13:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 927, loss 4.2293524742126465\n",
      "Epoch 4: |          | 928/? [13:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 928, loss 3.700411558151245\n",
      "Epoch 4: |          | 929/? [13:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 929, loss 3.8254222869873047\n",
      "Epoch 4: |          | 930/? [13:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 930, loss 3.7341532707214355\n",
      "Epoch 4: |          | 931/? [13:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 931, loss 3.426955461502075\n",
      "Epoch 4: |          | 932/? [13:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 932, loss 4.113287925720215\n",
      "Epoch 4: |          | 933/? [13:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 933, loss 3.882922410964966\n",
      "Epoch 4: |          | 934/? [13:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 934, loss 4.46274471282959\n",
      "Epoch 4: |          | 935/? [13:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 935, loss 4.754745006561279\n",
      "Epoch 4: |          | 936/? [13:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 936, loss 3.98783802986145\n",
      "Epoch 4: |          | 937/? [13:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 937, loss 4.005465030670166\n",
      "Epoch 4: |          | 938/? [13:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 938, loss 3.89711332321167\n",
      "Epoch 4: |          | 939/? [13:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 939, loss 4.151978969573975\n",
      "Epoch 4: |          | 940/? [13:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 940, loss 4.361050605773926\n",
      "Epoch 4: |          | 941/? [13:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 941, loss 3.853057861328125\n",
      "Epoch 4: |          | 942/? [13:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 942, loss 3.372271776199341\n",
      "Epoch 4: |          | 943/? [13:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 943, loss 4.250644683837891\n",
      "Epoch 4: |          | 944/? [13:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 944, loss 3.3590610027313232\n",
      "Epoch 4: |          | 945/? [13:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 945, loss 3.9666664600372314\n",
      "Epoch 4: |          | 946/? [13:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 946, loss 3.9637534618377686\n",
      "Epoch 4: |          | 947/? [13:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 947, loss 3.8502368927001953\n",
      "Epoch 4: |          | 948/? [13:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 948, loss 4.075722694396973\n",
      "Epoch 4: |          | 949/? [13:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 949, loss 3.9379563331604004\n",
      "Epoch 4: |          | 950/? [13:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 950, loss 3.7120068073272705\n",
      "Epoch 4: |          | 951/? [13:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 951, loss 4.384135723114014\n",
      "Epoch 4: |          | 952/? [13:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 952, loss 4.337436676025391\n",
      "Epoch 4: |          | 953/? [13:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 953, loss 4.883509159088135\n",
      "Epoch 4: |          | 954/? [13:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 954, loss 3.853351593017578\n",
      "Epoch 4: |          | 955/? [13:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 955, loss 4.546618938446045\n",
      "Epoch 4: |          | 956/? [13:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 956, loss 3.9238247871398926\n",
      "Epoch 4: |          | 957/? [13:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 957, loss 4.1528730392456055\n",
      "Epoch 4: |          | 958/? [13:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 958, loss 4.309792995452881\n",
      "Epoch 4: |          | 959/? [13:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 959, loss 3.896301746368408\n",
      "Epoch 4: |          | 960/? [13:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 960, loss 4.3250603675842285\n",
      "Epoch 4: |          | 961/? [13:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 961, loss 4.524237155914307\n",
      "Epoch 4: |          | 962/? [13:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 962, loss 4.062241077423096\n",
      "Epoch 4: |          | 963/? [13:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 963, loss 3.8138680458068848\n",
      "Epoch 4: |          | 964/? [13:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 964, loss 4.269556045532227\n",
      "Epoch 4: |          | 965/? [13:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 965, loss 3.7506630420684814\n",
      "Epoch 4: |          | 966/? [13:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 966, loss 3.6560797691345215\n",
      "Epoch 4: |          | 967/? [13:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 967, loss 3.9052810668945312\n",
      "Epoch 4: |          | 968/? [13:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 968, loss 3.849492311477661\n",
      "Epoch 4: |          | 969/? [13:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 969, loss 3.6798884868621826\n",
      "Epoch 4: |          | 970/? [13:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 970, loss 4.174541473388672\n",
      "Epoch 4: |          | 971/? [13:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 971, loss 4.444931983947754\n",
      "Epoch 4: |          | 972/? [13:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 972, loss 3.803987979888916\n",
      "Epoch 4: |          | 973/? [13:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 973, loss 4.0413007736206055\n",
      "Epoch 4: |          | 974/? [13:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 974, loss 4.053573131561279\n",
      "Epoch 4: |          | 975/? [13:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 975, loss 4.066380977630615\n",
      "Epoch 4: |          | 976/? [13:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 976, loss 4.1334052085876465\n",
      "Epoch 4: |          | 977/? [13:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 977, loss 4.717889308929443\n",
      "Epoch 4: |          | 978/? [13:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 978, loss 4.2456464767456055\n",
      "Epoch 4: |          | 979/? [13:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 979, loss 4.4351806640625\n",
      "Epoch 4: |          | 980/? [13:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 980, loss 3.6148178577423096\n",
      "Epoch 4: |          | 981/? [13:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 981, loss 3.437145709991455\n",
      "Epoch 4: |          | 982/? [13:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 982, loss 4.078525066375732\n",
      "Epoch 4: |          | 983/? [13:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 983, loss 4.478151321411133\n",
      "Epoch 4: |          | 984/? [13:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 984, loss 3.545231342315674\n",
      "Epoch 4: |          | 985/? [13:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 985, loss 3.8030967712402344\n",
      "Epoch 4: |          | 986/? [13:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 986, loss 3.8065483570098877\n",
      "Epoch 4: |          | 987/? [13:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 987, loss 3.3604226112365723\n",
      "Epoch 4: |          | 988/? [13:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 988, loss 4.394357681274414\n",
      "Epoch 4: |          | 989/? [13:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 989, loss 4.018643379211426\n",
      "Epoch 4: |          | 990/? [13:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 990, loss 3.304426908493042\n",
      "Epoch 4: |          | 991/? [13:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 991, loss 4.078996181488037\n",
      "Epoch 4: |          | 992/? [13:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 992, loss 4.760467052459717\n",
      "Epoch 4: |          | 993/? [13:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 993, loss 3.853437900543213\n",
      "Epoch 4: |          | 994/? [14:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 994, loss 3.861963987350464\n",
      "Epoch 4: |          | 995/? [14:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 995, loss 4.312751770019531\n",
      "Epoch 4: |          | 996/? [14:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 996, loss 4.2844133377075195\n",
      "Epoch 4: |          | 997/? [14:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 997, loss 3.9004008769989014\n",
      "Epoch 4: |          | 998/? [14:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 998, loss 4.124866008758545\n",
      "Epoch 4: |          | 999/? [14:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 999, loss 4.18922233581543\n",
      "Epoch 4: |          | 1000/? [14:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1000, loss 3.616100311279297\n",
      "Epoch 4: |          | 1001/? [14:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1001, loss 4.322365760803223\n",
      "Epoch 4: |          | 1002/? [14:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1002, loss 4.25168514251709\n",
      "Epoch 4: |          | 1003/? [14:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1003, loss 4.436556816101074\n",
      "Epoch 4: |          | 1004/? [14:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1004, loss 3.4028782844543457\n",
      "Epoch 4: |          | 1005/? [14:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1005, loss 3.968702793121338\n",
      "Epoch 4: |          | 1006/? [14:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1006, loss 4.327294826507568\n",
      "Epoch 4: |          | 1007/? [14:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1007, loss 3.92069673538208\n",
      "Epoch 4: |          | 1008/? [14:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1008, loss 4.012824058532715\n",
      "Epoch 4: |          | 1009/? [14:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1009, loss 4.328601837158203\n",
      "Epoch 4: |          | 1010/? [14:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1010, loss 3.423964738845825\n",
      "Epoch 4: |          | 1011/? [14:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1011, loss 4.024174690246582\n",
      "Epoch 4: |          | 1012/? [14:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1012, loss 3.7959156036376953\n",
      "Epoch 4: |          | 1013/? [14:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1013, loss 4.0223822593688965\n",
      "Epoch 4: |          | 1014/? [14:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1014, loss 4.413539886474609\n",
      "Epoch 4: |          | 1015/? [14:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1015, loss 4.070939540863037\n",
      "Epoch 4: |          | 1016/? [14:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1016, loss 3.8714518547058105\n",
      "Epoch 4: |          | 1017/? [14:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1017, loss 3.328965425491333\n",
      "Epoch 4: |          | 1018/? [14:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1018, loss 3.9360976219177246\n",
      "Epoch 4: |          | 1019/? [14:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1019, loss 4.001589775085449\n",
      "Epoch 4: |          | 1020/? [14:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1020, loss 3.5630135536193848\n",
      "Epoch 4: |          | 1021/? [14:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1021, loss 3.8265833854675293\n",
      "Epoch 4: |          | 1022/? [14:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1022, loss 3.5951743125915527\n",
      "Epoch 4: |          | 1023/? [14:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1023, loss 3.324683427810669\n",
      "Epoch 4: |          | 1024/? [14:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1024, loss 3.859355926513672\n",
      "Epoch 4: |          | 1025/? [14:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1025, loss 3.78322172164917\n",
      "Epoch 4: |          | 1026/? [14:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1026, loss 2.8420121669769287\n",
      "Epoch 4: |          | 1027/? [14:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1027, loss 4.066781044006348\n",
      "Epoch 4: |          | 1028/? [14:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1028, loss 3.9037890434265137\n",
      "Epoch 4: |          | 1029/? [14:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1029, loss 3.8077902793884277\n",
      "Epoch 4: |          | 1030/? [14:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1030, loss 3.595261335372925\n",
      "Epoch 4: |          | 1031/? [14:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1031, loss 3.6611809730529785\n",
      "Epoch 4: |          | 1032/? [14:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1032, loss 4.166225910186768\n",
      "Epoch 4: |          | 1033/? [14:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1033, loss 4.388980388641357\n",
      "Epoch 4: |          | 1034/? [14:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1034, loss 3.761309862136841\n",
      "Epoch 4: |          | 1035/? [14:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1035, loss 3.7721142768859863\n",
      "Epoch 4: |          | 1036/? [14:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1036, loss 3.686681032180786\n",
      "Epoch 4: |          | 1037/? [14:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1037, loss 4.346482276916504\n",
      "Epoch 4: |          | 1038/? [14:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1038, loss 4.487374782562256\n",
      "Epoch 4: |          | 1039/? [14:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1039, loss 4.773218154907227\n",
      "Epoch 4: |          | 1040/? [14:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1040, loss 4.090768814086914\n",
      "Epoch 4: |          | 1041/? [14:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1041, loss 4.398959159851074\n",
      "Epoch 4: |          | 1042/? [14:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1042, loss 3.9601027965545654\n",
      "Epoch 4: |          | 1043/? [14:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1043, loss 4.324353218078613\n",
      "Epoch 4: |          | 1044/? [14:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1044, loss 3.880927562713623\n",
      "Epoch 4: |          | 1045/? [14:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1045, loss 3.5019326210021973\n",
      "Epoch 4: |          | 1046/? [14:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1046, loss 3.344067096710205\n",
      "Epoch 4: |          | 1047/? [14:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1047, loss 4.492398262023926\n",
      "Epoch 4: |          | 1048/? [14:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1048, loss 3.881909132003784\n",
      "Epoch 4: |          | 1049/? [14:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1049, loss 4.12929630279541\n",
      "Epoch 4: |          | 1050/? [14:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1050, loss 3.6810264587402344\n",
      "Epoch 4: |          | 1051/? [14:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1051, loss 3.6808924674987793\n",
      "Epoch 4: |          | 1052/? [14:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1052, loss 4.298272132873535\n",
      "Epoch 4: |          | 1053/? [14:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1053, loss 4.427824974060059\n",
      "Epoch 4: |          | 1054/? [14:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1054, loss 3.8513236045837402\n",
      "Epoch 4: |          | 1055/? [14:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1055, loss 3.5340240001678467\n",
      "Epoch 4: |          | 1056/? [14:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1056, loss 3.5008270740509033\n",
      "Epoch 4: |          | 1057/? [14:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1057, loss 4.1866607666015625\n",
      "Epoch 4: |          | 1058/? [14:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1058, loss 3.7367405891418457\n",
      "Epoch 4: |          | 1059/? [14:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1059, loss 4.359504699707031\n",
      "Epoch 4: |          | 1060/? [14:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1060, loss 4.236152172088623\n",
      "Epoch 4: |          | 1061/? [14:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1061, loss 2.950821876525879\n",
      "Epoch 4: |          | 1062/? [14:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1062, loss 3.7977218627929688\n",
      "Epoch 4: |          | 1063/? [14:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1063, loss 3.9369118213653564\n",
      "Epoch 4: |          | 1064/? [14:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1064, loss 4.1471638679504395\n",
      "Epoch 4: |          | 1065/? [15:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1065, loss 2.7597219944000244\n",
      "Epoch 4: |          | 1066/? [15:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1066, loss 4.095303535461426\n",
      "Epoch 4: |          | 1067/? [15:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1067, loss 3.5876712799072266\n",
      "Epoch 4: |          | 1068/? [15:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1068, loss 3.7031052112579346\n",
      "Epoch 4: |          | 1069/? [15:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1069, loss 4.0979437828063965\n",
      "Epoch 4: |          | 1070/? [15:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1070, loss 3.88114595413208\n",
      "Epoch 4: |          | 1071/? [15:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1071, loss 4.22002649307251\n",
      "Epoch 4: |          | 1072/? [15:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1072, loss 4.272130012512207\n",
      "Epoch 4: |          | 1073/? [15:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1073, loss 4.487011909484863\n",
      "Epoch 4: |          | 1074/? [15:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1074, loss 3.7853622436523438\n",
      "Epoch 4: |          | 1075/? [15:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1075, loss 3.6370291709899902\n",
      "Epoch 4: |          | 1076/? [15:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1076, loss 4.257282257080078\n",
      "Epoch 4: |          | 1077/? [15:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1077, loss 3.7552897930145264\n",
      "Epoch 4: |          | 1078/? [15:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1078, loss 3.991288661956787\n",
      "Epoch 4: |          | 1079/? [15:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1079, loss 4.530841827392578\n",
      "Epoch 4: |          | 1080/? [15:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1080, loss 3.9268925189971924\n",
      "Epoch 4: |          | 1081/? [15:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1081, loss 4.205897808074951\n",
      "Epoch 4: |          | 1082/? [15:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1082, loss 3.7818057537078857\n",
      "Epoch 4: |          | 1083/? [15:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1083, loss 3.373459577560425\n",
      "Epoch 4: |          | 1084/? [15:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1084, loss 3.179013729095459\n",
      "Epoch 4: |          | 1085/? [15:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1085, loss 3.846503496170044\n",
      "Epoch 4: |          | 1086/? [15:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1086, loss 4.143848419189453\n",
      "Epoch 4: |          | 1087/? [15:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1087, loss 4.640143394470215\n",
      "Epoch 4: |          | 1088/? [15:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1088, loss 4.230587482452393\n",
      "Epoch 4: |          | 1089/? [15:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1089, loss 4.212581634521484\n",
      "Epoch 4: |          | 1090/? [15:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1090, loss 3.9830498695373535\n",
      "Epoch 4: |          | 1091/? [15:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1091, loss 3.7578952312469482\n",
      "Epoch 4: |          | 1092/? [15:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1092, loss 4.05594539642334\n",
      "Epoch 4: |          | 1093/? [15:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1093, loss 3.528200149536133\n",
      "Epoch 4: |          | 1094/? [15:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1094, loss 4.063223361968994\n",
      "Epoch 4: |          | 1095/? [15:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1095, loss 4.104435443878174\n",
      "Epoch 4: |          | 1096/? [15:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1096, loss 4.338595390319824\n",
      "Epoch 4: |          | 1097/? [15:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1097, loss 3.9170756340026855\n",
      "Epoch 4: |          | 1098/? [15:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1098, loss 3.1932966709136963\n",
      "Epoch 4: |          | 1099/? [15:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1099, loss 3.8705356121063232\n",
      "Epoch 4: |          | 1100/? [15:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1100, loss 4.097706317901611\n",
      "Epoch 4: |          | 1101/? [15:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1101, loss 3.6794562339782715\n",
      "Epoch 4: |          | 1102/? [15:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1102, loss 4.522011756896973\n",
      "Epoch 4: |          | 1103/? [15:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1103, loss 4.986611843109131\n",
      "Epoch 4: |          | 1104/? [15:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1104, loss 4.24948787689209\n",
      "Epoch 4: |          | 1105/? [15:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1105, loss 4.386331558227539\n",
      "Epoch 4: |          | 1106/? [15:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1106, loss 3.8668506145477295\n",
      "Epoch 4: |          | 1107/? [15:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1107, loss 4.003727436065674\n",
      "Epoch 4: |          | 1108/? [15:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1108, loss 3.9863674640655518\n",
      "Epoch 4: |          | 1109/? [15:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1109, loss 3.5797524452209473\n",
      "Epoch 4: |          | 1110/? [15:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1110, loss 4.537521839141846\n",
      "Epoch 4: |          | 1111/? [15:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1111, loss 4.2137603759765625\n",
      "Epoch 4: |          | 1112/? [15:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1112, loss 4.0570807456970215\n",
      "Epoch 4: |          | 1113/? [15:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1113, loss 3.870812177658081\n",
      "Epoch 4: |          | 1114/? [15:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1114, loss 3.3258423805236816\n",
      "Epoch 4: |          | 1115/? [15:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1115, loss 3.0933356285095215\n",
      "Epoch 4: |          | 1116/? [15:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1116, loss 3.4874515533447266\n",
      "Epoch 4: |          | 1117/? [15:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1117, loss 3.724064588546753\n",
      "Epoch 4: |          | 1118/? [15:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1118, loss 3.7943992614746094\n",
      "Epoch 4: |          | 1119/? [15:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1119, loss 4.417794227600098\n",
      "Epoch 4: |          | 1120/? [15:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1120, loss 3.955799102783203\n",
      "Epoch 4: |          | 1121/? [15:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1121, loss 4.198502540588379\n",
      "Epoch 4: |          | 1122/? [15:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1122, loss 3.8188834190368652\n",
      "Epoch 4: |          | 1123/? [15:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1123, loss 3.9958274364471436\n",
      "Epoch 4: |          | 1124/? [15:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1124, loss 4.356612205505371\n",
      "Epoch 4: |          | 1125/? [15:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1125, loss 3.653041124343872\n",
      "Epoch 4: |          | 1126/? [15:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1126, loss 3.5699901580810547\n",
      "Epoch 4: |          | 1127/? [15:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1127, loss 3.9053714275360107\n",
      "Epoch 4: |          | 1128/? [15:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1128, loss 3.944974899291992\n",
      "Epoch 4: |          | 1129/? [15:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1129, loss 4.059283256530762\n",
      "Epoch 4: |          | 1130/? [15:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1130, loss 4.249600410461426\n",
      "Epoch 4: |          | 1131/? [15:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1131, loss 4.306546688079834\n",
      "Epoch 4: |          | 1132/? [15:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1132, loss 3.074317216873169\n",
      "Epoch 4: |          | 1133/? [15:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1133, loss 3.9522175788879395\n",
      "Epoch 4: |          | 1134/? [15:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1134, loss 3.7228477001190186\n",
      "Epoch 4: |          | 1135/? [16:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1135, loss 4.3327107429504395\n",
      "Epoch 4: |          | 1136/? [16:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1136, loss 3.979309558868408\n",
      "Epoch 4: |          | 1137/? [16:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1137, loss 4.017293453216553\n",
      "Epoch 4: |          | 1138/? [16:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1138, loss 4.484070301055908\n",
      "Epoch 4: |          | 1139/? [16:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1139, loss 5.064736366271973\n",
      "Epoch 4: |          | 1140/? [16:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1140, loss 3.9117634296417236\n",
      "Epoch 4: |          | 1141/? [16:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1141, loss 4.363440036773682\n",
      "Epoch 4: |          | 1142/? [16:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1142, loss 4.47035551071167\n",
      "Epoch 4: |          | 1143/? [16:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1143, loss 4.4477338790893555\n",
      "Epoch 4: |          | 1144/? [16:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1144, loss 3.84613037109375\n",
      "Epoch 4: |          | 1145/? [16:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1145, loss 3.9903130531311035\n",
      "Epoch 4: |          | 1146/? [16:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1146, loss 3.654613971710205\n",
      "Epoch 4: |          | 1147/? [16:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1147, loss 3.524536609649658\n",
      "Epoch 4: |          | 1148/? [16:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1148, loss 3.793548107147217\n",
      "Epoch 4: |          | 1149/? [16:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1149, loss 4.709710121154785\n",
      "Epoch 4: |          | 1150/? [16:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1150, loss 4.157347202301025\n",
      "Epoch 4: |          | 1151/? [16:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1151, loss 4.476770401000977\n",
      "Epoch 4: |          | 1152/? [16:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1152, loss 3.7098851203918457\n",
      "Epoch 4: |          | 1153/? [16:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1153, loss 4.061415672302246\n",
      "Epoch 4: |          | 1154/? [16:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1154, loss 3.70324969291687\n",
      "Epoch 4: |          | 1155/? [16:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1155, loss 3.94633412361145\n",
      "Epoch 4: |          | 1156/? [16:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1156, loss 3.957523822784424\n",
      "Epoch 4: |          | 1157/? [16:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1157, loss 4.246498107910156\n",
      "Epoch 4: |          | 1158/? [16:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1158, loss 4.452755928039551\n",
      "Epoch 4: |          | 1159/? [16:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1159, loss 3.191673755645752\n",
      "Epoch 4: |          | 1160/? [16:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1160, loss 4.396357536315918\n",
      "Epoch 4: |          | 1161/? [16:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1161, loss 4.276314735412598\n",
      "Epoch 4: |          | 1162/? [16:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1162, loss 4.167179584503174\n",
      "Epoch 4: |          | 1163/? [16:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1163, loss 4.736149311065674\n",
      "Epoch 4: |          | 1164/? [16:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1164, loss 4.5177178382873535\n",
      "Epoch 4: |          | 1165/? [16:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1165, loss 3.6565747261047363\n",
      "Epoch 4: |          | 1166/? [16:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1166, loss 4.174746513366699\n",
      "Epoch 4: |          | 1167/? [16:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1167, loss 4.239212512969971\n",
      "Epoch 4: |          | 1168/? [16:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1168, loss 4.68517541885376\n",
      "Epoch 4: |          | 1169/? [16:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1169, loss 3.7418060302734375\n",
      "Epoch 4: |          | 1170/? [16:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1170, loss 4.2921319007873535\n",
      "Epoch 4: |          | 1171/? [16:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1171, loss 3.661046266555786\n",
      "Epoch 4: |          | 1172/? [16:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1172, loss 3.5595717430114746\n",
      "Epoch 4: |          | 1173/? [16:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1173, loss 4.144244194030762\n",
      "Epoch 4: |          | 1174/? [16:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1174, loss 3.640660524368286\n",
      "Epoch 4: |          | 1175/? [16:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1175, loss 4.214756965637207\n",
      "Epoch 4: |          | 1176/? [16:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1176, loss 4.327296257019043\n",
      "Epoch 4: |          | 1177/? [16:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1177, loss 4.438591480255127\n",
      "Epoch 4: |          | 1178/? [16:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1178, loss 3.8453803062438965\n",
      "Epoch 4: |          | 1179/? [16:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1179, loss 4.388646125793457\n",
      "Epoch 4: |          | 1180/? [16:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1180, loss 4.23539400100708\n",
      "Epoch 4: |          | 1181/? [16:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1181, loss 4.164179801940918\n",
      "Epoch 4: |          | 1182/? [16:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1182, loss 3.9830451011657715\n",
      "Epoch 4: |          | 1183/? [16:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1183, loss 3.706751585006714\n",
      "Epoch 4: |          | 1184/? [16:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1184, loss 4.0738325119018555\n",
      "Epoch 4: |          | 1185/? [16:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1185, loss 3.8631081581115723\n",
      "Epoch 4: |          | 1186/? [16:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1186, loss 4.0913848876953125\n",
      "Epoch 4: |          | 1187/? [16:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1187, loss 3.933150053024292\n",
      "Epoch 4: |          | 1188/? [16:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1188, loss 4.324561595916748\n",
      "Epoch 4: |          | 1189/? [16:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1189, loss 4.400569438934326\n",
      "Epoch 4: |          | 1190/? [16:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1190, loss 3.985621690750122\n",
      "Epoch 4: |          | 1191/? [16:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1191, loss 4.00918436050415\n",
      "Epoch 4: |          | 1192/? [16:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1192, loss 4.271432399749756\n",
      "Epoch 4: |          | 1193/? [16:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1193, loss 3.7859299182891846\n",
      "Epoch 4: |          | 1194/? [16:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1194, loss 3.4384217262268066\n",
      "Epoch 4: |          | 1195/? [16:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1195, loss 4.018755912780762\n",
      "Epoch 4: |          | 1196/? [16:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1196, loss 4.229947566986084\n",
      "Epoch 4: |          | 1197/? [16:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1197, loss 4.011064052581787\n",
      "Epoch 4: |          | 1198/? [16:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1198, loss 4.0965986251831055\n",
      "Epoch 4: |          | 1199/? [16:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1199, loss 4.348749160766602\n",
      "Epoch 4: |          | 1200/? [16:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1200, loss 3.5548481941223145\n",
      "Epoch 4: |          | 1201/? [16:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1201, loss 4.208666801452637\n",
      "Epoch 4: |          | 1202/? [16:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1202, loss 3.851198673248291\n",
      "Epoch 4: |          | 1203/? [16:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1203, loss 3.874406337738037\n",
      "Epoch 4: |          | 1204/? [17:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1204, loss 3.3721134662628174\n",
      "Epoch 4: |          | 1205/? [17:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1205, loss 3.9896533489227295\n",
      "Epoch 4: |          | 1206/? [17:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1206, loss 3.9871699810028076\n",
      "Epoch 4: |          | 1207/? [17:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1207, loss 4.286459922790527\n",
      "Epoch 4: |          | 1208/? [17:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1208, loss 4.471558094024658\n",
      "Epoch 4: |          | 1209/? [17:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1209, loss 4.025792598724365\n",
      "Epoch 4: |          | 1210/? [17:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1210, loss 4.330574989318848\n",
      "Epoch 4: |          | 1211/? [17:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1211, loss 4.324141979217529\n",
      "Epoch 4: |          | 1212/? [17:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1212, loss 4.099611282348633\n",
      "Epoch 4: |          | 1213/? [17:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1213, loss 3.8229033946990967\n",
      "Epoch 4: |          | 1214/? [17:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1214, loss 4.421599388122559\n",
      "Epoch 4: |          | 1215/? [17:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1215, loss 3.841470241546631\n",
      "Epoch 4: |          | 1216/? [17:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1216, loss 3.991459369659424\n",
      "Epoch 4: |          | 1217/? [17:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1217, loss 4.116781234741211\n",
      "Epoch 4: |          | 1218/? [17:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1218, loss 4.190004825592041\n",
      "Epoch 4: |          | 1219/? [17:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1219, loss 3.826141357421875\n",
      "Epoch 4: |          | 1220/? [17:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1220, loss 4.56893253326416\n",
      "Epoch 4: |          | 1221/? [17:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1221, loss 4.130953311920166\n",
      "Epoch 4: |          | 1222/? [17:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1222, loss 3.1689376831054688\n",
      "Epoch 4: |          | 1223/? [17:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1223, loss 3.3745505809783936\n",
      "Epoch 4: |          | 1224/? [17:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1224, loss 3.744133710861206\n",
      "Epoch 4: |          | 1225/? [17:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1225, loss 4.433923244476318\n",
      "Epoch 4: |          | 1226/? [17:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1226, loss 4.418887615203857\n",
      "Epoch 4: |          | 1227/? [17:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1227, loss 4.0266828536987305\n",
      "Epoch 4: |          | 1228/? [17:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1228, loss 3.897637128829956\n",
      "Epoch 4: |          | 1229/? [17:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1229, loss 3.5013270378112793\n",
      "Epoch 4: |          | 1230/? [17:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1230, loss 4.166325569152832\n",
      "Epoch 4: |          | 1231/? [17:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1231, loss 4.1801347732543945\n",
      "Epoch 4: |          | 1232/? [17:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1232, loss 4.340115547180176\n",
      "Epoch 4: |          | 1233/? [17:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1233, loss 4.1677141189575195\n",
      "Epoch 4: |          | 1234/? [17:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1234, loss 3.142566680908203\n",
      "Epoch 4: |          | 1235/? [17:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1235, loss 4.236423015594482\n",
      "Epoch 4: |          | 1236/? [17:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1236, loss 3.6420135498046875\n",
      "Epoch 4: |          | 1237/? [17:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1237, loss 3.9763424396514893\n",
      "Epoch 4: |          | 1238/? [17:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1238, loss 3.984990358352661\n",
      "Epoch 4: |          | 1239/? [17:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1239, loss 3.855048418045044\n",
      "Epoch 4: |          | 1240/? [17:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1240, loss 4.485187530517578\n",
      "Epoch 4: |          | 1241/? [17:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1241, loss 4.013316631317139\n",
      "Epoch 4: |          | 1242/? [17:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1242, loss 3.845569133758545\n",
      "Epoch 4: |          | 1243/? [17:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1243, loss 3.724810838699341\n",
      "Epoch 4: |          | 1244/? [17:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1244, loss 3.888324022293091\n",
      "Epoch 4: |          | 1245/? [17:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1245, loss 3.461381435394287\n",
      "Epoch 4: |          | 1246/? [17:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1246, loss 4.158747673034668\n",
      "Epoch 4: |          | 1247/? [17:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1247, loss 4.192317485809326\n",
      "Epoch 4: |          | 1248/? [17:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1248, loss 3.706050395965576\n",
      "Epoch 4: |          | 1249/? [17:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1249, loss 3.891664505004883\n",
      "Epoch 4: |          | 1250/? [17:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1250, loss 4.049361705780029\n",
      "Epoch 4: |          | 1251/? [17:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1251, loss 3.7568275928497314\n",
      "Epoch 4: |          | 1252/? [17:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1252, loss 4.545044422149658\n",
      "Epoch 4: |          | 1253/? [17:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1253, loss 3.8893001079559326\n",
      "Epoch 4: |          | 1254/? [17:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1254, loss 3.2264785766601562\n",
      "Epoch 4: |          | 1255/? [17:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1255, loss 4.608230113983154\n",
      "Epoch 4: |          | 1256/? [17:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1256, loss 3.6327292919158936\n",
      "Epoch 4: |          | 1257/? [17:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1257, loss 3.671041965484619\n",
      "Epoch 4: |          | 1258/? [17:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1258, loss 4.388200283050537\n",
      "Epoch 4: |          | 1259/? [17:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1259, loss 4.079679489135742\n",
      "Epoch 4: |          | 1260/? [17:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1260, loss 4.488383769989014\n",
      "Epoch 4: |          | 1261/? [17:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1261, loss 3.857759952545166\n",
      "Epoch 4: |          | 1262/? [17:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1262, loss 3.79303240776062\n",
      "Epoch 4: |          | 1263/? [17:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1263, loss 4.1764631271362305\n",
      "Epoch 4: |          | 1264/? [17:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1264, loss 4.447267532348633\n",
      "Epoch 4: |          | 1265/? [17:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1265, loss 4.255375862121582\n",
      "Epoch 4: |          | 1266/? [17:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1266, loss 3.9362854957580566\n",
      "Epoch 4: |          | 1267/? [17:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1267, loss 4.041594505310059\n",
      "Epoch 4: |          | 1268/? [17:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1268, loss 3.9204330444335938\n",
      "Epoch 4: |          | 1269/? [17:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1269, loss 3.438830614089966\n",
      "Epoch 4: |          | 1270/? [17:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1270, loss 3.7892258167266846\n",
      "Epoch 4: |          | 1271/? [17:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1271, loss 3.989839553833008\n",
      "Epoch 4: |          | 1272/? [17:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1272, loss 3.534571886062622\n",
      "Epoch 4: |          | 1273/? [18:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1273, loss 4.268940448760986\n",
      "Epoch 4: |          | 1274/? [18:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1274, loss 3.1178247928619385\n",
      "Epoch 4: |          | 1275/? [18:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1275, loss 3.6941661834716797\n",
      "Epoch 4: |          | 1276/? [18:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1276, loss 4.013704776763916\n",
      "Epoch 4: |          | 1277/? [18:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1277, loss 3.712601900100708\n",
      "Epoch 4: |          | 1278/? [18:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1278, loss 3.4215500354766846\n",
      "Epoch 4: |          | 1279/? [18:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1279, loss 4.121768951416016\n",
      "Epoch 4: |          | 1280/? [18:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1280, loss 3.347334384918213\n",
      "Epoch 4: |          | 1281/? [18:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1281, loss 3.807241916656494\n",
      "Epoch 4: |          | 1282/? [18:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1282, loss 3.5215015411376953\n",
      "Epoch 4: |          | 1283/? [18:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1283, loss 4.2724785804748535\n",
      "Epoch 4: |          | 1284/? [18:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1284, loss 3.36238169670105\n",
      "Epoch 4: |          | 1285/? [18:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1285, loss 4.454728126525879\n",
      "Epoch 4: |          | 1286/? [18:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1286, loss 2.9124085903167725\n",
      "Epoch 4: |          | 1287/? [18:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1287, loss 4.33289909362793\n",
      "Epoch 4: |          | 1288/? [18:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1288, loss 4.149657249450684\n",
      "Epoch 4: |          | 1289/? [18:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1289, loss 3.2627155780792236\n",
      "Epoch 4: |          | 1290/? [18:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1290, loss 4.078949928283691\n",
      "Epoch 4: |          | 1291/? [18:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1291, loss 4.971385955810547\n",
      "Epoch 4: |          | 1292/? [18:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1292, loss 4.238874912261963\n",
      "Epoch 4: |          | 1293/? [18:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1293, loss 3.75834584236145\n",
      "Epoch 4: |          | 1294/? [18:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1294, loss 3.989016056060791\n",
      "Epoch 4: |          | 1295/? [18:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1295, loss 4.064789772033691\n",
      "Epoch 4: |          | 1296/? [18:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1296, loss 3.2798855304718018\n",
      "Epoch 4: |          | 1297/? [18:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1297, loss 4.245914936065674\n",
      "Epoch 4: |          | 1298/? [18:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1298, loss 3.9735946655273438\n",
      "Epoch 4: |          | 1299/? [18:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1299, loss 3.091564178466797\n",
      "Epoch 4: |          | 1300/? [18:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1300, loss 4.053021430969238\n",
      "Epoch 4: |          | 1301/? [18:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1301, loss 3.7041497230529785\n",
      "Epoch 4: |          | 1302/? [18:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1302, loss 3.854417085647583\n",
      "Epoch 4: |          | 1303/? [18:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1303, loss 3.7578492164611816\n",
      "Epoch 4: |          | 1304/? [18:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1304, loss 4.5408501625061035\n",
      "Epoch 4: |          | 1305/? [18:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1305, loss 3.336121082305908\n",
      "Epoch 4: |          | 1306/? [18:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1306, loss 3.9726009368896484\n",
      "Epoch 4: |          | 1307/? [18:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1307, loss 3.5652942657470703\n",
      "Epoch 4: |          | 1308/? [18:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1308, loss 3.50956392288208\n",
      "Epoch 4: |          | 1309/? [18:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1309, loss 3.582277297973633\n",
      "Epoch 4: |          | 1310/? [18:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1310, loss 4.135500907897949\n",
      "Epoch 4: |          | 1311/? [18:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1311, loss 3.5519020557403564\n",
      "Epoch 4: |          | 1312/? [18:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1312, loss 3.29852557182312\n",
      "Epoch 4: |          | 1313/? [18:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1313, loss 4.469842433929443\n",
      "Epoch 4: |          | 1314/? [18:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1314, loss 3.6779379844665527\n",
      "Epoch 4: |          | 1315/? [18:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1315, loss 4.441426753997803\n",
      "Epoch 4: |          | 1316/? [18:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1316, loss 4.209753513336182\n",
      "Epoch 4: |          | 1317/? [18:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1317, loss 3.825674533843994\n",
      "Epoch 4: |          | 1318/? [18:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1318, loss 4.033068656921387\n",
      "Epoch 4: |          | 1319/? [18:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1319, loss 4.200074672698975\n",
      "Epoch 4: |          | 1320/? [18:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1320, loss 3.768559217453003\n",
      "Epoch 4: |          | 1321/? [18:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1321, loss 4.206191539764404\n",
      "Epoch 4: |          | 1322/? [18:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1322, loss 4.121491432189941\n",
      "Epoch 4: |          | 1323/? [18:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1323, loss 3.6068458557128906\n",
      "Epoch 4: |          | 1324/? [18:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1324, loss 4.531614303588867\n",
      "Epoch 4: |          | 1325/? [18:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1325, loss 4.632678031921387\n",
      "Epoch 4: |          | 1326/? [18:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1326, loss 4.080763339996338\n",
      "Epoch 4: |          | 1327/? [18:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1327, loss 3.9544334411621094\n",
      "Epoch 4: |          | 1328/? [18:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1328, loss 3.6434123516082764\n",
      "Epoch 4: |          | 1329/? [18:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1329, loss 4.244841575622559\n",
      "Epoch 4: |          | 1330/? [18:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1330, loss 3.981992721557617\n",
      "Epoch 4: |          | 1331/? [18:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1331, loss 4.06516170501709\n",
      "Epoch 4: |          | 1332/? [18:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1332, loss 3.8045878410339355\n",
      "Epoch 4: |          | 1333/? [18:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1333, loss 3.756743907928467\n",
      "Epoch 4: |          | 1334/? [18:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1334, loss 3.8966803550720215\n",
      "Epoch 4: |          | 1335/? [18:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1335, loss 3.8821616172790527\n",
      "Epoch 4: |          | 1336/? [18:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1336, loss 3.4123566150665283\n",
      "Epoch 4: |          | 1337/? [18:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1337, loss 4.085649490356445\n",
      "Epoch 4: |          | 1338/? [18:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1338, loss 3.300654172897339\n",
      "Epoch 4: |          | 1339/? [18:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1339, loss 3.952850341796875\n",
      "Epoch 4: |          | 1340/? [18:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1340, loss 3.3780956268310547\n",
      "Epoch 4: |          | 1341/? [18:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1341, loss 4.150664806365967\n",
      "Epoch 4: |          | 1342/? [18:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1342, loss 4.369753837585449\n",
      "Epoch 4: |          | 1343/? [18:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1343, loss 3.8618292808532715\n",
      "Epoch 4: |          | 1344/? [19:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1344, loss 3.978034257888794\n",
      "Epoch 4: |          | 1345/? [19:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1345, loss 4.132050514221191\n",
      "Epoch 4: |          | 1346/? [19:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1346, loss 5.233641147613525\n",
      "Epoch 4: |          | 1347/? [19:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1347, loss 4.211983680725098\n",
      "Epoch 4: |          | 1348/? [19:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1348, loss 4.308392524719238\n",
      "Epoch 4: |          | 1349/? [19:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1349, loss 4.155365943908691\n",
      "Epoch 4: |          | 1350/? [19:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1350, loss 4.261467456817627\n",
      "Epoch 4: |          | 1351/? [19:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1351, loss 4.192113399505615\n",
      "Epoch 4: |          | 1352/? [19:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1352, loss 3.4127469062805176\n",
      "Epoch 4: |          | 1353/? [19:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1353, loss 3.734931230545044\n",
      "Epoch 4: |          | 1354/? [19:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1354, loss 4.201264381408691\n",
      "Epoch 4: |          | 1355/? [19:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1355, loss 4.345547676086426\n",
      "Epoch 4: |          | 1356/? [19:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1356, loss 4.051342964172363\n",
      "Epoch 4: |          | 1357/? [19:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1357, loss 3.815444231033325\n",
      "Epoch 4: |          | 1358/? [19:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1358, loss 3.981872081756592\n",
      "Epoch 4: |          | 1359/? [19:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1359, loss 3.8784797191619873\n",
      "Epoch 4: |          | 1360/? [19:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1360, loss 4.059482097625732\n",
      "Epoch 4: |          | 1361/? [19:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1361, loss 4.012050151824951\n",
      "Epoch 4: |          | 1362/? [19:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1362, loss 3.871278762817383\n",
      "Epoch 4: |          | 1363/? [19:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1363, loss 3.2961974143981934\n",
      "Epoch 4: |          | 1364/? [19:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1364, loss 3.7922542095184326\n",
      "Epoch 4: |          | 1365/? [19:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1365, loss 3.4950547218322754\n",
      "Epoch 4: |          | 1366/? [19:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1366, loss 4.2265167236328125\n",
      "Epoch 4: |          | 1367/? [19:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1367, loss 3.514191150665283\n",
      "Epoch 4: |          | 1368/? [19:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1368, loss 3.2950758934020996\n",
      "Epoch 4: |          | 1369/? [19:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1369, loss 4.028053283691406\n",
      "Epoch 4: |          | 1370/? [19:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1370, loss 3.4977879524230957\n",
      "Epoch 4: |          | 1371/? [19:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1371, loss 4.482043743133545\n",
      "Epoch 4: |          | 1372/? [19:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1372, loss 3.896724224090576\n",
      "Epoch 4: |          | 1373/? [19:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1373, loss 4.316200256347656\n",
      "Epoch 4: |          | 1374/? [19:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1374, loss 3.415846586227417\n",
      "Epoch 4: |          | 1375/? [19:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1375, loss 4.041314125061035\n",
      "Epoch 4: |          | 1376/? [19:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1376, loss 3.9119277000427246\n",
      "Epoch 4: |          | 1377/? [19:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1377, loss 3.8967576026916504\n",
      "Epoch 4: |          | 1378/? [19:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1378, loss 3.9062016010284424\n",
      "Epoch 4: |          | 1379/? [19:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1379, loss 3.919158458709717\n",
      "Epoch 4: |          | 1380/? [19:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1380, loss 4.030756950378418\n",
      "Epoch 4: |          | 1381/? [19:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1381, loss 4.198426246643066\n",
      "Epoch 4: |          | 1382/? [19:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1382, loss 3.6794872283935547\n",
      "Epoch 4: |          | 1383/? [19:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1383, loss 3.951598644256592\n",
      "Epoch 4: |          | 1384/? [19:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1384, loss 3.9457035064697266\n",
      "Epoch 4: |          | 1385/? [19:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1385, loss 3.9001879692077637\n",
      "Epoch 4: |          | 1386/? [19:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1386, loss 4.004260540008545\n",
      "Epoch 4: |          | 1387/? [19:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1387, loss 3.9457664489746094\n",
      "Epoch 4: |          | 1388/? [19:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1388, loss 3.4326891899108887\n",
      "Epoch 4: |          | 1389/? [19:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1389, loss 4.16113805770874\n",
      "Epoch 4: |          | 1390/? [19:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1390, loss 4.445435523986816\n",
      "Epoch 4: |          | 1391/? [19:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1391, loss 4.078601360321045\n",
      "Epoch 4: |          | 1392/? [19:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1392, loss 3.557835817337036\n",
      "Epoch 4: |          | 1393/? [19:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1393, loss 3.743912935256958\n",
      "Epoch 4: |          | 1394/? [19:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1394, loss 3.361440658569336\n",
      "Epoch 4: |          | 1395/? [19:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1395, loss 4.091494083404541\n",
      "Epoch 4: |          | 1396/? [19:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1396, loss 3.982051134109497\n",
      "Epoch 4: |          | 1397/? [19:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1397, loss 3.1909537315368652\n",
      "Epoch 4: |          | 1398/? [19:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1398, loss 4.4023542404174805\n",
      "Epoch 4: |          | 1399/? [19:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1399, loss 4.44911527633667\n",
      "Epoch 4: |          | 1400/? [19:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1400, loss 3.522005796432495\n",
      "Epoch 4: |          | 1401/? [19:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1401, loss 4.306033134460449\n",
      "Epoch 4: |          | 1402/? [19:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1402, loss 3.9183475971221924\n",
      "Epoch 4: |          | 1403/? [19:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1403, loss 4.136857032775879\n",
      "Epoch 4: |          | 1404/? [19:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1404, loss 4.0868821144104\n",
      "Epoch 4: |          | 1405/? [19:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1405, loss 4.425156593322754\n",
      "Epoch 4: |          | 1406/? [19:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1406, loss 4.3393235206604\n",
      "Epoch 4: |          | 1407/? [19:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1407, loss 4.435572624206543\n",
      "Epoch 4: |          | 1408/? [19:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1408, loss 3.6252212524414062\n",
      "Epoch 4: |          | 1409/? [19:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1409, loss 3.6829795837402344\n",
      "Epoch 4: |          | 1410/? [20:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1410, loss 3.7532825469970703\n",
      "Epoch 4: |          | 1411/? [20:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1411, loss 4.083298683166504\n",
      "Epoch 4: |          | 1412/? [20:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1412, loss 3.7078490257263184\n",
      "Epoch 4: |          | 1413/? [20:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1413, loss 3.5844550132751465\n",
      "Epoch 4: |          | 1414/? [20:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1414, loss 3.726823329925537\n",
      "Epoch 4: |          | 1415/? [20:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1415, loss 3.9913582801818848\n",
      "Epoch 4: |          | 1416/? [20:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1416, loss 4.387124538421631\n",
      "Epoch 4: |          | 1417/? [20:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1417, loss 4.018912315368652\n",
      "Epoch 4: |          | 1418/? [20:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1418, loss 4.174370765686035\n",
      "Epoch 4: |          | 1419/? [20:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1419, loss 3.886666774749756\n",
      "Epoch 4: |          | 1420/? [20:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1420, loss 3.7685439586639404\n",
      "Epoch 4: |          | 1421/? [20:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1421, loss 3.4874606132507324\n",
      "Epoch 4: |          | 1422/? [20:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1422, loss 4.20942497253418\n",
      "Epoch 4: |          | 1423/? [20:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1423, loss 4.253347873687744\n",
      "Epoch 4: |          | 1424/? [20:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1424, loss 3.864384174346924\n",
      "Epoch 4: |          | 1425/? [20:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1425, loss 4.117961406707764\n",
      "Epoch 4: |          | 1426/? [20:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1426, loss 3.636147975921631\n",
      "Epoch 4: |          | 1427/? [20:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1427, loss 4.2835588455200195\n",
      "Epoch 4: |          | 1428/? [20:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1428, loss 4.247759819030762\n",
      "Epoch 4: |          | 1429/? [20:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1429, loss 4.153110027313232\n",
      "Epoch 4: |          | 1430/? [20:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1430, loss 4.228091239929199\n",
      "Epoch 4: |          | 1431/? [20:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1431, loss 4.001733303070068\n",
      "Epoch 4: |          | 1432/? [20:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1432, loss 4.0143842697143555\n",
      "Epoch 4: |          | 1433/? [20:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1433, loss 3.93461537361145\n",
      "Epoch 4: |          | 1434/? [20:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1434, loss 4.078300952911377\n",
      "Epoch 4: |          | 1435/? [20:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1435, loss 3.668766736984253\n",
      "Epoch 4: |          | 1436/? [20:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1436, loss 4.019776344299316\n",
      "Epoch 4: |          | 1437/? [20:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1437, loss 3.2878661155700684\n",
      "Epoch 4: |          | 1438/? [20:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1438, loss 4.934366703033447\n",
      "Epoch 4: |          | 1439/? [20:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1439, loss 4.036353588104248\n",
      "Epoch 4: |          | 1440/? [20:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1440, loss 4.183653831481934\n",
      "Epoch 4: |          | 1441/? [20:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1441, loss 4.490971565246582\n",
      "Epoch 4: |          | 1442/? [20:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1442, loss 4.508848667144775\n",
      "Epoch 4: |          | 1443/? [20:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1443, loss 3.640812397003174\n",
      "Epoch 4: |          | 1444/? [20:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1444, loss 3.705533981323242\n",
      "Epoch 4: |          | 1445/? [20:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1445, loss 4.239043235778809\n",
      "Epoch 4: |          | 1446/? [20:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1446, loss 3.7583422660827637\n",
      "Epoch 4: |          | 1447/? [20:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1447, loss 3.8858113288879395\n",
      "Epoch 4: |          | 1448/? [20:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1448, loss 3.7554938793182373\n",
      "Epoch 4: |          | 1449/? [20:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1449, loss 3.9875340461730957\n",
      "Epoch 4: |          | 1450/? [20:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1450, loss 4.047484397888184\n",
      "Epoch 4: |          | 1451/? [20:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1451, loss 4.367083549499512\n",
      "Epoch 4: |          | 1452/? [20:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1452, loss 4.025933742523193\n",
      "Epoch 4: |          | 1453/? [20:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1453, loss 3.3876636028289795\n",
      "Epoch 4: |          | 1454/? [20:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1454, loss 3.9689698219299316\n",
      "Epoch 4: |          | 1455/? [20:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1455, loss 4.109592914581299\n",
      "Epoch 4: |          | 1456/? [20:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1456, loss 3.6130242347717285\n",
      "Epoch 4: |          | 1457/? [20:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1457, loss 3.7800514698028564\n",
      "Epoch 4: |          | 1458/? [20:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1458, loss 3.9660849571228027\n",
      "Epoch 4: |          | 1459/? [20:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1459, loss 4.164221286773682\n",
      "Epoch 4: |          | 1460/? [20:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1460, loss 4.064352035522461\n",
      "Epoch 4: |          | 1461/? [20:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1461, loss 4.036341190338135\n",
      "Epoch 4: |          | 1462/? [20:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1462, loss 4.318316459655762\n",
      "Epoch 4: |          | 1463/? [20:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1463, loss 4.248879909515381\n",
      "Epoch 4: |          | 1464/? [20:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1464, loss 3.5152626037597656\n",
      "Epoch 4: |          | 1465/? [20:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1465, loss 3.9571030139923096\n",
      "Epoch 4: |          | 1466/? [20:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1466, loss 3.60680890083313\n",
      "Epoch 4: |          | 1467/? [20:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1467, loss 4.300295352935791\n",
      "Epoch 4: |          | 1468/? [20:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1468, loss 3.845088243484497\n",
      "Epoch 4: |          | 1469/? [20:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1469, loss 3.5553760528564453\n",
      "Epoch 4: |          | 1470/? [20:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1470, loss 4.0713605880737305\n",
      "Epoch 4: |          | 1471/? [20:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1471, loss 4.3356523513793945\n",
      "Epoch 4: |          | 1472/? [20:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1472, loss 4.087100028991699\n",
      "Epoch 4: |          | 1473/? [20:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1473, loss 3.8662781715393066\n",
      "Epoch 4: |          | 1474/? [20:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1474, loss 3.785630464553833\n",
      "Epoch 4: |          | 1475/? [20:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1475, loss 3.3934593200683594\n",
      "Epoch 4: |          | 1476/? [20:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1476, loss 3.976566791534424\n",
      "Epoch 4: |          | 1477/? [20:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1477, loss 4.006640911102295\n",
      "Epoch 4: |          | 1478/? [20:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1478, loss 3.8708457946777344\n",
      "Epoch 4: |          | 1479/? [20:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1479, loss 4.429479598999023\n",
      "Epoch 4: |          | 1480/? [20:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1480, loss 4.1515607833862305\n",
      "Epoch 4: |          | 1481/? [21:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1481, loss 3.886439085006714\n",
      "Epoch 4: |          | 1482/? [21:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1482, loss 4.0446977615356445\n",
      "Epoch 4: |          | 1483/? [21:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1483, loss 3.6502442359924316\n",
      "Epoch 4: |          | 1484/? [21:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1484, loss 3.849491596221924\n",
      "Epoch 4: |          | 1485/? [21:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1485, loss 4.026263236999512\n",
      "Epoch 4: |          | 1486/? [21:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1486, loss 4.032917022705078\n",
      "Epoch 4: |          | 1487/? [21:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1487, loss 3.480307102203369\n",
      "Epoch 4: |          | 1488/? [21:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1488, loss 4.149883270263672\n",
      "Epoch 4: |          | 1489/? [21:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1489, loss 4.076355934143066\n",
      "Epoch 4: |          | 1490/? [21:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1490, loss 3.955798625946045\n",
      "Epoch 4: |          | 1491/? [21:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1491, loss 2.930422306060791\n",
      "Epoch 4: |          | 1492/? [21:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1492, loss 3.5194602012634277\n",
      "Epoch 4: |          | 1493/? [21:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1493, loss 3.157367706298828\n",
      "Epoch 4: |          | 1494/? [21:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1494, loss 3.881115674972534\n",
      "Epoch 4: |          | 1495/? [21:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1495, loss 3.7876334190368652\n",
      "Epoch 4: |          | 1496/? [21:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1496, loss 4.098756790161133\n",
      "Epoch 4: |          | 1497/? [21:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1497, loss 3.3691203594207764\n",
      "Epoch 4: |          | 1498/? [21:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1498, loss 3.6859240531921387\n",
      "Epoch 4: |          | 1499/? [21:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1499, loss 4.228100776672363\n",
      "Epoch 4: |          | 1500/? [21:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1500, loss 4.13095760345459\n",
      "Epoch 4: |          | 1501/? [21:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1501, loss 4.00515079498291\n",
      "Epoch 4: |          | 1502/? [21:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1502, loss 4.066674709320068\n",
      "Epoch 4: |          | 1503/? [21:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1503, loss 3.8176193237304688\n",
      "Epoch 4: |          | 1504/? [21:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1504, loss 4.491929531097412\n",
      "Epoch 4: |          | 1505/? [21:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1505, loss 4.293672561645508\n",
      "Epoch 4: |          | 1506/? [21:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1506, loss 3.895249843597412\n",
      "Epoch 4: |          | 1507/? [21:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1507, loss 3.7719433307647705\n",
      "Epoch 4: |          | 1508/? [21:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1508, loss 3.9262187480926514\n",
      "Epoch 4: |          | 1509/? [21:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1509, loss 3.9034690856933594\n",
      "Epoch 4: |          | 1510/? [21:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1510, loss 4.174204349517822\n",
      "Epoch 4: |          | 1511/? [21:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1511, loss 3.7530357837677\n",
      "Epoch 4: |          | 1512/? [21:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1512, loss 4.3872551918029785\n",
      "Epoch 4: |          | 1513/? [21:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1513, loss 4.560062408447266\n",
      "Epoch 4: |          | 1514/? [21:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1514, loss 3.631298542022705\n",
      "Epoch 4: |          | 1515/? [21:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1515, loss 4.4927215576171875\n",
      "Epoch 4: |          | 1516/? [21:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1516, loss 4.405299186706543\n",
      "Epoch 4: |          | 1517/? [21:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1517, loss 3.8775382041931152\n",
      "Epoch 4: |          | 1518/? [21:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1518, loss 3.6332449913024902\n",
      "Epoch 4: |          | 1519/? [21:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1519, loss 4.180661201477051\n",
      "Epoch 4: |          | 1520/? [21:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1520, loss 4.240166664123535\n",
      "Epoch 4: |          | 1521/? [21:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1521, loss 3.9775466918945312\n",
      "Epoch 4: |          | 1522/? [21:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1522, loss 3.721848249435425\n",
      "Epoch 4: |          | 1523/? [21:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1523, loss 4.05705451965332\n",
      "Epoch 4: |          | 1524/? [21:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1524, loss 3.934772491455078\n",
      "Epoch 4: |          | 1525/? [21:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1525, loss 3.6693413257598877\n",
      "Epoch 4: |          | 1526/? [21:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1526, loss 4.1999640464782715\n",
      "Epoch 4: |          | 1527/? [21:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1527, loss 4.214954853057861\n",
      "Epoch 4: |          | 1528/? [21:39<00:00,  1.18it/s, v_num=30]ERROR: Input has inproper shape\n",
      "Epoch 4: |          | 1529/? [21:39<00:00,  1.18it/s, v_num=30]   VALIDATION: Batch 0, loss 4.631120204925537\n",
      "   VALIDATION: Batch 1, loss 3.6076583862304688\n",
      "   VALIDATION: Batch 2, loss 4.813231468200684\n",
      "   VALIDATION: Batch 3, loss 4.482441425323486\n",
      "   VALIDATION: Batch 4, loss 4.078371524810791\n",
      "   VALIDATION: Batch 5, loss 3.745494842529297\n",
      "   VALIDATION: Batch 6, loss 4.0490546226501465\n",
      "   VALIDATION: Batch 7, loss 4.6734619140625\n",
      "   VALIDATION: Batch 8, loss 4.535223007202148\n",
      "   VALIDATION: Batch 9, loss 4.660305500030518\n",
      "   VALIDATION: Batch 10, loss 4.380593776702881\n",
      "   VALIDATION: Batch 11, loss 3.995095729827881\n",
      "   VALIDATION: Batch 12, loss 4.250563621520996\n",
      "   VALIDATION: Batch 13, loss 4.760598182678223\n",
      "   VALIDATION: Batch 14, loss 4.115448951721191\n",
      "   VALIDATION: Batch 15, loss 4.003304481506348\n",
      "   VALIDATION: Batch 16, loss 4.582688331604004\n",
      "   VALIDATION: Batch 17, loss 4.242664813995361\n",
      "   VALIDATION: Batch 18, loss 3.5632739067077637\n",
      "   VALIDATION: Batch 19, loss 4.501752853393555\n",
      "   VALIDATION: Batch 20, loss 4.750683784484863\n",
      "   VALIDATION: Batch 21, loss 5.015684604644775\n",
      "   VALIDATION: Batch 22, loss 4.6240715980529785\n",
      "   VALIDATION: Batch 23, loss 4.129807949066162\n",
      "   VALIDATION: Batch 24, loss 4.004021644592285\n",
      "   VALIDATION: Batch 25, loss 4.429753303527832\n",
      "   VALIDATION: Batch 26, loss 4.651049613952637\n",
      "   VALIDATION: Batch 27, loss 4.533393859863281\n",
      "   VALIDATION: Batch 28, loss 4.287180423736572\n",
      "   VALIDATION: Batch 29, loss 4.517724990844727\n",
      "   VALIDATION: Batch 30, loss 4.079188823699951\n",
      "   VALIDATION: Batch 31, loss 4.410031318664551\n",
      "   VALIDATION: Batch 32, loss 4.989293575286865\n",
      "   VALIDATION: Batch 33, loss 3.1557650566101074\n",
      "   VALIDATION: Batch 34, loss 4.375807285308838\n",
      "   VALIDATION: Batch 35, loss 4.588930130004883\n",
      "   VALIDATION: Batch 36, loss 3.892869234085083\n",
      "   VALIDATION: Batch 37, loss 3.8833560943603516\n",
      "   VALIDATION: Batch 38, loss 3.9466793537139893\n",
      "   VALIDATION: Batch 39, loss 4.357390403747559\n",
      "   VALIDATION: Batch 40, loss 4.437398433685303\n",
      "   VALIDATION: Batch 41, loss 3.292546033859253\n",
      "   VALIDATION: Batch 42, loss 4.597108364105225\n",
      "   VALIDATION: Batch 43, loss 4.6026105880737305\n",
      "   VALIDATION: Batch 44, loss 4.1806535720825195\n",
      "   VALIDATION: Batch 45, loss 4.603533744812012\n",
      "   VALIDATION: Batch 46, loss 3.7330563068389893\n",
      "   VALIDATION: Batch 47, loss 4.728078842163086\n",
      "   VALIDATION: Batch 48, loss 4.860476493835449\n",
      "   VALIDATION: Batch 49, loss 4.477882385253906\n",
      "   VALIDATION: Batch 50, loss 4.450887203216553\n",
      "   VALIDATION: Batch 51, loss 4.889163017272949\n",
      "   VALIDATION: Batch 52, loss 4.076229095458984\n",
      "   VALIDATION: Batch 53, loss 3.9635510444641113\n",
      "   VALIDATION: Batch 54, loss 4.026606559753418\n",
      "   VALIDATION: Batch 55, loss 4.806110858917236\n",
      "   VALIDATION: Batch 56, loss 4.169853687286377\n",
      "   VALIDATION: Batch 57, loss 5.686585426330566\n",
      "   VALIDATION: Batch 58, loss 4.296445369720459\n",
      "   VALIDATION: Batch 59, loss 3.9429054260253906\n",
      "   VALIDATION: Batch 60, loss 3.514004945755005\n",
      "   VALIDATION: Batch 61, loss 4.344266891479492\n",
      "   VALIDATION: Batch 62, loss 4.317934513092041\n",
      "   VALIDATION: Batch 63, loss 4.855856418609619\n",
      "   VALIDATION: Batch 64, loss 4.615761756896973\n",
      "   VALIDATION: Batch 65, loss 3.7895779609680176\n",
      "   VALIDATION: Batch 66, loss 4.698914527893066\n",
      "   VALIDATION: Batch 67, loss 4.111166954040527\n",
      "   VALIDATION: Batch 68, loss 4.293404579162598\n",
      "   VALIDATION: Batch 69, loss 4.531323432922363\n",
      "   VALIDATION: Batch 70, loss 4.691032886505127\n",
      "   VALIDATION: Batch 71, loss 4.180936813354492\n",
      "   VALIDATION: Batch 72, loss 5.0575714111328125\n",
      "   VALIDATION: Batch 73, loss 3.8621463775634766\n",
      "   VALIDATION: Batch 74, loss 4.489346504211426\n",
      "   VALIDATION: Batch 75, loss 4.5158233642578125\n",
      "   VALIDATION: Batch 76, loss 4.3534770011901855\n",
      "   VALIDATION: Batch 77, loss 4.615814208984375\n",
      "   VALIDATION: Batch 78, loss 4.44339656829834\n",
      "   VALIDATION: Batch 79, loss 4.38932991027832\n",
      "   VALIDATION: Batch 80, loss 4.4771504402160645\n",
      "   VALIDATION: Batch 81, loss 4.250052452087402\n",
      "   VALIDATION: Batch 82, loss 4.6182074546813965\n",
      "   VALIDATION: Batch 83, loss 3.8710949420928955\n",
      "   VALIDATION: Batch 84, loss 4.57204008102417\n",
      "   VALIDATION: Batch 85, loss 4.269960880279541\n",
      "   VALIDATION: Batch 86, loss 4.277665138244629\n",
      "   VALIDATION: Batch 87, loss 4.1571879386901855\n",
      "   VALIDATION: Batch 88, loss 3.7532589435577393\n",
      "   VALIDATION: Batch 89, loss 4.067039489746094\n",
      "   VALIDATION: Batch 90, loss 4.31967830657959\n",
      "   VALIDATION: Batch 91, loss 4.405487537384033\n",
      "   VALIDATION: Batch 92, loss 4.047769069671631\n",
      "   VALIDATION: Batch 93, loss 4.8062310218811035\n",
      "   VALIDATION: Batch 94, loss 4.303637504577637\n",
      "   VALIDATION: Batch 95, loss 3.762171983718872\n",
      "   VALIDATION: Batch 96, loss 4.226059436798096\n",
      "   VALIDATION: Batch 97, loss 3.955078601837158\n",
      "   VALIDATION: Batch 98, loss 4.586424827575684\n",
      "   VALIDATION: Batch 99, loss 4.6406378746032715\n",
      "   VALIDATION: Batch 100, loss 4.99454402923584\n",
      "   VALIDATION: Batch 101, loss 3.5948867797851562\n",
      "   VALIDATION: Batch 102, loss 5.026007652282715\n",
      "   VALIDATION: Batch 103, loss 4.931670188903809\n",
      "   VALIDATION: Batch 104, loss 3.8995468616485596\n",
      "   VALIDATION: Batch 105, loss 4.415333271026611\n",
      "   VALIDATION: Batch 106, loss 4.18820858001709\n",
      "   VALIDATION: Batch 107, loss 4.332297325134277\n",
      "   VALIDATION: Batch 108, loss 4.058740139007568\n",
      "   VALIDATION: Batch 109, loss 4.675263404846191\n",
      "   VALIDATION: Batch 110, loss 4.354510307312012\n",
      "   VALIDATION: Batch 111, loss 4.687382698059082\n",
      "   VALIDATION: Batch 112, loss 5.513692378997803\n",
      "   VALIDATION: Batch 113, loss 4.840793609619141\n",
      "   VALIDATION: Batch 114, loss 4.63460111618042\n",
      "   VALIDATION: Batch 115, loss 4.076269149780273\n",
      "   VALIDATION: Batch 116, loss 3.931234836578369\n",
      "   VALIDATION: Batch 117, loss 4.609201431274414\n",
      "   VALIDATION: Batch 118, loss 4.7527947425842285\n",
      "   VALIDATION: Batch 119, loss 3.9055519104003906\n",
      "   VALIDATION: Batch 120, loss 3.5468106269836426\n",
      "   VALIDATION: Batch 121, loss 3.8857216835021973\n",
      "   VALIDATION: Batch 122, loss 4.282632350921631\n",
      "   VALIDATION: Batch 123, loss 4.293229579925537\n",
      "   VALIDATION: Batch 124, loss 3.656085252761841\n",
      "   VALIDATION: Batch 125, loss 4.259734153747559\n",
      "   VALIDATION: Batch 126, loss 4.472809314727783\n",
      "   VALIDATION: Batch 127, loss 4.267596244812012\n",
      "   VALIDATION: Batch 128, loss 4.431094646453857\n",
      "   VALIDATION: Batch 129, loss 4.0666656494140625\n",
      "   VALIDATION: Batch 130, loss 3.6527340412139893\n",
      "   VALIDATION: Batch 131, loss 3.703258514404297\n",
      "   VALIDATION: Batch 132, loss 4.3322272300720215\n",
      "   VALIDATION: Batch 133, loss 4.522599220275879\n",
      "   VALIDATION: Batch 134, loss 4.377734184265137\n",
      "   VALIDATION: Batch 135, loss 4.670590400695801\n",
      "   VALIDATION: Batch 136, loss 4.7564263343811035\n",
      "   VALIDATION: Batch 137, loss 4.5652689933776855\n",
      "   VALIDATION: Batch 138, loss 4.292026519775391\n",
      "   VALIDATION: Batch 139, loss 4.683102607727051\n",
      "   VALIDATION: Batch 140, loss 3.7906107902526855\n",
      "   VALIDATION: Batch 141, loss 4.719313621520996\n",
      "   VALIDATION: Batch 142, loss 3.4570326805114746\n",
      "   VALIDATION: Batch 143, loss 4.255888938903809\n",
      "   VALIDATION: Batch 144, loss 4.5159430503845215\n",
      "   VALIDATION: Batch 145, loss 4.329537391662598\n",
      "   VALIDATION: Batch 146, loss 4.137308120727539\n",
      "   VALIDATION: Batch 147, loss 4.462238311767578\n",
      "   VALIDATION: Batch 148, loss 4.603421211242676\n",
      "   VALIDATION: Batch 149, loss 5.043654918670654\n",
      "   VALIDATION: Batch 150, loss 4.671563625335693\n",
      "   VALIDATION: Batch 151, loss 4.923758506774902\n",
      "   VALIDATION: Batch 152, loss 4.301527500152588\n",
      "   VALIDATION: Batch 153, loss 4.524412631988525\n",
      "   VALIDATION: Batch 154, loss 4.391598701477051\n",
      "   VALIDATION: Batch 155, loss 4.123068809509277\n",
      "   VALIDATION: Batch 156, loss 4.770255088806152\n",
      "   VALIDATION: Batch 157, loss 4.450295448303223\n",
      "   VALIDATION: Batch 158, loss 3.866945266723633\n",
      "   VALIDATION: Batch 159, loss 4.332261562347412\n",
      "   VALIDATION: Batch 160, loss 4.6834845542907715\n",
      "   VALIDATION: Batch 161, loss 4.947373390197754\n",
      "   VALIDATION: Batch 162, loss 4.379652976989746\n",
      "   VALIDATION: Batch 163, loss 3.794257402420044\n",
      "   VALIDATION: Batch 164, loss 4.288775444030762\n",
      "   VALIDATION: Batch 165, loss 4.759624004364014\n",
      "   VALIDATION: Batch 166, loss 4.210842132568359\n",
      "   VALIDATION: Batch 167, loss 4.632491111755371\n",
      "   VALIDATION: Batch 168, loss 3.494158983230591\n",
      "   VALIDATION: Batch 169, loss 4.152638912200928\n",
      "   VALIDATION: Batch 170, loss 4.372076988220215\n",
      "   VALIDATION: Batch 171, loss 4.5154218673706055\n",
      "   VALIDATION: Batch 172, loss 4.340012550354004\n",
      "   VALIDATION: Batch 173, loss 4.241799831390381\n",
      "   VALIDATION: Batch 174, loss 4.644406318664551\n",
      "   VALIDATION: Batch 175, loss 4.423033237457275\n",
      "   VALIDATION: Batch 176, loss 4.212924480438232\n",
      "   VALIDATION: Batch 177, loss 4.244563579559326\n",
      "   VALIDATION: Batch 178, loss 5.236893653869629\n",
      "   VALIDATION: Batch 179, loss 4.5134382247924805\n",
      "   VALIDATION: Batch 180, loss 4.030512809753418\n",
      "   VALIDATION: Batch 181, loss 4.242801189422607\n",
      "   VALIDATION: Batch 182, loss 4.4692912101745605\n",
      "   VALIDATION: Batch 183, loss 3.51707124710083\n",
      "   VALIDATION: Batch 184, loss 3.260657787322998\n",
      "   VALIDATION: Batch 185, loss 4.04541015625\n",
      "   VALIDATION: Batch 186, loss 3.961425304412842\n",
      "   VALIDATION: Batch 187, loss 4.181426048278809\n",
      "   VALIDATION: Batch 188, loss 4.565183639526367\n",
      "   VALIDATION: Batch 189, loss 3.902618408203125\n",
      "   VALIDATION: Batch 190, loss 3.9545645713806152\n",
      "   VALIDATION: Batch 191, loss 4.477386951446533\n",
      "   VALIDATION: Batch 192, loss 4.838128089904785\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=30]              TRRAINING: Batch 0, loss 4.171280860900879\n",
      "Epoch 5: |          | 1/? [00:01<00:00,  0.88it/s, v_num=30]   TRRAINING: Batch 1, loss 3.7503418922424316\n",
      "Epoch 5: |          | 2/? [00:01<00:00,  1.00it/s, v_num=30]   TRRAINING: Batch 2, loss 3.8529231548309326\n",
      "Epoch 5: |          | 3/? [00:02<00:00,  1.04it/s, v_num=30]   TRRAINING: Batch 3, loss 3.5315330028533936\n",
      "Epoch 5: |          | 4/? [00:03<00:00,  1.06it/s, v_num=30]   TRRAINING: Batch 4, loss 3.921327590942383\n",
      "Epoch 5: |          | 5/? [00:04<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 5, loss 4.710413932800293\n",
      "Epoch 5: |          | 6/? [00:05<00:00,  1.13it/s, v_num=30]   TRRAINING: Batch 6, loss 4.239999771118164\n",
      "Epoch 5: |          | 7/? [00:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 7, loss 3.540531635284424\n",
      "Epoch 5: |          | 8/? [00:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 8, loss 3.673252820968628\n",
      "Epoch 5: |          | 9/? [00:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 9, loss 3.943265438079834\n",
      "Epoch 5: |          | 10/? [00:08<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 10, loss 4.208836555480957\n",
      "Epoch 5: |          | 11/? [00:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 11, loss 4.092140197753906\n",
      "Epoch 5: |          | 12/? [00:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 12, loss 4.729859352111816\n",
      "Epoch 5: |          | 13/? [00:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 13, loss 4.047394275665283\n",
      "Epoch 5: |          | 14/? [00:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 14, loss 4.249150276184082\n",
      "Epoch 5: |          | 15/? [00:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 15, loss 3.511096239089966\n",
      "Epoch 5: |          | 16/? [00:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 16, loss 3.326897382736206\n",
      "Epoch 5: |          | 17/? [00:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 17, loss 4.487290859222412\n",
      "Epoch 5: |          | 18/? [00:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 18, loss 4.002685070037842\n",
      "Epoch 5: |          | 19/? [00:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 19, loss 3.7982895374298096\n",
      "Epoch 5: |          | 20/? [00:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 20, loss 4.079648494720459\n",
      "Epoch 5: |          | 21/? [00:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 21, loss 4.183969497680664\n",
      "Epoch 5: |          | 22/? [00:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 22, loss 4.072608947753906\n",
      "Epoch 5: |          | 23/? [00:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 23, loss 3.4410972595214844\n",
      "Epoch 5: |          | 24/? [00:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 24, loss 4.082475185394287\n",
      "Epoch 5: |          | 25/? [00:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 25, loss 3.9475626945495605\n",
      "Epoch 5: |          | 26/? [00:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 26, loss 3.642364978790283\n",
      "Epoch 5: |          | 27/? [00:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 27, loss 3.588524341583252\n",
      "Epoch 5: |          | 28/? [00:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 28, loss 4.517973899841309\n",
      "Epoch 5: |          | 29/? [00:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 29, loss 4.035998344421387\n",
      "Epoch 5: |          | 30/? [00:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 30, loss 3.8997020721435547\n",
      "Epoch 5: |          | 31/? [00:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 31, loss 4.52179479598999\n",
      "Epoch 5: |          | 32/? [00:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 32, loss 4.041796684265137\n",
      "Epoch 5: |          | 33/? [00:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 33, loss 3.840024471282959\n",
      "Epoch 5: |          | 34/? [00:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 34, loss 3.8473663330078125\n",
      "Epoch 5: |          | 35/? [00:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 35, loss 3.293139696121216\n",
      "Epoch 5: |          | 36/? [00:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 36, loss 4.082573890686035\n",
      "Epoch 5: |          | 37/? [00:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 37, loss 4.256707668304443\n",
      "Epoch 5: |          | 38/? [00:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 38, loss 4.559095859527588\n",
      "Epoch 5: |          | 39/? [00:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 39, loss 4.469441890716553\n",
      "Epoch 5: |          | 40/? [00:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 40, loss 3.861675977706909\n",
      "Epoch 5: |          | 41/? [00:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 41, loss 3.91766619682312\n",
      "Epoch 5: |          | 42/? [00:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 42, loss 3.685866117477417\n",
      "Epoch 5: |          | 43/? [00:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 43, loss 3.886542797088623\n",
      "Epoch 5: |          | 44/? [00:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 44, loss 3.1019084453582764\n",
      "Epoch 5: |          | 45/? [00:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 45, loss 2.739588499069214\n",
      "Epoch 5: |          | 46/? [00:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 46, loss 4.492827415466309\n",
      "Epoch 5: |          | 47/? [00:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 47, loss 3.6815876960754395\n",
      "Epoch 5: |          | 48/? [00:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 48, loss 3.488063097000122\n",
      "Epoch 5: |          | 49/? [00:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 49, loss 3.9903602600097656\n",
      "Epoch 5: |          | 50/? [00:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 50, loss 3.8531570434570312\n",
      "Epoch 5: |          | 51/? [00:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 51, loss 3.803478717803955\n",
      "Epoch 5: |          | 52/? [00:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 52, loss 4.477642059326172\n",
      "Epoch 5: |          | 53/? [00:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 53, loss 4.122715950012207\n",
      "Epoch 5: |          | 54/? [00:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 54, loss 3.9644546508789062\n",
      "Epoch 5: |          | 55/? [00:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 55, loss 3.9764626026153564\n",
      "Epoch 5: |          | 56/? [00:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 56, loss 4.086934566497803\n",
      "Epoch 5: |          | 57/? [00:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 57, loss 4.01425838470459\n",
      "Epoch 5: |          | 58/? [00:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 58, loss 5.248830318450928\n",
      "Epoch 5: |          | 59/? [00:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 59, loss 4.143415451049805\n",
      "Epoch 5: |          | 60/? [00:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 60, loss 4.289920806884766\n",
      "Epoch 5: |          | 61/? [00:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 61, loss 4.267552852630615\n",
      "Epoch 5: |          | 62/? [00:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 62, loss 3.9037766456604004\n",
      "Epoch 5: |          | 63/? [00:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 63, loss 4.096545696258545\n",
      "Epoch 5: |          | 64/? [00:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 64, loss 3.8928961753845215\n",
      "Epoch 5: |          | 65/? [00:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 65, loss 3.9521431922912598\n",
      "Epoch 5: |          | 66/? [00:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 66, loss 3.3705596923828125\n",
      "Epoch 5: |          | 67/? [00:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 67, loss 4.099179744720459\n",
      "Epoch 5: |          | 68/? [00:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 68, loss 4.103012561798096\n",
      "Epoch 5: |          | 69/? [00:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 69, loss 3.992955446243286\n",
      "Epoch 5: |          | 70/? [00:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 70, loss 3.685455799102783\n",
      "Epoch 5: |          | 71/? [01:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 71, loss 3.6542961597442627\n",
      "Epoch 5: |          | 72/? [01:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 72, loss 3.8889148235321045\n",
      "Epoch 5: |          | 73/? [01:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 73, loss 4.0499043464660645\n",
      "Epoch 5: |          | 74/? [01:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 74, loss 3.7681655883789062\n",
      "Epoch 5: |          | 75/? [01:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 75, loss 3.9070045948028564\n",
      "Epoch 5: |          | 76/? [01:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 76, loss 3.8973190784454346\n",
      "Epoch 5: |          | 77/? [01:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 77, loss 3.9207663536071777\n",
      "Epoch 5: |          | 78/? [01:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 78, loss 3.7222743034362793\n",
      "Epoch 5: |          | 79/? [01:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 79, loss 3.922673463821411\n",
      "Epoch 5: |          | 80/? [01:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 80, loss 3.853682041168213\n",
      "Epoch 5: |          | 81/? [01:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 81, loss 3.316460132598877\n",
      "Epoch 5: |          | 82/? [01:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 82, loss 4.148432731628418\n",
      "Epoch 5: |          | 83/? [01:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 83, loss 3.5969936847686768\n",
      "Epoch 5: |          | 84/? [01:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 84, loss 3.445143461227417\n",
      "Epoch 5: |          | 85/? [01:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 85, loss 3.3292675018310547\n",
      "Epoch 5: |          | 86/? [01:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 86, loss 3.4682674407958984\n",
      "Epoch 5: |          | 87/? [01:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 87, loss 3.6450724601745605\n",
      "Epoch 5: |          | 88/? [01:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 88, loss 4.325806617736816\n",
      "Epoch 5: |          | 89/? [01:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 89, loss 4.236088752746582\n",
      "Epoch 5: |          | 90/? [01:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 90, loss 4.023688316345215\n",
      "Epoch 5: |          | 91/? [01:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 91, loss 3.767242431640625\n",
      "Epoch 5: |          | 92/? [01:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 92, loss 4.219542503356934\n",
      "Epoch 5: |          | 93/? [01:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 93, loss 4.353750228881836\n",
      "Epoch 5: |          | 94/? [01:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 94, loss 4.250148773193359\n",
      "Epoch 5: |          | 95/? [01:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 95, loss 4.57743501663208\n",
      "Epoch 5: |          | 96/? [01:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 96, loss 3.743398666381836\n",
      "Epoch 5: |          | 97/? [01:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 97, loss 3.5695528984069824\n",
      "Epoch 5: |          | 98/? [01:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 98, loss 4.093704700469971\n",
      "Epoch 5: |          | 99/? [01:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 99, loss 4.2705979347229\n",
      "Epoch 5: |          | 100/? [01:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 100, loss 4.24130916595459\n",
      "Epoch 5: |          | 101/? [01:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 101, loss 3.887385845184326\n",
      "Epoch 5: |          | 102/? [01:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 102, loss 3.9044361114501953\n",
      "Epoch 5: |          | 103/? [01:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 103, loss 3.6936211585998535\n",
      "Epoch 5: |          | 104/? [01:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 104, loss 4.04534912109375\n",
      "Epoch 5: |          | 105/? [01:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 105, loss 3.9638984203338623\n",
      "Epoch 5: |          | 106/? [01:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 106, loss 4.059910774230957\n",
      "Epoch 5: |          | 107/? [01:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 107, loss 4.126120090484619\n",
      "Epoch 5: |          | 108/? [01:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 108, loss 4.090863227844238\n",
      "Epoch 5: |          | 109/? [01:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 109, loss 3.7778091430664062\n",
      "Epoch 5: |          | 110/? [01:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 110, loss 4.014366149902344\n",
      "Epoch 5: |          | 111/? [01:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 111, loss 4.634045600891113\n",
      "Epoch 5: |          | 112/? [01:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 112, loss 3.348494052886963\n",
      "Epoch 5: |          | 113/? [01:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 113, loss 2.92478084564209\n",
      "Epoch 5: |          | 114/? [01:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 114, loss 4.2192912101745605\n",
      "Epoch 5: |          | 115/? [01:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 115, loss 4.353812217712402\n",
      "Epoch 5: |          | 116/? [01:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 116, loss 3.4959588050842285\n",
      "Epoch 5: |          | 117/? [01:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 117, loss 3.4498863220214844\n",
      "Epoch 5: |          | 118/? [01:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 118, loss 4.262117862701416\n",
      "Epoch 5: |          | 119/? [01:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 119, loss 4.46334981918335\n",
      "Epoch 5: |          | 120/? [01:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 120, loss 4.255467414855957\n",
      "Epoch 5: |          | 121/? [01:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 121, loss 3.99023699760437\n",
      "Epoch 5: |          | 122/? [01:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 122, loss 3.449309825897217\n",
      "Epoch 5: |          | 123/? [01:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 123, loss 3.9380364418029785\n",
      "Epoch 5: |          | 124/? [01:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 124, loss 4.078821182250977\n",
      "Epoch 5: |          | 125/? [01:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 125, loss 3.736380100250244\n",
      "Epoch 5: |          | 126/? [01:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 126, loss 4.224480628967285\n",
      "Epoch 5: |          | 127/? [01:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 127, loss 4.325416088104248\n",
      "Epoch 5: |          | 128/? [01:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 128, loss 3.424938201904297\n",
      "Epoch 5: |          | 129/? [01:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 129, loss 4.044924736022949\n",
      "Epoch 5: |          | 130/? [01:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 130, loss 3.1585443019866943\n",
      "Epoch 5: |          | 131/? [01:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 131, loss 3.984679698944092\n",
      "Epoch 5: |          | 132/? [01:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 132, loss 4.029024124145508\n",
      "Epoch 5: |          | 133/? [01:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 133, loss 3.9510254859924316\n",
      "Epoch 5: |          | 134/? [01:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 134, loss 4.108681678771973\n",
      "Epoch 5: |          | 135/? [01:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 135, loss 4.153864860534668\n",
      "Epoch 5: |          | 136/? [01:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 136, loss 4.207100868225098\n",
      "Epoch 5: |          | 137/? [01:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 137, loss 3.2058608531951904\n",
      "Epoch 5: |          | 138/? [01:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 138, loss 3.8391616344451904\n",
      "Epoch 5: |          | 139/? [01:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 139, loss 4.24685001373291\n",
      "Epoch 5: |          | 140/? [01:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 140, loss 3.4878296852111816\n",
      "Epoch 5: |          | 141/? [01:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 141, loss 3.5951476097106934\n",
      "Epoch 5: |          | 142/? [01:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 142, loss 5.11205530166626\n",
      "Epoch 5: |          | 143/? [02:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 143, loss 4.7315287590026855\n",
      "Epoch 5: |          | 144/? [02:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 144, loss 3.960686206817627\n",
      "Epoch 5: |          | 145/? [02:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 145, loss 3.4366660118103027\n",
      "Epoch 5: |          | 146/? [02:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 146, loss 3.6089210510253906\n",
      "Epoch 5: |          | 147/? [02:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 147, loss 3.943505048751831\n",
      "Epoch 5: |          | 148/? [02:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 148, loss 3.6540908813476562\n",
      "Epoch 5: |          | 149/? [02:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 149, loss 3.256972074508667\n",
      "Epoch 5: |          | 150/? [02:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 150, loss 4.09964656829834\n",
      "Epoch 5: |          | 151/? [02:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 151, loss 4.204845905303955\n",
      "Epoch 5: |          | 152/? [02:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 152, loss 4.167710781097412\n",
      "Epoch 5: |          | 153/? [02:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 153, loss 3.275949478149414\n",
      "Epoch 5: |          | 154/? [02:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 154, loss 4.503023147583008\n",
      "Epoch 5: |          | 155/? [02:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 155, loss 4.024317741394043\n",
      "Epoch 5: |          | 156/? [02:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 156, loss 3.3703880310058594\n",
      "Epoch 5: |          | 157/? [02:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 157, loss 4.106060981750488\n",
      "Epoch 5: |          | 158/? [02:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 158, loss 4.194081783294678\n",
      "Epoch 5: |          | 159/? [02:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 159, loss 3.8912291526794434\n",
      "Epoch 5: |          | 160/? [02:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 160, loss 3.671619415283203\n",
      "Epoch 5: |          | 161/? [02:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 161, loss 4.070522308349609\n",
      "Epoch 5: |          | 162/? [02:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 162, loss 4.090611457824707\n",
      "Epoch 5: |          | 163/? [02:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 163, loss 3.1828343868255615\n",
      "Epoch 5: |          | 164/? [02:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 164, loss 3.639456272125244\n",
      "Epoch 5: |          | 165/? [02:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 165, loss 4.452850341796875\n",
      "Epoch 5: |          | 166/? [02:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 166, loss 4.132793426513672\n",
      "Epoch 5: |          | 167/? [02:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 167, loss 4.234003067016602\n",
      "Epoch 5: |          | 168/? [02:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 168, loss 3.7819836139678955\n",
      "Epoch 5: |          | 169/? [02:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 169, loss 3.4096577167510986\n",
      "Epoch 5: |          | 170/? [02:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 170, loss 3.667722225189209\n",
      "Epoch 5: |          | 171/? [02:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 171, loss 4.02347993850708\n",
      "Epoch 5: |          | 172/? [02:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 172, loss 3.8196702003479004\n",
      "Epoch 5: |          | 173/? [02:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 173, loss 4.472506523132324\n",
      "Epoch 5: |          | 174/? [02:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 174, loss 4.1116042137146\n",
      "Epoch 5: |          | 175/? [02:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 175, loss 4.562248706817627\n",
      "Epoch 5: |          | 176/? [02:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 176, loss 3.7640419006347656\n",
      "Epoch 5: |          | 177/? [02:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 177, loss 3.778949022293091\n",
      "Epoch 5: |          | 178/? [02:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 178, loss 3.625258684158325\n",
      "Epoch 5: |          | 179/? [02:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 179, loss 4.3676347732543945\n",
      "Epoch 5: |          | 180/? [02:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 180, loss 3.8697056770324707\n",
      "Epoch 5: |          | 181/? [02:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 181, loss 3.6739699840545654\n",
      "Epoch 5: |          | 182/? [02:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 182, loss 4.08725643157959\n",
      "Epoch 5: |          | 183/? [02:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 183, loss 3.5563442707061768\n",
      "Epoch 5: |          | 184/? [02:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 184, loss 3.749488115310669\n",
      "Epoch 5: |          | 185/? [02:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 185, loss 4.282750129699707\n",
      "Epoch 5: |          | 186/? [02:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 186, loss 3.7964954376220703\n",
      "Epoch 5: |          | 187/? [02:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 187, loss 4.335556983947754\n",
      "Epoch 5: |          | 188/? [02:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 188, loss 3.6937012672424316\n",
      "Epoch 5: |          | 189/? [02:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 189, loss 4.330994606018066\n",
      "Epoch 5: |          | 190/? [02:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 190, loss 3.885009765625\n",
      "Epoch 5: |          | 191/? [02:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 191, loss 4.634308815002441\n",
      "Epoch 5: |          | 192/? [02:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 192, loss 4.35854434967041\n",
      "Epoch 5: |          | 193/? [02:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 193, loss 3.729703187942505\n",
      "Epoch 5: |          | 194/? [02:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 194, loss 3.616713047027588\n",
      "Epoch 5: |          | 195/? [02:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 195, loss 4.3206706047058105\n",
      "Epoch 5: |          | 196/? [02:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 196, loss 4.204733371734619\n",
      "Epoch 5: |          | 197/? [02:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 197, loss 4.020230293273926\n",
      "Epoch 5: |          | 198/? [02:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 198, loss 3.369612216949463\n",
      "Epoch 5: |          | 199/? [02:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 199, loss 4.257586479187012\n",
      "Epoch 5: |          | 200/? [02:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 200, loss 3.75059175491333\n",
      "Epoch 5: |          | 201/? [02:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 201, loss 4.027864456176758\n",
      "Epoch 5: |          | 202/? [02:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 202, loss 4.152814865112305\n",
      "Epoch 5: |          | 203/? [02:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 203, loss 3.8089072704315186\n",
      "Epoch 5: |          | 204/? [02:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 204, loss 3.8418281078338623\n",
      "Epoch 5: |          | 205/? [02:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 205, loss 3.7284767627716064\n",
      "Epoch 5: |          | 206/? [02:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 206, loss 3.598053455352783\n",
      "Epoch 5: |          | 207/? [02:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 207, loss 4.104626655578613\n",
      "Epoch 5: |          | 208/? [02:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 208, loss 4.0200419425964355\n",
      "Epoch 5: |          | 209/? [02:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 209, loss 3.802095890045166\n",
      "Epoch 5: |          | 210/? [02:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 210, loss 4.490945816040039\n",
      "Epoch 5: |          | 211/? [02:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 211, loss 3.8499362468719482\n",
      "Epoch 5: |          | 212/? [02:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 212, loss 4.091783046722412\n",
      "Epoch 5: |          | 213/? [02:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 213, loss 3.9196410179138184\n",
      "Epoch 5: |          | 214/? [03:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 214, loss 3.799544095993042\n",
      "Epoch 5: |          | 215/? [03:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 215, loss 3.46093487739563\n",
      "Epoch 5: |          | 216/? [03:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 216, loss 4.123641014099121\n",
      "Epoch 5: |          | 217/? [03:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 217, loss 4.0710062980651855\n",
      "Epoch 5: |          | 218/? [03:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 218, loss 4.051628112792969\n",
      "Epoch 5: |          | 219/? [03:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 219, loss 4.007439613342285\n",
      "Epoch 5: |          | 220/? [03:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 220, loss 4.092360496520996\n",
      "Epoch 5: |          | 221/? [03:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 221, loss 3.89782452583313\n",
      "Epoch 5: |          | 222/? [03:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 222, loss 3.11558198928833\n",
      "Epoch 5: |          | 223/? [03:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 223, loss 4.184954643249512\n",
      "Epoch 5: |          | 224/? [03:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 224, loss 4.240496635437012\n",
      "Epoch 5: |          | 225/? [03:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 225, loss 3.9863200187683105\n",
      "Epoch 5: |          | 226/? [03:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 226, loss 3.8728034496307373\n",
      "Epoch 5: |          | 227/? [03:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 227, loss 4.225322723388672\n",
      "Epoch 5: |          | 228/? [03:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 228, loss 3.9442410469055176\n",
      "Epoch 5: |          | 229/? [03:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 229, loss 4.0142316818237305\n",
      "Epoch 5: |          | 230/? [03:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 230, loss 3.9253666400909424\n",
      "Epoch 5: |          | 231/? [03:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 231, loss 3.8893730640411377\n",
      "Epoch 5: |          | 232/? [03:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 232, loss 3.611283540725708\n",
      "Epoch 5: |          | 233/? [03:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 233, loss 4.32314395904541\n",
      "Epoch 5: |          | 234/? [03:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 234, loss 4.307895660400391\n",
      "Epoch 5: |          | 235/? [03:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 235, loss 4.35369348526001\n",
      "Epoch 5: |          | 236/? [03:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 236, loss 3.8031444549560547\n",
      "Epoch 5: |          | 237/? [03:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 237, loss 3.9749350547790527\n",
      "Epoch 5: |          | 238/? [03:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 238, loss 4.235230445861816\n",
      "Epoch 5: |          | 239/? [03:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 239, loss 3.795368194580078\n",
      "Epoch 5: |          | 240/? [03:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 240, loss 3.3646435737609863\n",
      "Epoch 5: |          | 241/? [03:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 241, loss 3.970306396484375\n",
      "Epoch 5: |          | 242/? [03:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 242, loss 4.339663982391357\n",
      "Epoch 5: |          | 243/? [03:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 243, loss 3.190497875213623\n",
      "Epoch 5: |          | 244/? [03:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 244, loss 3.6399312019348145\n",
      "Epoch 5: |          | 245/? [03:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 245, loss 3.8832802772521973\n",
      "Epoch 5: |          | 246/? [03:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 246, loss 4.052487373352051\n",
      "Epoch 5: |          | 247/? [03:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 247, loss 4.123085975646973\n",
      "Epoch 5: |          | 248/? [03:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 248, loss 3.613847255706787\n",
      "Epoch 5: |          | 249/? [03:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 249, loss 3.4720215797424316\n",
      "Epoch 5: |          | 250/? [03:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 250, loss 4.074951648712158\n",
      "Epoch 5: |          | 251/? [03:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 251, loss 4.052640438079834\n",
      "Epoch 5: |          | 252/? [03:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 252, loss 3.840672731399536\n",
      "Epoch 5: |          | 253/? [03:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 253, loss 4.657055377960205\n",
      "Epoch 5: |          | 254/? [03:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 254, loss 4.3101396560668945\n",
      "Epoch 5: |          | 255/? [03:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 255, loss 3.9360175132751465\n",
      "Epoch 5: |          | 256/? [03:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 256, loss 4.822770595550537\n",
      "Epoch 5: |          | 257/? [03:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 257, loss 3.8063836097717285\n",
      "Epoch 5: |          | 258/? [03:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 258, loss 3.9012389183044434\n",
      "Epoch 5: |          | 259/? [03:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 259, loss 3.7818832397460938\n",
      "Epoch 5: |          | 260/? [03:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 260, loss 3.711601972579956\n",
      "Epoch 5: |          | 261/? [03:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 261, loss 3.563816785812378\n",
      "Epoch 5: |          | 262/? [03:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 262, loss 4.215020179748535\n",
      "Epoch 5: |          | 263/? [03:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 263, loss 3.892683744430542\n",
      "Epoch 5: |          | 264/? [03:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 264, loss 4.028069496154785\n",
      "Epoch 5: |          | 265/? [03:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 265, loss 3.4697189331054688\n",
      "Epoch 5: |          | 266/? [03:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 266, loss 3.9068970680236816\n",
      "Epoch 5: |          | 267/? [03:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 267, loss 3.5269157886505127\n",
      "Epoch 5: |          | 268/? [03:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 268, loss 3.869685649871826\n",
      "Epoch 5: |          | 269/? [03:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 269, loss 4.226241111755371\n",
      "Epoch 5: |          | 270/? [03:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 270, loss 3.8624351024627686\n",
      "Epoch 5: |          | 271/? [03:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 271, loss 4.164085865020752\n",
      "Epoch 5: |          | 272/? [03:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 272, loss 4.369812965393066\n",
      "Epoch 5: |          | 273/? [03:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 273, loss 3.5760750770568848\n",
      "Epoch 5: |          | 274/? [03:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 274, loss 4.437045097351074\n",
      "Epoch 5: |          | 275/? [03:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 275, loss 4.13067626953125\n",
      "Epoch 5: |          | 276/? [03:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 276, loss 3.3732082843780518\n",
      "Epoch 5: |          | 277/? [03:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 277, loss 4.046637058258057\n",
      "Epoch 5: |          | 278/? [03:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 278, loss 3.0850884914398193\n",
      "Epoch 5: |          | 279/? [03:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 279, loss 3.774477481842041\n",
      "Epoch 5: |          | 280/? [03:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 280, loss 3.490690231323242\n",
      "Epoch 5: |          | 281/? [03:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 281, loss 4.155705451965332\n",
      "Epoch 5: |          | 282/? [03:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 282, loss 3.7512130737304688\n",
      "Epoch 5: |          | 283/? [03:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 283, loss 3.850878953933716\n",
      "Epoch 5: |          | 284/? [03:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 284, loss 3.733710527420044\n",
      "Epoch 5: |          | 285/? [03:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 285, loss 3.267221450805664\n",
      "Epoch 5: |          | 286/? [04:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 286, loss 3.8198254108428955\n",
      "Epoch 5: |          | 287/? [04:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 287, loss 3.557091474533081\n",
      "Epoch 5: |          | 288/? [04:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 288, loss 3.5563952922821045\n",
      "Epoch 5: |          | 289/? [04:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 289, loss 3.367748975753784\n",
      "Epoch 5: |          | 290/? [04:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 290, loss 2.9595162868499756\n",
      "Epoch 5: |          | 291/? [04:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 291, loss 4.002705097198486\n",
      "Epoch 5: |          | 292/? [04:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 292, loss 3.669541597366333\n",
      "Epoch 5: |          | 293/? [04:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 293, loss 3.9669413566589355\n",
      "Epoch 5: |          | 294/? [04:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 294, loss 3.8558096885681152\n",
      "Epoch 5: |          | 295/? [04:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 295, loss 4.161213397979736\n",
      "Epoch 5: |          | 296/? [04:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 296, loss 3.6311898231506348\n",
      "Epoch 5: |          | 297/? [04:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 297, loss 4.3250627517700195\n",
      "Epoch 5: |          | 298/? [04:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 298, loss 4.116339683532715\n",
      "Epoch 5: |          | 299/? [04:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 299, loss 4.491673946380615\n",
      "Epoch 5: |          | 300/? [04:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 300, loss 3.9238879680633545\n",
      "Epoch 5: |          | 301/? [04:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 301, loss 3.731688976287842\n",
      "Epoch 5: |          | 302/? [04:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 302, loss 4.157646179199219\n",
      "Epoch 5: |          | 303/? [04:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 303, loss 3.9564735889434814\n",
      "Epoch 5: |          | 304/? [04:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 304, loss 4.16543436050415\n",
      "Epoch 5: |          | 305/? [04:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 305, loss 4.178670883178711\n",
      "Epoch 5: |          | 306/? [04:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 306, loss 3.860151767730713\n",
      "Epoch 5: |          | 307/? [04:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 307, loss 4.0966925621032715\n",
      "Epoch 5: |          | 308/? [04:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 308, loss 4.24121618270874\n",
      "Epoch 5: |          | 309/? [04:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 309, loss 3.8836007118225098\n",
      "Epoch 5: |          | 310/? [04:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 310, loss 4.297242164611816\n",
      "Epoch 5: |          | 311/? [04:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 311, loss 3.850426435470581\n",
      "Epoch 5: |          | 312/? [04:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 312, loss 3.9005303382873535\n",
      "Epoch 5: |          | 313/? [04:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 313, loss 3.684974193572998\n",
      "Epoch 5: |          | 314/? [04:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 314, loss 4.012546539306641\n",
      "Epoch 5: |          | 315/? [04:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 315, loss 3.6863510608673096\n",
      "Epoch 5: |          | 316/? [04:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 316, loss 4.244733810424805\n",
      "Epoch 5: |          | 317/? [04:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 317, loss 4.062333106994629\n",
      "Epoch 5: |          | 318/? [04:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 318, loss 4.174788475036621\n",
      "Epoch 5: |          | 319/? [04:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 319, loss 3.4204583168029785\n",
      "Epoch 5: |          | 320/? [04:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 320, loss 3.933757781982422\n",
      "Epoch 5: |          | 321/? [04:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 321, loss 3.719320774078369\n",
      "Epoch 5: |          | 322/? [04:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 322, loss 4.344831943511963\n",
      "Epoch 5: |          | 323/? [04:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 323, loss 4.300136566162109\n",
      "Epoch 5: |          | 324/? [04:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 324, loss 4.002599239349365\n",
      "Epoch 5: |          | 325/? [04:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 325, loss 4.448873519897461\n",
      "Epoch 5: |          | 326/? [04:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 326, loss 4.019827842712402\n",
      "Epoch 5: |          | 327/? [04:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 327, loss 3.7487921714782715\n",
      "Epoch 5: |          | 328/? [04:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 328, loss 3.5500903129577637\n",
      "Epoch 5: |          | 329/? [04:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 329, loss 4.123168468475342\n",
      "Epoch 5: |          | 330/? [04:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 330, loss 4.5995917320251465\n",
      "Epoch 5: |          | 331/? [04:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 331, loss 2.880690813064575\n",
      "Epoch 5: |          | 332/? [04:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 332, loss 3.9571824073791504\n",
      "Epoch 5: |          | 333/? [04:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 333, loss 3.809187412261963\n",
      "Epoch 5: |          | 334/? [04:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 334, loss 4.437984466552734\n",
      "Epoch 5: |          | 335/? [04:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 335, loss 4.3767924308776855\n",
      "Epoch 5: |          | 336/? [04:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 336, loss 4.346210956573486\n",
      "Epoch 5: |          | 337/? [04:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 337, loss 4.9340386390686035\n",
      "Epoch 5: |          | 338/? [04:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 338, loss 4.5679931640625\n",
      "Epoch 5: |          | 339/? [04:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 339, loss 3.670318603515625\n",
      "Epoch 5: |          | 340/? [04:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 340, loss 3.5229358673095703\n",
      "Epoch 5: |          | 341/? [04:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 341, loss 3.4007022380828857\n",
      "Epoch 5: |          | 342/? [04:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 342, loss 4.020817756652832\n",
      "Epoch 5: |          | 343/? [04:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 343, loss 3.7544116973876953\n",
      "Epoch 5: |          | 344/? [04:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 344, loss 4.587323188781738\n",
      "Epoch 5: |          | 345/? [04:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 345, loss 3.771761655807495\n",
      "Epoch 5: |          | 346/? [04:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 346, loss 4.03944730758667\n",
      "Epoch 5: |          | 347/? [04:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 347, loss 3.8155949115753174\n",
      "Epoch 5: |          | 348/? [04:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 348, loss 3.200415849685669\n",
      "Epoch 5: |          | 349/? [04:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 349, loss 3.056215524673462\n",
      "Epoch 5: |          | 350/? [04:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 350, loss 4.310781002044678\n",
      "Epoch 5: |          | 351/? [04:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 351, loss 4.337477684020996\n",
      "Epoch 5: |          | 352/? [04:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 352, loss 3.601353406906128\n",
      "Epoch 5: |          | 353/? [04:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 353, loss 3.3637242317199707\n",
      "Epoch 5: |          | 354/? [04:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 354, loss 3.792444944381714\n",
      "Epoch 5: |          | 355/? [04:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 355, loss 4.106106758117676\n",
      "Epoch 5: |          | 356/? [05:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 356, loss 4.1112518310546875\n",
      "Epoch 5: |          | 357/? [05:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 357, loss 3.583725690841675\n",
      "Epoch 5: |          | 358/? [05:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 358, loss 3.539682388305664\n",
      "Epoch 5: |          | 359/? [05:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 359, loss 4.142218589782715\n",
      "Epoch 5: |          | 360/? [05:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 360, loss 3.742863893508911\n",
      "Epoch 5: |          | 361/? [05:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 361, loss 3.867921829223633\n",
      "Epoch 5: |          | 362/? [05:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 362, loss 3.634125232696533\n",
      "Epoch 5: |          | 363/? [05:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 363, loss 3.5643393993377686\n",
      "Epoch 5: |          | 364/? [05:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 364, loss 4.2272629737854\n",
      "Epoch 5: |          | 365/? [05:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 365, loss 4.221194267272949\n",
      "Epoch 5: |          | 366/? [05:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 366, loss 3.9942703247070312\n",
      "Epoch 5: |          | 367/? [05:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 367, loss 3.9434151649475098\n",
      "Epoch 5: |          | 368/? [05:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 368, loss 3.526589870452881\n",
      "Epoch 5: |          | 369/? [05:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 369, loss 3.808619260787964\n",
      "Epoch 5: |          | 370/? [05:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 370, loss 3.482224941253662\n",
      "Epoch 5: |          | 371/? [05:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 371, loss 4.429746627807617\n",
      "Epoch 5: |          | 372/? [05:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 372, loss 3.745739698410034\n",
      "Epoch 5: |          | 373/? [05:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 373, loss 4.081584930419922\n",
      "Epoch 5: |          | 374/? [05:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 374, loss 3.744131088256836\n",
      "Epoch 5: |          | 375/? [05:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 375, loss 4.41542911529541\n",
      "Epoch 5: |          | 376/? [05:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 376, loss 3.84218168258667\n",
      "Epoch 5: |          | 377/? [05:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 377, loss 4.031802177429199\n",
      "Epoch 5: |          | 378/? [05:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 378, loss 4.181789398193359\n",
      "Epoch 5: |          | 379/? [05:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 379, loss 3.9845283031463623\n",
      "Epoch 5: |          | 380/? [05:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 380, loss 4.032451152801514\n",
      "Epoch 5: |          | 381/? [05:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 381, loss 4.101088523864746\n",
      "Epoch 5: |          | 382/? [05:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 382, loss 3.8023924827575684\n",
      "Epoch 5: |          | 383/? [05:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 383, loss 3.8951683044433594\n",
      "Epoch 5: |          | 384/? [05:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 384, loss 4.321631908416748\n",
      "Epoch 5: |          | 385/? [05:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 385, loss 3.9199626445770264\n",
      "Epoch 5: |          | 386/? [05:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 386, loss 2.8961617946624756\n",
      "Epoch 5: |          | 387/? [05:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 387, loss 3.7554478645324707\n",
      "Epoch 5: |          | 388/? [05:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 388, loss 3.581535816192627\n",
      "Epoch 5: |          | 389/? [05:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 389, loss 4.286032676696777\n",
      "Epoch 5: |          | 390/? [05:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 390, loss 3.745173692703247\n",
      "Epoch 5: |          | 391/? [05:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 391, loss 4.169852256774902\n",
      "Epoch 5: |          | 392/? [05:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 392, loss 4.253549575805664\n",
      "Epoch 5: |          | 393/? [05:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 393, loss 4.274160385131836\n",
      "Epoch 5: |          | 394/? [05:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 394, loss 3.929428815841675\n",
      "Epoch 5: |          | 395/? [05:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 395, loss 4.171490669250488\n",
      "Epoch 5: |          | 396/? [05:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 396, loss 4.053918838500977\n",
      "Epoch 5: |          | 397/? [05:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 397, loss 3.85127592086792\n",
      "Epoch 5: |          | 398/? [05:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 398, loss 3.7644639015197754\n",
      "Epoch 5: |          | 399/? [05:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 399, loss 3.8618011474609375\n",
      "Epoch 5: |          | 400/? [05:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 400, loss 3.8300795555114746\n",
      "Epoch 5: |          | 401/? [05:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 401, loss 3.798499584197998\n",
      "Epoch 5: |          | 402/? [05:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 402, loss 4.197767734527588\n",
      "Epoch 5: |          | 403/? [05:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 403, loss 4.078435897827148\n",
      "Epoch 5: |          | 404/? [05:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 404, loss 3.600149154663086\n",
      "Epoch 5: |          | 405/? [05:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 405, loss 3.6384036540985107\n",
      "Epoch 5: |          | 406/? [05:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 406, loss 3.981797695159912\n",
      "Epoch 5: |          | 407/? [05:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 407, loss 3.8821816444396973\n",
      "Epoch 5: |          | 408/? [05:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 408, loss 4.299225330352783\n",
      "Epoch 5: |          | 409/? [05:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 409, loss 4.222320079803467\n",
      "Epoch 5: |          | 410/? [05:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 410, loss 3.8945956230163574\n",
      "Epoch 5: |          | 411/? [05:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 411, loss 3.815192461013794\n",
      "Epoch 5: |          | 412/? [05:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 412, loss 3.332012891769409\n",
      "Epoch 5: |          | 413/? [05:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 413, loss 4.057951927185059\n",
      "Epoch 5: |          | 414/? [05:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 414, loss 3.6125450134277344\n",
      "Epoch 5: |          | 415/? [05:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 415, loss 4.046736240386963\n",
      "Epoch 5: |          | 416/? [05:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 416, loss 4.478135108947754\n",
      "Epoch 5: |          | 417/? [05:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 417, loss 4.499998569488525\n",
      "Epoch 5: |          | 418/? [05:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 418, loss 4.108831405639648\n",
      "Epoch 5: |          | 419/? [05:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 419, loss 3.923102617263794\n",
      "Epoch 5: |          | 420/? [05:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 420, loss 4.01069974899292\n",
      "Epoch 5: |          | 421/? [05:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 421, loss 4.536166191101074\n",
      "Epoch 5: |          | 422/? [05:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 422, loss 4.062790870666504\n",
      "Epoch 5: |          | 423/? [06:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 423, loss 3.65669584274292\n",
      "Epoch 5: |          | 424/? [06:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 424, loss 4.2304205894470215\n",
      "Epoch 5: |          | 425/? [06:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 425, loss 4.040223121643066\n",
      "Epoch 5: |          | 426/? [06:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 426, loss 3.6754775047302246\n",
      "Epoch 5: |          | 427/? [06:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 427, loss 3.728412628173828\n",
      "Epoch 5: |          | 428/? [06:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 428, loss 4.492474555969238\n",
      "Epoch 5: |          | 429/? [06:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 429, loss 3.380854368209839\n",
      "Epoch 5: |          | 430/? [06:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 430, loss 4.0347208976745605\n",
      "Epoch 5: |          | 431/? [06:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 431, loss 3.931349277496338\n",
      "Epoch 5: |          | 432/? [06:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 432, loss 4.057508945465088\n",
      "Epoch 5: |          | 433/? [06:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 433, loss 3.9978675842285156\n",
      "Epoch 5: |          | 434/? [06:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 434, loss 3.9072353839874268\n",
      "Epoch 5: |          | 435/? [06:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 435, loss 3.564804792404175\n",
      "Epoch 5: |          | 436/? [06:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 436, loss 3.975785493850708\n",
      "Epoch 5: |          | 437/? [06:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 437, loss 4.167640209197998\n",
      "Epoch 5: |          | 438/? [06:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 438, loss 3.795745849609375\n",
      "Epoch 5: |          | 439/? [06:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 439, loss 3.7072913646698\n",
      "Epoch 5: |          | 440/? [06:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 440, loss 3.641634702682495\n",
      "Epoch 5: |          | 441/? [06:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 441, loss 4.018146991729736\n",
      "Epoch 5: |          | 442/? [06:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 442, loss 3.8569588661193848\n",
      "Epoch 5: |          | 443/? [06:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 443, loss 3.995307207107544\n",
      "Epoch 5: |          | 444/? [06:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 444, loss 3.976844310760498\n",
      "Epoch 5: |          | 445/? [06:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 445, loss 4.890444755554199\n",
      "Epoch 5: |          | 446/? [06:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 446, loss 3.9072208404541016\n",
      "Epoch 5: |          | 447/? [06:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 447, loss 4.4629292488098145\n",
      "Epoch 5: |          | 448/? [06:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 448, loss 3.5529658794403076\n",
      "Epoch 5: |          | 449/? [06:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 449, loss 3.9419071674346924\n",
      "Epoch 5: |          | 450/? [06:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 450, loss 4.254273891448975\n",
      "Epoch 5: |          | 451/? [06:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 451, loss 3.93278169631958\n",
      "Epoch 5: |          | 452/? [06:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 452, loss 3.6360573768615723\n",
      "Epoch 5: |          | 453/? [06:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 453, loss 4.374421119689941\n",
      "Epoch 5: |          | 454/? [06:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 454, loss 3.749966859817505\n",
      "Epoch 5: |          | 455/? [06:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 455, loss 4.110904693603516\n",
      "Epoch 5: |          | 456/? [06:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 456, loss 3.418724775314331\n",
      "Epoch 5: |          | 457/? [06:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 457, loss 3.874689817428589\n",
      "Epoch 5: |          | 458/? [06:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 458, loss 4.292781829833984\n",
      "Epoch 5: |          | 459/? [06:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 459, loss 4.301603317260742\n",
      "Epoch 5: |          | 460/? [06:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 460, loss 4.037082672119141\n",
      "Epoch 5: |          | 461/? [06:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 461, loss 4.004598617553711\n",
      "Epoch 5: |          | 462/? [06:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 462, loss 4.076943397521973\n",
      "Epoch 5: |          | 463/? [06:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 463, loss 3.934295177459717\n",
      "Epoch 5: |          | 464/? [06:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 464, loss 3.431356430053711\n",
      "Epoch 5: |          | 465/? [06:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 465, loss 3.7206339836120605\n",
      "Epoch 5: |          | 466/? [06:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 466, loss 4.170633316040039\n",
      "Epoch 5: |          | 467/? [06:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 467, loss 3.9527812004089355\n",
      "Epoch 5: |          | 468/? [06:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 468, loss 3.88053822517395\n",
      "Epoch 5: |          | 469/? [06:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 469, loss 4.035338401794434\n",
      "Epoch 5: |          | 470/? [06:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 470, loss 3.414090633392334\n",
      "Epoch 5: |          | 471/? [06:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 471, loss 4.2036333084106445\n",
      "Epoch 5: |          | 472/? [06:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 472, loss 3.7035536766052246\n",
      "Epoch 5: |          | 473/? [06:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 473, loss 3.736123561859131\n",
      "Epoch 5: |          | 474/? [06:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 474, loss 3.3619236946105957\n",
      "Epoch 5: |          | 475/? [06:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 475, loss 4.630579948425293\n",
      "Epoch 5: |          | 476/? [06:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 476, loss 3.650914430618286\n",
      "Epoch 5: |          | 477/? [06:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 477, loss 3.17973256111145\n",
      "Epoch 5: |          | 478/? [06:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 478, loss 3.3978779315948486\n",
      "Epoch 5: |          | 479/? [06:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 479, loss 3.8961920738220215\n",
      "Epoch 5: |          | 480/? [06:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 480, loss 3.782747745513916\n",
      "Epoch 5: |          | 481/? [06:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 481, loss 3.3844153881073\n",
      "Epoch 5: |          | 482/? [06:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 482, loss 3.692091703414917\n",
      "Epoch 5: |          | 483/? [06:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 483, loss 3.375746250152588\n",
      "Epoch 5: |          | 484/? [06:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 484, loss 4.288751602172852\n",
      "Epoch 5: |          | 485/? [06:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 485, loss 4.138524055480957\n",
      "Epoch 5: |          | 486/? [06:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 486, loss 3.7703471183776855\n",
      "Epoch 5: |          | 487/? [06:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 487, loss 4.134713172912598\n",
      "Epoch 5: |          | 488/? [06:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 488, loss 3.863452911376953\n",
      "Epoch 5: |          | 489/? [06:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 489, loss 3.382272243499756\n",
      "Epoch 5: |          | 490/? [06:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 490, loss 3.991396427154541\n",
      "Epoch 5: |          | 491/? [06:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 491, loss 3.8100483417510986\n",
      "Epoch 5: |          | 492/? [06:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 492, loss 3.163935422897339\n",
      "Epoch 5: |          | 493/? [06:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 493, loss 4.183094501495361\n",
      "Epoch 5: |          | 494/? [06:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 494, loss 4.015705585479736\n",
      "Epoch 5: |          | 495/? [06:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 495, loss 4.0735650062561035\n",
      "Epoch 5: |          | 496/? [07:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 496, loss 3.711630344390869\n",
      "Epoch 5: |          | 497/? [07:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 497, loss 4.246129512786865\n",
      "Epoch 5: |          | 498/? [07:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 498, loss 3.886685848236084\n",
      "Epoch 5: |          | 499/? [07:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 499, loss 4.001463890075684\n",
      "Epoch 5: |          | 500/? [07:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 500, loss 3.7620913982391357\n",
      "Epoch 5: |          | 501/? [07:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 501, loss 3.54292631149292\n",
      "Epoch 5: |          | 502/? [07:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 502, loss 3.9692916870117188\n",
      "Epoch 5: |          | 503/? [07:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 503, loss 3.8954825401306152\n",
      "Epoch 5: |          | 504/? [07:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 504, loss 3.829496383666992\n",
      "Epoch 5: |          | 505/? [07:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 505, loss 3.296177625656128\n",
      "Epoch 5: |          | 506/? [07:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 506, loss 3.88547945022583\n",
      "Epoch 5: |          | 507/? [07:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 507, loss 3.924039363861084\n",
      "Epoch 5: |          | 508/? [07:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 508, loss 4.233237266540527\n",
      "Epoch 5: |          | 509/? [07:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 509, loss 3.5962727069854736\n",
      "Epoch 5: |          | 510/? [07:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 510, loss 4.069863796234131\n",
      "Epoch 5: |          | 511/? [07:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 511, loss 3.9294230937957764\n",
      "Epoch 5: |          | 512/? [07:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 512, loss 3.3586413860321045\n",
      "Epoch 5: |          | 513/? [07:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 513, loss 3.620182514190674\n",
      "Epoch 5: |          | 514/? [07:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 514, loss 3.7871410846710205\n",
      "Epoch 5: |          | 515/? [07:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 515, loss 3.4194495677948\n",
      "Epoch 5: |          | 516/? [07:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 516, loss 3.7543063163757324\n",
      "Epoch 5: |          | 517/? [07:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 517, loss 3.9836997985839844\n",
      "Epoch 5: |          | 518/? [07:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 518, loss 3.5294735431671143\n",
      "Epoch 5: |          | 519/? [07:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 519, loss 3.964667797088623\n",
      "Epoch 5: |          | 520/? [07:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 520, loss 3.819138765335083\n",
      "Epoch 5: |          | 521/? [07:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 521, loss 3.843744993209839\n",
      "Epoch 5: |          | 522/? [07:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 522, loss 4.380610466003418\n",
      "Epoch 5: |          | 523/? [07:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 523, loss 4.451883792877197\n",
      "Epoch 5: |          | 524/? [07:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 524, loss 4.224132537841797\n",
      "Epoch 5: |          | 525/? [07:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 525, loss 3.8080925941467285\n",
      "Epoch 5: |          | 526/? [07:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 526, loss 3.601149082183838\n",
      "Epoch 5: |          | 527/? [07:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 527, loss 4.267064094543457\n",
      "Epoch 5: |          | 528/? [07:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 528, loss 4.046960830688477\n",
      "Epoch 5: |          | 529/? [07:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 529, loss 3.630074977874756\n",
      "Epoch 5: |          | 530/? [07:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 530, loss 4.1942291259765625\n",
      "Epoch 5: |          | 531/? [07:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 531, loss 3.7163093090057373\n",
      "Epoch 5: |          | 532/? [07:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 532, loss 3.981527328491211\n",
      "Epoch 5: |          | 533/? [07:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 533, loss 3.6408283710479736\n",
      "Epoch 5: |          | 534/? [07:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 534, loss 3.3146183490753174\n",
      "Epoch 5: |          | 535/? [07:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 535, loss 3.727444887161255\n",
      "Epoch 5: |          | 536/? [07:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 536, loss 4.263464450836182\n",
      "Epoch 5: |          | 537/? [07:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 537, loss 4.031736850738525\n",
      "Epoch 5: |          | 538/? [07:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 538, loss 3.657447338104248\n",
      "Epoch 5: |          | 539/? [07:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 539, loss 3.772909164428711\n",
      "Epoch 5: |          | 540/? [07:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 540, loss 4.170502662658691\n",
      "Epoch 5: |          | 541/? [07:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 541, loss 3.9130358695983887\n",
      "Epoch 5: |          | 542/? [07:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 542, loss 3.6607863903045654\n",
      "Epoch 5: |          | 543/? [07:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 543, loss 4.082062721252441\n",
      "Epoch 5: |          | 544/? [07:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 544, loss 3.9833626747131348\n",
      "Epoch 5: |          | 545/? [07:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 545, loss 3.2539801597595215\n",
      "Epoch 5: |          | 546/? [07:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 546, loss 4.020562648773193\n",
      "Epoch 5: |          | 547/? [07:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 547, loss 4.474156379699707\n",
      "Epoch 5: |          | 548/? [07:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 548, loss 4.118211269378662\n",
      "Epoch 5: |          | 549/? [07:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 549, loss 4.013211727142334\n",
      "Epoch 5: |          | 550/? [07:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 550, loss 4.331194877624512\n",
      "Epoch 5: |          | 551/? [07:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 551, loss 4.000280380249023\n",
      "Epoch 5: |          | 552/? [07:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 552, loss 3.995678663253784\n",
      "Epoch 5: |          | 553/? [07:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 553, loss 3.4585819244384766\n",
      "Epoch 5: |          | 554/? [07:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 554, loss 4.022355079650879\n",
      "Epoch 5: |          | 555/? [07:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 555, loss 4.300295829772949\n",
      "Epoch 5: |          | 556/? [07:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 556, loss 4.073284149169922\n",
      "Epoch 5: |          | 557/? [07:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 557, loss 3.583056688308716\n",
      "Epoch 5: |          | 558/? [07:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 558, loss 3.784924030303955\n",
      "Epoch 5: |          | 559/? [07:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 559, loss 3.7601852416992188\n",
      "Epoch 5: |          | 560/? [07:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 560, loss 3.298595905303955\n",
      "Epoch 5: |          | 561/? [07:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 561, loss 3.14457106590271\n",
      "Epoch 5: |          | 562/? [07:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 562, loss 4.184898853302002\n",
      "Epoch 5: |          | 563/? [07:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 563, loss 3.28263521194458\n",
      "Epoch 5: |          | 564/? [07:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 564, loss 3.751422882080078\n",
      "Epoch 5: |          | 565/? [07:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 565, loss 4.115332126617432\n",
      "Epoch 5: |          | 566/? [07:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 566, loss 4.19948673248291\n",
      "Epoch 5: |          | 567/? [07:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 567, loss 4.29856538772583\n",
      "Epoch 5: |          | 568/? [08:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 568, loss 3.375406265258789\n",
      "Epoch 5: |          | 569/? [08:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 569, loss 3.9728500843048096\n",
      "Epoch 5: |          | 570/? [08:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 570, loss 4.080941200256348\n",
      "Epoch 5: |          | 571/? [08:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 571, loss 3.6575260162353516\n",
      "Epoch 5: |          | 572/? [08:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 572, loss 4.57638692855835\n",
      "Epoch 5: |          | 573/? [08:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 573, loss 3.0059895515441895\n",
      "Epoch 5: |          | 574/? [08:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 574, loss 4.1812238693237305\n",
      "Epoch 5: |          | 575/? [08:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 575, loss 3.523465394973755\n",
      "Epoch 5: |          | 576/? [08:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 576, loss 3.7305641174316406\n",
      "Epoch 5: |          | 577/? [08:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 577, loss 3.9510693550109863\n",
      "Epoch 5: |          | 578/? [08:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 578, loss 4.2054314613342285\n",
      "Epoch 5: |          | 579/? [08:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 579, loss 3.3515000343322754\n",
      "Epoch 5: |          | 580/? [08:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 580, loss 4.033998966217041\n",
      "Epoch 5: |          | 581/? [08:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 581, loss 4.029112339019775\n",
      "Epoch 5: |          | 582/? [08:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 582, loss 4.081145286560059\n",
      "Epoch 5: |          | 583/? [08:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 583, loss 3.852395534515381\n",
      "Epoch 5: |          | 584/? [08:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 584, loss 4.098004341125488\n",
      "Epoch 5: |          | 585/? [08:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 585, loss 4.012789726257324\n",
      "Epoch 5: |          | 586/? [08:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 586, loss 4.0960845947265625\n",
      "Epoch 5: |          | 587/? [08:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 587, loss 4.089976787567139\n",
      "Epoch 5: |          | 588/? [08:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 588, loss 4.019774913787842\n",
      "Epoch 5: |          | 589/? [08:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 589, loss 3.5171687602996826\n",
      "Epoch 5: |          | 590/? [08:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 590, loss 4.052605628967285\n",
      "Epoch 5: |          | 591/? [08:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 591, loss 3.963911533355713\n",
      "Epoch 5: |          | 592/? [08:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 592, loss 3.630296230316162\n",
      "Epoch 5: |          | 593/? [08:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 593, loss 3.9504570960998535\n",
      "Epoch 5: |          | 594/? [08:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 594, loss 4.727950572967529\n",
      "Epoch 5: |          | 595/? [08:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 595, loss 3.48789644241333\n",
      "Epoch 5: |          | 596/? [08:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 596, loss 3.52500581741333\n",
      "Epoch 5: |          | 597/? [08:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 597, loss 3.780642032623291\n",
      "Epoch 5: |          | 598/? [08:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 598, loss 4.253759384155273\n",
      "Epoch 5: |          | 599/? [08:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 599, loss 3.8842976093292236\n",
      "Epoch 5: |          | 600/? [08:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 600, loss 3.6813578605651855\n",
      "Epoch 5: |          | 601/? [08:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 601, loss 3.9778335094451904\n",
      "Epoch 5: |          | 602/? [08:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 602, loss 3.520535707473755\n",
      "Epoch 5: |          | 603/? [08:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 603, loss 3.628079652786255\n",
      "Epoch 5: |          | 604/? [08:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 604, loss 5.3092241287231445\n",
      "Epoch 5: |          | 605/? [08:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 605, loss 3.453258991241455\n",
      "Epoch 5: |          | 606/? [08:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 606, loss 3.7087242603302\n",
      "Epoch 5: |          | 607/? [08:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 607, loss 4.030144691467285\n",
      "Epoch 5: |          | 608/? [08:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 608, loss 3.8075828552246094\n",
      "Epoch 5: |          | 609/? [08:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 609, loss 3.734846591949463\n",
      "Epoch 5: |          | 610/? [08:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 610, loss 3.803596019744873\n",
      "Epoch 5: |          | 611/? [08:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 611, loss 3.9386868476867676\n",
      "Epoch 5: |          | 612/? [08:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 612, loss 3.672886371612549\n",
      "Epoch 5: |          | 613/? [08:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 613, loss 4.0030622482299805\n",
      "Epoch 5: |          | 614/? [08:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 614, loss 3.7719669342041016\n",
      "Epoch 5: |          | 615/? [08:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 615, loss 4.307178020477295\n",
      "Epoch 5: |          | 616/? [08:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 616, loss 4.492921829223633\n",
      "Epoch 5: |          | 617/? [08:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 617, loss 3.098290205001831\n",
      "Epoch 5: |          | 618/? [08:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 618, loss 4.028087615966797\n",
      "Epoch 5: |          | 619/? [08:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 619, loss 3.6172337532043457\n",
      "Epoch 5: |          | 620/? [08:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 620, loss 4.1300368309021\n",
      "Epoch 5: |          | 621/? [08:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 621, loss 3.60111665725708\n",
      "Epoch 5: |          | 622/? [08:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 622, loss 3.388209819793701\n",
      "Epoch 5: |          | 623/? [08:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 623, loss 3.1521823406219482\n",
      "Epoch 5: |          | 624/? [08:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 624, loss 2.86962890625\n",
      "Epoch 5: |          | 625/? [08:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 625, loss 4.3345046043396\n",
      "Epoch 5: |          | 626/? [08:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 626, loss 3.817073345184326\n",
      "Epoch 5: |          | 627/? [08:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 627, loss 3.7437758445739746\n",
      "Epoch 5: |          | 628/? [08:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 628, loss 3.780925750732422\n",
      "Epoch 5: |          | 629/? [08:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 629, loss 4.140712738037109\n",
      "Epoch 5: |          | 630/? [08:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 630, loss 3.904998302459717\n",
      "Epoch 5: |          | 631/? [08:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 631, loss 4.01438045501709\n",
      "Epoch 5: |          | 632/? [08:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 632, loss 3.3358206748962402\n",
      "Epoch 5: |          | 633/? [08:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 633, loss 4.132251262664795\n",
      "Epoch 5: |          | 634/? [08:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 634, loss 3.648106813430786\n",
      "Epoch 5: |          | 635/? [08:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 635, loss 3.493455410003662\n",
      "Epoch 5: |          | 636/? [08:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 636, loss 3.9048519134521484\n",
      "Epoch 5: |          | 637/? [08:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 637, loss 3.711879253387451\n",
      "Epoch 5: |          | 638/? [08:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 638, loss 3.969926357269287\n",
      "Epoch 5: |          | 639/? [08:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 639, loss 3.7193877696990967\n",
      "Epoch 5: |          | 640/? [09:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 640, loss 4.250818729400635\n",
      "Epoch 5: |          | 641/? [09:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 641, loss 3.2797553539276123\n",
      "Epoch 5: |          | 642/? [09:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 642, loss 4.101931095123291\n",
      "Epoch 5: |          | 643/? [09:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 643, loss 3.9666106700897217\n",
      "Epoch 5: |          | 644/? [09:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 644, loss 3.9169490337371826\n",
      "Epoch 5: |          | 645/? [09:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 645, loss 3.639122724533081\n",
      "Epoch 5: |          | 646/? [09:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 646, loss 3.6721997261047363\n",
      "Epoch 5: |          | 647/? [09:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 647, loss 4.241215229034424\n",
      "Epoch 5: |          | 648/? [09:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 648, loss 3.6394805908203125\n",
      "Epoch 5: |          | 649/? [09:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 649, loss 3.215029239654541\n",
      "Epoch 5: |          | 650/? [09:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 650, loss 4.180447578430176\n",
      "Epoch 5: |          | 651/? [09:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 651, loss 4.309464454650879\n",
      "Epoch 5: |          | 652/? [09:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 652, loss 3.7700564861297607\n",
      "Epoch 5: |          | 653/? [09:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 653, loss 3.922494888305664\n",
      "Epoch 5: |          | 654/? [09:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 654, loss 4.024174690246582\n",
      "Epoch 5: |          | 655/? [09:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 655, loss 3.8049254417419434\n",
      "Epoch 5: |          | 656/? [09:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 656, loss 3.4678447246551514\n",
      "Epoch 5: |          | 657/? [09:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 657, loss 5.777848243713379\n",
      "Epoch 5: |          | 658/? [09:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 658, loss 3.392164707183838\n",
      "Epoch 5: |          | 659/? [09:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 659, loss 3.8649399280548096\n",
      "Epoch 5: |          | 660/? [09:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 660, loss 4.251645565032959\n",
      "Epoch 5: |          | 661/? [09:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 661, loss 4.172304630279541\n",
      "Epoch 5: |          | 662/? [09:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 662, loss 4.00576114654541\n",
      "Epoch 5: |          | 663/? [09:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 663, loss 3.756770372390747\n",
      "Epoch 5: |          | 664/? [09:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 664, loss 3.714423418045044\n",
      "Epoch 5: |          | 665/? [09:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 665, loss 4.003427982330322\n",
      "Epoch 5: |          | 666/? [09:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 666, loss 3.800412654876709\n",
      "Epoch 5: |          | 667/? [09:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 667, loss 4.627805233001709\n",
      "Epoch 5: |          | 668/? [09:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 668, loss 3.4104561805725098\n",
      "Epoch 5: |          | 669/? [09:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 669, loss 3.6192755699157715\n",
      "Epoch 5: |          | 670/? [09:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 670, loss 4.300000190734863\n",
      "Epoch 5: |          | 671/? [09:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 671, loss 4.053722858428955\n",
      "Epoch 5: |          | 672/? [09:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 672, loss 4.0438971519470215\n",
      "Epoch 5: |          | 673/? [09:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 673, loss 3.8932971954345703\n",
      "Epoch 5: |          | 674/? [09:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 674, loss 2.3338510990142822\n",
      "Epoch 5: |          | 675/? [09:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 675, loss 0.8949737548828125\n",
      "Epoch 5: |          | 676/? [09:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 676, loss 0.773317813873291\n",
      "Epoch 5: |          | 677/? [09:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 677, loss 0.6617163419723511\n",
      "Epoch 5: |          | 678/? [09:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 678, loss 1.764865517616272\n",
      "Epoch 5: |          | 679/? [09:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 679, loss 3.2993063926696777\n",
      "Epoch 5: |          | 680/? [09:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 680, loss 3.795099973678589\n",
      "Epoch 5: |          | 681/? [09:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 681, loss 3.362046003341675\n",
      "Epoch 5: |          | 682/? [09:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 682, loss 3.6814143657684326\n",
      "Epoch 5: |          | 683/? [09:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 683, loss 3.3934097290039062\n",
      "Epoch 5: |          | 684/? [09:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 684, loss 4.4045820236206055\n",
      "Epoch 5: |          | 685/? [09:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 685, loss 4.014988899230957\n",
      "Epoch 5: |          | 686/? [09:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 686, loss 3.6352639198303223\n",
      "Epoch 5: |          | 687/? [09:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 687, loss 4.1637773513793945\n",
      "Epoch 5: |          | 688/? [09:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 688, loss 3.653722047805786\n",
      "Epoch 5: |          | 689/? [09:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 689, loss 3.7472598552703857\n",
      "Epoch 5: |          | 690/? [09:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 690, loss 4.353863716125488\n",
      "Epoch 5: |          | 691/? [09:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 691, loss 3.8515846729278564\n",
      "Epoch 5: |          | 692/? [09:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 692, loss 3.8753936290740967\n",
      "Epoch 5: |          | 693/? [09:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 693, loss 4.393738746643066\n",
      "Epoch 5: |          | 694/? [09:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 694, loss 3.7418792247772217\n",
      "Epoch 5: |          | 695/? [09:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 695, loss 4.318001747131348\n",
      "Epoch 5: |          | 696/? [09:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 696, loss 3.6224722862243652\n",
      "Epoch 5: |          | 697/? [09:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 697, loss 3.8093421459198\n",
      "Epoch 5: |          | 698/? [09:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 698, loss 3.2390987873077393\n",
      "Epoch 5: |          | 699/? [09:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 699, loss 3.9754605293273926\n",
      "Epoch 5: |          | 700/? [09:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 700, loss 4.051678657531738\n",
      "Epoch 5: |          | 701/? [09:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 701, loss 3.6854605674743652\n",
      "Epoch 5: |          | 702/? [09:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 702, loss 3.971303939819336\n",
      "Epoch 5: |          | 703/? [09:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 703, loss 4.086304187774658\n",
      "Epoch 5: |          | 704/? [09:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 704, loss 3.9341132640838623\n",
      "Epoch 5: |          | 705/? [09:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 705, loss 3.5735976696014404\n",
      "Epoch 5: |          | 706/? [09:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 706, loss 3.6336770057678223\n",
      "Epoch 5: |          | 707/? [09:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 707, loss 4.102394104003906\n",
      "Epoch 5: |          | 708/? [09:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 708, loss 3.8422131538391113\n",
      "Epoch 5: |          | 709/? [09:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 709, loss 3.7395291328430176\n",
      "Epoch 5: |          | 710/? [09:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 710, loss 4.299850940704346\n",
      "Epoch 5: |          | 711/? [09:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 711, loss 4.346485137939453\n",
      "Epoch 5: |          | 712/? [09:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 712, loss 4.088107109069824\n",
      "Epoch 5: |          | 713/? [10:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 713, loss 4.15964412689209\n",
      "Epoch 5: |          | 714/? [10:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 714, loss 4.2440972328186035\n",
      "Epoch 5: |          | 715/? [10:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 715, loss 3.1916515827178955\n",
      "Epoch 5: |          | 716/? [10:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 716, loss 3.9048430919647217\n",
      "Epoch 5: |          | 717/? [10:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 717, loss 3.779195785522461\n",
      "Epoch 5: |          | 718/? [10:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 718, loss 3.3040060997009277\n",
      "Epoch 5: |          | 719/? [10:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 719, loss 3.8046622276306152\n",
      "Epoch 5: |          | 720/? [10:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 720, loss 3.5026962757110596\n",
      "Epoch 5: |          | 721/? [10:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 721, loss 4.204998016357422\n",
      "Epoch 5: |          | 722/? [10:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 722, loss 3.497149705886841\n",
      "Epoch 5: |          | 723/? [10:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 723, loss 4.0086774826049805\n",
      "Epoch 5: |          | 724/? [10:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 724, loss 3.6071250438690186\n",
      "Epoch 5: |          | 725/? [10:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 725, loss 3.544527530670166\n",
      "Epoch 5: |          | 726/? [10:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 726, loss 3.733992338180542\n",
      "Epoch 5: |          | 727/? [10:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 727, loss 3.525179386138916\n",
      "Epoch 5: |          | 728/? [10:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 728, loss 3.3539395332336426\n",
      "Epoch 5: |          | 729/? [10:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 729, loss 3.816567897796631\n",
      "Epoch 5: |          | 730/? [10:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 730, loss 3.8018155097961426\n",
      "Epoch 5: |          | 731/? [10:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 731, loss 3.9325034618377686\n",
      "Epoch 5: |          | 732/? [10:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 732, loss 4.1750407218933105\n",
      "Epoch 5: |          | 733/? [10:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 733, loss 3.870800018310547\n",
      "Epoch 5: |          | 734/? [10:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 734, loss 4.031440734863281\n",
      "Epoch 5: |          | 735/? [10:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 735, loss 3.959888458251953\n",
      "Epoch 5: |          | 736/? [10:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 736, loss 3.5223922729492188\n",
      "Epoch 5: |          | 737/? [10:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 737, loss 4.281132698059082\n",
      "Epoch 5: |          | 738/? [10:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 738, loss 3.492464542388916\n",
      "Epoch 5: |          | 739/? [10:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 739, loss 3.978569746017456\n",
      "Epoch 5: |          | 740/? [10:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 740, loss 3.6608033180236816\n",
      "Epoch 5: |          | 741/? [10:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 741, loss 3.8185248374938965\n",
      "Epoch 5: |          | 742/? [10:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 742, loss 4.180675506591797\n",
      "Epoch 5: |          | 743/? [10:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 743, loss 4.068612098693848\n",
      "Epoch 5: |          | 744/? [10:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 744, loss 4.0116400718688965\n",
      "Epoch 5: |          | 745/? [10:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 745, loss 3.637500286102295\n",
      "Epoch 5: |          | 746/? [10:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 746, loss 3.936650037765503\n",
      "Epoch 5: |          | 747/? [10:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 747, loss 3.6745407581329346\n",
      "Epoch 5: |          | 748/? [10:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 748, loss 2.672517776489258\n",
      "Epoch 5: |          | 749/? [10:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 749, loss 3.7974534034729004\n",
      "Epoch 5: |          | 750/? [10:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 750, loss 4.073121070861816\n",
      "Epoch 5: |          | 751/? [10:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 751, loss 2.472254514694214\n",
      "Epoch 5: |          | 752/? [10:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 752, loss 3.994335889816284\n",
      "Epoch 5: |          | 753/? [10:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 753, loss 3.1690471172332764\n",
      "Epoch 5: |          | 754/? [10:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 754, loss 3.6542258262634277\n",
      "Epoch 5: |          | 755/? [10:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 755, loss 3.4553427696228027\n",
      "Epoch 5: |          | 756/? [10:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 756, loss 3.838442325592041\n",
      "Epoch 5: |          | 757/? [10:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 757, loss 3.8894972801208496\n",
      "Epoch 5: |          | 758/? [10:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 758, loss 3.6380019187927246\n",
      "Epoch 5: |          | 759/? [10:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 759, loss 3.5769877433776855\n",
      "Epoch 5: |          | 760/? [10:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 760, loss 4.089659690856934\n",
      "Epoch 5: |          | 761/? [10:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 761, loss 4.118221282958984\n",
      "Epoch 5: |          | 762/? [10:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 762, loss 3.788012742996216\n",
      "Epoch 5: |          | 763/? [10:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 763, loss 3.903437376022339\n",
      "Epoch 5: |          | 764/? [10:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 764, loss 4.191222190856934\n",
      "Epoch 5: |          | 765/? [10:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 765, loss 3.967886447906494\n",
      "Epoch 5: |          | 766/? [10:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 766, loss 4.322691917419434\n",
      "Epoch 5: |          | 767/? [10:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 767, loss 4.399306297302246\n",
      "Epoch 5: |          | 768/? [10:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 768, loss 3.925323963165283\n",
      "Epoch 5: |          | 769/? [10:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 769, loss 3.1307389736175537\n",
      "Epoch 5: |          | 770/? [10:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 770, loss 3.680696964263916\n",
      "Epoch 5: |          | 771/? [10:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 771, loss 4.4038310050964355\n",
      "Epoch 5: |          | 772/? [10:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 772, loss 4.14093017578125\n",
      "Epoch 5: |          | 773/? [10:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 773, loss 3.8121585845947266\n",
      "Epoch 5: |          | 774/? [10:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 774, loss 3.9846596717834473\n",
      "Epoch 5: |          | 775/? [10:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 775, loss 4.416974067687988\n",
      "Epoch 5: |          | 776/? [10:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 776, loss 3.86332368850708\n",
      "Epoch 5: |          | 777/? [10:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 777, loss 3.7358734607696533\n",
      "Epoch 5: |          | 778/? [10:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 778, loss 4.114837646484375\n",
      "Epoch 5: |          | 779/? [10:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 779, loss 4.568922996520996\n",
      "Epoch 5: |          | 780/? [10:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 780, loss 3.5513007640838623\n",
      "Epoch 5: |          | 781/? [10:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 781, loss 3.6642329692840576\n",
      "Epoch 5: |          | 782/? [10:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 782, loss 4.019974708557129\n",
      "Epoch 5: |          | 783/? [11:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 783, loss 4.08869743347168\n",
      "Epoch 5: |          | 784/? [11:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 784, loss 3.6721351146698\n",
      "Epoch 5: |          | 785/? [11:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 785, loss 3.4574837684631348\n",
      "Epoch 5: |          | 786/? [11:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 786, loss 4.3363871574401855\n",
      "Epoch 5: |          | 787/? [11:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 787, loss 4.295274257659912\n",
      "Epoch 5: |          | 788/? [11:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 788, loss 2.263758659362793\n",
      "Epoch 5: |          | 789/? [11:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 789, loss 3.7714285850524902\n",
      "Epoch 5: |          | 790/? [11:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 790, loss 4.614957332611084\n",
      "Epoch 5: |          | 791/? [11:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 791, loss 4.30459451675415\n",
      "Epoch 5: |          | 792/? [11:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 792, loss 3.526599884033203\n",
      "Epoch 5: |          | 793/? [11:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 793, loss 4.0040602684021\n",
      "Epoch 5: |          | 794/? [11:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 794, loss 4.320796012878418\n",
      "Epoch 5: |          | 795/? [11:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 795, loss 3.8359313011169434\n",
      "Epoch 5: |          | 796/? [11:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 796, loss 4.213312149047852\n",
      "Epoch 5: |          | 797/? [11:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 797, loss 3.2180256843566895\n",
      "Epoch 5: |          | 798/? [11:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 798, loss 3.314537763595581\n",
      "Epoch 5: |          | 799/? [11:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 799, loss 4.243049144744873\n",
      "Epoch 5: |          | 800/? [11:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 800, loss 4.080251216888428\n",
      "Epoch 5: |          | 801/? [11:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 801, loss 3.6373984813690186\n",
      "Epoch 5: |          | 802/? [11:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 802, loss 3.955885648727417\n",
      "Epoch 5: |          | 803/? [11:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 803, loss 3.8038246631622314\n",
      "Epoch 5: |          | 804/? [11:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 804, loss 3.9507274627685547\n",
      "Epoch 5: |          | 805/? [11:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 805, loss 4.130328178405762\n",
      "Epoch 5: |          | 806/? [11:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 806, loss 4.531630039215088\n",
      "Epoch 5: |          | 807/? [11:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 807, loss 3.923349380493164\n",
      "Epoch 5: |          | 808/? [11:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 808, loss 3.5383999347686768\n",
      "Epoch 5: |          | 809/? [11:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 809, loss 4.071082592010498\n",
      "Epoch 5: |          | 810/? [11:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 810, loss 3.8652358055114746\n",
      "Epoch 5: |          | 811/? [11:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 811, loss 4.096606731414795\n",
      "Epoch 5: |          | 812/? [11:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 812, loss 4.731528282165527\n",
      "Epoch 5: |          | 813/? [11:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 813, loss 4.479243278503418\n",
      "Epoch 5: |          | 814/? [11:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 814, loss 3.506829023361206\n",
      "Epoch 5: |          | 815/? [11:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 815, loss 4.2411417961120605\n",
      "Epoch 5: |          | 816/? [11:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 816, loss 4.0587921142578125\n",
      "Epoch 5: |          | 817/? [11:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 817, loss 3.3790550231933594\n",
      "Epoch 5: |          | 818/? [11:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 818, loss 4.367637634277344\n",
      "Epoch 5: |          | 819/? [11:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 819, loss 4.074820518493652\n",
      "Epoch 5: |          | 820/? [11:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 820, loss 3.893090009689331\n",
      "Epoch 5: |          | 821/? [11:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 821, loss 3.8548831939697266\n",
      "Epoch 5: |          | 822/? [11:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 822, loss 3.5408852100372314\n",
      "Epoch 5: |          | 823/? [11:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 823, loss 3.572859287261963\n",
      "Epoch 5: |          | 824/? [11:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 824, loss 4.0224785804748535\n",
      "Epoch 5: |          | 825/? [11:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 825, loss 3.57475209236145\n",
      "Epoch 5: |          | 826/? [11:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 826, loss 4.105188846588135\n",
      "Epoch 5: |          | 827/? [11:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 827, loss 3.7337448596954346\n",
      "Epoch 5: |          | 828/? [11:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 828, loss 4.216372013092041\n",
      "Epoch 5: |          | 829/? [11:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 829, loss 3.893113613128662\n",
      "Epoch 5: |          | 830/? [11:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 830, loss 4.492767810821533\n",
      "Epoch 5: |          | 831/? [11:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 831, loss 2.3828883171081543\n",
      "Epoch 5: |          | 832/? [11:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 832, loss 3.849951982498169\n",
      "Epoch 5: |          | 833/? [11:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 833, loss 3.736664295196533\n",
      "Epoch 5: |          | 834/? [11:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 834, loss 4.485064506530762\n",
      "Epoch 5: |          | 835/? [11:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 835, loss 3.8349850177764893\n",
      "Epoch 5: |          | 836/? [11:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 836, loss 4.5012407302856445\n",
      "Epoch 5: |          | 837/? [11:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 837, loss 3.925116777420044\n",
      "Epoch 5: |          | 838/? [11:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 838, loss 3.2830090522766113\n",
      "Epoch 5: |          | 839/? [11:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 839, loss 3.6358630657196045\n",
      "Epoch 5: |          | 840/? [11:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 840, loss 4.176136016845703\n",
      "Epoch 5: |          | 841/? [11:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 841, loss 4.215704441070557\n",
      "Epoch 5: |          | 842/? [11:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 842, loss 3.889782428741455\n",
      "Epoch 5: |          | 843/? [11:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 843, loss 4.230023384094238\n",
      "Epoch 5: |          | 844/? [11:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 844, loss 3.5749099254608154\n",
      "Epoch 5: |          | 845/? [11:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 845, loss 3.9986464977264404\n",
      "Epoch 5: |          | 846/? [11:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 846, loss 4.428102493286133\n",
      "Epoch 5: |          | 847/? [11:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 847, loss 4.020174980163574\n",
      "Epoch 5: |          | 848/? [11:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 848, loss 3.5725951194763184\n",
      "Epoch 5: |          | 849/? [11:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 849, loss 3.6437668800354004\n",
      "Epoch 5: |          | 850/? [11:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 850, loss 3.770603656768799\n",
      "Epoch 5: |          | 851/? [11:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 851, loss 4.071061611175537\n",
      "Epoch 5: |          | 852/? [12:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 852, loss 4.182043552398682\n",
      "Epoch 5: |          | 853/? [12:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 853, loss 4.019500732421875\n",
      "Epoch 5: |          | 854/? [12:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 854, loss 3.3520915508270264\n",
      "Epoch 5: |          | 855/? [12:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 855, loss 3.598402500152588\n",
      "Epoch 5: |          | 856/? [12:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 856, loss 3.5552711486816406\n",
      "Epoch 5: |          | 857/? [12:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 857, loss 4.100423336029053\n",
      "Epoch 5: |          | 858/? [12:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 858, loss 4.006969928741455\n",
      "Epoch 5: |          | 859/? [12:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 859, loss 4.0116496086120605\n",
      "Epoch 5: |          | 860/? [12:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 860, loss 4.398959159851074\n",
      "Epoch 5: |          | 861/? [12:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 861, loss 3.694479465484619\n",
      "Epoch 5: |          | 862/? [12:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 862, loss 4.023449897766113\n",
      "Epoch 5: |          | 863/? [12:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 863, loss 3.4018726348876953\n",
      "Epoch 5: |          | 864/? [12:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 864, loss 3.945305585861206\n",
      "Epoch 5: |          | 865/? [12:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 865, loss 3.9559314250946045\n",
      "Epoch 5: |          | 866/? [12:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 866, loss 2.9867727756500244\n",
      "Epoch 5: |          | 867/? [12:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 867, loss 3.157832384109497\n",
      "Epoch 5: |          | 868/? [12:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 868, loss 4.035052299499512\n",
      "Epoch 5: |          | 869/? [12:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 869, loss 4.063724994659424\n",
      "Epoch 5: |          | 870/? [12:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 870, loss 3.671199083328247\n",
      "Epoch 5: |          | 871/? [12:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 871, loss 4.044081211090088\n",
      "Epoch 5: |          | 872/? [12:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 872, loss 3.840527296066284\n",
      "Epoch 5: |          | 873/? [12:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 873, loss 3.837057590484619\n",
      "Epoch 5: |          | 874/? [12:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 874, loss 3.3664402961730957\n",
      "Epoch 5: |          | 875/? [12:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 875, loss 4.070322513580322\n",
      "Epoch 5: |          | 876/? [12:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 876, loss 3.6285605430603027\n",
      "Epoch 5: |          | 877/? [12:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 877, loss 4.0193400382995605\n",
      "Epoch 5: |          | 878/? [12:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 878, loss 3.444592237472534\n",
      "Epoch 5: |          | 879/? [12:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 879, loss 3.5098724365234375\n",
      "Epoch 5: |          | 880/? [12:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 880, loss 4.623307228088379\n",
      "Epoch 5: |          | 881/? [12:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 881, loss 4.0066938400268555\n",
      "Epoch 5: |          | 882/? [12:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 882, loss 3.790724992752075\n",
      "Epoch 5: |          | 883/? [12:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 883, loss 3.9389488697052\n",
      "Epoch 5: |          | 884/? [12:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 884, loss 3.966407299041748\n",
      "Epoch 5: |          | 885/? [12:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 885, loss 3.7302918434143066\n",
      "Epoch 5: |          | 886/? [12:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 886, loss 4.410271644592285\n",
      "Epoch 5: |          | 887/? [12:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 887, loss 4.395217418670654\n",
      "Epoch 5: |          | 888/? [12:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 888, loss 4.099993705749512\n",
      "Epoch 5: |          | 889/? [12:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 889, loss 3.67547345161438\n",
      "Epoch 5: |          | 890/? [12:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 890, loss 3.9191641807556152\n",
      "Epoch 5: |          | 891/? [12:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 891, loss 3.7051501274108887\n",
      "Epoch 5: |          | 892/? [12:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 892, loss 4.329283714294434\n",
      "Epoch 5: |          | 893/? [12:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 893, loss 3.7552857398986816\n",
      "Epoch 5: |          | 894/? [12:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 894, loss 3.262463092803955\n",
      "Epoch 5: |          | 895/? [12:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 895, loss 4.414580821990967\n",
      "Epoch 5: |          | 896/? [12:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 896, loss 3.987067461013794\n",
      "Epoch 5: |          | 897/? [12:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 897, loss 4.024806976318359\n",
      "Epoch 5: |          | 898/? [12:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 898, loss 4.000235080718994\n",
      "Epoch 5: |          | 899/? [12:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 899, loss 3.7660155296325684\n",
      "Epoch 5: |          | 900/? [12:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 900, loss 3.7200522422790527\n",
      "Epoch 5: |          | 901/? [12:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 901, loss 4.083698272705078\n",
      "Epoch 5: |          | 902/? [12:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 902, loss 4.21754789352417\n",
      "Epoch 5: |          | 903/? [12:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 903, loss 3.5239779949188232\n",
      "Epoch 5: |          | 904/? [12:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 904, loss 3.995546340942383\n",
      "Epoch 5: |          | 905/? [12:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 905, loss 4.172652244567871\n",
      "Epoch 5: |          | 906/? [12:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 906, loss 3.915410280227661\n",
      "Epoch 5: |          | 907/? [12:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 907, loss 4.00538969039917\n",
      "Epoch 5: |          | 908/? [12:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 908, loss 4.0743513107299805\n",
      "Epoch 5: |          | 909/? [12:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 909, loss 4.052306175231934\n",
      "Epoch 5: |          | 910/? [12:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 910, loss 3.813302516937256\n",
      "Epoch 5: |          | 911/? [12:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 911, loss 3.880021572113037\n",
      "Epoch 5: |          | 912/? [12:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 912, loss 3.8421778678894043\n",
      "Epoch 5: |          | 913/? [12:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 913, loss 3.8511645793914795\n",
      "Epoch 5: |          | 914/? [12:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 914, loss 4.1074018478393555\n",
      "Epoch 5: |          | 915/? [12:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 915, loss 3.941818952560425\n",
      "Epoch 5: |          | 916/? [12:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 916, loss 3.880526065826416\n",
      "Epoch 5: |          | 917/? [13:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 917, loss 3.8359005451202393\n",
      "Epoch 5: |          | 918/? [13:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 918, loss 3.7348523139953613\n",
      "Epoch 5: |          | 919/? [13:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 919, loss 3.7620391845703125\n",
      "Epoch 5: |          | 920/? [13:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 920, loss 3.9184298515319824\n",
      "Epoch 5: |          | 921/? [13:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 921, loss 3.7429542541503906\n",
      "Epoch 5: |          | 922/? [13:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 922, loss 3.899587631225586\n",
      "Epoch 5: |          | 923/? [13:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 923, loss 3.756455659866333\n",
      "Epoch 5: |          | 924/? [13:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 924, loss 3.754323959350586\n",
      "Epoch 5: |          | 925/? [13:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 925, loss 4.056856155395508\n",
      "Epoch 5: |          | 926/? [13:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 926, loss 3.8217902183532715\n",
      "Epoch 5: |          | 927/? [13:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 927, loss 4.117762565612793\n",
      "Epoch 5: |          | 928/? [13:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 928, loss 3.595649242401123\n",
      "Epoch 5: |          | 929/? [13:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 929, loss 3.703638792037964\n",
      "Epoch 5: |          | 930/? [13:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 930, loss 3.624980926513672\n",
      "Epoch 5: |          | 931/? [13:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 931, loss 3.329939365386963\n",
      "Epoch 5: |          | 932/? [13:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 932, loss 3.9829001426696777\n",
      "Epoch 5: |          | 933/? [13:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 933, loss 3.7448413372039795\n",
      "Epoch 5: |          | 934/? [13:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 934, loss 4.32395076751709\n",
      "Epoch 5: |          | 935/? [13:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 935, loss 4.623033046722412\n",
      "Epoch 5: |          | 936/? [13:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 936, loss 3.858898162841797\n",
      "Epoch 5: |          | 937/? [13:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 937, loss 3.76580548286438\n",
      "Epoch 5: |          | 938/? [13:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 938, loss 3.7836670875549316\n",
      "Epoch 5: |          | 939/? [13:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 939, loss 4.023523807525635\n",
      "Epoch 5: |          | 940/? [13:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 940, loss 4.218395233154297\n",
      "Epoch 5: |          | 941/? [13:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 941, loss 3.740495204925537\n",
      "Epoch 5: |          | 942/? [13:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 942, loss 3.2494029998779297\n",
      "Epoch 5: |          | 943/? [13:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 943, loss 4.110622406005859\n",
      "Epoch 5: |          | 944/? [13:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 944, loss 3.145740032196045\n",
      "Epoch 5: |          | 945/? [13:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 945, loss 3.856956958770752\n",
      "Epoch 5: |          | 946/? [13:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 946, loss 3.8226158618927\n",
      "Epoch 5: |          | 947/? [13:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 947, loss 3.7166621685028076\n",
      "Epoch 5: |          | 948/? [13:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 948, loss 3.9805920124053955\n",
      "Epoch 5: |          | 949/? [13:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 949, loss 3.8277294635772705\n",
      "Epoch 5: |          | 950/? [13:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 950, loss 3.583216905593872\n",
      "Epoch 5: |          | 951/? [13:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 951, loss 4.2633562088012695\n",
      "Epoch 5: |          | 952/? [13:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 952, loss 4.197761058807373\n",
      "Epoch 5: |          | 953/? [13:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 953, loss 4.750672340393066\n",
      "Epoch 5: |          | 954/? [13:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 954, loss 3.7203757762908936\n",
      "Epoch 5: |          | 955/? [13:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 955, loss 4.377960681915283\n",
      "Epoch 5: |          | 956/? [13:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 956, loss 3.825281858444214\n",
      "Epoch 5: |          | 957/? [13:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 957, loss 3.9920246601104736\n",
      "Epoch 5: |          | 958/? [13:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 958, loss 4.106196880340576\n",
      "Epoch 5: |          | 959/? [13:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 959, loss 3.627709150314331\n",
      "Epoch 5: |          | 960/? [13:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 960, loss 4.168895721435547\n",
      "Epoch 5: |          | 961/? [13:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 961, loss 4.373805046081543\n",
      "Epoch 5: |          | 962/? [13:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 962, loss 3.9014344215393066\n",
      "Epoch 5: |          | 963/? [13:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 963, loss 3.7001426219940186\n",
      "Epoch 5: |          | 964/? [13:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 964, loss 4.151289939880371\n",
      "Epoch 5: |          | 965/? [13:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 965, loss 3.635733127593994\n",
      "Epoch 5: |          | 966/? [13:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 966, loss 3.5302653312683105\n",
      "Epoch 5: |          | 967/? [13:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 967, loss 3.7827134132385254\n",
      "Epoch 5: |          | 968/? [13:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 968, loss 3.6983935832977295\n",
      "Epoch 5: |          | 969/? [13:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 969, loss 3.569326400756836\n",
      "Epoch 5: |          | 970/? [13:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 970, loss 4.024168968200684\n",
      "Epoch 5: |          | 971/? [13:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 971, loss 4.274369716644287\n",
      "Epoch 5: |          | 972/? [13:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 972, loss 3.6863369941711426\n",
      "Epoch 5: |          | 973/? [13:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 973, loss 3.9051432609558105\n",
      "Epoch 5: |          | 974/? [13:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 974, loss 3.9410736560821533\n",
      "Epoch 5: |          | 975/? [13:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 975, loss 3.9423015117645264\n",
      "Epoch 5: |          | 976/? [13:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 976, loss 3.993328094482422\n",
      "Epoch 5: |          | 977/? [13:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 977, loss 4.567614555358887\n",
      "Epoch 5: |          | 978/? [13:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 978, loss 4.063356876373291\n",
      "Epoch 5: |          | 979/? [13:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 979, loss 4.302298545837402\n",
      "Epoch 5: |          | 980/? [13:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 980, loss 3.4655487537384033\n",
      "Epoch 5: |          | 981/? [13:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 981, loss 3.3136653900146484\n",
      "Epoch 5: |          | 982/? [13:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 982, loss 3.9508285522460938\n",
      "Epoch 5: |          | 983/? [13:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 983, loss 4.374330997467041\n",
      "Epoch 5: |          | 984/? [13:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 984, loss 3.443009853363037\n",
      "Epoch 5: |          | 985/? [13:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 985, loss 3.6864771842956543\n",
      "Epoch 5: |          | 986/? [13:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 986, loss 3.6865501403808594\n",
      "Epoch 5: |          | 987/? [13:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 987, loss 3.270143985748291\n",
      "Epoch 5: |          | 988/? [14:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 988, loss 4.280135154724121\n",
      "Epoch 5: |          | 989/? [14:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 989, loss 3.886730194091797\n",
      "Epoch 5: |          | 990/? [14:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 990, loss 3.2386250495910645\n",
      "Epoch 5: |          | 991/? [14:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 991, loss 3.9663619995117188\n",
      "Epoch 5: |          | 992/? [14:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 992, loss 4.635308265686035\n",
      "Epoch 5: |          | 993/? [14:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 993, loss 3.753579616546631\n",
      "Epoch 5: |          | 994/? [14:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 994, loss 3.739164352416992\n",
      "Epoch 5: |          | 995/? [14:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 995, loss 4.18063497543335\n",
      "Epoch 5: |          | 996/? [14:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 996, loss 4.136120796203613\n",
      "Epoch 5: |          | 997/? [14:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 997, loss 3.768052339553833\n",
      "Epoch 5: |          | 998/? [14:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 998, loss 4.0127458572387695\n",
      "Epoch 5: |          | 999/? [14:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 999, loss 4.027151584625244\n",
      "Epoch 5: |          | 1000/? [14:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1000, loss 3.4856181144714355\n",
      "Epoch 5: |          | 1001/? [14:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1001, loss 4.183101177215576\n",
      "Epoch 5: |          | 1002/? [14:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1002, loss 4.115907669067383\n",
      "Epoch 5: |          | 1003/? [14:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1003, loss 4.313277721405029\n",
      "Epoch 5: |          | 1004/? [14:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1004, loss 3.312272310256958\n",
      "Epoch 5: |          | 1005/? [14:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1005, loss 3.862773895263672\n",
      "Epoch 5: |          | 1006/? [14:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1006, loss 4.195341110229492\n",
      "Epoch 5: |          | 1007/? [14:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1007, loss 3.7619242668151855\n",
      "Epoch 5: |          | 1008/? [14:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1008, loss 3.908377170562744\n",
      "Epoch 5: |          | 1009/? [14:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1009, loss 4.200641632080078\n",
      "Epoch 5: |          | 1010/? [14:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1010, loss 3.3158907890319824\n",
      "Epoch 5: |          | 1011/? [14:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1011, loss 3.8847954273223877\n",
      "Epoch 5: |          | 1012/? [14:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1012, loss 3.684736967086792\n",
      "Epoch 5: |          | 1013/? [14:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1013, loss 3.8481736183166504\n",
      "Epoch 5: |          | 1014/? [14:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1014, loss 4.295495510101318\n",
      "Epoch 5: |          | 1015/? [14:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1015, loss 3.9308249950408936\n",
      "Epoch 5: |          | 1016/? [14:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1016, loss 3.742412567138672\n",
      "Epoch 5: |          | 1017/? [14:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1017, loss 3.2987656593322754\n",
      "Epoch 5: |          | 1018/? [14:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1018, loss 3.8262722492218018\n",
      "Epoch 5: |          | 1019/? [14:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1019, loss 3.889251232147217\n",
      "Epoch 5: |          | 1020/? [14:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1020, loss 3.445103883743286\n",
      "Epoch 5: |          | 1021/? [14:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1021, loss 3.704219341278076\n",
      "Epoch 5: |          | 1022/? [14:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1022, loss 3.502809524536133\n",
      "Epoch 5: |          | 1023/? [14:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1023, loss 3.2376246452331543\n",
      "Epoch 5: |          | 1024/? [14:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1024, loss 3.7467479705810547\n",
      "Epoch 5: |          | 1025/? [14:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1025, loss 3.6472461223602295\n",
      "Epoch 5: |          | 1026/? [14:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1026, loss 2.741696357727051\n",
      "Epoch 5: |          | 1027/? [14:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1027, loss 3.947105884552002\n",
      "Epoch 5: |          | 1028/? [14:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1028, loss 3.8016979694366455\n",
      "Epoch 5: |          | 1029/? [14:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1029, loss 3.682910203933716\n",
      "Epoch 5: |          | 1030/? [14:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1030, loss 3.5005462169647217\n",
      "Epoch 5: |          | 1031/? [14:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1031, loss 3.5657451152801514\n",
      "Epoch 5: |          | 1032/? [14:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1032, loss 3.9929308891296387\n",
      "Epoch 5: |          | 1033/? [14:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1033, loss 4.266028881072998\n",
      "Epoch 5: |          | 1034/? [14:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1034, loss 3.6610145568847656\n",
      "Epoch 5: |          | 1035/? [14:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1035, loss 3.6567699909210205\n",
      "Epoch 5: |          | 1036/? [14:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1036, loss 3.6042752265930176\n",
      "Epoch 5: |          | 1037/? [14:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1037, loss 4.216973304748535\n",
      "Epoch 5: |          | 1038/? [14:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1038, loss 4.363970756530762\n",
      "Epoch 5: |          | 1039/? [14:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1039, loss 4.660191535949707\n",
      "Epoch 5: |          | 1040/? [14:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1040, loss 3.969491481781006\n",
      "Epoch 5: |          | 1041/? [14:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1041, loss 4.286527156829834\n",
      "Epoch 5: |          | 1042/? [14:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1042, loss 3.865070343017578\n",
      "Epoch 5: |          | 1043/? [14:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1043, loss 4.211567401885986\n",
      "Epoch 5: |          | 1044/? [14:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1044, loss 3.7821667194366455\n",
      "Epoch 5: |          | 1045/? [14:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1045, loss 3.375518321990967\n",
      "Epoch 5: |          | 1046/? [14:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1046, loss 3.2301011085510254\n",
      "Epoch 5: |          | 1047/? [14:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1047, loss 4.339450836181641\n",
      "Epoch 5: |          | 1048/? [14:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1048, loss 3.788695812225342\n",
      "Epoch 5: |          | 1049/? [14:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1049, loss 4.023280143737793\n",
      "Epoch 5: |          | 1050/? [14:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1050, loss 3.5926761627197266\n",
      "Epoch 5: |          | 1051/? [14:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1051, loss 3.5435471534729004\n",
      "Epoch 5: |          | 1052/? [14:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1052, loss 4.172813415527344\n",
      "Epoch 5: |          | 1053/? [14:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1053, loss 4.30618953704834\n",
      "Epoch 5: |          | 1054/? [14:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1054, loss 3.7384040355682373\n",
      "Epoch 5: |          | 1055/? [14:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1055, loss 3.422423839569092\n",
      "Epoch 5: |          | 1056/? [14:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1056, loss 3.4083919525146484\n",
      "Epoch 5: |          | 1057/? [14:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1057, loss 4.071027755737305\n",
      "Epoch 5: |          | 1058/? [14:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1058, loss 3.6160330772399902\n",
      "Epoch 5: |          | 1059/? [15:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1059, loss 4.22991943359375\n",
      "Epoch 5: |          | 1060/? [15:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1060, loss 4.089428901672363\n",
      "Epoch 5: |          | 1061/? [15:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1061, loss 2.8259034156799316\n",
      "Epoch 5: |          | 1062/? [15:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1062, loss 3.7266407012939453\n",
      "Epoch 5: |          | 1063/? [15:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1063, loss 3.8304858207702637\n",
      "Epoch 5: |          | 1064/? [15:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1064, loss 4.0091657638549805\n",
      "Epoch 5: |          | 1065/? [15:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1065, loss 2.6742091178894043\n",
      "Epoch 5: |          | 1066/? [15:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1066, loss 3.9757087230682373\n",
      "Epoch 5: |          | 1067/? [15:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1067, loss 3.4731502532958984\n",
      "Epoch 5: |          | 1068/? [15:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1068, loss 3.599658966064453\n",
      "Epoch 5: |          | 1069/? [15:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1069, loss 4.001718997955322\n",
      "Epoch 5: |          | 1070/? [15:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1070, loss 3.759767532348633\n",
      "Epoch 5: |          | 1071/? [15:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1071, loss 4.122227668762207\n",
      "Epoch 5: |          | 1072/? [15:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1072, loss 4.159384250640869\n",
      "Epoch 5: |          | 1073/? [15:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1073, loss 4.3365159034729\n",
      "Epoch 5: |          | 1074/? [15:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1074, loss 3.6710400581359863\n",
      "Epoch 5: |          | 1075/? [15:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1075, loss 3.5078186988830566\n",
      "Epoch 5: |          | 1076/? [15:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1076, loss 4.11016845703125\n",
      "Epoch 5: |          | 1077/? [15:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1077, loss 3.6133289337158203\n",
      "Epoch 5: |          | 1078/? [15:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1078, loss 3.882371425628662\n",
      "Epoch 5: |          | 1079/? [15:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1079, loss 4.34712028503418\n",
      "Epoch 5: |          | 1080/? [15:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1080, loss 3.83910870552063\n",
      "Epoch 5: |          | 1081/? [15:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1081, loss 4.089242458343506\n",
      "Epoch 5: |          | 1082/? [15:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1082, loss 3.683884859085083\n",
      "Epoch 5: |          | 1083/? [15:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1083, loss 3.2506191730499268\n",
      "Epoch 5: |          | 1084/? [15:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1084, loss 3.0645365715026855\n",
      "Epoch 5: |          | 1085/? [15:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1085, loss 3.739508867263794\n",
      "Epoch 5: |          | 1086/? [15:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1086, loss 4.030589580535889\n",
      "Epoch 5: |          | 1087/? [15:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1087, loss 4.50774621963501\n",
      "Epoch 5: |          | 1088/? [15:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1088, loss 4.088107585906982\n",
      "Epoch 5: |          | 1089/? [15:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1089, loss 4.060635089874268\n",
      "Epoch 5: |          | 1090/? [15:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1090, loss 3.851043224334717\n",
      "Epoch 5: |          | 1091/? [15:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1091, loss 3.6507067680358887\n",
      "Epoch 5: |          | 1092/? [15:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1092, loss 3.961815357208252\n",
      "Epoch 5: |          | 1093/? [15:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1093, loss 3.4265658855438232\n",
      "Epoch 5: |          | 1094/? [15:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1094, loss 3.9376659393310547\n",
      "Epoch 5: |          | 1095/? [15:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1095, loss 3.9702892303466797\n",
      "Epoch 5: |          | 1096/? [15:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1096, loss 4.225489139556885\n",
      "Epoch 5: |          | 1097/? [15:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1097, loss 3.803168535232544\n",
      "Epoch 5: |          | 1098/? [15:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1098, loss 3.105874538421631\n",
      "Epoch 5: |          | 1099/? [15:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1099, loss 3.7414767742156982\n",
      "Epoch 5: |          | 1100/? [15:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1100, loss 3.964395523071289\n",
      "Epoch 5: |          | 1101/? [15:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1101, loss 3.567473888397217\n",
      "Epoch 5: |          | 1102/? [15:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1102, loss 4.379045486450195\n",
      "Epoch 5: |          | 1103/? [15:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1103, loss 4.789440155029297\n",
      "Epoch 5: |          | 1104/? [15:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1104, loss 4.12290096282959\n",
      "Epoch 5: |          | 1105/? [15:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1105, loss 4.257131099700928\n",
      "Epoch 5: |          | 1106/? [15:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1106, loss 3.7306370735168457\n",
      "Epoch 5: |          | 1107/? [15:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1107, loss 3.883150815963745\n",
      "Epoch 5: |          | 1108/? [15:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1108, loss 3.882237195968628\n",
      "Epoch 5: |          | 1109/? [15:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1109, loss 3.4855384826660156\n",
      "Epoch 5: |          | 1110/? [15:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1110, loss 4.408279895782471\n",
      "Epoch 5: |          | 1111/? [15:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1111, loss 4.121504783630371\n",
      "Epoch 5: |          | 1112/? [15:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1112, loss 3.915767192840576\n",
      "Epoch 5: |          | 1113/? [15:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1113, loss 3.742284059524536\n",
      "Epoch 5: |          | 1114/? [15:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1114, loss 3.2137703895568848\n",
      "Epoch 5: |          | 1115/? [15:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1115, loss 3.011622905731201\n",
      "Epoch 5: |          | 1116/? [15:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1116, loss 3.3674614429473877\n",
      "Epoch 5: |          | 1117/? [15:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1117, loss 3.5786845684051514\n",
      "Epoch 5: |          | 1118/? [15:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1118, loss 3.6882681846618652\n",
      "Epoch 5: |          | 1119/? [15:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1119, loss 4.280219554901123\n",
      "Epoch 5: |          | 1120/? [15:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1120, loss 3.836005449295044\n",
      "Epoch 5: |          | 1121/? [15:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1121, loss 4.065523147583008\n",
      "Epoch 5: |          | 1122/? [15:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1122, loss 3.6310248374938965\n",
      "Epoch 5: |          | 1123/? [15:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1123, loss 3.8728275299072266\n",
      "Epoch 5: |          | 1124/? [15:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1124, loss 4.218349456787109\n",
      "Epoch 5: |          | 1125/? [15:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1125, loss 3.5383129119873047\n",
      "Epoch 5: |          | 1126/? [15:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1126, loss 3.4592223167419434\n",
      "Epoch 5: |          | 1127/? [15:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1127, loss 3.7607970237731934\n",
      "Epoch 5: |          | 1128/? [15:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1128, loss 3.842679262161255\n",
      "Epoch 5: |          | 1129/? [15:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1129, loss 3.9430320262908936\n",
      "Epoch 5: |          | 1130/? [16:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1130, loss 4.135542869567871\n",
      "Epoch 5: |          | 1131/? [16:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1131, loss 4.191880226135254\n",
      "Epoch 5: |          | 1132/? [16:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1132, loss 2.992283821105957\n",
      "Epoch 5: |          | 1133/? [16:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1133, loss 3.8125178813934326\n",
      "Epoch 5: |          | 1134/? [16:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1134, loss 3.5992069244384766\n",
      "Epoch 5: |          | 1135/? [16:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1135, loss 4.215769290924072\n",
      "Epoch 5: |          | 1136/? [16:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1136, loss 3.8532989025115967\n",
      "Epoch 5: |          | 1137/? [16:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1137, loss 3.9047858715057373\n",
      "Epoch 5: |          | 1138/? [16:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1138, loss 4.345055103302002\n",
      "Epoch 5: |          | 1139/? [16:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1139, loss 4.198441028594971\n",
      "Epoch 5: |          | 1140/? [16:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1140, loss 3.781165361404419\n",
      "Epoch 5: |          | 1141/? [16:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1141, loss 4.195910930633545\n",
      "Epoch 5: |          | 1142/? [16:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1142, loss 4.314172744750977\n",
      "Epoch 5: |          | 1143/? [16:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1143, loss 4.2937517166137695\n",
      "Epoch 5: |          | 1144/? [16:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1144, loss 3.695861339569092\n",
      "Epoch 5: |          | 1145/? [16:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1145, loss 3.8830535411834717\n",
      "Epoch 5: |          | 1146/? [16:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1146, loss 3.4934661388397217\n",
      "Epoch 5: |          | 1147/? [16:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1147, loss 3.4283745288848877\n",
      "Epoch 5: |          | 1148/? [16:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1148, loss 3.678783893585205\n",
      "Epoch 5: |          | 1149/? [16:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1149, loss 4.545186996459961\n",
      "Epoch 5: |          | 1150/? [16:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1150, loss 4.052579879760742\n",
      "Epoch 5: |          | 1151/? [16:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1151, loss 4.335105895996094\n",
      "Epoch 5: |          | 1152/? [16:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1152, loss 3.575261354446411\n",
      "Epoch 5: |          | 1153/? [16:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1153, loss 3.9343008995056152\n",
      "Epoch 5: |          | 1154/? [16:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1154, loss 3.6015396118164062\n",
      "Epoch 5: |          | 1155/? [16:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1155, loss 3.8269202709198\n",
      "Epoch 5: |          | 1156/? [16:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1156, loss 3.8342556953430176\n",
      "Epoch 5: |          | 1157/? [16:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1157, loss 4.1222429275512695\n",
      "Epoch 5: |          | 1158/? [16:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1158, loss 4.311582088470459\n",
      "Epoch 5: |          | 1159/? [16:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1159, loss 3.08528995513916\n",
      "Epoch 5: |          | 1160/? [16:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1160, loss 4.2525248527526855\n",
      "Epoch 5: |          | 1161/? [16:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1161, loss 4.141397476196289\n",
      "Epoch 5: |          | 1162/? [16:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1162, loss 4.023573398590088\n",
      "Epoch 5: |          | 1163/? [16:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1163, loss 4.623061180114746\n",
      "Epoch 5: |          | 1164/? [16:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1164, loss 4.387889862060547\n",
      "Epoch 5: |          | 1165/? [16:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1165, loss 3.5557613372802734\n",
      "Epoch 5: |          | 1166/? [16:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1166, loss 4.0608110427856445\n",
      "Epoch 5: |          | 1167/? [16:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1167, loss 4.105576515197754\n",
      "Epoch 5: |          | 1168/? [16:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1168, loss 4.544342517852783\n",
      "Epoch 5: |          | 1169/? [16:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1169, loss 3.634702682495117\n",
      "Epoch 5: |          | 1170/? [16:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1170, loss 4.161129474639893\n",
      "Epoch 5: |          | 1171/? [16:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1171, loss 3.536832094192505\n",
      "Epoch 5: |          | 1172/? [16:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1172, loss 3.4555251598358154\n",
      "Epoch 5: |          | 1173/? [16:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1173, loss 4.022800445556641\n",
      "Epoch 5: |          | 1174/? [16:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1174, loss 3.5145652294158936\n",
      "Epoch 5: |          | 1175/? [16:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1175, loss 4.093169212341309\n",
      "Epoch 5: |          | 1176/? [16:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1176, loss 4.181361198425293\n",
      "Epoch 5: |          | 1177/? [16:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1177, loss 4.31474494934082\n",
      "Epoch 5: |          | 1178/? [16:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1178, loss 3.7387657165527344\n",
      "Epoch 5: |          | 1179/? [16:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1179, loss 4.264841079711914\n",
      "Epoch 5: |          | 1180/? [16:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1180, loss 4.11527156829834\n",
      "Epoch 5: |          | 1181/? [16:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1181, loss 4.026391983032227\n",
      "Epoch 5: |          | 1182/? [16:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1182, loss 3.850064754486084\n",
      "Epoch 5: |          | 1183/? [16:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1183, loss 3.565281391143799\n",
      "Epoch 5: |          | 1184/? [16:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1184, loss 3.9803695678710938\n",
      "Epoch 5: |          | 1185/? [16:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1185, loss 3.7305305004119873\n",
      "Epoch 5: |          | 1186/? [16:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1186, loss 3.958252429962158\n",
      "Epoch 5: |          | 1187/? [16:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1187, loss 3.8281102180480957\n",
      "Epoch 5: |          | 1188/? [16:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1188, loss 4.200216293334961\n",
      "Epoch 5: |          | 1189/? [16:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1189, loss 4.265963554382324\n",
      "Epoch 5: |          | 1190/? [16:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1190, loss 3.871250629425049\n",
      "Epoch 5: |          | 1191/? [16:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1191, loss 3.888258457183838\n",
      "Epoch 5: |          | 1192/? [16:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1192, loss 4.161375999450684\n",
      "Epoch 5: |          | 1193/? [16:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1193, loss 3.665217876434326\n",
      "Epoch 5: |          | 1194/? [16:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1194, loss 3.354189395904541\n",
      "Epoch 5: |          | 1195/? [16:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1195, loss 3.8852105140686035\n",
      "Epoch 5: |          | 1196/? [16:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1196, loss 4.106454849243164\n",
      "Epoch 5: |          | 1197/? [16:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1197, loss 3.875920057296753\n",
      "Epoch 5: |          | 1198/? [16:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1198, loss 3.9678568840026855\n",
      "Epoch 5: |          | 1199/? [16:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1199, loss 4.236546993255615\n",
      "Epoch 5: |          | 1200/? [16:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1200, loss 3.4321815967559814\n",
      "Epoch 5: |          | 1201/? [17:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1201, loss 4.074164390563965\n",
      "Epoch 5: |          | 1202/? [17:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1202, loss 3.753650188446045\n",
      "Epoch 5: |          | 1203/? [17:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1203, loss 3.7364342212677\n",
      "Epoch 5: |          | 1204/? [17:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1204, loss 3.291067123413086\n",
      "Epoch 5: |          | 1205/? [17:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1205, loss 3.8842759132385254\n",
      "Epoch 5: |          | 1206/? [17:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1206, loss 3.869366407394409\n",
      "Epoch 5: |          | 1207/? [17:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1207, loss 4.15000581741333\n",
      "Epoch 5: |          | 1208/? [17:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1208, loss 4.344483375549316\n",
      "Epoch 5: |          | 1209/? [17:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1209, loss 3.915381908416748\n",
      "Epoch 5: |          | 1210/? [17:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1210, loss 4.1891188621521\n",
      "Epoch 5: |          | 1211/? [17:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1211, loss 4.182585716247559\n",
      "Epoch 5: |          | 1212/? [17:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1212, loss 3.9731528759002686\n",
      "Epoch 5: |          | 1213/? [17:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1213, loss 3.710197925567627\n",
      "Epoch 5: |          | 1214/? [17:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1214, loss 4.277496814727783\n",
      "Epoch 5: |          | 1215/? [17:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1215, loss 3.737694501876831\n",
      "Epoch 5: |          | 1216/? [17:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1216, loss 3.8604259490966797\n",
      "Epoch 5: |          | 1217/? [17:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1217, loss 3.971851348876953\n",
      "Epoch 5: |          | 1218/? [17:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1218, loss 4.0633344650268555\n",
      "Epoch 5: |          | 1219/? [17:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1219, loss 3.70497465133667\n",
      "Epoch 5: |          | 1220/? [17:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1220, loss 4.468039035797119\n",
      "Epoch 5: |          | 1221/? [17:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1221, loss 3.9592297077178955\n",
      "Epoch 5: |          | 1222/? [17:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1222, loss 3.085829734802246\n",
      "Epoch 5: |          | 1223/? [17:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1223, loss 3.306130886077881\n",
      "Epoch 5: |          | 1224/? [17:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1224, loss 3.6491215229034424\n",
      "Epoch 5: |          | 1225/? [17:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1225, loss 4.308887958526611\n",
      "Epoch 5: |          | 1226/? [17:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1226, loss 4.2836527824401855\n",
      "Epoch 5: |          | 1227/? [17:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1227, loss 3.913504123687744\n",
      "Epoch 5: |          | 1228/? [17:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1228, loss 3.770263671875\n",
      "Epoch 5: |          | 1229/? [17:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1229, loss 3.3973910808563232\n",
      "Epoch 5: |          | 1230/? [17:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1230, loss 4.053592681884766\n",
      "Epoch 5: |          | 1231/? [17:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1231, loss 4.066941261291504\n",
      "Epoch 5: |          | 1232/? [17:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1232, loss 4.2267537117004395\n",
      "Epoch 5: |          | 1233/? [17:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1233, loss 4.044736385345459\n",
      "Epoch 5: |          | 1234/? [17:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1234, loss 3.043109893798828\n",
      "Epoch 5: |          | 1235/? [17:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1235, loss 4.129830360412598\n",
      "Epoch 5: |          | 1236/? [17:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1236, loss 3.5421810150146484\n",
      "Epoch 5: |          | 1237/? [17:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1237, loss 3.841400623321533\n",
      "Epoch 5: |          | 1238/? [17:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1238, loss 3.886798858642578\n",
      "Epoch 5: |          | 1239/? [17:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1239, loss 3.756317615509033\n",
      "Epoch 5: |          | 1240/? [17:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1240, loss 4.338730812072754\n",
      "Epoch 5: |          | 1241/? [17:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1241, loss 3.8972511291503906\n",
      "Epoch 5: |          | 1242/? [17:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1242, loss 3.7501537799835205\n",
      "Epoch 5: |          | 1243/? [17:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1243, loss 3.6257781982421875\n",
      "Epoch 5: |          | 1244/? [17:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1244, loss 3.785841464996338\n",
      "Epoch 5: |          | 1245/? [17:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1245, loss 3.3602747917175293\n",
      "Epoch 5: |          | 1246/? [17:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1246, loss 4.034615516662598\n",
      "Epoch 5: |          | 1247/? [17:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1247, loss 4.05521297454834\n",
      "Epoch 5: |          | 1248/? [17:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1248, loss 3.6088593006134033\n",
      "Epoch 5: |          | 1249/? [17:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1249, loss 3.7803261280059814\n",
      "Epoch 5: |          | 1250/? [17:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1250, loss 3.9524714946746826\n",
      "Epoch 5: |          | 1251/? [17:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1251, loss 3.66157603263855\n",
      "Epoch 5: |          | 1252/? [17:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1252, loss 4.4437665939331055\n",
      "Epoch 5: |          | 1253/? [17:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1253, loss 3.789094924926758\n",
      "Epoch 5: |          | 1254/? [17:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1254, loss 3.1372101306915283\n",
      "Epoch 5: |          | 1255/? [17:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1255, loss 4.485318660736084\n",
      "Epoch 5: |          | 1256/? [17:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1256, loss 3.5278420448303223\n",
      "Epoch 5: |          | 1257/? [17:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1257, loss 3.553220748901367\n",
      "Epoch 5: |          | 1258/? [17:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1258, loss 4.258286476135254\n",
      "Epoch 5: |          | 1259/? [17:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1259, loss 3.9360694885253906\n",
      "Epoch 5: |          | 1260/? [17:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1260, loss 4.359763145446777\n",
      "Epoch 5: |          | 1261/? [17:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1261, loss 3.7474124431610107\n",
      "Epoch 5: |          | 1262/? [17:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1262, loss 3.6924376487731934\n",
      "Epoch 5: |          | 1263/? [17:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1263, loss 4.030238151550293\n",
      "Epoch 5: |          | 1264/? [17:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1264, loss 4.297638416290283\n",
      "Epoch 5: |          | 1265/? [17:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1265, loss 4.129683971405029\n",
      "Epoch 5: |          | 1266/? [17:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1266, loss 3.8278942108154297\n",
      "Epoch 5: |          | 1267/? [17:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1267, loss 3.930455446243286\n",
      "Epoch 5: |          | 1268/? [17:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1268, loss 3.797766923904419\n",
      "Epoch 5: |          | 1269/? [17:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1269, loss 3.3305416107177734\n",
      "Epoch 5: |          | 1270/? [18:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1270, loss 3.6793017387390137\n",
      "Epoch 5: |          | 1271/? [18:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1271, loss 3.8706634044647217\n",
      "Epoch 5: |          | 1272/? [18:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1272, loss 3.440235137939453\n",
      "Epoch 5: |          | 1273/? [18:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1273, loss 4.138327598571777\n",
      "Epoch 5: |          | 1274/? [18:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1274, loss 3.0191361904144287\n",
      "Epoch 5: |          | 1275/? [18:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1275, loss 3.581817150115967\n",
      "Epoch 5: |          | 1276/? [18:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1276, loss 3.8842902183532715\n",
      "Epoch 5: |          | 1277/? [18:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1277, loss 3.594712018966675\n",
      "Epoch 5: |          | 1278/? [18:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1278, loss 3.308899402618408\n",
      "Epoch 5: |          | 1279/? [18:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1279, loss 3.9914913177490234\n",
      "Epoch 5: |          | 1280/? [18:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1280, loss 3.2330756187438965\n",
      "Epoch 5: |          | 1281/? [18:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1281, loss 3.700338363647461\n",
      "Epoch 5: |          | 1282/? [18:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1282, loss 3.420848846435547\n",
      "Epoch 5: |          | 1283/? [18:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1283, loss 4.140961647033691\n",
      "Epoch 5: |          | 1284/? [18:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1284, loss 3.27160906791687\n",
      "Epoch 5: |          | 1285/? [18:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1285, loss 4.312085151672363\n",
      "Epoch 5: |          | 1286/? [18:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1286, loss 2.8043105602264404\n",
      "Epoch 5: |          | 1287/? [18:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1287, loss 4.217263698577881\n",
      "Epoch 5: |          | 1288/? [18:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1288, loss 4.0260186195373535\n",
      "Epoch 5: |          | 1289/? [18:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1289, loss 3.175567865371704\n",
      "Epoch 5: |          | 1290/? [18:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1290, loss 3.9394683837890625\n",
      "Epoch 5: |          | 1291/? [18:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1291, loss 4.822595596313477\n",
      "Epoch 5: |          | 1292/? [18:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1292, loss 4.082139492034912\n",
      "Epoch 5: |          | 1293/? [18:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1293, loss 3.6368396282196045\n",
      "Epoch 5: |          | 1294/? [18:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1294, loss 3.862288236618042\n",
      "Epoch 5: |          | 1295/? [18:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1295, loss 3.964071750640869\n",
      "Epoch 5: |          | 1296/? [18:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1296, loss 3.1864051818847656\n",
      "Epoch 5: |          | 1297/? [18:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1297, loss 4.1137800216674805\n",
      "Epoch 5: |          | 1298/? [18:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1298, loss 3.8435089588165283\n",
      "Epoch 5: |          | 1299/? [18:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1299, loss 2.9939799308776855\n",
      "Epoch 5: |          | 1300/? [18:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1300, loss 3.9282803535461426\n",
      "Epoch 5: |          | 1301/? [18:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1301, loss 3.5924649238586426\n",
      "Epoch 5: |          | 1302/? [18:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1302, loss 3.728713274002075\n",
      "Epoch 5: |          | 1303/? [18:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1303, loss 3.6545536518096924\n",
      "Epoch 5: |          | 1304/? [18:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1304, loss 4.407070636749268\n",
      "Epoch 5: |          | 1305/? [18:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1305, loss 3.2215499877929688\n",
      "Epoch 5: |          | 1306/? [18:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1306, loss 3.8489041328430176\n",
      "Epoch 5: |          | 1307/? [18:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1307, loss 3.460237979888916\n",
      "Epoch 5: |          | 1308/? [18:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1308, loss 3.408003330230713\n",
      "Epoch 5: |          | 1309/? [18:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1309, loss 3.4502875804901123\n",
      "Epoch 5: |          | 1310/? [18:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1310, loss 4.015942573547363\n",
      "Epoch 5: |          | 1311/? [18:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1311, loss 3.454573154449463\n",
      "Epoch 5: |          | 1312/? [18:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1312, loss 3.194275379180908\n",
      "Epoch 5: |          | 1313/? [18:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1313, loss 4.351449489593506\n",
      "Epoch 5: |          | 1314/? [18:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1314, loss 3.5675759315490723\n",
      "Epoch 5: |          | 1315/? [18:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1315, loss 4.322350978851318\n",
      "Epoch 5: |          | 1316/? [18:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1316, loss 4.09487247467041\n",
      "Epoch 5: |          | 1317/? [18:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1317, loss 3.7233452796936035\n",
      "Epoch 5: |          | 1318/? [18:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1318, loss 3.907288074493408\n",
      "Epoch 5: |          | 1319/? [18:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1319, loss 4.09039306640625\n",
      "Epoch 5: |          | 1320/? [18:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1320, loss 3.659252643585205\n",
      "Epoch 5: |          | 1321/? [18:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1321, loss 4.092545986175537\n",
      "Epoch 5: |          | 1322/? [18:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1322, loss 3.984675645828247\n",
      "Epoch 5: |          | 1323/? [18:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1323, loss 3.4897830486297607\n",
      "Epoch 5: |          | 1324/? [18:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1324, loss 4.372549533843994\n",
      "Epoch 5: |          | 1325/? [18:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1325, loss 4.508406639099121\n",
      "Epoch 5: |          | 1326/? [18:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1326, loss 3.967400074005127\n",
      "Epoch 5: |          | 1327/? [18:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1327, loss 3.8295631408691406\n",
      "Epoch 5: |          | 1328/? [18:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1328, loss 3.5232748985290527\n",
      "Epoch 5: |          | 1329/? [18:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1329, loss 4.1039509773254395\n",
      "Epoch 5: |          | 1330/? [18:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1330, loss 3.8477516174316406\n",
      "Epoch 5: |          | 1331/? [18:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1331, loss 3.95569109916687\n",
      "Epoch 5: |          | 1332/? [18:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1332, loss 3.6657822132110596\n",
      "Epoch 5: |          | 1333/? [18:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1333, loss 3.6398282051086426\n",
      "Epoch 5: |          | 1334/? [18:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1334, loss 3.7648987770080566\n",
      "Epoch 5: |          | 1335/? [18:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1335, loss 3.729703187942505\n",
      "Epoch 5: |          | 1336/? [18:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1336, loss 3.2983527183532715\n",
      "Epoch 5: |          | 1337/? [18:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1337, loss 3.9415295124053955\n",
      "Epoch 5: |          | 1338/? [18:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1338, loss 3.1940886974334717\n",
      "Epoch 5: |          | 1339/? [18:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1339, loss 3.8289380073547363\n",
      "Epoch 5: |          | 1340/? [19:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1340, loss 3.2736923694610596\n",
      "Epoch 5: |          | 1341/? [19:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1341, loss 4.0116071701049805\n",
      "Epoch 5: |          | 1342/? [19:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1342, loss 4.234490871429443\n",
      "Epoch 5: |          | 1343/? [19:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1343, loss 3.744115114212036\n",
      "Epoch 5: |          | 1344/? [19:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1344, loss 3.850989580154419\n",
      "Epoch 5: |          | 1345/? [19:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1345, loss 4.0203704833984375\n",
      "Epoch 5: |          | 1346/? [19:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1346, loss 5.060303688049316\n",
      "Epoch 5: |          | 1347/? [19:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1347, loss 4.04379940032959\n",
      "Epoch 5: |          | 1348/? [19:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1348, loss 4.118295192718506\n",
      "Epoch 5: |          | 1349/? [19:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1349, loss 4.026766300201416\n",
      "Epoch 5: |          | 1350/? [19:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1350, loss 4.080802917480469\n",
      "Epoch 5: |          | 1351/? [19:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1351, loss 4.039071559906006\n",
      "Epoch 5: |          | 1352/? [19:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1352, loss 3.3001904487609863\n",
      "Epoch 5: |          | 1353/? [19:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1353, loss 3.6349034309387207\n",
      "Epoch 5: |          | 1354/? [19:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1354, loss 4.043126583099365\n",
      "Epoch 5: |          | 1355/? [19:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1355, loss 4.191237449645996\n",
      "Epoch 5: |          | 1356/? [19:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1356, loss 3.9338932037353516\n",
      "Epoch 5: |          | 1357/? [19:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1357, loss 3.7049717903137207\n",
      "Epoch 5: |          | 1358/? [19:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1358, loss 3.857351779937744\n",
      "Epoch 5: |          | 1359/? [19:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1359, loss 3.760657787322998\n",
      "Epoch 5: |          | 1360/? [19:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1360, loss 3.925090789794922\n",
      "Epoch 5: |          | 1361/? [19:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1361, loss 3.8874378204345703\n",
      "Epoch 5: |          | 1362/? [19:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1362, loss 3.7617557048797607\n",
      "Epoch 5: |          | 1363/? [19:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1363, loss 3.186533212661743\n",
      "Epoch 5: |          | 1364/? [19:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1364, loss 3.6692652702331543\n",
      "Epoch 5: |          | 1365/? [19:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1365, loss 3.3902251720428467\n",
      "Epoch 5: |          | 1366/? [19:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1366, loss 4.093930244445801\n",
      "Epoch 5: |          | 1367/? [19:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1367, loss 3.401118516921997\n",
      "Epoch 5: |          | 1368/? [19:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1368, loss 3.1750595569610596\n",
      "Epoch 5: |          | 1369/? [19:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1369, loss 3.8781368732452393\n",
      "Epoch 5: |          | 1370/? [19:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1370, loss 3.400053024291992\n",
      "Epoch 5: |          | 1371/? [19:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1371, loss 4.346445083618164\n",
      "Epoch 5: |          | 1372/? [19:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1372, loss 3.786625623703003\n",
      "Epoch 5: |          | 1373/? [19:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1373, loss 4.17722225189209\n",
      "Epoch 5: |          | 1374/? [19:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1374, loss 3.29707407951355\n",
      "Epoch 5: |          | 1375/? [19:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1375, loss 3.9179351329803467\n",
      "Epoch 5: |          | 1376/? [19:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1376, loss 3.7694270610809326\n",
      "Epoch 5: |          | 1377/? [19:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1377, loss 3.764453172683716\n",
      "Epoch 5: |          | 1378/? [19:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1378, loss 3.758678913116455\n",
      "Epoch 5: |          | 1379/? [19:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1379, loss 3.7738661766052246\n",
      "Epoch 5: |          | 1380/? [19:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1380, loss 3.917789936065674\n",
      "Epoch 5: |          | 1381/? [19:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1381, loss 4.059173583984375\n",
      "Epoch 5: |          | 1382/? [19:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1382, loss 3.5691890716552734\n",
      "Epoch 5: |          | 1383/? [19:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1383, loss 3.8095390796661377\n",
      "Epoch 5: |          | 1384/? [19:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1384, loss 3.805123805999756\n",
      "Epoch 5: |          | 1385/? [19:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1385, loss 3.777468204498291\n",
      "Epoch 5: |          | 1386/? [19:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1386, loss 3.8840270042419434\n",
      "Epoch 5: |          | 1387/? [19:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1387, loss 3.828665256500244\n",
      "Epoch 5: |          | 1388/? [19:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1388, loss 3.3364224433898926\n",
      "Epoch 5: |          | 1389/? [19:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1389, loss 4.024512767791748\n",
      "Epoch 5: |          | 1390/? [19:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1390, loss 4.311964988708496\n",
      "Epoch 5: |          | 1391/? [19:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1391, loss 3.9469780921936035\n",
      "Epoch 5: |          | 1392/? [19:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1392, loss 3.432903289794922\n",
      "Epoch 5: |          | 1393/? [19:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1393, loss 3.6023049354553223\n",
      "Epoch 5: |          | 1394/? [19:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1394, loss 3.2143428325653076\n",
      "Epoch 5: |          | 1395/? [19:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1395, loss 3.9483795166015625\n",
      "Epoch 5: |          | 1396/? [19:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1396, loss 3.820774793624878\n",
      "Epoch 5: |          | 1397/? [19:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1397, loss 3.0810916423797607\n",
      "Epoch 5: |          | 1398/? [19:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1398, loss 4.277445316314697\n",
      "Epoch 5: |          | 1399/? [19:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1399, loss 4.306779861450195\n",
      "Epoch 5: |          | 1400/? [19:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1400, loss 3.419177293777466\n",
      "Epoch 5: |          | 1401/? [19:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1401, loss 4.146852016448975\n",
      "Epoch 5: |          | 1402/? [19:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1402, loss 3.80171537399292\n",
      "Epoch 5: |          | 1403/? [19:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1403, loss 4.008700847625732\n",
      "Epoch 5: |          | 1404/? [19:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1404, loss 3.9647724628448486\n",
      "Epoch 5: |          | 1405/? [19:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1405, loss 4.2830281257629395\n",
      "Epoch 5: |          | 1406/? [20:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1406, loss 4.213959217071533\n",
      "Epoch 5: |          | 1407/? [20:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1407, loss 4.272942066192627\n",
      "Epoch 5: |          | 1408/? [20:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1408, loss 3.510540723800659\n",
      "Epoch 5: |          | 1409/? [20:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1409, loss 3.5589759349823\n",
      "Epoch 5: |          | 1410/? [20:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1410, loss 3.632153272628784\n",
      "Epoch 5: |          | 1411/? [20:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1411, loss 3.937504291534424\n",
      "Epoch 5: |          | 1412/? [20:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1412, loss 3.613020420074463\n",
      "Epoch 5: |          | 1413/? [20:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1413, loss 3.4666781425476074\n",
      "Epoch 5: |          | 1414/? [20:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1414, loss 3.6185145378112793\n",
      "Epoch 5: |          | 1415/? [20:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1415, loss 3.8686842918395996\n",
      "Epoch 5: |          | 1416/? [20:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1416, loss 4.237491130828857\n",
      "Epoch 5: |          | 1417/? [20:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1417, loss 3.9144248962402344\n",
      "Epoch 5: |          | 1418/? [20:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1418, loss 4.03010892868042\n",
      "Epoch 5: |          | 1419/? [20:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1419, loss 3.7659249305725098\n",
      "Epoch 5: |          | 1420/? [20:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1420, loss 3.640082836151123\n",
      "Epoch 5: |          | 1421/? [20:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1421, loss 3.3982226848602295\n",
      "Epoch 5: |          | 1422/? [20:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1422, loss 4.072895050048828\n",
      "Epoch 5: |          | 1423/? [20:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1423, loss 4.115995407104492\n",
      "Epoch 5: |          | 1424/? [20:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1424, loss 3.743176221847534\n",
      "Epoch 5: |          | 1425/? [20:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1425, loss 3.9845809936523438\n",
      "Epoch 5: |          | 1426/? [20:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1426, loss 3.525202989578247\n",
      "Epoch 5: |          | 1427/? [20:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1427, loss 4.154582977294922\n",
      "Epoch 5: |          | 1428/? [20:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1428, loss 4.117345809936523\n",
      "Epoch 5: |          | 1429/? [20:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1429, loss 4.024020195007324\n",
      "Epoch 5: |          | 1430/? [20:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1430, loss 4.101243019104004\n",
      "Epoch 5: |          | 1431/? [20:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1431, loss 3.860393524169922\n",
      "Epoch 5: |          | 1432/? [20:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1432, loss 3.9025161266326904\n",
      "Epoch 5: |          | 1433/? [20:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1433, loss 3.8120949268341064\n",
      "Epoch 5: |          | 1434/? [20:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1434, loss 3.957380771636963\n",
      "Epoch 5: |          | 1435/? [20:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1435, loss 3.5540013313293457\n",
      "Epoch 5: |          | 1436/? [20:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1436, loss 3.9033408164978027\n",
      "Epoch 5: |          | 1437/? [20:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1437, loss 3.1671411991119385\n",
      "Epoch 5: |          | 1438/? [20:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1438, loss 4.806981563568115\n",
      "Epoch 5: |          | 1439/? [20:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1439, loss 3.901794910430908\n",
      "Epoch 5: |          | 1440/? [20:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1440, loss 4.063325881958008\n",
      "Epoch 5: |          | 1441/? [20:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1441, loss 4.3686699867248535\n",
      "Epoch 5: |          | 1442/? [20:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1442, loss 4.387486457824707\n",
      "Epoch 5: |          | 1443/? [20:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1443, loss 3.553157329559326\n",
      "Epoch 5: |          | 1444/? [20:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1444, loss 3.5885791778564453\n",
      "Epoch 5: |          | 1445/? [20:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1445, loss 4.0842061042785645\n",
      "Epoch 5: |          | 1446/? [20:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1446, loss 3.634345293045044\n",
      "Epoch 5: |          | 1447/? [20:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1447, loss 3.769650936126709\n",
      "Epoch 5: |          | 1448/? [20:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1448, loss 3.6441948413848877\n",
      "Epoch 5: |          | 1449/? [20:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1449, loss 3.862459182739258\n",
      "Epoch 5: |          | 1450/? [20:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1450, loss 3.9071033000946045\n",
      "Epoch 5: |          | 1451/? [20:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1451, loss 4.235201358795166\n",
      "Epoch 5: |          | 1452/? [20:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1452, loss 3.89996075630188\n",
      "Epoch 5: |          | 1453/? [20:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1453, loss 3.269923448562622\n",
      "Epoch 5: |          | 1454/? [20:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1454, loss 3.852682113647461\n",
      "Epoch 5: |          | 1455/? [20:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1455, loss 3.980093479156494\n",
      "Epoch 5: |          | 1456/? [20:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1456, loss 3.5143446922302246\n",
      "Epoch 5: |          | 1457/? [20:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1457, loss 3.6647186279296875\n",
      "Epoch 5: |          | 1458/? [20:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1458, loss 3.8731937408447266\n",
      "Epoch 5: |          | 1459/? [20:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1459, loss 4.032203674316406\n",
      "Epoch 5: |          | 1460/? [20:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1460, loss 3.9556286334991455\n",
      "Epoch 5: |          | 1461/? [20:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1461, loss 3.90598726272583\n",
      "Epoch 5: |          | 1462/? [20:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1462, loss 4.22097635269165\n",
      "Epoch 5: |          | 1463/? [20:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1463, loss 4.120150566101074\n",
      "Epoch 5: |          | 1464/? [20:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1464, loss 3.3837146759033203\n",
      "Epoch 5: |          | 1465/? [20:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1465, loss 3.835073471069336\n",
      "Epoch 5: |          | 1466/? [20:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1466, loss 3.4918951988220215\n",
      "Epoch 5: |          | 1467/? [20:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1467, loss 4.175337791442871\n",
      "Epoch 5: |          | 1468/? [20:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1468, loss 3.6973583698272705\n",
      "Epoch 5: |          | 1469/? [20:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1469, loss 3.4590213298797607\n",
      "Epoch 5: |          | 1470/? [20:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1470, loss 3.9427273273468018\n",
      "Epoch 5: |          | 1471/? [20:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1471, loss 4.2017741203308105\n",
      "Epoch 5: |          | 1472/? [20:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1472, loss 3.975189208984375\n",
      "Epoch 5: |          | 1473/? [20:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1473, loss 3.759004592895508\n",
      "Epoch 5: |          | 1474/? [20:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1474, loss 3.6781506538391113\n",
      "Epoch 5: |          | 1475/? [20:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1475, loss 3.2687935829162598\n",
      "Epoch 5: |          | 1476/? [20:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1476, loss 3.858483076095581\n",
      "Epoch 5: |          | 1477/? [21:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1477, loss 3.879591464996338\n",
      "Epoch 5: |          | 1478/? [21:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1478, loss 3.7608230113983154\n",
      "Epoch 5: |          | 1479/? [21:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1479, loss 4.299456596374512\n",
      "Epoch 5: |          | 1480/? [21:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1480, loss 4.015915870666504\n",
      "Epoch 5: |          | 1481/? [21:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1481, loss 3.760587215423584\n",
      "Epoch 5: |          | 1482/? [21:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1482, loss 3.939940929412842\n",
      "Epoch 5: |          | 1483/? [21:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1483, loss 3.5370445251464844\n",
      "Epoch 5: |          | 1484/? [21:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1484, loss 3.7293834686279297\n",
      "Epoch 5: |          | 1485/? [21:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1485, loss 3.9005672931671143\n",
      "Epoch 5: |          | 1486/? [21:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1486, loss 3.926158905029297\n",
      "Epoch 5: |          | 1487/? [21:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1487, loss 3.3346946239471436\n",
      "Epoch 5: |          | 1488/? [21:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1488, loss 4.020434379577637\n",
      "Epoch 5: |          | 1489/? [21:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1489, loss 3.9554505348205566\n",
      "Epoch 5: |          | 1490/? [21:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1490, loss 3.7989590167999268\n",
      "Epoch 5: |          | 1491/? [21:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1491, loss 2.8507766723632812\n",
      "Epoch 5: |          | 1492/? [21:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1492, loss 3.4220099449157715\n",
      "Epoch 5: |          | 1493/? [21:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1493, loss 2.9859941005706787\n",
      "Epoch 5: |          | 1494/? [21:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1494, loss 3.7681221961975098\n",
      "Epoch 5: |          | 1495/? [21:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1495, loss 3.6613776683807373\n",
      "Epoch 5: |          | 1496/? [21:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1496, loss 3.9776065349578857\n",
      "Epoch 5: |          | 1497/? [21:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1497, loss 3.2252285480499268\n",
      "Epoch 5: |          | 1498/? [21:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1498, loss 3.558619737625122\n",
      "Epoch 5: |          | 1499/? [21:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1499, loss 4.083152770996094\n",
      "Epoch 5: |          | 1500/? [21:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1500, loss 4.003640174865723\n",
      "Epoch 5: |          | 1501/? [21:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1501, loss 3.869657516479492\n",
      "Epoch 5: |          | 1502/? [21:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1502, loss 3.919888734817505\n",
      "Epoch 5: |          | 1503/? [21:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1503, loss 3.689218044281006\n",
      "Epoch 5: |          | 1504/? [21:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1504, loss 4.377991199493408\n",
      "Epoch 5: |          | 1505/? [21:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1505, loss 4.145862579345703\n",
      "Epoch 5: |          | 1506/? [21:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1506, loss 3.7529265880584717\n",
      "Epoch 5: |          | 1507/? [21:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1507, loss 3.655362367630005\n",
      "Epoch 5: |          | 1508/? [21:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1508, loss 3.815438747406006\n",
      "Epoch 5: |          | 1509/? [21:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1509, loss 3.7818779945373535\n",
      "Epoch 5: |          | 1510/? [21:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1510, loss 4.057772159576416\n",
      "Epoch 5: |          | 1511/? [21:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1511, loss 3.6528868675231934\n",
      "Epoch 5: |          | 1512/? [21:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1512, loss 4.234318256378174\n",
      "Epoch 5: |          | 1513/? [21:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1513, loss 4.458718776702881\n",
      "Epoch 5: |          | 1514/? [21:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1514, loss 3.5227456092834473\n",
      "Epoch 5: |          | 1515/? [21:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1515, loss 4.367320537567139\n",
      "Epoch 5: |          | 1516/? [21:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1516, loss 4.299534797668457\n",
      "Epoch 5: |          | 1517/? [21:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1517, loss 3.755786895751953\n",
      "Epoch 5: |          | 1518/? [21:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1518, loss 3.5314903259277344\n",
      "Epoch 5: |          | 1519/? [21:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1519, loss 4.062527656555176\n",
      "Epoch 5: |          | 1520/? [21:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1520, loss 4.125330924987793\n",
      "Epoch 5: |          | 1521/? [21:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1521, loss 3.8550667762756348\n",
      "Epoch 5: |          | 1522/? [21:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1522, loss 3.6256840229034424\n",
      "Epoch 5: |          | 1523/? [21:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1523, loss 3.9369568824768066\n",
      "Epoch 5: |          | 1524/? [21:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1524, loss 3.8223350048065186\n",
      "Epoch 5: |          | 1525/? [21:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1525, loss 3.515160322189331\n",
      "Epoch 5: |          | 1526/? [21:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1526, loss 4.091855525970459\n",
      "Epoch 5: |          | 1527/? [21:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1527, loss 4.096503734588623\n",
      "Epoch 5: |          | 1528/? [21:43<00:00,  1.17it/s, v_num=30]ERROR: Input has inproper shape\n",
      "Epoch 5: |          | 1529/? [21:43<00:00,  1.17it/s, v_num=30]   VALIDATION: Batch 0, loss 4.616659164428711\n",
      "   VALIDATION: Batch 1, loss 3.5860581398010254\n",
      "   VALIDATION: Batch 2, loss 4.769078254699707\n",
      "   VALIDATION: Batch 3, loss 4.485729217529297\n",
      "   VALIDATION: Batch 4, loss 4.035737991333008\n",
      "   VALIDATION: Batch 5, loss 3.7202529907226562\n",
      "   VALIDATION: Batch 6, loss 4.031302452087402\n",
      "   VALIDATION: Batch 7, loss 4.644421577453613\n",
      "   VALIDATION: Batch 8, loss 4.521975517272949\n",
      "   VALIDATION: Batch 9, loss 4.630923748016357\n",
      "   VALIDATION: Batch 10, loss 4.348220348358154\n",
      "   VALIDATION: Batch 11, loss 3.964409351348877\n",
      "   VALIDATION: Batch 12, loss 4.210890293121338\n",
      "   VALIDATION: Batch 13, loss 4.737516403198242\n",
      "   VALIDATION: Batch 14, loss 4.11002254486084\n",
      "   VALIDATION: Batch 15, loss 3.9794628620147705\n",
      "   VALIDATION: Batch 16, loss 4.560906410217285\n",
      "   VALIDATION: Batch 17, loss 4.202866554260254\n",
      "   VALIDATION: Batch 18, loss 3.5540974140167236\n",
      "   VALIDATION: Batch 19, loss 4.476213455200195\n",
      "   VALIDATION: Batch 20, loss 4.734292507171631\n",
      "   VALIDATION: Batch 21, loss 5.040684223175049\n",
      "   VALIDATION: Batch 22, loss 4.611636161804199\n",
      "   VALIDATION: Batch 23, loss 4.108048915863037\n",
      "   VALIDATION: Batch 24, loss 3.9846701622009277\n",
      "   VALIDATION: Batch 25, loss 4.414704322814941\n",
      "   VALIDATION: Batch 26, loss 4.613669395446777\n",
      "   VALIDATION: Batch 27, loss 4.525053977966309\n",
      "   VALIDATION: Batch 28, loss 4.285621643066406\n",
      "   VALIDATION: Batch 29, loss 4.477560997009277\n",
      "   VALIDATION: Batch 30, loss 4.05474328994751\n",
      "   VALIDATION: Batch 31, loss 4.372332572937012\n",
      "   VALIDATION: Batch 32, loss 4.931426048278809\n",
      "   VALIDATION: Batch 33, loss 3.1271109580993652\n",
      "   VALIDATION: Batch 34, loss 4.337192058563232\n",
      "   VALIDATION: Batch 35, loss 4.576363563537598\n",
      "   VALIDATION: Batch 36, loss 3.8454902172088623\n",
      "   VALIDATION: Batch 37, loss 3.8612029552459717\n",
      "   VALIDATION: Batch 38, loss 3.93408465385437\n",
      "   VALIDATION: Batch 39, loss 4.342052459716797\n",
      "   VALIDATION: Batch 40, loss 4.399829387664795\n",
      "   VALIDATION: Batch 41, loss 3.1918702125549316\n",
      "   VALIDATION: Batch 42, loss 4.3800859451293945\n",
      "   VALIDATION: Batch 43, loss 4.564577102661133\n",
      "   VALIDATION: Batch 44, loss 4.161452293395996\n",
      "   VALIDATION: Batch 45, loss 4.5492048263549805\n",
      "   VALIDATION: Batch 46, loss 3.6912803649902344\n",
      "   VALIDATION: Batch 47, loss 4.721583366394043\n",
      "   VALIDATION: Batch 48, loss 4.841535568237305\n",
      "   VALIDATION: Batch 49, loss 4.434201240539551\n",
      "   VALIDATION: Batch 50, loss 4.425848007202148\n",
      "   VALIDATION: Batch 51, loss 4.868350505828857\n",
      "   VALIDATION: Batch 52, loss 4.045785903930664\n",
      "   VALIDATION: Batch 53, loss 3.9412314891815186\n",
      "   VALIDATION: Batch 54, loss 3.9998183250427246\n",
      "   VALIDATION: Batch 55, loss 4.770514488220215\n",
      "   VALIDATION: Batch 56, loss 4.142985820770264\n",
      "   VALIDATION: Batch 57, loss 5.725391387939453\n",
      "   VALIDATION: Batch 58, loss 4.272589683532715\n",
      "   VALIDATION: Batch 59, loss 3.920121669769287\n",
      "   VALIDATION: Batch 60, loss 3.4973559379577637\n",
      "   VALIDATION: Batch 61, loss 4.326363563537598\n",
      "   VALIDATION: Batch 62, loss 4.2920427322387695\n",
      "   VALIDATION: Batch 63, loss 4.834566116333008\n",
      "   VALIDATION: Batch 64, loss 4.589785575866699\n",
      "   VALIDATION: Batch 65, loss 3.768709897994995\n",
      "   VALIDATION: Batch 66, loss 4.667362213134766\n",
      "   VALIDATION: Batch 67, loss 4.0883097648620605\n",
      "   VALIDATION: Batch 68, loss 4.263945579528809\n",
      "   VALIDATION: Batch 69, loss 4.508142948150635\n",
      "   VALIDATION: Batch 70, loss 4.6598052978515625\n",
      "   VALIDATION: Batch 71, loss 4.172948837280273\n",
      "   VALIDATION: Batch 72, loss 5.044734001159668\n",
      "   VALIDATION: Batch 73, loss 3.829598903656006\n",
      "   VALIDATION: Batch 74, loss 4.4688262939453125\n",
      "   VALIDATION: Batch 75, loss 4.484801292419434\n",
      "   VALIDATION: Batch 76, loss 4.325512886047363\n",
      "   VALIDATION: Batch 77, loss 4.5897417068481445\n",
      "   VALIDATION: Batch 78, loss 4.4684624671936035\n",
      "   VALIDATION: Batch 79, loss 4.370545387268066\n",
      "   VALIDATION: Batch 80, loss 4.460672855377197\n",
      "   VALIDATION: Batch 81, loss 4.225244998931885\n",
      "   VALIDATION: Batch 82, loss 4.595777988433838\n",
      "   VALIDATION: Batch 83, loss 3.84625244140625\n",
      "   VALIDATION: Batch 84, loss 4.57025671005249\n",
      "   VALIDATION: Batch 85, loss 4.218198776245117\n",
      "   VALIDATION: Batch 86, loss 4.250847816467285\n",
      "   VALIDATION: Batch 87, loss 4.139535903930664\n",
      "   VALIDATION: Batch 88, loss 3.7336337566375732\n",
      "   VALIDATION: Batch 89, loss 4.0266194343566895\n",
      "   VALIDATION: Batch 90, loss 4.29284143447876\n",
      "   VALIDATION: Batch 91, loss 4.414512634277344\n",
      "   VALIDATION: Batch 92, loss 4.070432186126709\n",
      "   VALIDATION: Batch 93, loss 4.794718265533447\n",
      "   VALIDATION: Batch 94, loss 4.279991149902344\n",
      "   VALIDATION: Batch 95, loss 3.728848934173584\n",
      "   VALIDATION: Batch 96, loss 4.20482873916626\n",
      "   VALIDATION: Batch 97, loss 3.9304721355438232\n",
      "   VALIDATION: Batch 98, loss 4.564769744873047\n",
      "   VALIDATION: Batch 99, loss 4.625135898590088\n",
      "   VALIDATION: Batch 100, loss 4.980999946594238\n",
      "   VALIDATION: Batch 101, loss 3.590877056121826\n",
      "   VALIDATION: Batch 102, loss 4.977051734924316\n",
      "   VALIDATION: Batch 103, loss 4.912846565246582\n",
      "   VALIDATION: Batch 104, loss 3.8843321800231934\n",
      "   VALIDATION: Batch 105, loss 4.376801490783691\n",
      "   VALIDATION: Batch 106, loss 4.17954683303833\n",
      "   VALIDATION: Batch 107, loss 4.31028413772583\n",
      "   VALIDATION: Batch 108, loss 4.0363969802856445\n",
      "   VALIDATION: Batch 109, loss 4.651716232299805\n",
      "   VALIDATION: Batch 110, loss 4.321569442749023\n",
      "   VALIDATION: Batch 111, loss 4.658534049987793\n",
      "   VALIDATION: Batch 112, loss 5.5064191818237305\n",
      "   VALIDATION: Batch 113, loss 4.828193187713623\n",
      "   VALIDATION: Batch 114, loss 4.605454921722412\n",
      "   VALIDATION: Batch 115, loss 4.042055606842041\n",
      "   VALIDATION: Batch 116, loss 3.9081473350524902\n",
      "   VALIDATION: Batch 117, loss 4.588370323181152\n",
      "   VALIDATION: Batch 118, loss 4.74820613861084\n",
      "   VALIDATION: Batch 119, loss 3.8922500610351562\n",
      "   VALIDATION: Batch 120, loss 3.528573513031006\n",
      "   VALIDATION: Batch 121, loss 3.8478896617889404\n",
      "   VALIDATION: Batch 122, loss 4.244223117828369\n",
      "   VALIDATION: Batch 123, loss 4.246405124664307\n",
      "   VALIDATION: Batch 124, loss 3.6521153450012207\n",
      "   VALIDATION: Batch 125, loss 4.2445292472839355\n",
      "   VALIDATION: Batch 126, loss 4.461812496185303\n",
      "   VALIDATION: Batch 127, loss 4.2647929191589355\n",
      "   VALIDATION: Batch 128, loss 4.382444381713867\n",
      "   VALIDATION: Batch 129, loss 4.042537212371826\n",
      "   VALIDATION: Batch 130, loss 3.635988235473633\n",
      "   VALIDATION: Batch 131, loss 3.6777758598327637\n",
      "   VALIDATION: Batch 132, loss 4.325924396514893\n",
      "   VALIDATION: Batch 133, loss 4.497509956359863\n",
      "   VALIDATION: Batch 134, loss 4.367819786071777\n",
      "   VALIDATION: Batch 135, loss 4.660121440887451\n",
      "   VALIDATION: Batch 136, loss 4.729711055755615\n",
      "   VALIDATION: Batch 137, loss 4.540249824523926\n",
      "   VALIDATION: Batch 138, loss 4.252647876739502\n",
      "   VALIDATION: Batch 139, loss 4.6704421043396\n",
      "   VALIDATION: Batch 140, loss 3.7769057750701904\n",
      "   VALIDATION: Batch 141, loss 4.706900119781494\n",
      "   VALIDATION: Batch 142, loss 3.4250969886779785\n",
      "   VALIDATION: Batch 143, loss 4.226444721221924\n",
      "   VALIDATION: Batch 144, loss 4.490504264831543\n",
      "   VALIDATION: Batch 145, loss 4.299853324890137\n",
      "   VALIDATION: Batch 146, loss 4.118682384490967\n",
      "   VALIDATION: Batch 147, loss 4.43753719329834\n",
      "   VALIDATION: Batch 148, loss 4.567773818969727\n",
      "   VALIDATION: Batch 149, loss 5.024294376373291\n",
      "   VALIDATION: Batch 150, loss 4.616375923156738\n",
      "   VALIDATION: Batch 151, loss 4.912713050842285\n",
      "   VALIDATION: Batch 152, loss 4.261849403381348\n",
      "   VALIDATION: Batch 153, loss 4.49816370010376\n",
      "   VALIDATION: Batch 154, loss 4.3669281005859375\n",
      "   VALIDATION: Batch 155, loss 4.089664936065674\n",
      "   VALIDATION: Batch 156, loss 4.759056568145752\n",
      "   VALIDATION: Batch 157, loss 4.4235639572143555\n",
      "   VALIDATION: Batch 158, loss 3.8547096252441406\n",
      "   VALIDATION: Batch 159, loss 4.336457252502441\n",
      "   VALIDATION: Batch 160, loss 4.6835455894470215\n",
      "   VALIDATION: Batch 161, loss 4.911782741546631\n",
      "   VALIDATION: Batch 162, loss 4.362946510314941\n",
      "   VALIDATION: Batch 163, loss 3.757918119430542\n",
      "   VALIDATION: Batch 164, loss 4.267237663269043\n",
      "   VALIDATION: Batch 165, loss 4.728656768798828\n",
      "   VALIDATION: Batch 166, loss 4.2263569831848145\n",
      "   VALIDATION: Batch 167, loss 4.612901210784912\n",
      "   VALIDATION: Batch 168, loss 3.468646287918091\n",
      "   VALIDATION: Batch 169, loss 4.128080368041992\n",
      "   VALIDATION: Batch 170, loss 4.335967063903809\n",
      "   VALIDATION: Batch 171, loss 4.498202323913574\n",
      "   VALIDATION: Batch 172, loss 4.3205647468566895\n",
      "   VALIDATION: Batch 173, loss 4.191061973571777\n",
      "   VALIDATION: Batch 174, loss 4.60550594329834\n",
      "   VALIDATION: Batch 175, loss 4.403449058532715\n",
      "   VALIDATION: Batch 176, loss 4.199408531188965\n",
      "   VALIDATION: Batch 177, loss 4.158916473388672\n",
      "   VALIDATION: Batch 178, loss 5.226439476013184\n",
      "   VALIDATION: Batch 179, loss 4.48789119720459\n",
      "   VALIDATION: Batch 180, loss 3.9937477111816406\n",
      "   VALIDATION: Batch 181, loss 4.147037506103516\n",
      "   VALIDATION: Batch 182, loss 4.457165718078613\n",
      "   VALIDATION: Batch 183, loss 3.500370502471924\n",
      "   VALIDATION: Batch 184, loss 3.2418384552001953\n",
      "   VALIDATION: Batch 185, loss 4.030407905578613\n",
      "   VALIDATION: Batch 186, loss 3.9476406574249268\n",
      "   VALIDATION: Batch 187, loss 4.163018226623535\n",
      "   VALIDATION: Batch 188, loss 4.5465922355651855\n",
      "   VALIDATION: Batch 189, loss 3.8836822509765625\n",
      "   VALIDATION: Batch 190, loss 3.9519801139831543\n",
      "   VALIDATION: Batch 191, loss 4.451852321624756\n",
      "   VALIDATION: Batch 192, loss 4.805502891540527\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=30]              TRRAINING: Batch 0, loss 4.061451435089111\n",
      "Epoch 6: |          | 1/? [00:01<00:00,  0.88it/s, v_num=30]   TRRAINING: Batch 1, loss 3.6644415855407715\n",
      "Epoch 6: |          | 2/? [00:02<00:00,  0.99it/s, v_num=30]   TRRAINING: Batch 2, loss 3.743065595626831\n",
      "Epoch 6: |          | 3/? [00:02<00:00,  1.03it/s, v_num=30]   TRRAINING: Batch 3, loss 3.434757709503174\n",
      "Epoch 6: |          | 4/? [00:03<00:00,  1.06it/s, v_num=30]   TRRAINING: Batch 4, loss 3.810542583465576\n",
      "Epoch 6: |          | 5/? [00:04<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 5, loss 4.6032514572143555\n",
      "Epoch 6: |          | 6/? [00:05<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 6, loss 4.074629306793213\n",
      "Epoch 6: |          | 7/? [00:06<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 7, loss 3.4147896766662598\n",
      "Epoch 6: |          | 8/? [00:06<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 8, loss 3.5239155292510986\n",
      "Epoch 6: |          | 9/? [00:07<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 9, loss 3.819453477859497\n",
      "Epoch 6: |          | 10/? [00:08<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 10, loss 4.084216117858887\n",
      "Epoch 6: |          | 11/? [00:09<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 11, loss 3.9499893188476562\n",
      "Epoch 6: |          | 12/? [00:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 12, loss 4.486000061035156\n",
      "Epoch 6: |          | 13/? [00:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 13, loss 3.9282212257385254\n",
      "Epoch 6: |          | 14/? [00:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 14, loss 4.108105182647705\n",
      "Epoch 6: |          | 15/? [00:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 15, loss 3.3941187858581543\n",
      "Epoch 6: |          | 16/? [00:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 16, loss 3.233198642730713\n",
      "Epoch 6: |          | 17/? [00:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 17, loss 4.360379695892334\n",
      "Epoch 6: |          | 18/? [00:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 18, loss 3.8778464794158936\n",
      "Epoch 6: |          | 19/? [00:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 19, loss 3.6654324531555176\n",
      "Epoch 6: |          | 20/? [00:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 20, loss 3.941713333129883\n",
      "Epoch 6: |          | 21/? [00:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 21, loss 4.073635578155518\n",
      "Epoch 6: |          | 22/? [00:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 22, loss 3.964611053466797\n",
      "Epoch 6: |          | 23/? [00:19<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 23, loss 3.3410160541534424\n",
      "Epoch 6: |          | 24/? [00:20<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 24, loss 3.9751334190368652\n",
      "Epoch 6: |          | 25/? [00:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 25, loss 3.8261847496032715\n",
      "Epoch 6: |          | 26/? [00:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 26, loss 3.5008111000061035\n",
      "Epoch 6: |          | 27/? [00:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 27, loss 3.485126495361328\n",
      "Epoch 6: |          | 28/? [00:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 28, loss 4.382292747497559\n",
      "Epoch 6: |          | 29/? [00:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 29, loss 3.9240798950195312\n",
      "Epoch 6: |          | 30/? [00:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 30, loss 3.785041093826294\n",
      "Epoch 6: |          | 31/? [00:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 31, loss 4.361396312713623\n",
      "Epoch 6: |          | 32/? [00:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 32, loss 3.9253687858581543\n",
      "Epoch 6: |          | 33/? [00:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 33, loss 3.6986935138702393\n",
      "Epoch 6: |          | 34/? [00:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 34, loss 3.7197399139404297\n",
      "Epoch 6: |          | 35/? [00:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 35, loss 3.195340156555176\n",
      "Epoch 6: |          | 36/? [00:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 36, loss 3.9516468048095703\n",
      "Epoch 6: |          | 37/? [00:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 37, loss 4.137589454650879\n",
      "Epoch 6: |          | 38/? [00:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 38, loss 4.424152851104736\n",
      "Epoch 6: |          | 39/? [00:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 39, loss 4.318172931671143\n",
      "Epoch 6: |          | 40/? [00:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 40, loss 3.7312541007995605\n",
      "Epoch 6: |          | 41/? [00:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 41, loss 3.802567720413208\n",
      "Epoch 6: |          | 42/? [00:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 42, loss 3.5828983783721924\n",
      "Epoch 6: |          | 43/? [00:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 43, loss 3.776057004928589\n",
      "Epoch 6: |          | 44/? [00:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 44, loss 2.968146800994873\n",
      "Epoch 6: |          | 45/? [00:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 45, loss 2.5373082160949707\n",
      "Epoch 6: |          | 46/? [00:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 46, loss 4.361451148986816\n",
      "Epoch 6: |          | 47/? [00:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 47, loss 3.579143524169922\n",
      "Epoch 6: |          | 48/? [00:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 48, loss 3.400421619415283\n",
      "Epoch 6: |          | 49/? [00:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 49, loss 3.898787260055542\n",
      "Epoch 6: |          | 50/? [00:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 50, loss 3.7360033988952637\n",
      "Epoch 6: |          | 51/? [00:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 51, loss 3.6673786640167236\n",
      "Epoch 6: |          | 52/? [00:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 52, loss 4.3719892501831055\n",
      "Epoch 6: |          | 53/? [00:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 53, loss 4.017658710479736\n",
      "Epoch 6: |          | 54/? [00:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 54, loss 3.844278335571289\n",
      "Epoch 6: |          | 55/? [00:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 55, loss 3.870746612548828\n",
      "Epoch 6: |          | 56/? [00:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 56, loss 3.981761932373047\n",
      "Epoch 6: |          | 57/? [00:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 57, loss 3.8934974670410156\n",
      "Epoch 6: |          | 58/? [00:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 58, loss 5.082505226135254\n",
      "Epoch 6: |          | 59/? [00:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 59, loss 4.006229877471924\n",
      "Epoch 6: |          | 60/? [00:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 60, loss 4.158627510070801\n",
      "Epoch 6: |          | 61/? [00:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 61, loss 4.141712665557861\n",
      "Epoch 6: |          | 62/? [00:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 62, loss 3.794005870819092\n",
      "Epoch 6: |          | 63/? [00:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 63, loss 3.990363359451294\n",
      "Epoch 6: |          | 64/? [00:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 64, loss 3.782520294189453\n",
      "Epoch 6: |          | 65/? [00:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 65, loss 3.8304104804992676\n",
      "Epoch 6: |          | 66/? [00:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 66, loss 3.2624824047088623\n",
      "Epoch 6: |          | 67/? [00:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 67, loss 3.9985949993133545\n",
      "Epoch 6: |          | 68/? [00:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 68, loss 3.96565318107605\n",
      "Epoch 6: |          | 69/? [00:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 69, loss 3.887836456298828\n",
      "Epoch 6: |          | 70/? [00:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 70, loss 3.5726895332336426\n",
      "Epoch 6: |          | 71/? [01:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 71, loss 3.5410003662109375\n",
      "Epoch 6: |          | 72/? [01:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 72, loss 3.753293991088867\n",
      "Epoch 6: |          | 73/? [01:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 73, loss 3.9263832569122314\n",
      "Epoch 6: |          | 74/? [01:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 74, loss 3.6675095558166504\n",
      "Epoch 6: |          | 75/? [01:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 75, loss 3.786306858062744\n",
      "Epoch 6: |          | 76/? [01:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 76, loss 3.7881646156311035\n",
      "Epoch 6: |          | 77/? [01:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 77, loss 3.82279634475708\n",
      "Epoch 6: |          | 78/? [01:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 78, loss 3.6488585472106934\n",
      "Epoch 6: |          | 79/? [01:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 79, loss 3.834866762161255\n",
      "Epoch 6: |          | 80/? [01:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 80, loss 3.7415549755096436\n",
      "Epoch 6: |          | 81/? [01:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 81, loss 3.2164719104766846\n",
      "Epoch 6: |          | 82/? [01:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 82, loss 4.01041316986084\n",
      "Epoch 6: |          | 83/? [01:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 83, loss 3.502018451690674\n",
      "Epoch 6: |          | 84/? [01:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 84, loss 3.3361904621124268\n",
      "Epoch 6: |          | 85/? [01:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 85, loss 3.2198052406311035\n",
      "Epoch 6: |          | 86/? [01:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 86, loss 3.368117570877075\n",
      "Epoch 6: |          | 87/? [01:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 87, loss 3.5472378730773926\n",
      "Epoch 6: |          | 88/? [01:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 88, loss 4.192478656768799\n",
      "Epoch 6: |          | 89/? [01:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 89, loss 4.134315013885498\n",
      "Epoch 6: |          | 90/? [01:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 90, loss 3.9129843711853027\n",
      "Epoch 6: |          | 91/? [01:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 91, loss 3.6339023113250732\n",
      "Epoch 6: |          | 92/? [01:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 92, loss 4.07296085357666\n",
      "Epoch 6: |          | 93/? [01:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 93, loss 4.223771095275879\n",
      "Epoch 6: |          | 94/? [01:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 94, loss 4.115176200866699\n",
      "Epoch 6: |          | 95/? [01:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 95, loss 3.914281129837036\n",
      "Epoch 6: |          | 96/? [01:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 96, loss 3.6155433654785156\n",
      "Epoch 6: |          | 97/? [01:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 97, loss 3.4318954944610596\n",
      "Epoch 6: |          | 98/? [01:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 98, loss 3.9668164253234863\n",
      "Epoch 6: |          | 99/? [01:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 99, loss 4.150333881378174\n",
      "Epoch 6: |          | 100/? [01:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 100, loss 4.0984907150268555\n",
      "Epoch 6: |          | 101/? [01:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 101, loss 3.7510452270507812\n",
      "Epoch 6: |          | 102/? [01:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 102, loss 3.7627170085906982\n",
      "Epoch 6: |          | 103/? [01:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 103, loss 3.5947422981262207\n",
      "Epoch 6: |          | 104/? [01:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 104, loss 3.897475481033325\n",
      "Epoch 6: |          | 105/? [01:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 105, loss 3.8451366424560547\n",
      "Epoch 6: |          | 106/? [01:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 106, loss 3.948089122772217\n",
      "Epoch 6: |          | 107/? [01:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 107, loss 3.981790065765381\n",
      "Epoch 6: |          | 108/? [01:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 108, loss 3.945845127105713\n",
      "Epoch 6: |          | 109/? [01:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 109, loss 3.6884894371032715\n",
      "Epoch 6: |          | 110/? [01:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 110, loss 3.8772239685058594\n",
      "Epoch 6: |          | 111/? [01:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 111, loss 4.499351978302002\n",
      "Epoch 6: |          | 112/? [01:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 112, loss 3.2087082862854004\n",
      "Epoch 6: |          | 113/? [01:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 113, loss 2.7999348640441895\n",
      "Epoch 6: |          | 114/? [01:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 114, loss 4.091457843780518\n",
      "Epoch 6: |          | 115/? [01:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 115, loss 4.22626256942749\n",
      "Epoch 6: |          | 116/? [01:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 116, loss 3.397486925125122\n",
      "Epoch 6: |          | 117/? [01:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 117, loss 3.3276703357696533\n",
      "Epoch 6: |          | 118/? [01:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 118, loss 4.135263442993164\n",
      "Epoch 6: |          | 119/? [01:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 119, loss 4.3480730056762695\n",
      "Epoch 6: |          | 120/? [01:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 120, loss 4.152884483337402\n",
      "Epoch 6: |          | 121/? [01:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 121, loss 3.8689987659454346\n",
      "Epoch 6: |          | 122/? [01:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 122, loss 3.3493239879608154\n",
      "Epoch 6: |          | 123/? [01:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 123, loss 3.8428053855895996\n",
      "Epoch 6: |          | 124/? [01:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 124, loss 3.950984477996826\n",
      "Epoch 6: |          | 125/? [01:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 125, loss 3.622351884841919\n",
      "Epoch 6: |          | 126/? [01:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 126, loss 4.1002020835876465\n",
      "Epoch 6: |          | 127/? [01:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 127, loss 4.229272365570068\n",
      "Epoch 6: |          | 128/? [01:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 128, loss 3.3461391925811768\n",
      "Epoch 6: |          | 129/? [01:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 129, loss 3.9333271980285645\n",
      "Epoch 6: |          | 130/? [01:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 130, loss 3.071063756942749\n",
      "Epoch 6: |          | 131/? [01:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 131, loss 3.8773128986358643\n",
      "Epoch 6: |          | 132/? [01:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 132, loss 3.940216064453125\n",
      "Epoch 6: |          | 133/? [01:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 133, loss 3.8181655406951904\n",
      "Epoch 6: |          | 134/? [01:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 134, loss 4.031886100769043\n",
      "Epoch 6: |          | 135/? [01:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 135, loss 4.031216621398926\n",
      "Epoch 6: |          | 136/? [01:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 136, loss 4.0916547775268555\n",
      "Epoch 6: |          | 137/? [01:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 137, loss 3.1075432300567627\n",
      "Epoch 6: |          | 138/? [01:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 138, loss 3.7214255332946777\n",
      "Epoch 6: |          | 139/? [01:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 139, loss 4.118870258331299\n",
      "Epoch 6: |          | 140/? [01:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 140, loss 3.4171595573425293\n",
      "Epoch 6: |          | 141/? [01:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 141, loss 3.513894557952881\n",
      "Epoch 6: |          | 142/? [01:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 142, loss 4.93520450592041\n",
      "Epoch 6: |          | 143/? [02:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 143, loss 4.575657844543457\n",
      "Epoch 6: |          | 144/? [02:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 144, loss 3.8723807334899902\n",
      "Epoch 6: |          | 145/? [02:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 145, loss 3.347522020339966\n",
      "Epoch 6: |          | 146/? [02:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 146, loss 3.496666669845581\n",
      "Epoch 6: |          | 147/? [02:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 147, loss 3.819777011871338\n",
      "Epoch 6: |          | 148/? [02:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 148, loss 3.553410053253174\n",
      "Epoch 6: |          | 149/? [02:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 149, loss 3.165980815887451\n",
      "Epoch 6: |          | 150/? [02:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 150, loss 4.0060858726501465\n",
      "Epoch 6: |          | 151/? [02:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 151, loss 4.099482536315918\n",
      "Epoch 6: |          | 152/? [02:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 152, loss 4.054040908813477\n",
      "Epoch 6: |          | 153/? [02:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 153, loss 3.1809496879577637\n",
      "Epoch 6: |          | 154/? [02:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 154, loss 4.388091564178467\n",
      "Epoch 6: |          | 155/? [02:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 155, loss 3.911215305328369\n",
      "Epoch 6: |          | 156/? [02:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 156, loss 3.2918267250061035\n",
      "Epoch 6: |          | 157/? [02:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 157, loss 3.983262300491333\n",
      "Epoch 6: |          | 158/? [02:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 158, loss 4.084322452545166\n",
      "Epoch 6: |          | 159/? [02:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 159, loss 3.803849458694458\n",
      "Epoch 6: |          | 160/? [02:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 160, loss 3.5671725273132324\n",
      "Epoch 6: |          | 161/? [02:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 161, loss 3.9406135082244873\n",
      "Epoch 6: |          | 162/? [02:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 162, loss 3.968742847442627\n",
      "Epoch 6: |          | 163/? [02:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 163, loss 3.100761890411377\n",
      "Epoch 6: |          | 164/? [02:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 164, loss 3.54943585395813\n",
      "Epoch 6: |          | 165/? [02:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 165, loss 4.342947959899902\n",
      "Epoch 6: |          | 166/? [02:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 166, loss 4.004494667053223\n",
      "Epoch 6: |          | 167/? [02:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 167, loss 4.118227481842041\n",
      "Epoch 6: |          | 168/? [02:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 168, loss 3.672853946685791\n",
      "Epoch 6: |          | 169/? [02:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 169, loss 3.3174824714660645\n",
      "Epoch 6: |          | 170/? [02:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 170, loss 3.5556533336639404\n",
      "Epoch 6: |          | 171/? [02:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 171, loss 3.8939831256866455\n",
      "Epoch 6: |          | 172/? [02:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 172, loss 3.7053322792053223\n",
      "Epoch 6: |          | 173/? [02:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 173, loss 4.377001762390137\n",
      "Epoch 6: |          | 174/? [02:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 174, loss 3.9600512981414795\n",
      "Epoch 6: |          | 175/? [02:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 175, loss 4.4392218589782715\n",
      "Epoch 6: |          | 176/? [02:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 176, loss 3.667931079864502\n",
      "Epoch 6: |          | 177/? [02:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 177, loss 3.6790664196014404\n",
      "Epoch 6: |          | 178/? [02:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 178, loss 3.526169538497925\n",
      "Epoch 6: |          | 179/? [02:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 179, loss 4.2523193359375\n",
      "Epoch 6: |          | 180/? [02:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 180, loss 3.7562756538391113\n",
      "Epoch 6: |          | 181/? [02:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 181, loss 3.5683658123016357\n",
      "Epoch 6: |          | 182/? [02:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 182, loss 3.9586358070373535\n",
      "Epoch 6: |          | 183/? [02:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 183, loss 3.4546732902526855\n",
      "Epoch 6: |          | 184/? [02:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 184, loss 3.654327392578125\n",
      "Epoch 6: |          | 185/? [02:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 185, loss 4.158588886260986\n",
      "Epoch 6: |          | 186/? [02:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 186, loss 3.705252170562744\n",
      "Epoch 6: |          | 187/? [02:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 187, loss 4.226486682891846\n",
      "Epoch 6: |          | 188/? [02:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 188, loss 3.5924301147460938\n",
      "Epoch 6: |          | 189/? [02:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 189, loss 4.219719409942627\n",
      "Epoch 6: |          | 190/? [02:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 190, loss 3.77410626411438\n",
      "Epoch 6: |          | 191/? [02:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 191, loss 4.510709285736084\n",
      "Epoch 6: |          | 192/? [02:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 192, loss 4.2289862632751465\n",
      "Epoch 6: |          | 193/? [02:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 193, loss 3.628303050994873\n",
      "Epoch 6: |          | 194/? [02:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 194, loss 3.525001049041748\n",
      "Epoch 6: |          | 195/? [02:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 195, loss 4.213120937347412\n",
      "Epoch 6: |          | 196/? [02:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 196, loss 4.08402156829834\n",
      "Epoch 6: |          | 197/? [02:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 197, loss 3.8955256938934326\n",
      "Epoch 6: |          | 198/? [02:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 198, loss 3.2606539726257324\n",
      "Epoch 6: |          | 199/? [02:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 199, loss 4.166320323944092\n",
      "Epoch 6: |          | 200/? [02:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 200, loss 3.6391379833221436\n",
      "Epoch 6: |          | 201/? [02:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 201, loss 3.896441698074341\n",
      "Epoch 6: |          | 202/? [02:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 202, loss 4.0364670753479\n",
      "Epoch 6: |          | 203/? [02:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 203, loss 3.6770730018615723\n",
      "Epoch 6: |          | 204/? [02:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 204, loss 3.693129301071167\n",
      "Epoch 6: |          | 205/? [02:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 205, loss 3.597216844558716\n",
      "Epoch 6: |          | 206/? [02:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 206, loss 3.4794998168945312\n",
      "Epoch 6: |          | 207/? [02:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 207, loss 3.984473466873169\n",
      "Epoch 6: |          | 208/? [02:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 208, loss 3.913796901702881\n",
      "Epoch 6: |          | 209/? [02:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 209, loss 3.669999361038208\n",
      "Epoch 6: |          | 210/? [02:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 210, loss 4.36581563949585\n",
      "Epoch 6: |          | 211/? [02:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 211, loss 3.755160093307495\n",
      "Epoch 6: |          | 212/? [02:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 212, loss 3.974013090133667\n",
      "Epoch 6: |          | 213/? [02:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 213, loss 3.8117878437042236\n",
      "Epoch 6: |          | 214/? [02:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 214, loss 3.68983793258667\n",
      "Epoch 6: |          | 215/? [03:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 215, loss 3.356240749359131\n",
      "Epoch 6: |          | 216/? [03:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 216, loss 3.997091293334961\n",
      "Epoch 6: |          | 217/? [03:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 217, loss 3.9503719806671143\n",
      "Epoch 6: |          | 218/? [03:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 218, loss 3.935321092605591\n",
      "Epoch 6: |          | 219/? [03:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 219, loss 3.892421007156372\n",
      "Epoch 6: |          | 220/? [03:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 220, loss 3.9552650451660156\n",
      "Epoch 6: |          | 221/? [03:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 221, loss 3.8081507682800293\n",
      "Epoch 6: |          | 222/? [03:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 222, loss 3.041257381439209\n",
      "Epoch 6: |          | 223/? [03:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 223, loss 4.043474197387695\n",
      "Epoch 6: |          | 224/? [03:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 224, loss 4.119284629821777\n",
      "Epoch 6: |          | 225/? [03:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 225, loss 3.8717613220214844\n",
      "Epoch 6: |          | 226/? [03:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 226, loss 3.7825095653533936\n",
      "Epoch 6: |          | 227/? [03:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 227, loss 4.123710632324219\n",
      "Epoch 6: |          | 228/? [03:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 228, loss 3.8353397846221924\n",
      "Epoch 6: |          | 229/? [03:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 229, loss 3.887293577194214\n",
      "Epoch 6: |          | 230/? [03:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 230, loss 3.8319575786590576\n",
      "Epoch 6: |          | 231/? [03:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 231, loss 3.771191120147705\n",
      "Epoch 6: |          | 232/? [03:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 232, loss 3.4914748668670654\n",
      "Epoch 6: |          | 233/? [03:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 233, loss 4.183366298675537\n",
      "Epoch 6: |          | 234/? [03:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 234, loss 4.179157257080078\n",
      "Epoch 6: |          | 235/? [03:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 235, loss 4.230556011199951\n",
      "Epoch 6: |          | 236/? [03:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 236, loss 3.6948604583740234\n",
      "Epoch 6: |          | 237/? [03:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 237, loss 3.8727409839630127\n",
      "Epoch 6: |          | 238/? [03:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 238, loss 4.118840217590332\n",
      "Epoch 6: |          | 239/? [03:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 239, loss 3.6541266441345215\n",
      "Epoch 6: |          | 240/? [03:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 240, loss 3.2657790184020996\n",
      "Epoch 6: |          | 241/? [03:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 241, loss 3.827643871307373\n",
      "Epoch 6: |          | 242/? [03:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 242, loss 4.223759651184082\n",
      "Epoch 6: |          | 243/? [03:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 243, loss 3.0857839584350586\n",
      "Epoch 6: |          | 244/? [03:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 244, loss 3.5338222980499268\n",
      "Epoch 6: |          | 245/? [03:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 245, loss 3.7778003215789795\n",
      "Epoch 6: |          | 246/? [03:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 246, loss 3.9244141578674316\n",
      "Epoch 6: |          | 247/? [03:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 247, loss 3.9952950477600098\n",
      "Epoch 6: |          | 248/? [03:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 248, loss 3.4962425231933594\n",
      "Epoch 6: |          | 249/? [03:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 249, loss 3.3663172721862793\n",
      "Epoch 6: |          | 250/? [03:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 250, loss 3.968233585357666\n",
      "Epoch 6: |          | 251/? [03:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 251, loss 3.94042706489563\n",
      "Epoch 6: |          | 252/? [03:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 252, loss 3.7357115745544434\n",
      "Epoch 6: |          | 253/? [03:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 253, loss 4.536688804626465\n",
      "Epoch 6: |          | 254/? [03:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 254, loss 4.1779069900512695\n",
      "Epoch 6: |          | 255/? [03:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 255, loss 3.8280887603759766\n",
      "Epoch 6: |          | 256/? [03:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 256, loss 4.58450984954834\n",
      "Epoch 6: |          | 257/? [03:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 257, loss 3.702430009841919\n",
      "Epoch 6: |          | 258/? [03:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 258, loss 3.8170082569122314\n",
      "Epoch 6: |          | 259/? [03:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 259, loss 3.6936404705047607\n",
      "Epoch 6: |          | 260/? [03:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 260, loss 3.6080803871154785\n",
      "Epoch 6: |          | 261/? [03:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 261, loss 3.4282145500183105\n",
      "Epoch 6: |          | 262/? [03:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 262, loss 4.122877597808838\n",
      "Epoch 6: |          | 263/? [03:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 263, loss 3.7752246856689453\n",
      "Epoch 6: |          | 264/? [03:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 264, loss 3.931084156036377\n",
      "Epoch 6: |          | 265/? [03:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 265, loss 3.367875337600708\n",
      "Epoch 6: |          | 266/? [03:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 266, loss 3.8061270713806152\n",
      "Epoch 6: |          | 267/? [03:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 267, loss 3.4441452026367188\n",
      "Epoch 6: |          | 268/? [03:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 268, loss 3.7613697052001953\n",
      "Epoch 6: |          | 269/? [03:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 269, loss 4.126880645751953\n",
      "Epoch 6: |          | 270/? [03:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 270, loss 3.7274956703186035\n",
      "Epoch 6: |          | 271/? [03:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 271, loss 4.010757923126221\n",
      "Epoch 6: |          | 272/? [03:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 272, loss 4.2548322677612305\n",
      "Epoch 6: |          | 273/? [03:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 273, loss 3.4758193492889404\n",
      "Epoch 6: |          | 274/? [03:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 274, loss 4.29019832611084\n",
      "Epoch 6: |          | 275/? [03:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 275, loss 4.007576942443848\n",
      "Epoch 6: |          | 276/? [03:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 276, loss 3.299023151397705\n",
      "Epoch 6: |          | 277/? [03:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 277, loss 3.954998731613159\n",
      "Epoch 6: |          | 278/? [03:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 278, loss 3.0055110454559326\n",
      "Epoch 6: |          | 279/? [03:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 279, loss 3.6828243732452393\n",
      "Epoch 6: |          | 280/? [03:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 280, loss 3.3808250427246094\n",
      "Epoch 6: |          | 281/? [03:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 281, loss 4.040936470031738\n",
      "Epoch 6: |          | 282/? [03:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 282, loss 3.6281700134277344\n",
      "Epoch 6: |          | 283/? [03:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 283, loss 3.6897149085998535\n",
      "Epoch 6: |          | 284/? [03:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 284, loss 3.6372668743133545\n",
      "Epoch 6: |          | 285/? [03:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 285, loss 3.169968843460083\n",
      "Epoch 6: |          | 286/? [04:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 286, loss 3.7366950511932373\n",
      "Epoch 6: |          | 287/? [04:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 287, loss 3.4533209800720215\n",
      "Epoch 6: |          | 288/? [04:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 288, loss 3.4535439014434814\n",
      "Epoch 6: |          | 289/? [04:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 289, loss 3.243593215942383\n",
      "Epoch 6: |          | 290/? [04:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 290, loss 2.860351800918579\n",
      "Epoch 6: |          | 291/? [04:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 291, loss 3.8926074504852295\n",
      "Epoch 6: |          | 292/? [04:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 292, loss 3.5795750617980957\n",
      "Epoch 6: |          | 293/? [04:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 293, loss 3.8509914875030518\n",
      "Epoch 6: |          | 294/? [04:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 294, loss 3.7700934410095215\n",
      "Epoch 6: |          | 295/? [04:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 295, loss 4.0475358963012695\n",
      "Epoch 6: |          | 296/? [04:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 296, loss 3.523587465286255\n",
      "Epoch 6: |          | 297/? [04:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 297, loss 4.2133588790893555\n",
      "Epoch 6: |          | 298/? [04:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 298, loss 3.997544527053833\n",
      "Epoch 6: |          | 299/? [04:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 299, loss 4.324747085571289\n",
      "Epoch 6: |          | 300/? [04:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 300, loss 3.8085217475891113\n",
      "Epoch 6: |          | 301/? [04:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 301, loss 3.6160576343536377\n",
      "Epoch 6: |          | 302/? [04:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 302, loss 4.044143199920654\n",
      "Epoch 6: |          | 303/? [04:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 303, loss 3.84228515625\n",
      "Epoch 6: |          | 304/? [04:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 304, loss 4.030239582061768\n",
      "Epoch 6: |          | 305/? [04:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 305, loss 4.050806522369385\n",
      "Epoch 6: |          | 306/? [04:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 306, loss 3.7548816204071045\n",
      "Epoch 6: |          | 307/? [04:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 307, loss 3.978764295578003\n",
      "Epoch 6: |          | 308/? [04:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 308, loss 4.131232261657715\n",
      "Epoch 6: |          | 309/? [04:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 309, loss 3.767077922821045\n",
      "Epoch 6: |          | 310/? [04:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 310, loss 4.185137748718262\n",
      "Epoch 6: |          | 311/? [04:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 311, loss 3.7338385581970215\n",
      "Epoch 6: |          | 312/? [04:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 312, loss 3.7732436656951904\n",
      "Epoch 6: |          | 313/? [04:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 313, loss 3.58795428276062\n",
      "Epoch 6: |          | 314/? [04:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 314, loss 3.904405117034912\n",
      "Epoch 6: |          | 315/? [04:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 315, loss 3.5990500450134277\n",
      "Epoch 6: |          | 316/? [04:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 316, loss 4.109532356262207\n",
      "Epoch 6: |          | 317/? [04:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 317, loss 3.950035810470581\n",
      "Epoch 6: |          | 318/? [04:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 318, loss 4.070820331573486\n",
      "Epoch 6: |          | 319/? [04:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 319, loss 3.3212738037109375\n",
      "Epoch 6: |          | 320/? [04:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 320, loss 3.8539326190948486\n",
      "Epoch 6: |          | 321/? [04:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 321, loss 3.639857530593872\n",
      "Epoch 6: |          | 322/? [04:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 322, loss 4.242865562438965\n",
      "Epoch 6: |          | 323/? [04:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 323, loss 4.175869941711426\n",
      "Epoch 6: |          | 324/? [04:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 324, loss 3.916013717651367\n",
      "Epoch 6: |          | 325/? [04:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 325, loss 4.316311836242676\n",
      "Epoch 6: |          | 326/? [04:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 326, loss 3.9133079051971436\n",
      "Epoch 6: |          | 327/? [04:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 327, loss 3.6480159759521484\n",
      "Epoch 6: |          | 328/? [04:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 328, loss 3.433488368988037\n",
      "Epoch 6: |          | 329/? [04:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 329, loss 3.991675615310669\n",
      "Epoch 6: |          | 330/? [04:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 330, loss 4.4886369705200195\n",
      "Epoch 6: |          | 331/? [04:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 331, loss 2.7975735664367676\n",
      "Epoch 6: |          | 332/? [04:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 332, loss 3.8458847999572754\n",
      "Epoch 6: |          | 333/? [04:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 333, loss 3.721640110015869\n",
      "Epoch 6: |          | 334/? [04:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 334, loss 4.299395561218262\n",
      "Epoch 6: |          | 335/? [04:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 335, loss 4.2269697189331055\n",
      "Epoch 6: |          | 336/? [04:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 336, loss 4.227477073669434\n",
      "Epoch 6: |          | 337/? [04:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 337, loss 4.773567199707031\n",
      "Epoch 6: |          | 338/? [04:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 338, loss 4.438485622406006\n",
      "Epoch 6: |          | 339/? [04:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 339, loss 3.574092149734497\n",
      "Epoch 6: |          | 340/? [04:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 340, loss 3.3982036113739014\n",
      "Epoch 6: |          | 341/? [04:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 341, loss 3.3254787921905518\n",
      "Epoch 6: |          | 342/? [04:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 342, loss 3.8975930213928223\n",
      "Epoch 6: |          | 343/? [04:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 343, loss 3.6465728282928467\n",
      "Epoch 6: |          | 344/? [04:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 344, loss 4.48748254776001\n",
      "Epoch 6: |          | 345/? [04:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 345, loss 3.656156539916992\n",
      "Epoch 6: |          | 346/? [04:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 346, loss 3.930067539215088\n",
      "Epoch 6: |          | 347/? [04:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 347, loss 3.7321696281433105\n",
      "Epoch 6: |          | 348/? [04:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 348, loss 3.1192705631256104\n",
      "Epoch 6: |          | 349/? [04:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 349, loss 2.984003782272339\n",
      "Epoch 6: |          | 350/? [04:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 350, loss 4.178719997406006\n",
      "Epoch 6: |          | 351/? [04:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 351, loss 4.225561618804932\n",
      "Epoch 6: |          | 352/? [05:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 352, loss 3.5091166496276855\n",
      "Epoch 6: |          | 353/? [05:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 353, loss 3.286212205886841\n",
      "Epoch 6: |          | 354/? [05:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 354, loss 3.685077667236328\n",
      "Epoch 6: |          | 355/? [05:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 355, loss 4.001272201538086\n",
      "Epoch 6: |          | 356/? [05:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 356, loss 4.009570121765137\n",
      "Epoch 6: |          | 357/? [05:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 357, loss 3.479485273361206\n",
      "Epoch 6: |          | 358/? [05:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 358, loss 3.4453060626983643\n",
      "Epoch 6: |          | 359/? [05:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 359, loss 4.0193071365356445\n",
      "Epoch 6: |          | 360/? [05:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 360, loss 3.627279281616211\n",
      "Epoch 6: |          | 361/? [05:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 361, loss 3.7612948417663574\n",
      "Epoch 6: |          | 362/? [05:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 362, loss 3.5204861164093018\n",
      "Epoch 6: |          | 363/? [05:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 363, loss 3.4567999839782715\n",
      "Epoch 6: |          | 364/? [05:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 364, loss 4.036498546600342\n",
      "Epoch 6: |          | 365/? [05:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 365, loss 4.093544960021973\n",
      "Epoch 6: |          | 366/? [05:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 366, loss 3.8967056274414062\n",
      "Epoch 6: |          | 367/? [05:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 367, loss 3.823265790939331\n",
      "Epoch 6: |          | 368/? [05:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 368, loss 3.422689914703369\n",
      "Epoch 6: |          | 369/? [05:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 369, loss 3.6976733207702637\n",
      "Epoch 6: |          | 370/? [05:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 370, loss 3.403210163116455\n",
      "Epoch 6: |          | 371/? [05:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 371, loss 4.301839351654053\n",
      "Epoch 6: |          | 372/? [05:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 372, loss 3.603262424468994\n",
      "Epoch 6: |          | 373/? [05:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 373, loss 3.969146251678467\n",
      "Epoch 6: |          | 374/? [05:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 374, loss 3.644414186477661\n",
      "Epoch 6: |          | 375/? [05:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 375, loss 4.294238090515137\n",
      "Epoch 6: |          | 376/? [05:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 376, loss 3.7419400215148926\n",
      "Epoch 6: |          | 377/? [05:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 377, loss 3.9338290691375732\n",
      "Epoch 6: |          | 378/? [05:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 378, loss 4.0653910636901855\n",
      "Epoch 6: |          | 379/? [05:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 379, loss 3.859382152557373\n",
      "Epoch 6: |          | 380/? [05:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 380, loss 3.8999035358428955\n",
      "Epoch 6: |          | 381/? [05:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 381, loss 4.001181602478027\n",
      "Epoch 6: |          | 382/? [05:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 382, loss 3.691589832305908\n",
      "Epoch 6: |          | 383/? [05:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 383, loss 3.7963881492614746\n",
      "Epoch 6: |          | 384/? [05:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 384, loss 4.195512771606445\n",
      "Epoch 6: |          | 385/? [05:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 385, loss 3.7961859703063965\n",
      "Epoch 6: |          | 386/? [05:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 386, loss 2.825880527496338\n",
      "Epoch 6: |          | 387/? [05:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 387, loss 3.6502461433410645\n",
      "Epoch 6: |          | 388/? [05:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 388, loss 3.375187635421753\n",
      "Epoch 6: |          | 389/? [05:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 389, loss 4.158929824829102\n",
      "Epoch 6: |          | 390/? [05:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 390, loss 3.6291160583496094\n",
      "Epoch 6: |          | 391/? [05:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 391, loss 4.052753448486328\n",
      "Epoch 6: |          | 392/? [05:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 392, loss 4.138001441955566\n",
      "Epoch 6: |          | 393/? [05:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 393, loss 4.156603813171387\n",
      "Epoch 6: |          | 394/? [05:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 394, loss 3.815718412399292\n",
      "Epoch 6: |          | 395/? [05:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 395, loss 4.042540073394775\n",
      "Epoch 6: |          | 396/? [05:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 396, loss 3.937386989593506\n",
      "Epoch 6: |          | 397/? [05:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 397, loss 3.720729351043701\n",
      "Epoch 6: |          | 398/? [05:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 398, loss 3.6400146484375\n",
      "Epoch 6: |          | 399/? [05:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 399, loss 3.741724729537964\n",
      "Epoch 6: |          | 400/? [05:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 400, loss 3.7200942039489746\n",
      "Epoch 6: |          | 401/? [05:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 401, loss 3.6900229454040527\n",
      "Epoch 6: |          | 402/? [05:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 402, loss 4.0610671043396\n",
      "Epoch 6: |          | 403/? [05:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 403, loss 3.973600387573242\n",
      "Epoch 6: |          | 404/? [05:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 404, loss 3.4969544410705566\n",
      "Epoch 6: |          | 405/? [05:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 405, loss 3.552201747894287\n",
      "Epoch 6: |          | 406/? [05:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 406, loss 3.8835525512695312\n",
      "Epoch 6: |          | 407/? [05:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 407, loss 3.767284870147705\n",
      "Epoch 6: |          | 408/? [05:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 408, loss 4.200700283050537\n",
      "Epoch 6: |          | 409/? [05:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 409, loss 4.105721950531006\n",
      "Epoch 6: |          | 410/? [05:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 410, loss 3.753350019454956\n",
      "Epoch 6: |          | 411/? [05:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 411, loss 3.701186418533325\n",
      "Epoch 6: |          | 412/? [05:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 412, loss 3.2548108100891113\n",
      "Epoch 6: |          | 413/? [05:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 413, loss 3.949782609939575\n",
      "Epoch 6: |          | 414/? [05:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 414, loss 3.571467638015747\n",
      "Epoch 6: |          | 415/? [05:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 415, loss 3.951251983642578\n",
      "Epoch 6: |          | 416/? [05:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 416, loss 4.371972560882568\n",
      "Epoch 6: |          | 417/? [05:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 417, loss 4.3697509765625\n",
      "Epoch 6: |          | 418/? [05:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 418, loss 3.999838352203369\n",
      "Epoch 6: |          | 419/? [05:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 419, loss 3.783477783203125\n",
      "Epoch 6: |          | 420/? [05:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 420, loss 3.9074559211730957\n",
      "Epoch 6: |          | 421/? [05:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 421, loss 4.4264702796936035\n",
      "Epoch 6: |          | 422/? [05:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 422, loss 3.955946683883667\n",
      "Epoch 6: |          | 423/? [06:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 423, loss 3.5648884773254395\n",
      "Epoch 6: |          | 424/? [06:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 424, loss 4.18691349029541\n",
      "Epoch 6: |          | 425/? [06:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 425, loss 3.8705220222473145\n",
      "Epoch 6: |          | 426/? [06:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 426, loss 3.571155071258545\n",
      "Epoch 6: |          | 427/? [06:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 427, loss 3.6240475177764893\n",
      "Epoch 6: |          | 428/? [06:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 428, loss 4.375773906707764\n",
      "Epoch 6: |          | 429/? [06:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 429, loss 3.3025543689727783\n",
      "Epoch 6: |          | 430/? [06:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 430, loss 3.9249355792999268\n",
      "Epoch 6: |          | 431/? [06:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 431, loss 3.802783250808716\n",
      "Epoch 6: |          | 432/? [06:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 432, loss 3.9461586475372314\n",
      "Epoch 6: |          | 433/? [06:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 433, loss 3.8943381309509277\n",
      "Epoch 6: |          | 434/? [06:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 434, loss 3.804441452026367\n",
      "Epoch 6: |          | 435/? [06:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 435, loss 3.471644639968872\n",
      "Epoch 6: |          | 436/? [06:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 436, loss 3.873413562774658\n",
      "Epoch 6: |          | 437/? [06:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 437, loss 4.07199239730835\n",
      "Epoch 6: |          | 438/? [06:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 438, loss 3.68986177444458\n",
      "Epoch 6: |          | 439/? [06:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 439, loss 3.5646090507507324\n",
      "Epoch 6: |          | 440/? [06:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 440, loss 3.4726970195770264\n",
      "Epoch 6: |          | 441/? [06:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 441, loss 3.898332118988037\n",
      "Epoch 6: |          | 442/? [06:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 442, loss 3.7544963359832764\n",
      "Epoch 6: |          | 443/? [06:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 443, loss 3.8721046447753906\n",
      "Epoch 6: |          | 444/? [06:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 444, loss 3.887638807296753\n",
      "Epoch 6: |          | 445/? [06:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 445, loss 4.775061130523682\n",
      "Epoch 6: |          | 446/? [06:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 446, loss 3.8344016075134277\n",
      "Epoch 6: |          | 447/? [06:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 447, loss 4.353055000305176\n",
      "Epoch 6: |          | 448/? [06:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 448, loss 3.426145553588867\n",
      "Epoch 6: |          | 449/? [06:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 449, loss 3.8386588096618652\n",
      "Epoch 6: |          | 450/? [06:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 450, loss 4.134129524230957\n",
      "Epoch 6: |          | 451/? [06:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 451, loss 3.8378117084503174\n",
      "Epoch 6: |          | 452/? [06:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 452, loss 3.5426108837127686\n",
      "Epoch 6: |          | 453/? [06:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 453, loss 4.270953178405762\n",
      "Epoch 6: |          | 454/? [06:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 454, loss 3.6435561180114746\n",
      "Epoch 6: |          | 455/? [06:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 455, loss 3.974722385406494\n",
      "Epoch 6: |          | 456/? [06:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 456, loss 3.3456063270568848\n",
      "Epoch 6: |          | 457/? [06:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 457, loss 3.7815380096435547\n",
      "Epoch 6: |          | 458/? [06:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 458, loss 4.188857078552246\n",
      "Epoch 6: |          | 459/? [06:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 459, loss 4.174275875091553\n",
      "Epoch 6: |          | 460/? [06:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 460, loss 3.9204089641571045\n",
      "Epoch 6: |          | 461/? [06:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 461, loss 3.8935863971710205\n",
      "Epoch 6: |          | 462/? [06:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 462, loss 3.9534714221954346\n",
      "Epoch 6: |          | 463/? [06:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 463, loss 3.814070224761963\n",
      "Epoch 6: |          | 464/? [06:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 464, loss 3.340466022491455\n",
      "Epoch 6: |          | 465/? [06:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 465, loss 3.615875244140625\n",
      "Epoch 6: |          | 466/? [06:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 466, loss 4.068339824676514\n",
      "Epoch 6: |          | 467/? [06:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 467, loss 3.855422258377075\n",
      "Epoch 6: |          | 468/? [06:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 468, loss 3.7836830615997314\n",
      "Epoch 6: |          | 469/? [06:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 469, loss 3.91520357131958\n",
      "Epoch 6: |          | 470/? [06:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 470, loss 3.3238956928253174\n",
      "Epoch 6: |          | 471/? [06:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 471, loss 4.114400863647461\n",
      "Epoch 6: |          | 472/? [06:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 472, loss 3.607978105545044\n",
      "Epoch 6: |          | 473/? [06:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 473, loss 3.6241557598114014\n",
      "Epoch 6: |          | 474/? [06:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 474, loss 3.284475803375244\n",
      "Epoch 6: |          | 475/? [06:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 475, loss 4.4954447746276855\n",
      "Epoch 6: |          | 476/? [06:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 476, loss 3.5711445808410645\n",
      "Epoch 6: |          | 477/? [06:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 477, loss 3.095738649368286\n",
      "Epoch 6: |          | 478/? [06:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 478, loss 3.3067588806152344\n",
      "Epoch 6: |          | 479/? [06:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 479, loss 3.800030469894409\n",
      "Epoch 6: |          | 480/? [06:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 480, loss 3.65924334526062\n",
      "Epoch 6: |          | 481/? [06:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 481, loss 3.329922914505005\n",
      "Epoch 6: |          | 482/? [06:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 482, loss 3.573603868484497\n",
      "Epoch 6: |          | 483/? [06:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 483, loss 3.282663345336914\n",
      "Epoch 6: |          | 484/? [06:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 484, loss 4.1660871505737305\n",
      "Epoch 6: |          | 485/? [06:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 485, loss 4.058926582336426\n",
      "Epoch 6: |          | 486/? [06:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 486, loss 3.6289355754852295\n",
      "Epoch 6: |          | 487/? [06:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 487, loss 4.010341644287109\n",
      "Epoch 6: |          | 488/? [06:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 488, loss 3.766381025314331\n",
      "Epoch 6: |          | 489/? [06:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 489, loss 3.3009254932403564\n",
      "Epoch 6: |          | 490/? [06:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 490, loss 3.871051788330078\n",
      "Epoch 6: |          | 491/? [06:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 491, loss 3.732459545135498\n",
      "Epoch 6: |          | 492/? [06:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 492, loss 3.0783238410949707\n",
      "Epoch 6: |          | 493/? [06:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 493, loss 4.0726823806762695\n",
      "Epoch 6: |          | 494/? [06:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 494, loss 3.876626968383789\n",
      "Epoch 6: |          | 495/? [07:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 495, loss 3.967679500579834\n",
      "Epoch 6: |          | 496/? [07:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 496, loss 3.599823474884033\n",
      "Epoch 6: |          | 497/? [07:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 497, loss 4.14469051361084\n",
      "Epoch 6: |          | 498/? [07:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 498, loss 3.770352840423584\n",
      "Epoch 6: |          | 499/? [07:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 499, loss 3.9006028175354004\n",
      "Epoch 6: |          | 500/? [07:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 500, loss 3.6588149070739746\n",
      "Epoch 6: |          | 501/? [07:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 501, loss 3.430938243865967\n",
      "Epoch 6: |          | 502/? [07:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 502, loss 3.852177381515503\n",
      "Epoch 6: |          | 503/? [07:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 503, loss 3.783940076828003\n",
      "Epoch 6: |          | 504/? [07:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 504, loss 3.722883701324463\n",
      "Epoch 6: |          | 505/? [07:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 505, loss 3.2011001110076904\n",
      "Epoch 6: |          | 506/? [07:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 506, loss 3.770606517791748\n",
      "Epoch 6: |          | 507/? [07:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 507, loss 3.8145534992218018\n",
      "Epoch 6: |          | 508/? [07:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 508, loss 4.1353020668029785\n",
      "Epoch 6: |          | 509/? [07:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 509, loss 3.5030219554901123\n",
      "Epoch 6: |          | 510/? [07:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 510, loss 3.9653067588806152\n",
      "Epoch 6: |          | 511/? [07:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 511, loss 3.830369234085083\n",
      "Epoch 6: |          | 512/? [07:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 512, loss 3.2737927436828613\n",
      "Epoch 6: |          | 513/? [07:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 513, loss 3.518913745880127\n",
      "Epoch 6: |          | 514/? [07:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 514, loss 3.690000057220459\n",
      "Epoch 6: |          | 515/? [07:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 515, loss 3.3119056224823\n",
      "Epoch 6: |          | 516/? [07:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 516, loss 3.63908052444458\n",
      "Epoch 6: |          | 517/? [07:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 517, loss 3.8326430320739746\n",
      "Epoch 6: |          | 518/? [07:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 518, loss 3.427872896194458\n",
      "Epoch 6: |          | 519/? [07:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 519, loss 3.837266445159912\n",
      "Epoch 6: |          | 520/? [07:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 520, loss 3.708484649658203\n",
      "Epoch 6: |          | 521/? [07:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 521, loss 3.7340893745422363\n",
      "Epoch 6: |          | 522/? [07:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 522, loss 4.251927375793457\n",
      "Epoch 6: |          | 523/? [07:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 523, loss 4.334378242492676\n",
      "Epoch 6: |          | 524/? [07:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 524, loss 4.110713005065918\n",
      "Epoch 6: |          | 525/? [07:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 525, loss 3.7012264728546143\n",
      "Epoch 6: |          | 526/? [07:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 526, loss 3.4996414184570312\n",
      "Epoch 6: |          | 527/? [07:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 527, loss 4.177277088165283\n",
      "Epoch 6: |          | 528/? [07:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 528, loss 3.935464859008789\n",
      "Epoch 6: |          | 529/? [07:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 529, loss 3.531442642211914\n",
      "Epoch 6: |          | 530/? [07:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 530, loss 4.06732177734375\n",
      "Epoch 6: |          | 531/? [07:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 531, loss 3.5963268280029297\n",
      "Epoch 6: |          | 532/? [07:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 532, loss 3.8733534812927246\n",
      "Epoch 6: |          | 533/? [07:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 533, loss 3.510878086090088\n",
      "Epoch 6: |          | 534/? [07:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 534, loss 3.210324764251709\n",
      "Epoch 6: |          | 535/? [07:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 535, loss 3.5600948333740234\n",
      "Epoch 6: |          | 536/? [07:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 536, loss 4.136480331420898\n",
      "Epoch 6: |          | 537/? [07:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 537, loss 3.9281928539276123\n",
      "Epoch 6: |          | 538/? [07:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 538, loss 3.5499634742736816\n",
      "Epoch 6: |          | 539/? [07:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 539, loss 3.6756045818328857\n",
      "Epoch 6: |          | 540/? [07:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 540, loss 4.050723552703857\n",
      "Epoch 6: |          | 541/? [07:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 541, loss 3.8069329261779785\n",
      "Epoch 6: |          | 542/? [07:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 542, loss 3.5673470497131348\n",
      "Epoch 6: |          | 543/? [07:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 543, loss 3.942943572998047\n",
      "Epoch 6: |          | 544/? [07:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 544, loss 3.8616244792938232\n",
      "Epoch 6: |          | 545/? [07:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 545, loss 3.168560266494751\n",
      "Epoch 6: |          | 546/? [07:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 546, loss 3.9200451374053955\n",
      "Epoch 6: |          | 547/? [07:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 547, loss 4.3502888679504395\n",
      "Epoch 6: |          | 548/? [07:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 548, loss 4.017543315887451\n",
      "Epoch 6: |          | 549/? [07:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 549, loss 3.9142069816589355\n",
      "Epoch 6: |          | 550/? [07:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 550, loss 4.215391159057617\n",
      "Epoch 6: |          | 551/? [07:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 551, loss 3.895491361618042\n",
      "Epoch 6: |          | 552/? [07:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 552, loss 3.8633620738983154\n",
      "Epoch 6: |          | 553/? [07:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 553, loss 3.3530240058898926\n",
      "Epoch 6: |          | 554/? [07:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 554, loss 3.908316135406494\n",
      "Epoch 6: |          | 555/? [07:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 555, loss 4.175625801086426\n",
      "Epoch 6: |          | 556/? [07:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 556, loss 3.9761862754821777\n",
      "Epoch 6: |          | 557/? [07:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 557, loss 3.4613616466522217\n",
      "Epoch 6: |          | 558/? [07:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 558, loss 3.689854383468628\n",
      "Epoch 6: |          | 559/? [07:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 559, loss 3.6408958435058594\n",
      "Epoch 6: |          | 560/? [07:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 560, loss 3.2199714183807373\n",
      "Epoch 6: |          | 561/? [07:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 561, loss 3.0565574169158936\n",
      "Epoch 6: |          | 562/? [07:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 562, loss 4.076715469360352\n",
      "Epoch 6: |          | 563/? [07:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 563, loss 3.188121795654297\n",
      "Epoch 6: |          | 564/? [07:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 564, loss 3.648743152618408\n",
      "Epoch 6: |          | 565/? [07:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 565, loss 3.984429121017456\n",
      "Epoch 6: |          | 566/? [08:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 566, loss 4.087811470031738\n",
      "Epoch 6: |          | 567/? [08:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 567, loss 4.185523986816406\n",
      "Epoch 6: |          | 568/? [08:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 568, loss 3.274171829223633\n",
      "Epoch 6: |          | 569/? [08:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 569, loss 3.859515428543091\n",
      "Epoch 6: |          | 570/? [08:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 570, loss 3.9650981426239014\n",
      "Epoch 6: |          | 571/? [08:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 571, loss 3.5670418739318848\n",
      "Epoch 6: |          | 572/? [08:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 572, loss 4.474240303039551\n",
      "Epoch 6: |          | 573/? [08:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 573, loss 2.924483060836792\n",
      "Epoch 6: |          | 574/? [08:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 574, loss 4.080240249633789\n",
      "Epoch 6: |          | 575/? [08:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 575, loss 3.4259555339813232\n",
      "Epoch 6: |          | 576/? [08:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 576, loss 3.6338553428649902\n",
      "Epoch 6: |          | 577/? [08:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 577, loss 3.833857774734497\n",
      "Epoch 6: |          | 578/? [08:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 578, loss 4.0974531173706055\n",
      "Epoch 6: |          | 579/? [08:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 579, loss 3.264078140258789\n",
      "Epoch 6: |          | 580/? [08:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 580, loss 3.932408094406128\n",
      "Epoch 6: |          | 581/? [08:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 581, loss 3.9154601097106934\n",
      "Epoch 6: |          | 582/? [08:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 582, loss 3.98069429397583\n",
      "Epoch 6: |          | 583/? [08:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 583, loss 3.7629292011260986\n",
      "Epoch 6: |          | 584/? [08:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 584, loss 3.996521472930908\n",
      "Epoch 6: |          | 585/? [08:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 585, loss 3.8982810974121094\n",
      "Epoch 6: |          | 586/? [08:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 586, loss 3.9987213611602783\n",
      "Epoch 6: |          | 587/? [08:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 587, loss 3.997743606567383\n",
      "Epoch 6: |          | 588/? [08:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 588, loss 3.897595167160034\n",
      "Epoch 6: |          | 589/? [08:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 589, loss 3.4301071166992188\n",
      "Epoch 6: |          | 590/? [08:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 590, loss 3.927311658859253\n",
      "Epoch 6: |          | 591/? [08:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 591, loss 3.855318069458008\n",
      "Epoch 6: |          | 592/? [08:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 592, loss 3.519895076751709\n",
      "Epoch 6: |          | 593/? [08:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 593, loss 3.8659820556640625\n",
      "Epoch 6: |          | 594/? [08:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 594, loss 4.621236324310303\n",
      "Epoch 6: |          | 595/? [08:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 595, loss 3.4080214500427246\n",
      "Epoch 6: |          | 596/? [08:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 596, loss 3.4432296752929688\n",
      "Epoch 6: |          | 597/? [08:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 597, loss 3.673368453979492\n",
      "Epoch 6: |          | 598/? [08:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 598, loss 4.1232099533081055\n",
      "Epoch 6: |          | 599/? [08:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 599, loss 3.797168016433716\n",
      "Epoch 6: |          | 600/? [08:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 600, loss 3.583775043487549\n",
      "Epoch 6: |          | 601/? [08:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 601, loss 3.874886989593506\n",
      "Epoch 6: |          | 602/? [08:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 602, loss 3.4239425659179688\n",
      "Epoch 6: |          | 603/? [08:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 603, loss 3.4930660724639893\n",
      "Epoch 6: |          | 604/? [08:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 604, loss 5.219477653503418\n",
      "Epoch 6: |          | 605/? [08:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 605, loss 3.337127208709717\n",
      "Epoch 6: |          | 606/? [08:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 606, loss 3.615562915802002\n",
      "Epoch 6: |          | 607/? [08:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 607, loss 3.9092965126037598\n",
      "Epoch 6: |          | 608/? [08:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 608, loss 3.7078781127929688\n",
      "Epoch 6: |          | 609/? [08:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 609, loss 3.614067792892456\n",
      "Epoch 6: |          | 610/? [08:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 610, loss 3.7133853435516357\n",
      "Epoch 6: |          | 611/? [08:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 611, loss 3.842836856842041\n",
      "Epoch 6: |          | 612/? [08:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 612, loss 3.58015513420105\n",
      "Epoch 6: |          | 613/? [08:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 613, loss 3.918792724609375\n",
      "Epoch 6: |          | 614/? [08:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 614, loss 3.692349672317505\n",
      "Epoch 6: |          | 615/? [08:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 615, loss 4.206984996795654\n",
      "Epoch 6: |          | 616/? [08:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 616, loss 4.387185096740723\n",
      "Epoch 6: |          | 617/? [08:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 617, loss 2.964487314224243\n",
      "Epoch 6: |          | 618/? [08:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 618, loss 3.9392566680908203\n",
      "Epoch 6: |          | 619/? [08:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 619, loss 3.51203989982605\n",
      "Epoch 6: |          | 620/? [08:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 620, loss 4.006152153015137\n",
      "Epoch 6: |          | 621/? [08:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 621, loss 3.508108615875244\n",
      "Epoch 6: |          | 622/? [08:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 622, loss 3.3097386360168457\n",
      "Epoch 6: |          | 623/? [08:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 623, loss 3.1014888286590576\n",
      "Epoch 6: |          | 624/? [08:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 624, loss 2.812028169631958\n",
      "Epoch 6: |          | 625/? [08:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 625, loss 4.2461347579956055\n",
      "Epoch 6: |          | 626/? [08:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 626, loss 3.722609043121338\n",
      "Epoch 6: |          | 627/? [08:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 627, loss 3.6633307933807373\n",
      "Epoch 6: |          | 628/? [08:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 628, loss 3.698512315750122\n",
      "Epoch 6: |          | 629/? [08:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 629, loss 4.049376487731934\n",
      "Epoch 6: |          | 630/? [08:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 630, loss 3.8026719093322754\n",
      "Epoch 6: |          | 631/? [08:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 631, loss 3.9134132862091064\n",
      "Epoch 6: |          | 632/? [08:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 632, loss 3.260957717895508\n",
      "Epoch 6: |          | 633/? [08:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 633, loss 4.0330915451049805\n",
      "Epoch 6: |          | 634/? [08:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 634, loss 3.559278964996338\n",
      "Epoch 6: |          | 635/? [08:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 635, loss 3.4151721000671387\n",
      "Epoch 6: |          | 636/? [08:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 636, loss 3.8095390796661377\n",
      "Epoch 6: |          | 637/? [09:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 637, loss 3.6337172985076904\n",
      "Epoch 6: |          | 638/? [09:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 638, loss 3.861515760421753\n",
      "Epoch 6: |          | 639/? [09:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 639, loss 3.6147923469543457\n",
      "Epoch 6: |          | 640/? [09:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 640, loss 4.152398109436035\n",
      "Epoch 6: |          | 641/? [09:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 641, loss 3.1770126819610596\n",
      "Epoch 6: |          | 642/? [09:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 642, loss 3.9949698448181152\n",
      "Epoch 6: |          | 643/? [09:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 643, loss 3.853206157684326\n",
      "Epoch 6: |          | 644/? [09:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 644, loss 3.832866668701172\n",
      "Epoch 6: |          | 645/? [09:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 645, loss 3.5406334400177\n",
      "Epoch 6: |          | 646/? [09:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 646, loss 3.5706756114959717\n",
      "Epoch 6: |          | 647/? [09:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 647, loss 4.1348371505737305\n",
      "Epoch 6: |          | 648/? [09:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 648, loss 3.520930528640747\n",
      "Epoch 6: |          | 649/? [09:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 649, loss 3.0613858699798584\n",
      "Epoch 6: |          | 650/? [09:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 650, loss 4.070165157318115\n",
      "Epoch 6: |          | 651/? [09:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 651, loss 4.206377983093262\n",
      "Epoch 6: |          | 652/? [09:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 652, loss 3.679062604904175\n",
      "Epoch 6: |          | 653/? [09:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 653, loss 3.8286566734313965\n",
      "Epoch 6: |          | 654/? [09:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 654, loss 3.895798444747925\n",
      "Epoch 6: |          | 655/? [09:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 655, loss 3.6962170600891113\n",
      "Epoch 6: |          | 656/? [09:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 656, loss 3.3617756366729736\n",
      "Epoch 6: |          | 657/? [09:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 657, loss 5.600522041320801\n",
      "Epoch 6: |          | 658/? [09:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 658, loss 3.265122175216675\n",
      "Epoch 6: |          | 659/? [09:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 659, loss 3.771768569946289\n",
      "Epoch 6: |          | 660/? [09:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 660, loss 4.1390838623046875\n",
      "Epoch 6: |          | 661/? [09:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 661, loss 4.065499782562256\n",
      "Epoch 6: |          | 662/? [09:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 662, loss 3.910571575164795\n",
      "Epoch 6: |          | 663/? [09:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 663, loss 3.6709351539611816\n",
      "Epoch 6: |          | 664/? [09:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 664, loss 3.6208221912384033\n",
      "Epoch 6: |          | 665/? [09:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 665, loss 3.9088664054870605\n",
      "Epoch 6: |          | 666/? [09:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 666, loss 3.7035820484161377\n",
      "Epoch 6: |          | 667/? [09:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 667, loss 4.533087730407715\n",
      "Epoch 6: |          | 668/? [09:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 668, loss 3.3211567401885986\n",
      "Epoch 6: |          | 669/? [09:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 669, loss 3.5237109661102295\n",
      "Epoch 6: |          | 670/? [09:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 670, loss 4.193919658660889\n",
      "Epoch 6: |          | 671/? [09:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 671, loss 3.9408066272735596\n",
      "Epoch 6: |          | 672/? [09:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 672, loss 3.9479269981384277\n",
      "Epoch 6: |          | 673/? [09:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 673, loss 3.813458204269409\n",
      "Epoch 6: |          | 674/? [09:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 674, loss 2.323124408721924\n",
      "Epoch 6: |          | 675/? [09:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 675, loss 0.9092992544174194\n",
      "Epoch 6: |          | 676/? [09:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 676, loss 0.7250489592552185\n",
      "Epoch 6: |          | 677/? [09:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 677, loss 0.6352198719978333\n",
      "Epoch 6: |          | 678/? [09:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 678, loss 1.7075719833374023\n",
      "Epoch 6: |          | 679/? [09:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 679, loss 3.2082018852233887\n",
      "Epoch 6: |          | 680/? [09:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 680, loss 3.6878390312194824\n",
      "Epoch 6: |          | 681/? [09:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 681, loss 3.27293062210083\n",
      "Epoch 6: |          | 682/? [09:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 682, loss 3.57218599319458\n",
      "Epoch 6: |          | 683/? [09:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 683, loss 3.2648730278015137\n",
      "Epoch 6: |          | 684/? [09:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 684, loss 4.2920942306518555\n",
      "Epoch 6: |          | 685/? [09:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 685, loss 3.9009132385253906\n",
      "Epoch 6: |          | 686/? [09:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 686, loss 3.5307488441467285\n",
      "Epoch 6: |          | 687/? [09:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 687, loss 4.034300804138184\n",
      "Epoch 6: |          | 688/? [09:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 688, loss 3.5180931091308594\n",
      "Epoch 6: |          | 689/? [09:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 689, loss 3.648508071899414\n",
      "Epoch 6: |          | 690/? [09:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 690, loss 4.271049976348877\n",
      "Epoch 6: |          | 691/? [09:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 691, loss 3.7554917335510254\n",
      "Epoch 6: |          | 692/? [09:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 692, loss 3.7842540740966797\n",
      "Epoch 6: |          | 693/? [09:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 693, loss 4.301976203918457\n",
      "Epoch 6: |          | 694/? [09:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 694, loss 3.6442840099334717\n",
      "Epoch 6: |          | 695/? [09:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 695, loss 4.209658622741699\n",
      "Epoch 6: |          | 696/? [09:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 696, loss 3.518199920654297\n",
      "Epoch 6: |          | 697/? [09:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 697, loss 3.710271120071411\n",
      "Epoch 6: |          | 698/? [09:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 698, loss 3.157306671142578\n",
      "Epoch 6: |          | 699/? [09:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 699, loss 3.8801894187927246\n",
      "Epoch 6: |          | 700/? [09:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 700, loss 3.9538161754608154\n",
      "Epoch 6: |          | 701/? [09:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 701, loss 3.6013588905334473\n",
      "Epoch 6: |          | 702/? [09:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 702, loss 3.8692219257354736\n",
      "Epoch 6: |          | 703/? [09:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 703, loss 3.9781899452209473\n",
      "Epoch 6: |          | 704/? [09:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 704, loss 3.826169967651367\n",
      "Epoch 6: |          | 705/? [09:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 705, loss 3.464743137359619\n",
      "Epoch 6: |          | 706/? [09:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 706, loss 3.5378143787384033\n",
      "Epoch 6: |          | 707/? [09:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 707, loss 3.9935073852539062\n",
      "Epoch 6: |          | 708/? [09:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 708, loss 3.7396435737609863\n",
      "Epoch 6: |          | 709/? [10:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 709, loss 3.6425087451934814\n",
      "Epoch 6: |          | 710/? [10:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 710, loss 4.189652442932129\n",
      "Epoch 6: |          | 711/? [10:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 711, loss 4.220503330230713\n",
      "Epoch 6: |          | 712/? [10:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 712, loss 3.9747531414031982\n",
      "Epoch 6: |          | 713/? [10:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 713, loss 4.062119483947754\n",
      "Epoch 6: |          | 714/? [10:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 714, loss 4.095755577087402\n",
      "Epoch 6: |          | 715/? [10:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 715, loss 3.0905277729034424\n",
      "Epoch 6: |          | 716/? [10:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 716, loss 3.7982287406921387\n",
      "Epoch 6: |          | 717/? [10:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 717, loss 3.6930928230285645\n",
      "Epoch 6: |          | 718/? [10:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 718, loss 3.2139217853546143\n",
      "Epoch 6: |          | 719/? [10:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 719, loss 3.7024197578430176\n",
      "Epoch 6: |          | 720/? [10:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 720, loss 3.3953328132629395\n",
      "Epoch 6: |          | 721/? [10:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 721, loss 4.080314636230469\n",
      "Epoch 6: |          | 722/? [10:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 722, loss 3.4159188270568848\n",
      "Epoch 6: |          | 723/? [10:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 723, loss 3.931140184402466\n",
      "Epoch 6: |          | 724/? [10:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 724, loss 3.488431930541992\n",
      "Epoch 6: |          | 725/? [10:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 725, loss 3.444622755050659\n",
      "Epoch 6: |          | 726/? [10:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 726, loss 3.6311538219451904\n",
      "Epoch 6: |          | 727/? [10:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 727, loss 3.424259901046753\n",
      "Epoch 6: |          | 728/? [10:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 728, loss 3.2467408180236816\n",
      "Epoch 6: |          | 729/? [10:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 729, loss 3.7373175621032715\n",
      "Epoch 6: |          | 730/? [10:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 730, loss 3.708348035812378\n",
      "Epoch 6: |          | 731/? [10:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 731, loss 3.842679262161255\n",
      "Epoch 6: |          | 732/? [10:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 732, loss 4.050886154174805\n",
      "Epoch 6: |          | 733/? [10:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 733, loss 3.7737159729003906\n",
      "Epoch 6: |          | 734/? [10:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 734, loss 3.9044597148895264\n",
      "Epoch 6: |          | 735/? [10:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 735, loss 3.854888916015625\n",
      "Epoch 6: |          | 736/? [10:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 736, loss 3.4345555305480957\n",
      "Epoch 6: |          | 737/? [10:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 737, loss 4.186221599578857\n",
      "Epoch 6: |          | 738/? [10:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 738, loss 3.3990445137023926\n",
      "Epoch 6: |          | 739/? [10:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 739, loss 3.847464084625244\n",
      "Epoch 6: |          | 740/? [10:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 740, loss 3.5710155963897705\n",
      "Epoch 6: |          | 741/? [10:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 741, loss 3.7137959003448486\n",
      "Epoch 6: |          | 742/? [10:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 742, loss 4.076213359832764\n",
      "Epoch 6: |          | 743/? [10:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 743, loss 3.9738717079162598\n",
      "Epoch 6: |          | 744/? [10:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 744, loss 3.9205665588378906\n",
      "Epoch 6: |          | 745/? [10:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 745, loss 3.5472846031188965\n",
      "Epoch 6: |          | 746/? [10:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 746, loss 3.8358378410339355\n",
      "Epoch 6: |          | 747/? [10:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 747, loss 3.5641849040985107\n",
      "Epoch 6: |          | 748/? [10:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 748, loss 2.5838406085968018\n",
      "Epoch 6: |          | 749/? [10:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 749, loss 3.701868772506714\n",
      "Epoch 6: |          | 750/? [10:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 750, loss 3.9379794597625732\n",
      "Epoch 6: |          | 751/? [10:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 751, loss 2.241631031036377\n",
      "Epoch 6: |          | 752/? [10:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 752, loss 3.8912582397460938\n",
      "Epoch 6: |          | 753/? [10:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 753, loss 3.08618426322937\n",
      "Epoch 6: |          | 754/? [10:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 754, loss 3.5351195335388184\n",
      "Epoch 6: |          | 755/? [10:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 755, loss 3.3548405170440674\n",
      "Epoch 6: |          | 756/? [10:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 756, loss 3.718806743621826\n",
      "Epoch 6: |          | 757/? [10:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 757, loss 3.7833385467529297\n",
      "Epoch 6: |          | 758/? [10:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 758, loss 3.5480926036834717\n",
      "Epoch 6: |          | 759/? [10:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 759, loss 3.480233669281006\n",
      "Epoch 6: |          | 760/? [10:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 760, loss 3.9782917499542236\n",
      "Epoch 6: |          | 761/? [10:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 761, loss 4.001071453094482\n",
      "Epoch 6: |          | 762/? [10:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 762, loss 3.667113780975342\n",
      "Epoch 6: |          | 763/? [10:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 763, loss 3.802011489868164\n",
      "Epoch 6: |          | 764/? [10:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 764, loss 4.062828063964844\n",
      "Epoch 6: |          | 765/? [10:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 765, loss 3.8455650806427\n",
      "Epoch 6: |          | 766/? [10:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 766, loss 4.2106032371521\n",
      "Epoch 6: |          | 767/? [10:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 767, loss 4.276195049285889\n",
      "Epoch 6: |          | 768/? [10:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 768, loss 3.8292248249053955\n",
      "Epoch 6: |          | 769/? [10:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 769, loss 3.051166534423828\n",
      "Epoch 6: |          | 770/? [10:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 770, loss 3.6106128692626953\n",
      "Epoch 6: |          | 771/? [10:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 771, loss 4.291069507598877\n",
      "Epoch 6: |          | 772/? [10:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 772, loss 4.029709815979004\n",
      "Epoch 6: |          | 773/? [10:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 773, loss 3.72013783454895\n",
      "Epoch 6: |          | 774/? [10:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 774, loss 3.870359420776367\n",
      "Epoch 6: |          | 775/? [10:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 775, loss 4.293622016906738\n",
      "Epoch 6: |          | 776/? [10:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 776, loss 3.755596876144409\n",
      "Epoch 6: |          | 777/? [10:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 777, loss 3.6024532318115234\n",
      "Epoch 6: |          | 778/? [10:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 778, loss 4.006843090057373\n",
      "Epoch 6: |          | 779/? [10:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 779, loss 4.451756000518799\n",
      "Epoch 6: |          | 780/? [11:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 780, loss 3.453533887863159\n",
      "Epoch 6: |          | 781/? [11:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 781, loss 3.561185836791992\n",
      "Epoch 6: |          | 782/? [11:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 782, loss 3.9162991046905518\n",
      "Epoch 6: |          | 783/? [11:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 783, loss 3.971158504486084\n",
      "Epoch 6: |          | 784/? [11:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 784, loss 3.5756173133850098\n",
      "Epoch 6: |          | 785/? [11:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 785, loss 3.3334484100341797\n",
      "Epoch 6: |          | 786/? [11:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 786, loss 4.218177795410156\n",
      "Epoch 6: |          | 787/? [11:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 787, loss 4.16334342956543\n",
      "Epoch 6: |          | 788/? [11:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 788, loss 2.0523626804351807\n",
      "Epoch 6: |          | 789/? [11:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 789, loss 3.6601951122283936\n",
      "Epoch 6: |          | 790/? [11:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 790, loss 4.483302116394043\n",
      "Epoch 6: |          | 791/? [11:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 791, loss 4.201712608337402\n",
      "Epoch 6: |          | 792/? [11:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 792, loss 3.4394307136535645\n",
      "Epoch 6: |          | 793/? [11:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 793, loss 3.9090943336486816\n",
      "Epoch 6: |          | 794/? [11:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 794, loss 4.219246864318848\n",
      "Epoch 6: |          | 795/? [11:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 795, loss 3.7335262298583984\n",
      "Epoch 6: |          | 796/? [11:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 796, loss 4.103501319885254\n",
      "Epoch 6: |          | 797/? [11:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 797, loss 3.124525785446167\n",
      "Epoch 6: |          | 798/? [11:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 798, loss 3.2135307788848877\n",
      "Epoch 6: |          | 799/? [11:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 799, loss 4.133772850036621\n",
      "Epoch 6: |          | 800/? [11:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 800, loss 3.9483447074890137\n",
      "Epoch 6: |          | 801/? [11:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 801, loss 3.5410239696502686\n",
      "Epoch 6: |          | 802/? [11:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 802, loss 3.8324081897735596\n",
      "Epoch 6: |          | 803/? [11:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 803, loss 3.6941781044006348\n",
      "Epoch 6: |          | 804/? [11:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 804, loss 3.8389649391174316\n",
      "Epoch 6: |          | 805/? [11:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 805, loss 3.9850082397460938\n",
      "Epoch 6: |          | 806/? [11:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 806, loss 4.438549041748047\n",
      "Epoch 6: |          | 807/? [11:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 807, loss 3.7915453910827637\n",
      "Epoch 6: |          | 808/? [11:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 808, loss 3.452634811401367\n",
      "Epoch 6: |          | 809/? [11:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 809, loss 3.95369029045105\n",
      "Epoch 6: |          | 810/? [11:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 810, loss 3.7521698474884033\n",
      "Epoch 6: |          | 811/? [11:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 811, loss 3.9846909046173096\n",
      "Epoch 6: |          | 812/? [11:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 812, loss 4.589581489562988\n",
      "Epoch 6: |          | 813/? [11:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 813, loss 4.367432594299316\n",
      "Epoch 6: |          | 814/? [11:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 814, loss 3.4191792011260986\n",
      "Epoch 6: |          | 815/? [11:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 815, loss 4.135062217712402\n",
      "Epoch 6: |          | 816/? [11:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 816, loss 3.94016695022583\n",
      "Epoch 6: |          | 817/? [11:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 817, loss 3.272874116897583\n",
      "Epoch 6: |          | 818/? [11:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 818, loss 4.236912727355957\n",
      "Epoch 6: |          | 819/? [11:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 819, loss 3.9606480598449707\n",
      "Epoch 6: |          | 820/? [11:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 820, loss 3.789005756378174\n",
      "Epoch 6: |          | 821/? [11:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 821, loss 3.7270073890686035\n",
      "Epoch 6: |          | 822/? [11:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 822, loss 3.4350082874298096\n",
      "Epoch 6: |          | 823/? [11:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 823, loss 3.482328414916992\n",
      "Epoch 6: |          | 824/? [11:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 824, loss 3.913220167160034\n",
      "Epoch 6: |          | 825/? [11:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 825, loss 3.490795135498047\n",
      "Epoch 6: |          | 826/? [11:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 826, loss 4.001112461090088\n",
      "Epoch 6: |          | 827/? [11:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 827, loss 3.6322338581085205\n",
      "Epoch 6: |          | 828/? [11:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 828, loss 4.0905022621154785\n",
      "Epoch 6: |          | 829/? [11:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 829, loss 3.7910525798797607\n",
      "Epoch 6: |          | 830/? [11:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 830, loss 4.367989540100098\n",
      "Epoch 6: |          | 831/? [11:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 831, loss 2.3064990043640137\n",
      "Epoch 6: |          | 832/? [11:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 832, loss 3.736069917678833\n",
      "Epoch 6: |          | 833/? [11:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 833, loss 3.64540433883667\n",
      "Epoch 6: |          | 834/? [11:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 834, loss 4.370473384857178\n",
      "Epoch 6: |          | 835/? [11:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 835, loss 3.722644329071045\n",
      "Epoch 6: |          | 836/? [11:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 836, loss 4.355964183807373\n",
      "Epoch 6: |          | 837/? [11:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 837, loss 3.8208377361297607\n",
      "Epoch 6: |          | 838/? [11:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 838, loss 3.2108588218688965\n",
      "Epoch 6: |          | 839/? [11:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 839, loss 3.5347561836242676\n",
      "Epoch 6: |          | 840/? [11:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 840, loss 4.085585594177246\n",
      "Epoch 6: |          | 841/? [11:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 841, loss 4.106769561767578\n",
      "Epoch 6: |          | 842/? [11:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 842, loss 3.787321090698242\n",
      "Epoch 6: |          | 843/? [11:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 843, loss 4.115985870361328\n",
      "Epoch 6: |          | 844/? [12:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 844, loss 3.4854347705841064\n",
      "Epoch 6: |          | 845/? [12:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 845, loss 3.8887462615966797\n",
      "Epoch 6: |          | 846/? [12:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 846, loss 4.302021026611328\n",
      "Epoch 6: |          | 847/? [12:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 847, loss 3.9120869636535645\n",
      "Epoch 6: |          | 848/? [12:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 848, loss 3.476191997528076\n",
      "Epoch 6: |          | 849/? [12:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 849, loss 3.540904998779297\n",
      "Epoch 6: |          | 850/? [12:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 850, loss 3.6551673412323\n",
      "Epoch 6: |          | 851/? [12:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 851, loss 3.9552173614501953\n",
      "Epoch 6: |          | 852/? [12:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 852, loss 4.087734222412109\n",
      "Epoch 6: |          | 853/? [12:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 853, loss 3.8879687786102295\n",
      "Epoch 6: |          | 854/? [12:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 854, loss 3.2679314613342285\n",
      "Epoch 6: |          | 855/? [12:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 855, loss 3.4989967346191406\n",
      "Epoch 6: |          | 856/? [12:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 856, loss 3.44905161857605\n",
      "Epoch 6: |          | 857/? [12:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 857, loss 3.9871697425842285\n",
      "Epoch 6: |          | 858/? [12:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 858, loss 3.894792079925537\n",
      "Epoch 6: |          | 859/? [12:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 859, loss 3.8932957649230957\n",
      "Epoch 6: |          | 860/? [12:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 860, loss 4.2768874168396\n",
      "Epoch 6: |          | 861/? [12:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 861, loss 3.5940794944763184\n",
      "Epoch 6: |          | 862/? [12:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 862, loss 3.915024995803833\n",
      "Epoch 6: |          | 863/? [12:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 863, loss 3.3054251670837402\n",
      "Epoch 6: |          | 864/? [12:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 864, loss 3.8358044624328613\n",
      "Epoch 6: |          | 865/? [12:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 865, loss 3.8637466430664062\n",
      "Epoch 6: |          | 866/? [12:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 866, loss 2.8956596851348877\n",
      "Epoch 6: |          | 867/? [12:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 867, loss 3.0399537086486816\n",
      "Epoch 6: |          | 868/? [12:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 868, loss 3.9237468242645264\n",
      "Epoch 6: |          | 869/? [12:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 869, loss 3.975238800048828\n",
      "Epoch 6: |          | 870/? [12:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 870, loss 3.589045763015747\n",
      "Epoch 6: |          | 871/? [12:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 871, loss 3.9240639209747314\n",
      "Epoch 6: |          | 872/? [12:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 872, loss 3.739474058151245\n",
      "Epoch 6: |          | 873/? [12:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 873, loss 3.7422382831573486\n",
      "Epoch 6: |          | 874/? [12:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 874, loss 3.270216464996338\n",
      "Epoch 6: |          | 875/? [12:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 875, loss 3.970810651779175\n",
      "Epoch 6: |          | 876/? [12:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 876, loss 3.50195574760437\n",
      "Epoch 6: |          | 877/? [12:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 877, loss 3.931203842163086\n",
      "Epoch 6: |          | 878/? [12:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 878, loss 3.3484489917755127\n",
      "Epoch 6: |          | 879/? [12:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 879, loss 3.400613307952881\n",
      "Epoch 6: |          | 880/? [12:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 880, loss 4.490334510803223\n",
      "Epoch 6: |          | 881/? [12:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 881, loss 3.897505283355713\n",
      "Epoch 6: |          | 882/? [12:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 882, loss 3.700596570968628\n",
      "Epoch 6: |          | 883/? [12:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 883, loss 3.846921920776367\n",
      "Epoch 6: |          | 884/? [12:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 884, loss 3.8499863147735596\n",
      "Epoch 6: |          | 885/? [12:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 885, loss 3.6327309608459473\n",
      "Epoch 6: |          | 886/? [12:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 886, loss 4.337904930114746\n",
      "Epoch 6: |          | 887/? [12:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 887, loss 4.284241676330566\n",
      "Epoch 6: |          | 888/? [12:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 888, loss 3.9924819469451904\n",
      "Epoch 6: |          | 889/? [12:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 889, loss 3.5725154876708984\n",
      "Epoch 6: |          | 890/? [12:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 890, loss 3.7814571857452393\n",
      "Epoch 6: |          | 891/? [12:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 891, loss 3.6224684715270996\n",
      "Epoch 6: |          | 892/? [12:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 892, loss 4.204886436462402\n",
      "Epoch 6: |          | 893/? [12:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 893, loss 3.6432368755340576\n",
      "Epoch 6: |          | 894/? [12:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 894, loss 3.150965452194214\n",
      "Epoch 6: |          | 895/? [12:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 895, loss 4.302635192871094\n",
      "Epoch 6: |          | 896/? [12:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 896, loss 3.894030809402466\n",
      "Epoch 6: |          | 897/? [12:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 897, loss 3.9225921630859375\n",
      "Epoch 6: |          | 898/? [12:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 898, loss 3.8829188346862793\n",
      "Epoch 6: |          | 899/? [12:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 899, loss 3.6637637615203857\n",
      "Epoch 6: |          | 900/? [12:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 900, loss 3.6061623096466064\n",
      "Epoch 6: |          | 901/? [12:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 901, loss 3.9835314750671387\n",
      "Epoch 6: |          | 902/? [12:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 902, loss 4.115097999572754\n",
      "Epoch 6: |          | 903/? [12:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 903, loss 3.428649425506592\n",
      "Epoch 6: |          | 904/? [12:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 904, loss 3.896768093109131\n",
      "Epoch 6: |          | 905/? [12:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 905, loss 4.071584224700928\n",
      "Epoch 6: |          | 906/? [12:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 906, loss 3.8324573040008545\n",
      "Epoch 6: |          | 907/? [12:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 907, loss 3.8995144367218018\n",
      "Epoch 6: |          | 908/? [12:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 908, loss 3.9703338146209717\n",
      "Epoch 6: |          | 909/? [12:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 909, loss 3.9349944591522217\n",
      "Epoch 6: |          | 910/? [12:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 910, loss 3.6969361305236816\n",
      "Epoch 6: |          | 911/? [12:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 911, loss 3.778461456298828\n",
      "Epoch 6: |          | 912/? [12:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 912, loss 3.7360901832580566\n",
      "Epoch 6: |          | 913/? [12:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 913, loss 3.7507567405700684\n",
      "Epoch 6: |          | 914/? [12:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 914, loss 3.99139666557312\n",
      "Epoch 6: |          | 915/? [13:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 915, loss 3.8471875190734863\n",
      "Epoch 6: |          | 916/? [13:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 916, loss 3.7999579906463623\n",
      "Epoch 6: |          | 917/? [13:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 917, loss 3.719348907470703\n",
      "Epoch 6: |          | 918/? [13:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 918, loss 3.653308153152466\n",
      "Epoch 6: |          | 919/? [13:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 919, loss 3.67197847366333\n",
      "Epoch 6: |          | 920/? [13:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 920, loss 3.821384906768799\n",
      "Epoch 6: |          | 921/? [13:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 921, loss 3.630344867706299\n",
      "Epoch 6: |          | 922/? [13:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 922, loss 3.798783540725708\n",
      "Epoch 6: |          | 923/? [13:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 923, loss 3.651038646697998\n",
      "Epoch 6: |          | 924/? [13:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 924, loss 3.668504238128662\n",
      "Epoch 6: |          | 925/? [13:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 925, loss 3.9650180339813232\n",
      "Epoch 6: |          | 926/? [13:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 926, loss 3.7277190685272217\n",
      "Epoch 6: |          | 927/? [13:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 927, loss 4.021535873413086\n",
      "Epoch 6: |          | 928/? [13:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 928, loss 3.5145316123962402\n",
      "Epoch 6: |          | 929/? [13:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 929, loss 3.60807728767395\n",
      "Epoch 6: |          | 930/? [13:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 930, loss 3.533254623413086\n",
      "Epoch 6: |          | 931/? [13:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 931, loss 3.2472968101501465\n",
      "Epoch 6: |          | 932/? [13:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 932, loss 3.886653184890747\n",
      "Epoch 6: |          | 933/? [13:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 933, loss 3.654634952545166\n",
      "Epoch 6: |          | 934/? [13:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 934, loss 4.2036237716674805\n",
      "Epoch 6: |          | 935/? [13:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 935, loss 4.511228561401367\n",
      "Epoch 6: |          | 936/? [13:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 936, loss 3.7504067420959473\n",
      "Epoch 6: |          | 937/? [13:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 937, loss 3.593339443206787\n",
      "Epoch 6: |          | 938/? [13:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 938, loss 3.6910526752471924\n",
      "Epoch 6: |          | 939/? [13:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 939, loss 3.928016185760498\n",
      "Epoch 6: |          | 940/? [13:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 940, loss 4.112295150756836\n",
      "Epoch 6: |          | 941/? [13:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 941, loss 3.6615073680877686\n",
      "Epoch 6: |          | 942/? [13:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 942, loss 3.1677334308624268\n",
      "Epoch 6: |          | 943/? [13:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 943, loss 3.9898171424865723\n",
      "Epoch 6: |          | 944/? [13:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 944, loss 3.1100313663482666\n",
      "Epoch 6: |          | 945/? [13:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 945, loss 3.7754948139190674\n",
      "Epoch 6: |          | 946/? [13:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 946, loss 3.6998767852783203\n",
      "Epoch 6: |          | 947/? [13:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 947, loss 3.6127846240997314\n",
      "Epoch 6: |          | 948/? [13:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 948, loss 3.878203868865967\n",
      "Epoch 6: |          | 949/? [13:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 949, loss 3.7256336212158203\n",
      "Epoch 6: |          | 950/? [13:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 950, loss 3.491300106048584\n",
      "Epoch 6: |          | 951/? [13:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 951, loss 4.160838603973389\n",
      "Epoch 6: |          | 952/? [13:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 952, loss 4.080230236053467\n",
      "Epoch 6: |          | 953/? [13:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 953, loss 4.635181427001953\n",
      "Epoch 6: |          | 954/? [13:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 954, loss 3.63236927986145\n",
      "Epoch 6: |          | 955/? [13:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 955, loss 4.248902320861816\n",
      "Epoch 6: |          | 956/? [13:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 956, loss 3.744762897491455\n",
      "Epoch 6: |          | 957/? [13:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 957, loss 3.8853232860565186\n",
      "Epoch 6: |          | 958/? [13:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 958, loss 3.963463544845581\n",
      "Epoch 6: |          | 959/? [13:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 959, loss 3.4076285362243652\n",
      "Epoch 6: |          | 960/? [13:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 960, loss 4.035312175750732\n",
      "Epoch 6: |          | 961/? [13:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 961, loss 4.300858497619629\n",
      "Epoch 6: |          | 962/? [13:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 962, loss 3.808467388153076\n",
      "Epoch 6: |          | 963/? [13:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 963, loss 3.6146953105926514\n",
      "Epoch 6: |          | 964/? [13:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 964, loss 4.045527458190918\n",
      "Epoch 6: |          | 965/? [13:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 965, loss 3.529212236404419\n",
      "Epoch 6: |          | 966/? [13:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 966, loss 3.429874897003174\n",
      "Epoch 6: |          | 967/? [13:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 967, loss 3.678820848464966\n",
      "Epoch 6: |          | 968/? [13:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 968, loss 3.574899673461914\n",
      "Epoch 6: |          | 969/? [13:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 969, loss 3.4836583137512207\n",
      "Epoch 6: |          | 970/? [13:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 970, loss 3.932543992996216\n",
      "Epoch 6: |          | 971/? [13:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 971, loss 4.1336774826049805\n",
      "Epoch 6: |          | 972/? [13:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 972, loss 3.5800957679748535\n",
      "Epoch 6: |          | 973/? [13:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 973, loss 3.751331329345703\n",
      "Epoch 6: |          | 974/? [13:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 974, loss 3.8376097679138184\n",
      "Epoch 6: |          | 975/? [13:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 975, loss 3.846595048904419\n",
      "Epoch 6: |          | 976/? [13:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 976, loss 3.894909620285034\n",
      "Epoch 6: |          | 977/? [13:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 977, loss 4.461775779724121\n",
      "Epoch 6: |          | 978/? [13:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 978, loss 3.9559226036071777\n",
      "Epoch 6: |          | 979/? [13:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 979, loss 4.1668901443481445\n",
      "Epoch 6: |          | 980/? [13:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 980, loss 3.357123613357544\n",
      "Epoch 6: |          | 981/? [13:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 981, loss 3.2112858295440674\n",
      "Epoch 6: |          | 982/? [13:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 982, loss 3.8489041328430176\n",
      "Epoch 6: |          | 983/? [13:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 983, loss 4.257518291473389\n",
      "Epoch 6: |          | 984/? [13:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 984, loss 3.356494188308716\n",
      "Epoch 6: |          | 985/? [13:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 985, loss 3.5962719917297363\n",
      "Epoch 6: |          | 986/? [13:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 986, loss 3.598750352859497\n",
      "Epoch 6: |          | 987/? [14:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 987, loss 3.1532111167907715\n",
      "Epoch 6: |          | 988/? [14:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 988, loss 4.169993877410889\n",
      "Epoch 6: |          | 989/? [14:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 989, loss 3.7845025062561035\n",
      "Epoch 6: |          | 990/? [14:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 990, loss 3.155890703201294\n",
      "Epoch 6: |          | 991/? [14:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 991, loss 3.889129161834717\n",
      "Epoch 6: |          | 992/? [14:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 992, loss 4.5369367599487305\n",
      "Epoch 6: |          | 993/? [14:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 993, loss 3.6532721519470215\n",
      "Epoch 6: |          | 994/? [14:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 994, loss 3.640038013458252\n",
      "Epoch 6: |          | 995/? [14:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 995, loss 4.088918209075928\n",
      "Epoch 6: |          | 996/? [14:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 996, loss 4.041502475738525\n",
      "Epoch 6: |          | 997/? [14:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 997, loss 3.6802818775177\n",
      "Epoch 6: |          | 998/? [14:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 998, loss 3.9017434120178223\n",
      "Epoch 6: |          | 999/? [14:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 999, loss 3.8935706615448\n",
      "Epoch 6: |          | 1000/? [14:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1000, loss 3.3554515838623047\n",
      "Epoch 6: |          | 1001/? [14:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1001, loss 4.070954322814941\n",
      "Epoch 6: |          | 1002/? [14:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1002, loss 4.008942127227783\n",
      "Epoch 6: |          | 1003/? [14:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1003, loss 4.213939666748047\n",
      "Epoch 6: |          | 1004/? [14:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1004, loss 3.2507712841033936\n",
      "Epoch 6: |          | 1005/? [14:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1005, loss 3.761914014816284\n",
      "Epoch 6: |          | 1006/? [14:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1006, loss 4.090752601623535\n",
      "Epoch 6: |          | 1007/? [14:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1007, loss 3.6873104572296143\n",
      "Epoch 6: |          | 1008/? [14:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1008, loss 3.8118464946746826\n",
      "Epoch 6: |          | 1009/? [14:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1009, loss 4.059538841247559\n",
      "Epoch 6: |          | 1010/? [14:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1010, loss 3.215759754180908\n",
      "Epoch 6: |          | 1011/? [14:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1011, loss 3.7880733013153076\n",
      "Epoch 6: |          | 1012/? [14:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1012, loss 3.5884006023406982\n",
      "Epoch 6: |          | 1013/? [14:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1013, loss 3.7531299591064453\n",
      "Epoch 6: |          | 1014/? [14:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1014, loss 4.196126937866211\n",
      "Epoch 6: |          | 1015/? [14:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1015, loss 3.8330540657043457\n",
      "Epoch 6: |          | 1016/? [14:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1016, loss 3.6226558685302734\n",
      "Epoch 6: |          | 1017/? [14:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1017, loss 3.125596523284912\n",
      "Epoch 6: |          | 1018/? [14:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1018, loss 3.703409194946289\n",
      "Epoch 6: |          | 1019/? [14:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1019, loss 3.7828783988952637\n",
      "Epoch 6: |          | 1020/? [14:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1020, loss 3.363327741622925\n",
      "Epoch 6: |          | 1021/? [14:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1021, loss 3.6163737773895264\n",
      "Epoch 6: |          | 1022/? [14:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1022, loss 3.421782970428467\n",
      "Epoch 6: |          | 1023/? [14:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1023, loss 3.1576590538024902\n",
      "Epoch 6: |          | 1024/? [14:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1024, loss 3.6213936805725098\n",
      "Epoch 6: |          | 1025/? [14:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1025, loss 3.5281548500061035\n",
      "Epoch 6: |          | 1026/? [14:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1026, loss 2.67083740234375\n",
      "Epoch 6: |          | 1027/? [14:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1027, loss 3.8323974609375\n",
      "Epoch 6: |          | 1028/? [14:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1028, loss 3.674805164337158\n",
      "Epoch 6: |          | 1029/? [14:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1029, loss 3.5835914611816406\n",
      "Epoch 6: |          | 1030/? [14:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1030, loss 3.4061694145202637\n",
      "Epoch 6: |          | 1031/? [14:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1031, loss 3.4677436351776123\n",
      "Epoch 6: |          | 1032/? [14:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1032, loss 3.889185667037964\n",
      "Epoch 6: |          | 1033/? [14:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1033, loss 4.165230751037598\n",
      "Epoch 6: |          | 1034/? [14:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1034, loss 3.544785261154175\n",
      "Epoch 6: |          | 1035/? [14:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1035, loss 3.5527892112731934\n",
      "Epoch 6: |          | 1036/? [14:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1036, loss 3.513988494873047\n",
      "Epoch 6: |          | 1037/? [14:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1037, loss 4.114129066467285\n",
      "Epoch 6: |          | 1038/? [14:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1038, loss 4.260336875915527\n",
      "Epoch 6: |          | 1039/? [14:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1039, loss 4.5585174560546875\n",
      "Epoch 6: |          | 1040/? [14:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1040, loss 3.8710625171661377\n",
      "Epoch 6: |          | 1041/? [14:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1041, loss 4.164670944213867\n",
      "Epoch 6: |          | 1042/? [14:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1042, loss 3.7545628547668457\n",
      "Epoch 6: |          | 1043/? [14:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1043, loss 4.104642391204834\n",
      "Epoch 6: |          | 1044/? [14:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1044, loss 3.6832594871520996\n",
      "Epoch 6: |          | 1045/? [14:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1045, loss 3.276175022125244\n",
      "Epoch 6: |          | 1046/? [14:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1046, loss 3.1441454887390137\n",
      "Epoch 6: |          | 1047/? [14:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1047, loss 4.217450141906738\n",
      "Epoch 6: |          | 1048/? [14:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1048, loss 3.703115940093994\n",
      "Epoch 6: |          | 1049/? [14:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1049, loss 3.9301791191101074\n",
      "Epoch 6: |          | 1050/? [14:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1050, loss 3.524014711380005\n",
      "Epoch 6: |          | 1051/? [14:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1051, loss 3.4274990558624268\n",
      "Epoch 6: |          | 1052/? [14:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1052, loss 4.056962013244629\n",
      "Epoch 6: |          | 1053/? [14:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1053, loss 4.2127275466918945\n",
      "Epoch 6: |          | 1054/? [14:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1054, loss 3.6562581062316895\n",
      "Epoch 6: |          | 1055/? [14:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1055, loss 3.323960065841675\n",
      "Epoch 6: |          | 1056/? [14:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1056, loss 3.314195156097412\n",
      "Epoch 6: |          | 1057/? [14:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1057, loss 3.955728530883789\n",
      "Epoch 6: |          | 1058/? [15:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1058, loss 3.540891647338867\n",
      "Epoch 6: |          | 1059/? [15:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1059, loss 4.127134323120117\n",
      "Epoch 6: |          | 1060/? [15:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1060, loss 3.9634156227111816\n",
      "Epoch 6: |          | 1061/? [15:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1061, loss 2.7743191719055176\n",
      "Epoch 6: |          | 1062/? [15:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1062, loss 3.633909225463867\n",
      "Epoch 6: |          | 1063/? [15:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1063, loss 3.7058157920837402\n",
      "Epoch 6: |          | 1064/? [15:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1064, loss 3.9076263904571533\n",
      "Epoch 6: |          | 1065/? [15:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1065, loss 2.534883499145508\n",
      "Epoch 6: |          | 1066/? [15:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1066, loss 3.852743625640869\n",
      "Epoch 6: |          | 1067/? [15:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1067, loss 3.3765311241149902\n",
      "Epoch 6: |          | 1068/? [15:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1068, loss 3.500947952270508\n",
      "Epoch 6: |          | 1069/? [15:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1069, loss 3.936196804046631\n",
      "Epoch 6: |          | 1070/? [15:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1070, loss 3.6621181964874268\n",
      "Epoch 6: |          | 1071/? [15:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1071, loss 4.016773700714111\n",
      "Epoch 6: |          | 1072/? [15:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1072, loss 4.062497615814209\n",
      "Epoch 6: |          | 1073/? [15:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1073, loss 4.22100830078125\n",
      "Epoch 6: |          | 1074/? [15:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1074, loss 3.5814247131347656\n",
      "Epoch 6: |          | 1075/? [15:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1075, loss 3.4097912311553955\n",
      "Epoch 6: |          | 1076/? [15:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1076, loss 4.0094146728515625\n",
      "Epoch 6: |          | 1077/? [15:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1077, loss 3.513216018676758\n",
      "Epoch 6: |          | 1078/? [15:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1078, loss 3.7804908752441406\n",
      "Epoch 6: |          | 1079/? [15:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1079, loss 4.243381977081299\n",
      "Epoch 6: |          | 1080/? [15:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1080, loss 3.6708412170410156\n",
      "Epoch 6: |          | 1081/? [15:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1081, loss 3.9858062267303467\n",
      "Epoch 6: |          | 1082/? [15:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1082, loss 3.6037230491638184\n",
      "Epoch 6: |          | 1083/? [15:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1083, loss 3.143620252609253\n",
      "Epoch 6: |          | 1084/? [15:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1084, loss 2.985396146774292\n",
      "Epoch 6: |          | 1085/? [15:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1085, loss 3.6452317237854004\n",
      "Epoch 6: |          | 1086/? [15:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1086, loss 3.9258227348327637\n",
      "Epoch 6: |          | 1087/? [15:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1087, loss 4.395442008972168\n",
      "Epoch 6: |          | 1088/? [15:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1088, loss 3.9760851860046387\n",
      "Epoch 6: |          | 1089/? [15:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1089, loss 3.9163947105407715\n",
      "Epoch 6: |          | 1090/? [15:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1090, loss 3.7387566566467285\n",
      "Epoch 6: |          | 1091/? [15:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1091, loss 3.554800033569336\n",
      "Epoch 6: |          | 1092/? [15:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1092, loss 3.8395466804504395\n",
      "Epoch 6: |          | 1093/? [15:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1093, loss 3.3344993591308594\n",
      "Epoch 6: |          | 1094/? [15:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1094, loss 3.8459300994873047\n",
      "Epoch 6: |          | 1095/? [15:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1095, loss 3.848949909210205\n",
      "Epoch 6: |          | 1096/? [15:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1096, loss 4.113486289978027\n",
      "Epoch 6: |          | 1097/? [15:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1097, loss 3.708840847015381\n",
      "Epoch 6: |          | 1098/? [15:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1098, loss 3.023465156555176\n",
      "Epoch 6: |          | 1099/? [15:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1099, loss 3.653224229812622\n",
      "Epoch 6: |          | 1100/? [15:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1100, loss 3.8512771129608154\n",
      "Epoch 6: |          | 1101/? [15:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1101, loss 3.4860026836395264\n",
      "Epoch 6: |          | 1102/? [15:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1102, loss 4.245152950286865\n",
      "Epoch 6: |          | 1103/? [15:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1103, loss 4.622158527374268\n",
      "Epoch 6: |          | 1104/? [15:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1104, loss 4.028628349304199\n",
      "Epoch 6: |          | 1105/? [15:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1105, loss 4.1560444831848145\n",
      "Epoch 6: |          | 1106/? [15:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1106, loss 3.6321861743927\n",
      "Epoch 6: |          | 1107/? [15:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1107, loss 3.7997946739196777\n",
      "Epoch 6: |          | 1108/? [15:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1108, loss 3.7920615673065186\n",
      "Epoch 6: |          | 1109/? [15:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1109, loss 3.404834747314453\n",
      "Epoch 6: |          | 1110/? [15:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1110, loss 4.283541679382324\n",
      "Epoch 6: |          | 1111/? [15:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1111, loss 3.984475612640381\n",
      "Epoch 6: |          | 1112/? [15:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1112, loss 3.8341987133026123\n",
      "Epoch 6: |          | 1113/? [15:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1113, loss 3.6541709899902344\n",
      "Epoch 6: |          | 1114/? [15:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1114, loss 3.12551212310791\n",
      "Epoch 6: |          | 1115/? [15:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1115, loss 2.932194471359253\n",
      "Epoch 6: |          | 1116/? [15:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1116, loss 3.2728328704833984\n",
      "Epoch 6: |          | 1117/? [15:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1117, loss 3.4854583740234375\n",
      "Epoch 6: |          | 1118/? [15:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1118, loss 3.58817720413208\n",
      "Epoch 6: |          | 1119/? [15:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1119, loss 4.175360202789307\n",
      "Epoch 6: |          | 1120/? [15:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1120, loss 3.782978057861328\n",
      "Epoch 6: |          | 1121/? [15:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1121, loss 3.971994400024414\n",
      "Epoch 6: |          | 1122/? [15:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1122, loss 3.519440174102783\n",
      "Epoch 6: |          | 1123/? [15:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1123, loss 3.7634353637695312\n",
      "Epoch 6: |          | 1124/? [15:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1124, loss 4.1012043952941895\n",
      "Epoch 6: |          | 1125/? [15:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1125, loss 3.4382381439208984\n",
      "Epoch 6: |          | 1126/? [15:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1126, loss 3.3534786701202393\n",
      "Epoch 6: |          | 1127/? [15:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1127, loss 3.65136456489563\n",
      "Epoch 6: |          | 1128/? [15:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1128, loss 3.7500674724578857\n",
      "Epoch 6: |          | 1129/? [15:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1129, loss 3.8312275409698486\n",
      "Epoch 6: |          | 1130/? [16:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1130, loss 4.021779537200928\n",
      "Epoch 6: |          | 1131/? [16:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1131, loss 4.0921220779418945\n",
      "Epoch 6: |          | 1132/? [16:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1132, loss 2.921050548553467\n",
      "Epoch 6: |          | 1133/? [16:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1133, loss 3.7038466930389404\n",
      "Epoch 6: |          | 1134/? [16:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1134, loss 3.512221097946167\n",
      "Epoch 6: |          | 1135/? [16:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1135, loss 4.115048408508301\n",
      "Epoch 6: |          | 1136/? [16:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1136, loss 3.751755952835083\n",
      "Epoch 6: |          | 1137/? [16:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1137, loss 3.814495086669922\n",
      "Epoch 6: |          | 1138/? [16:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1138, loss 4.2361650466918945\n",
      "Epoch 6: |          | 1139/? [16:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1139, loss 3.958881378173828\n",
      "Epoch 6: |          | 1140/? [16:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1140, loss 3.601811170578003\n",
      "Epoch 6: |          | 1141/? [16:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1141, loss 4.085984230041504\n",
      "Epoch 6: |          | 1142/? [16:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1142, loss 4.20819616317749\n",
      "Epoch 6: |          | 1143/? [16:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1143, loss 4.187460422515869\n",
      "Epoch 6: |          | 1144/? [16:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1144, loss 3.5990688800811768\n",
      "Epoch 6: |          | 1145/? [16:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1145, loss 3.766256332397461\n",
      "Epoch 6: |          | 1146/? [16:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1146, loss 3.356440782546997\n",
      "Epoch 6: |          | 1147/? [16:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1147, loss 3.34930157661438\n",
      "Epoch 6: |          | 1148/? [16:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1148, loss 3.5503406524658203\n",
      "Epoch 6: |          | 1149/? [16:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1149, loss 4.432971954345703\n",
      "Epoch 6: |          | 1150/? [16:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1150, loss 3.939887285232544\n",
      "Epoch 6: |          | 1151/? [16:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1151, loss 4.191235065460205\n",
      "Epoch 6: |          | 1152/? [16:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1152, loss 3.479339122772217\n",
      "Epoch 6: |          | 1153/? [16:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1153, loss 3.842092990875244\n",
      "Epoch 6: |          | 1154/? [16:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1154, loss 3.514150619506836\n",
      "Epoch 6: |          | 1155/? [16:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1155, loss 3.7503883838653564\n",
      "Epoch 6: |          | 1156/? [16:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1156, loss 3.725877046585083\n",
      "Epoch 6: |          | 1157/? [16:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1157, loss 4.016712665557861\n",
      "Epoch 6: |          | 1158/? [16:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1158, loss 4.1651997566223145\n",
      "Epoch 6: |          | 1159/? [16:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1159, loss 3.0033960342407227\n",
      "Epoch 6: |          | 1160/? [16:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1160, loss 4.145702362060547\n",
      "Epoch 6: |          | 1161/? [16:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1161, loss 4.022052764892578\n",
      "Epoch 6: |          | 1162/? [16:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1162, loss 3.929816484451294\n",
      "Epoch 6: |          | 1163/? [16:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1163, loss 4.5176496505737305\n",
      "Epoch 6: |          | 1164/? [16:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1164, loss 4.282207012176514\n",
      "Epoch 6: |          | 1165/? [16:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1165, loss 3.4760124683380127\n",
      "Epoch 6: |          | 1166/? [16:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1166, loss 3.950869083404541\n",
      "Epoch 6: |          | 1167/? [16:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1167, loss 3.9944095611572266\n",
      "Epoch 6: |          | 1168/? [16:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1168, loss 4.4230475425720215\n",
      "Epoch 6: |          | 1169/? [16:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1169, loss 3.5650126934051514\n",
      "Epoch 6: |          | 1170/? [16:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1170, loss 4.069636344909668\n",
      "Epoch 6: |          | 1171/? [16:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1171, loss 3.4382503032684326\n",
      "Epoch 6: |          | 1172/? [16:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1172, loss 3.393543243408203\n",
      "Epoch 6: |          | 1173/? [16:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1173, loss 3.949138641357422\n",
      "Epoch 6: |          | 1174/? [16:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1174, loss 3.4411754608154297\n",
      "Epoch 6: |          | 1175/? [16:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1175, loss 4.010720252990723\n",
      "Epoch 6: |          | 1176/? [16:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1176, loss 4.067607879638672\n",
      "Epoch 6: |          | 1177/? [16:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1177, loss 4.223182201385498\n",
      "Epoch 6: |          | 1178/? [16:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1178, loss 3.6420886516571045\n",
      "Epoch 6: |          | 1179/? [16:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1179, loss 4.159787178039551\n",
      "Epoch 6: |          | 1180/? [16:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1180, loss 4.016847610473633\n",
      "Epoch 6: |          | 1181/? [16:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1181, loss 3.9245522022247314\n",
      "Epoch 6: |          | 1182/? [16:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1182, loss 3.762989044189453\n",
      "Epoch 6: |          | 1183/? [16:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1183, loss 3.464362621307373\n",
      "Epoch 6: |          | 1184/? [16:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1184, loss 3.898232936859131\n",
      "Epoch 6: |          | 1185/? [16:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1185, loss 3.619353771209717\n",
      "Epoch 6: |          | 1186/? [16:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1186, loss 3.844669818878174\n",
      "Epoch 6: |          | 1187/? [16:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1187, loss 3.7397773265838623\n",
      "Epoch 6: |          | 1188/? [16:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1188, loss 4.114063262939453\n",
      "Epoch 6: |          | 1189/? [16:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1189, loss 4.170114517211914\n",
      "Epoch 6: |          | 1190/? [16:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1190, loss 3.7782223224639893\n",
      "Epoch 6: |          | 1191/? [16:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1191, loss 3.7973532676696777\n",
      "Epoch 6: |          | 1192/? [16:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1192, loss 4.064383506774902\n",
      "Epoch 6: |          | 1193/? [16:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1193, loss 3.5719685554504395\n",
      "Epoch 6: |          | 1194/? [16:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1194, loss 3.2921719551086426\n",
      "Epoch 6: |          | 1195/? [16:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1195, loss 3.7936482429504395\n",
      "Epoch 6: |          | 1196/? [16:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1196, loss 4.000059604644775\n",
      "Epoch 6: |          | 1197/? [16:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1197, loss 3.7595601081848145\n",
      "Epoch 6: |          | 1198/? [16:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1198, loss 3.883199691772461\n",
      "Epoch 6: |          | 1199/? [16:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1199, loss 4.159596920013428\n",
      "Epoch 6: |          | 1200/? [17:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1200, loss 3.3401801586151123\n",
      "Epoch 6: |          | 1201/? [17:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1201, loss 3.9807076454162598\n",
      "Epoch 6: |          | 1202/? [17:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1202, loss 3.6727046966552734\n",
      "Epoch 6: |          | 1203/? [17:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1203, loss 3.6391587257385254\n",
      "Epoch 6: |          | 1204/? [17:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1204, loss 3.2175045013427734\n",
      "Epoch 6: |          | 1205/? [17:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1205, loss 3.796722412109375\n",
      "Epoch 6: |          | 1206/? [17:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1206, loss 3.7993907928466797\n",
      "Epoch 6: |          | 1207/? [17:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1207, loss 4.0341315269470215\n",
      "Epoch 6: |          | 1208/? [17:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1208, loss 4.241371154785156\n",
      "Epoch 6: |          | 1209/? [17:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1209, loss 3.827023983001709\n",
      "Epoch 6: |          | 1210/? [17:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1210, loss 4.096879005432129\n",
      "Epoch 6: |          | 1211/? [17:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1211, loss 4.0933837890625\n",
      "Epoch 6: |          | 1212/? [17:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1212, loss 3.871623992919922\n",
      "Epoch 6: |          | 1213/? [17:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1213, loss 3.614227771759033\n",
      "Epoch 6: |          | 1214/? [17:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1214, loss 4.154516696929932\n",
      "Epoch 6: |          | 1215/? [17:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1215, loss 3.653883457183838\n",
      "Epoch 6: |          | 1216/? [17:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1216, loss 3.758906602859497\n",
      "Epoch 6: |          | 1217/? [17:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1217, loss 3.854466676712036\n",
      "Epoch 6: |          | 1218/? [17:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1218, loss 3.9673564434051514\n",
      "Epoch 6: |          | 1219/? [17:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1219, loss 3.6100049018859863\n",
      "Epoch 6: |          | 1220/? [17:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1220, loss 4.30409574508667\n",
      "Epoch 6: |          | 1221/? [17:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1221, loss 3.8083176612854004\n",
      "Epoch 6: |          | 1222/? [17:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1222, loss 3.0030062198638916\n",
      "Epoch 6: |          | 1223/? [17:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1223, loss 3.2019901275634766\n",
      "Epoch 6: |          | 1224/? [17:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1224, loss 3.544534683227539\n",
      "Epoch 6: |          | 1225/? [17:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1225, loss 4.201998233795166\n",
      "Epoch 6: |          | 1226/? [17:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1226, loss 4.16265869140625\n",
      "Epoch 6: |          | 1227/? [17:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1227, loss 3.8230648040771484\n",
      "Epoch 6: |          | 1228/? [17:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1228, loss 3.660717487335205\n",
      "Epoch 6: |          | 1229/? [17:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1229, loss 3.304664134979248\n",
      "Epoch 6: |          | 1230/? [17:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1230, loss 3.9407505989074707\n",
      "Epoch 6: |          | 1231/? [17:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1231, loss 3.949862241744995\n",
      "Epoch 6: |          | 1232/? [17:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1232, loss 4.111764430999756\n",
      "Epoch 6: |          | 1233/? [17:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1233, loss 3.945899248123169\n",
      "Epoch 6: |          | 1234/? [17:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1234, loss 2.956259250640869\n",
      "Epoch 6: |          | 1235/? [17:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1235, loss 4.045619010925293\n",
      "Epoch 6: |          | 1236/? [17:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1236, loss 3.4377923011779785\n",
      "Epoch 6: |          | 1237/? [17:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1237, loss 3.7454094886779785\n",
      "Epoch 6: |          | 1238/? [17:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1238, loss 3.797746181488037\n",
      "Epoch 6: |          | 1239/? [17:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1239, loss 3.6465277671813965\n",
      "Epoch 6: |          | 1240/? [17:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1240, loss 4.216939926147461\n",
      "Epoch 6: |          | 1241/? [17:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1241, loss 3.790614604949951\n",
      "Epoch 6: |          | 1242/? [17:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1242, loss 3.642267942428589\n",
      "Epoch 6: |          | 1243/? [17:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1243, loss 3.535429000854492\n",
      "Epoch 6: |          | 1244/? [17:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1244, loss 3.669031858444214\n",
      "Epoch 6: |          | 1245/? [17:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1245, loss 3.2668704986572266\n",
      "Epoch 6: |          | 1246/? [17:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1246, loss 3.934053897857666\n",
      "Epoch 6: |          | 1247/? [17:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1247, loss 3.9480011463165283\n",
      "Epoch 6: |          | 1248/? [17:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1248, loss 3.5227737426757812\n",
      "Epoch 6: |          | 1249/? [17:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1249, loss 3.690108060836792\n",
      "Epoch 6: |          | 1250/? [17:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1250, loss 3.8411381244659424\n",
      "Epoch 6: |          | 1251/? [17:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1251, loss 3.5806682109832764\n",
      "Epoch 6: |          | 1252/? [17:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 1252, loss 4.350837230682373\n",
      "Epoch 6: |          | 1253/? [17:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1253, loss 3.6835646629333496\n",
      "Epoch 6: |          | 1254/? [17:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1254, loss 3.049720287322998\n",
      "Epoch 6: |          | 1255/? [17:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1255, loss 4.381072044372559\n",
      "Epoch 6: |          | 1256/? [17:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1256, loss 3.4361050128936768\n",
      "Epoch 6: |          | 1257/? [17:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1257, loss 3.458693027496338\n",
      "Epoch 6: |          | 1258/? [17:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1258, loss 4.151613235473633\n",
      "Epoch 6: |          | 1259/? [17:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1259, loss 3.8363304138183594\n",
      "Epoch 6: |          | 1260/? [17:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1260, loss 4.267913341522217\n",
      "Epoch 6: |          | 1261/? [17:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1261, loss 3.6297359466552734\n",
      "Epoch 6: |          | 1262/? [17:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1262, loss 3.616114377975464\n",
      "Epoch 6: |          | 1263/? [17:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1263, loss 3.9298248291015625\n",
      "Epoch 6: |          | 1264/? [17:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1264, loss 4.184299468994141\n",
      "Epoch 6: |          | 1265/? [17:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1265, loss 4.0344085693359375\n",
      "Epoch 6: |          | 1266/? [17:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1266, loss 3.7361786365509033\n",
      "Epoch 6: |          | 1267/? [17:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1267, loss 3.84769868850708\n",
      "Epoch 6: |          | 1268/? [17:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1268, loss 3.7156596183776855\n",
      "Epoch 6: |          | 1269/? [18:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1269, loss 3.248486042022705\n",
      "Epoch 6: |          | 1270/? [18:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1270, loss 3.5775020122528076\n",
      "Epoch 6: |          | 1271/? [18:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1271, loss 3.7642664909362793\n",
      "Epoch 6: |          | 1272/? [18:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1272, loss 3.3682899475097656\n",
      "Epoch 6: |          | 1273/? [18:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1273, loss 4.0329132080078125\n",
      "Epoch 6: |          | 1274/? [18:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1274, loss 2.950655460357666\n",
      "Epoch 6: |          | 1275/? [18:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1275, loss 3.4864203929901123\n",
      "Epoch 6: |          | 1276/? [18:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1276, loss 3.7836170196533203\n",
      "Epoch 6: |          | 1277/? [18:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1277, loss 3.5129265785217285\n",
      "Epoch 6: |          | 1278/? [18:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1278, loss 3.1816818714141846\n",
      "Epoch 6: |          | 1279/? [18:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1279, loss 3.8814315795898438\n",
      "Epoch 6: |          | 1280/? [18:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1280, loss 3.168675184249878\n",
      "Epoch 6: |          | 1281/? [18:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1281, loss 3.628732681274414\n",
      "Epoch 6: |          | 1282/? [18:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1282, loss 3.341562271118164\n",
      "Epoch 6: |          | 1283/? [18:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1283, loss 4.030167579650879\n",
      "Epoch 6: |          | 1284/? [18:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1284, loss 3.1853954792022705\n",
      "Epoch 6: |          | 1285/? [18:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1285, loss 4.196691989898682\n",
      "Epoch 6: |          | 1286/? [18:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1286, loss 2.7158424854278564\n",
      "Epoch 6: |          | 1287/? [18:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1287, loss 4.135387420654297\n",
      "Epoch 6: |          | 1288/? [18:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1288, loss 3.9385929107666016\n",
      "Epoch 6: |          | 1289/? [18:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1289, loss 3.1261439323425293\n",
      "Epoch 6: |          | 1290/? [18:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1290, loss 3.840895175933838\n",
      "Epoch 6: |          | 1291/? [18:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1291, loss 4.7050557136535645\n",
      "Epoch 6: |          | 1292/? [18:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1292, loss 3.974255323410034\n",
      "Epoch 6: |          | 1293/? [18:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1293, loss 3.547394275665283\n",
      "Epoch 6: |          | 1294/? [18:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1294, loss 3.7623162269592285\n",
      "Epoch 6: |          | 1295/? [18:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1295, loss 3.8584837913513184\n",
      "Epoch 6: |          | 1296/? [18:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1296, loss 3.1135635375976562\n",
      "Epoch 6: |          | 1297/? [18:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1297, loss 4.03040885925293\n",
      "Epoch 6: |          | 1298/? [18:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1298, loss 3.741812229156494\n",
      "Epoch 6: |          | 1299/? [18:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1299, loss 2.9234182834625244\n",
      "Epoch 6: |          | 1300/? [18:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1300, loss 3.7907776832580566\n",
      "Epoch 6: |          | 1301/? [18:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1301, loss 3.490694522857666\n",
      "Epoch 6: |          | 1302/? [18:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1302, loss 3.6178436279296875\n",
      "Epoch 6: |          | 1303/? [18:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1303, loss 3.560236692428589\n",
      "Epoch 6: |          | 1304/? [18:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1304, loss 4.302732944488525\n",
      "Epoch 6: |          | 1305/? [18:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1305, loss 3.1278023719787598\n",
      "Epoch 6: |          | 1306/? [18:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1306, loss 3.744323253631592\n",
      "Epoch 6: |          | 1307/? [18:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1307, loss 3.3739631175994873\n",
      "Epoch 6: |          | 1308/? [18:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1308, loss 3.3303020000457764\n",
      "Epoch 6: |          | 1309/? [18:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1309, loss 3.3413329124450684\n",
      "Epoch 6: |          | 1310/? [18:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1310, loss 3.8925018310546875\n",
      "Epoch 6: |          | 1311/? [18:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1311, loss 3.3709263801574707\n",
      "Epoch 6: |          | 1312/? [18:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1312, loss 3.1179652214050293\n",
      "Epoch 6: |          | 1313/? [18:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1313, loss 4.248517990112305\n",
      "Epoch 6: |          | 1314/? [18:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1314, loss 3.491145372390747\n",
      "Epoch 6: |          | 1315/? [18:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1315, loss 4.216204643249512\n",
      "Epoch 6: |          | 1316/? [18:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1316, loss 3.9969398975372314\n",
      "Epoch 6: |          | 1317/? [18:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1317, loss 3.6337647438049316\n",
      "Epoch 6: |          | 1318/? [18:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1318, loss 3.799461841583252\n",
      "Epoch 6: |          | 1319/? [18:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1319, loss 3.988576889038086\n",
      "Epoch 6: |          | 1320/? [18:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1320, loss 3.5662713050842285\n",
      "Epoch 6: |          | 1321/? [18:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1321, loss 4.034979343414307\n",
      "Epoch 6: |          | 1322/? [18:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1322, loss 3.8636112213134766\n",
      "Epoch 6: |          | 1323/? [18:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1323, loss 3.396820545196533\n",
      "Epoch 6: |          | 1324/? [18:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1324, loss 4.284902572631836\n",
      "Epoch 6: |          | 1325/? [18:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1325, loss 4.423159122467041\n",
      "Epoch 6: |          | 1326/? [18:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1326, loss 3.871351957321167\n",
      "Epoch 6: |          | 1327/? [18:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1327, loss 3.7552623748779297\n",
      "Epoch 6: |          | 1328/? [18:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1328, loss 3.420668840408325\n",
      "Epoch 6: |          | 1329/? [18:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1329, loss 4.006855487823486\n",
      "Epoch 6: |          | 1330/? [18:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1330, loss 3.758990526199341\n",
      "Epoch 6: |          | 1331/? [18:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1331, loss 3.8613955974578857\n",
      "Epoch 6: |          | 1332/? [18:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1332, loss 3.5662734508514404\n",
      "Epoch 6: |          | 1333/? [18:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1333, loss 3.5470142364501953\n",
      "Epoch 6: |          | 1334/? [19:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1334, loss 3.664424180984497\n",
      "Epoch 6: |          | 1335/? [19:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1335, loss 3.6428534984588623\n",
      "Epoch 6: |          | 1336/? [19:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1336, loss 3.2297186851501465\n",
      "Epoch 6: |          | 1337/? [19:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1337, loss 3.8514695167541504\n",
      "Epoch 6: |          | 1338/? [19:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1338, loss 3.123443126678467\n",
      "Epoch 6: |          | 1339/? [19:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1339, loss 3.733905792236328\n",
      "Epoch 6: |          | 1340/? [19:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1340, loss 3.1856863498687744\n",
      "Epoch 6: |          | 1341/? [19:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1341, loss 3.912724018096924\n",
      "Epoch 6: |          | 1342/? [19:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1342, loss 4.133670806884766\n",
      "Epoch 6: |          | 1343/? [19:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1343, loss 3.659524917602539\n",
      "Epoch 6: |          | 1344/? [19:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1344, loss 3.7708640098571777\n",
      "Epoch 6: |          | 1345/? [19:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1345, loss 3.9285666942596436\n",
      "Epoch 6: |          | 1346/? [19:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1346, loss 4.923659801483154\n",
      "Epoch 6: |          | 1347/? [19:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1347, loss 3.912135601043701\n",
      "Epoch 6: |          | 1348/? [19:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1348, loss 3.985558271408081\n",
      "Epoch 6: |          | 1349/? [19:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1349, loss 3.947552442550659\n",
      "Epoch 6: |          | 1350/? [19:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1350, loss 3.9486725330352783\n",
      "Epoch 6: |          | 1351/? [19:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1351, loss 3.927013874053955\n",
      "Epoch 6: |          | 1352/? [19:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1352, loss 3.2109904289245605\n",
      "Epoch 6: |          | 1353/? [19:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1353, loss 3.556718111038208\n",
      "Epoch 6: |          | 1354/? [19:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1354, loss 3.9333386421203613\n",
      "Epoch 6: |          | 1355/? [19:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1355, loss 4.069543361663818\n",
      "Epoch 6: |          | 1356/? [19:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1356, loss 3.8393359184265137\n",
      "Epoch 6: |          | 1357/? [19:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1357, loss 3.6155879497528076\n",
      "Epoch 6: |          | 1358/? [19:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1358, loss 3.7661094665527344\n",
      "Epoch 6: |          | 1359/? [19:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1359, loss 3.669848680496216\n",
      "Epoch 6: |          | 1360/? [19:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1360, loss 3.8154404163360596\n",
      "Epoch 6: |          | 1361/? [19:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1361, loss 3.797536849975586\n",
      "Epoch 6: |          | 1362/? [19:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1362, loss 3.6589114665985107\n",
      "Epoch 6: |          | 1363/? [19:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1363, loss 3.0872175693511963\n",
      "Epoch 6: |          | 1364/? [19:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1364, loss 3.570333480834961\n",
      "Epoch 6: |          | 1365/? [19:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1365, loss 3.292959213256836\n",
      "Epoch 6: |          | 1366/? [19:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1366, loss 3.9955360889434814\n",
      "Epoch 6: |          | 1367/? [19:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1367, loss 3.315091371536255\n",
      "Epoch 6: |          | 1368/? [19:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1368, loss 3.0915238857269287\n",
      "Epoch 6: |          | 1369/? [19:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1369, loss 3.795985460281372\n",
      "Epoch 6: |          | 1370/? [19:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1370, loss 3.3225150108337402\n",
      "Epoch 6: |          | 1371/? [19:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1371, loss 4.239419460296631\n",
      "Epoch 6: |          | 1372/? [19:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1372, loss 3.7096118927001953\n",
      "Epoch 6: |          | 1373/? [19:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1373, loss 4.076684951782227\n",
      "Epoch 6: |          | 1374/? [19:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1374, loss 3.2116355895996094\n",
      "Epoch 6: |          | 1375/? [19:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1375, loss 3.820204973220825\n",
      "Epoch 6: |          | 1376/? [19:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1376, loss 3.6866328716278076\n",
      "Epoch 6: |          | 1377/? [19:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1377, loss 3.6853740215301514\n",
      "Epoch 6: |          | 1378/? [19:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1378, loss 3.6531753540039062\n",
      "Epoch 6: |          | 1379/? [19:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1379, loss 3.664137601852417\n",
      "Epoch 6: |          | 1380/? [19:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1380, loss 3.8304150104522705\n",
      "Epoch 6: |          | 1381/? [19:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1381, loss 3.950524091720581\n",
      "Epoch 6: |          | 1382/? [19:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1382, loss 3.481783628463745\n",
      "Epoch 6: |          | 1383/? [19:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1383, loss 3.718336582183838\n",
      "Epoch 6: |          | 1384/? [19:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1384, loss 3.718754291534424\n",
      "Epoch 6: |          | 1385/? [19:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1385, loss 3.6868038177490234\n",
      "Epoch 6: |          | 1386/? [19:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1386, loss 3.8063175678253174\n",
      "Epoch 6: |          | 1387/? [19:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1387, loss 3.721482753753662\n",
      "Epoch 6: |          | 1388/? [19:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1388, loss 3.259688138961792\n",
      "Epoch 6: |          | 1389/? [19:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1389, loss 3.928630828857422\n",
      "Epoch 6: |          | 1390/? [19:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1390, loss 4.2095537185668945\n",
      "Epoch 6: |          | 1391/? [19:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1391, loss 3.8353705406188965\n",
      "Epoch 6: |          | 1392/? [19:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1392, loss 3.3334522247314453\n",
      "Epoch 6: |          | 1393/? [19:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1393, loss 3.5106873512268066\n",
      "Epoch 6: |          | 1394/? [19:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1394, loss 3.115553379058838\n",
      "Epoch 6: |          | 1395/? [19:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1395, loss 3.8473880290985107\n",
      "Epoch 6: |          | 1396/? [19:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1396, loss 3.6998813152313232\n",
      "Epoch 6: |          | 1397/? [19:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1397, loss 2.9902148246765137\n",
      "Epoch 6: |          | 1398/? [19:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1398, loss 4.184253692626953\n",
      "Epoch 6: |          | 1399/? [19:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1399, loss 4.188985824584961\n",
      "Epoch 6: |          | 1400/? [19:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1400, loss 3.3314247131347656\n",
      "Epoch 6: |          | 1401/? [19:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1401, loss 4.0196404457092285\n",
      "Epoch 6: |          | 1402/? [19:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1402, loss 3.7051658630371094\n",
      "Epoch 6: |          | 1403/? [19:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1403, loss 3.924785614013672\n",
      "Epoch 6: |          | 1404/? [19:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1404, loss 3.8634285926818848\n",
      "Epoch 6: |          | 1405/? [20:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1405, loss 4.188695430755615\n",
      "Epoch 6: |          | 1406/? [20:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1406, loss 4.099278450012207\n",
      "Epoch 6: |          | 1407/? [20:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1407, loss 4.168962001800537\n",
      "Epoch 6: |          | 1408/? [20:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1408, loss 3.4202027320861816\n",
      "Epoch 6: |          | 1409/? [20:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1409, loss 3.467210292816162\n",
      "Epoch 6: |          | 1410/? [20:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1410, loss 3.512356996536255\n",
      "Epoch 6: |          | 1411/? [20:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1411, loss 3.829338550567627\n",
      "Epoch 6: |          | 1412/? [20:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1412, loss 3.5216267108917236\n",
      "Epoch 6: |          | 1413/? [20:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1413, loss 3.373088836669922\n",
      "Epoch 6: |          | 1414/? [20:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1414, loss 3.54323148727417\n",
      "Epoch 6: |          | 1415/? [20:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1415, loss 3.7873833179473877\n",
      "Epoch 6: |          | 1416/? [20:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1416, loss 4.140143871307373\n",
      "Epoch 6: |          | 1417/? [20:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1417, loss 3.8282229900360107\n",
      "Epoch 6: |          | 1418/? [20:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1418, loss 3.938079833984375\n",
      "Epoch 6: |          | 1419/? [20:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1419, loss 3.6743321418762207\n",
      "Epoch 6: |          | 1420/? [20:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1420, loss 3.5455639362335205\n",
      "Epoch 6: |          | 1421/? [20:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1421, loss 3.3336589336395264\n",
      "Epoch 6: |          | 1422/? [20:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1422, loss 3.975492477416992\n",
      "Epoch 6: |          | 1423/? [20:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1423, loss 4.016686916351318\n",
      "Epoch 6: |          | 1424/? [20:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1424, loss 3.6699600219726562\n",
      "Epoch 6: |          | 1425/? [20:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1425, loss 3.88834810256958\n",
      "Epoch 6: |          | 1426/? [20:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1426, loss 3.438554048538208\n",
      "Epoch 6: |          | 1427/? [20:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1427, loss 4.0598344802856445\n",
      "Epoch 6: |          | 1428/? [20:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1428, loss 4.014874458312988\n",
      "Epoch 6: |          | 1429/? [20:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1429, loss 3.925612688064575\n",
      "Epoch 6: |          | 1430/? [20:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1430, loss 3.9799981117248535\n",
      "Epoch 6: |          | 1431/? [20:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1431, loss 3.759425401687622\n",
      "Epoch 6: |          | 1432/? [20:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1432, loss 3.824995517730713\n",
      "Epoch 6: |          | 1433/? [20:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1433, loss 3.7082371711730957\n",
      "Epoch 6: |          | 1434/? [20:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1434, loss 3.8611903190612793\n",
      "Epoch 6: |          | 1435/? [20:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1435, loss 3.4673876762390137\n",
      "Epoch 6: |          | 1436/? [20:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1436, loss 3.8179678916931152\n",
      "Epoch 6: |          | 1437/? [20:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1437, loss 3.071953296661377\n",
      "Epoch 6: |          | 1438/? [20:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1438, loss 4.692483425140381\n",
      "Epoch 6: |          | 1439/? [20:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1439, loss 3.7766082286834717\n",
      "Epoch 6: |          | 1440/? [20:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1440, loss 3.9821155071258545\n",
      "Epoch 6: |          | 1441/? [20:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1441, loss 4.263970851898193\n",
      "Epoch 6: |          | 1442/? [20:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1442, loss 4.293520450592041\n",
      "Epoch 6: |          | 1443/? [20:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1443, loss 3.455409526824951\n",
      "Epoch 6: |          | 1444/? [20:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1444, loss 3.4967987537384033\n",
      "Epoch 6: |          | 1445/? [20:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1445, loss 3.999621868133545\n",
      "Epoch 6: |          | 1446/? [20:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1446, loss 3.540799617767334\n",
      "Epoch 6: |          | 1447/? [20:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1447, loss 3.668478488922119\n",
      "Epoch 6: |          | 1448/? [20:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1448, loss 3.5389785766601562\n",
      "Epoch 6: |          | 1449/? [20:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1449, loss 3.7761917114257812\n",
      "Epoch 6: |          | 1450/? [20:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1450, loss 3.8037819862365723\n",
      "Epoch 6: |          | 1451/? [20:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1451, loss 4.124781131744385\n",
      "Epoch 6: |          | 1452/? [20:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1452, loss 3.7997469902038574\n",
      "Epoch 6: |          | 1453/? [20:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1453, loss 3.1907260417938232\n",
      "Epoch 6: |          | 1454/? [20:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1454, loss 3.779226779937744\n",
      "Epoch 6: |          | 1455/? [20:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1455, loss 3.8606839179992676\n",
      "Epoch 6: |          | 1456/? [20:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1456, loss 3.432891368865967\n",
      "Epoch 6: |          | 1457/? [20:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1457, loss 3.5779712200164795\n",
      "Epoch 6: |          | 1458/? [20:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1458, loss 3.7927768230438232\n",
      "Epoch 6: |          | 1459/? [20:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1459, loss 3.9303805828094482\n",
      "Epoch 6: |          | 1460/? [20:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1460, loss 3.864285707473755\n",
      "Epoch 6: |          | 1461/? [20:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1461, loss 3.7978203296661377\n",
      "Epoch 6: |          | 1462/? [20:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1462, loss 4.103886127471924\n",
      "Epoch 6: |          | 1463/? [20:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1463, loss 4.023090839385986\n",
      "Epoch 6: |          | 1464/? [20:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1464, loss 3.27673602104187\n",
      "Epoch 6: |          | 1465/? [20:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1465, loss 3.7415363788604736\n",
      "Epoch 6: |          | 1466/? [20:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1466, loss 3.4131736755371094\n",
      "Epoch 6: |          | 1467/? [20:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1467, loss 4.08794641494751\n",
      "Epoch 6: |          | 1468/? [20:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1468, loss 3.586493968963623\n",
      "Epoch 6: |          | 1469/? [20:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1469, loss 3.3562443256378174\n",
      "Epoch 6: |          | 1470/? [20:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1470, loss 3.82904052734375\n",
      "Epoch 6: |          | 1471/? [20:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1471, loss 4.110798358917236\n",
      "Epoch 6: |          | 1472/? [20:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1472, loss 3.8913931846618652\n",
      "Epoch 6: |          | 1473/? [20:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1473, loss 3.6584784984588623\n",
      "Epoch 6: |          | 1474/? [20:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1474, loss 3.58494234085083\n",
      "Epoch 6: |          | 1475/? [21:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1475, loss 3.168003797531128\n",
      "Epoch 6: |          | 1476/? [21:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1476, loss 3.7642822265625\n",
      "Epoch 6: |          | 1477/? [21:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1477, loss 3.7999167442321777\n",
      "Epoch 6: |          | 1478/? [21:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1478, loss 3.6699461936950684\n",
      "Epoch 6: |          | 1479/? [21:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1479, loss 4.179633140563965\n",
      "Epoch 6: |          | 1480/? [21:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1480, loss 3.9011588096618652\n",
      "Epoch 6: |          | 1481/? [21:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1481, loss 3.6656017303466797\n",
      "Epoch 6: |          | 1482/? [21:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1482, loss 3.8421390056610107\n",
      "Epoch 6: |          | 1483/? [21:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1483, loss 3.4442782402038574\n",
      "Epoch 6: |          | 1484/? [21:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1484, loss 3.6447062492370605\n",
      "Epoch 6: |          | 1485/? [21:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1485, loss 3.79152250289917\n",
      "Epoch 6: |          | 1486/? [21:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1486, loss 3.8283867835998535\n",
      "Epoch 6: |          | 1487/? [21:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1487, loss 3.236619472503662\n",
      "Epoch 6: |          | 1488/? [21:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1488, loss 3.9156455993652344\n",
      "Epoch 6: |          | 1489/? [21:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1489, loss 3.8725218772888184\n",
      "Epoch 6: |          | 1490/? [21:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1490, loss 3.7805333137512207\n",
      "Epoch 6: |          | 1491/? [21:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1491, loss 2.790956497192383\n",
      "Epoch 6: |          | 1492/? [21:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1492, loss 3.345914125442505\n",
      "Epoch 6: |          | 1493/? [21:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1493, loss 2.8787100315093994\n",
      "Epoch 6: |          | 1494/? [21:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1494, loss 3.6877951622009277\n",
      "Epoch 6: |          | 1495/? [21:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1495, loss 3.5684523582458496\n",
      "Epoch 6: |          | 1496/? [21:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1496, loss 3.8858304023742676\n",
      "Epoch 6: |          | 1497/? [21:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1497, loss 3.141056537628174\n",
      "Epoch 6: |          | 1498/? [21:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1498, loss 3.466033458709717\n",
      "Epoch 6: |          | 1499/? [21:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1499, loss 3.9848599433898926\n",
      "Epoch 6: |          | 1500/? [21:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1500, loss 3.8815512657165527\n",
      "Epoch 6: |          | 1501/? [21:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1501, loss 3.773543119430542\n",
      "Epoch 6: |          | 1502/? [21:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1502, loss 3.8216748237609863\n",
      "Epoch 6: |          | 1503/? [21:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1503, loss 3.606678009033203\n",
      "Epoch 6: |          | 1504/? [21:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1504, loss 4.269192695617676\n",
      "Epoch 6: |          | 1505/? [21:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1505, loss 4.02703857421875\n",
      "Epoch 6: |          | 1506/? [21:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1506, loss 3.65134859085083\n",
      "Epoch 6: |          | 1507/? [21:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1507, loss 3.559321641921997\n",
      "Epoch 6: |          | 1508/? [21:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1508, loss 3.7192540168762207\n",
      "Epoch 6: |          | 1509/? [21:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1509, loss 3.6863765716552734\n",
      "Epoch 6: |          | 1510/? [21:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1510, loss 3.9705708026885986\n",
      "Epoch 6: |          | 1511/? [21:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1511, loss 3.57800555229187\n",
      "Epoch 6: |          | 1512/? [21:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1512, loss 4.128017902374268\n",
      "Epoch 6: |          | 1513/? [21:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1513, loss 4.374888896942139\n",
      "Epoch 6: |          | 1514/? [21:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1514, loss 3.451505184173584\n",
      "Epoch 6: |          | 1515/? [21:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1515, loss 4.234317302703857\n",
      "Epoch 6: |          | 1516/? [21:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1516, loss 4.212726593017578\n",
      "Epoch 6: |          | 1517/? [21:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1517, loss 3.6610100269317627\n",
      "Epoch 6: |          | 1518/? [21:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1518, loss 3.4598336219787598\n",
      "Epoch 6: |          | 1519/? [21:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1519, loss 3.9641366004943848\n",
      "Epoch 6: |          | 1520/? [21:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1520, loss 4.17292594909668\n",
      "Epoch 6: |          | 1521/? [21:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1521, loss 3.7679061889648438\n",
      "Epoch 6: |          | 1522/? [21:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1522, loss 3.546844482421875\n",
      "Epoch 6: |          | 1523/? [21:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1523, loss 3.839186429977417\n",
      "Epoch 6: |          | 1524/? [21:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1524, loss 3.712637424468994\n",
      "Epoch 6: |          | 1525/? [21:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1525, loss 3.4176135063171387\n",
      "Epoch 6: |          | 1526/? [21:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1526, loss 3.995068311691284\n",
      "Epoch 6: |          | 1527/? [21:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1527, loss 4.012829780578613\n",
      "Epoch 6: |          | 1528/? [21:44<00:00,  1.17it/s, v_num=30]ERROR: Input has inproper shape\n",
      "Epoch 6: |          | 1529/? [21:44<00:00,  1.17it/s, v_num=30]   VALIDATION: Batch 0, loss 4.620224952697754\n",
      "   VALIDATION: Batch 1, loss 3.591386318206787\n",
      "   VALIDATION: Batch 2, loss 4.773774147033691\n",
      "   VALIDATION: Batch 3, loss 4.490103244781494\n",
      "   VALIDATION: Batch 4, loss 4.021689414978027\n",
      "   VALIDATION: Batch 5, loss 3.719270706176758\n",
      "   VALIDATION: Batch 6, loss 4.027871131896973\n",
      "   VALIDATION: Batch 7, loss 4.663665294647217\n",
      "   VALIDATION: Batch 8, loss 4.50397253036499\n",
      "   VALIDATION: Batch 9, loss 4.645426273345947\n",
      "   VALIDATION: Batch 10, loss 4.358023643493652\n",
      "   VALIDATION: Batch 11, loss 3.9631264209747314\n",
      "   VALIDATION: Batch 12, loss 4.202581882476807\n",
      "   VALIDATION: Batch 13, loss 4.759500980377197\n",
      "   VALIDATION: Batch 14, loss 4.096236228942871\n",
      "   VALIDATION: Batch 15, loss 3.951889753341675\n",
      "   VALIDATION: Batch 16, loss 4.549939155578613\n",
      "   VALIDATION: Batch 17, loss 4.222188472747803\n",
      "   VALIDATION: Batch 18, loss 3.5458431243896484\n",
      "   VALIDATION: Batch 19, loss 4.4825520515441895\n",
      "   VALIDATION: Batch 20, loss 4.744480609893799\n",
      "   VALIDATION: Batch 21, loss 5.06771993637085\n",
      "   VALIDATION: Batch 22, loss 4.620638847351074\n",
      "   VALIDATION: Batch 23, loss 4.115221977233887\n",
      "   VALIDATION: Batch 24, loss 3.9768402576446533\n",
      "   VALIDATION: Batch 25, loss 4.400289535522461\n",
      "   VALIDATION: Batch 26, loss 4.604602813720703\n",
      "   VALIDATION: Batch 27, loss 4.523443698883057\n",
      "   VALIDATION: Batch 28, loss 4.2731122970581055\n",
      "   VALIDATION: Batch 29, loss 4.477894306182861\n",
      "   VALIDATION: Batch 30, loss 4.048295021057129\n",
      "   VALIDATION: Batch 31, loss 4.378826141357422\n",
      "   VALIDATION: Batch 32, loss 4.883367538452148\n",
      "   VALIDATION: Batch 33, loss 3.1304962635040283\n",
      "   VALIDATION: Batch 34, loss 4.350600242614746\n",
      "   VALIDATION: Batch 35, loss 4.582003593444824\n",
      "   VALIDATION: Batch 36, loss 3.856438398361206\n",
      "   VALIDATION: Batch 37, loss 3.8501129150390625\n",
      "   VALIDATION: Batch 38, loss 3.9385406970977783\n",
      "   VALIDATION: Batch 39, loss 4.3570475578308105\n",
      "   VALIDATION: Batch 40, loss 4.385324954986572\n",
      "   VALIDATION: Batch 41, loss 3.1799263954162598\n",
      "   VALIDATION: Batch 42, loss 4.414081573486328\n",
      "   VALIDATION: Batch 43, loss 4.578257083892822\n",
      "   VALIDATION: Batch 44, loss 4.173360347747803\n",
      "   VALIDATION: Batch 45, loss 4.548065662384033\n",
      "   VALIDATION: Batch 46, loss 3.6839821338653564\n",
      "   VALIDATION: Batch 47, loss 4.7266693115234375\n",
      "   VALIDATION: Batch 48, loss 4.843649864196777\n",
      "   VALIDATION: Batch 49, loss 4.430417060852051\n",
      "   VALIDATION: Batch 50, loss 4.390653133392334\n",
      "   VALIDATION: Batch 51, loss 4.893012523651123\n",
      "   VALIDATION: Batch 52, loss 4.0579352378845215\n",
      "   VALIDATION: Batch 53, loss 3.9297585487365723\n",
      "   VALIDATION: Batch 54, loss 4.010361671447754\n",
      "   VALIDATION: Batch 55, loss 4.769644737243652\n",
      "   VALIDATION: Batch 56, loss 4.152770042419434\n",
      "   VALIDATION: Batch 57, loss 5.723458290100098\n",
      "   VALIDATION: Batch 58, loss 4.289914608001709\n",
      "   VALIDATION: Batch 59, loss 3.9178466796875\n",
      "   VALIDATION: Batch 60, loss 3.479999542236328\n",
      "   VALIDATION: Batch 61, loss 4.323987007141113\n",
      "   VALIDATION: Batch 62, loss 4.308615207672119\n",
      "   VALIDATION: Batch 63, loss 4.835944175720215\n",
      "   VALIDATION: Batch 64, loss 4.603576183319092\n",
      "   VALIDATION: Batch 65, loss 3.765608549118042\n",
      "   VALIDATION: Batch 66, loss 4.664986610412598\n",
      "   VALIDATION: Batch 67, loss 4.087029457092285\n",
      "   VALIDATION: Batch 68, loss 4.2659101486206055\n",
      "   VALIDATION: Batch 69, loss 4.52144193649292\n",
      "   VALIDATION: Batch 70, loss 4.687426567077637\n",
      "   VALIDATION: Batch 71, loss 4.183257102966309\n",
      "   VALIDATION: Batch 72, loss 5.077864646911621\n",
      "   VALIDATION: Batch 73, loss 3.8393471240997314\n",
      "   VALIDATION: Batch 74, loss 4.4728498458862305\n",
      "   VALIDATION: Batch 75, loss 4.4762067794799805\n",
      "   VALIDATION: Batch 76, loss 4.329536437988281\n",
      "   VALIDATION: Batch 77, loss 4.580924034118652\n",
      "   VALIDATION: Batch 78, loss 4.439605712890625\n",
      "   VALIDATION: Batch 79, loss 4.376471519470215\n",
      "   VALIDATION: Batch 80, loss 4.458246231079102\n",
      "   VALIDATION: Batch 81, loss 4.217665672302246\n",
      "   VALIDATION: Batch 82, loss 4.612049579620361\n",
      "   VALIDATION: Batch 83, loss 3.8415706157684326\n",
      "   VALIDATION: Batch 84, loss 4.56682014465332\n",
      "   VALIDATION: Batch 85, loss 4.201765060424805\n",
      "   VALIDATION: Batch 86, loss 4.243538856506348\n",
      "   VALIDATION: Batch 87, loss 4.147948265075684\n",
      "   VALIDATION: Batch 88, loss 3.743412494659424\n",
      "   VALIDATION: Batch 89, loss 4.017264366149902\n",
      "   VALIDATION: Batch 90, loss 4.30799674987793\n",
      "   VALIDATION: Batch 91, loss 4.376673698425293\n",
      "   VALIDATION: Batch 92, loss 4.012154579162598\n",
      "   VALIDATION: Batch 93, loss 4.792606353759766\n",
      "   VALIDATION: Batch 94, loss 4.2695770263671875\n",
      "   VALIDATION: Batch 95, loss 3.74237322807312\n",
      "   VALIDATION: Batch 96, loss 4.213767051696777\n",
      "   VALIDATION: Batch 97, loss 3.9048678874969482\n",
      "   VALIDATION: Batch 98, loss 4.562386989593506\n",
      "   VALIDATION: Batch 99, loss 4.617888927459717\n",
      "   VALIDATION: Batch 100, loss 4.992987632751465\n",
      "   VALIDATION: Batch 101, loss 3.5778446197509766\n",
      "   VALIDATION: Batch 102, loss 4.98825740814209\n",
      "   VALIDATION: Batch 103, loss 4.927823066711426\n",
      "   VALIDATION: Batch 104, loss 3.8794968128204346\n",
      "   VALIDATION: Batch 105, loss 4.380638599395752\n",
      "   VALIDATION: Batch 106, loss 4.223856449127197\n",
      "   VALIDATION: Batch 107, loss 4.318423748016357\n",
      "   VALIDATION: Batch 108, loss 4.043391704559326\n",
      "   VALIDATION: Batch 109, loss 4.644724369049072\n",
      "   VALIDATION: Batch 110, loss 4.323818206787109\n",
      "   VALIDATION: Batch 111, loss 4.660693168640137\n",
      "   VALIDATION: Batch 112, loss 5.546914577484131\n",
      "   VALIDATION: Batch 113, loss 4.861536979675293\n",
      "   VALIDATION: Batch 114, loss 4.618740558624268\n",
      "   VALIDATION: Batch 115, loss 4.048159599304199\n",
      "   VALIDATION: Batch 116, loss 3.900857448577881\n",
      "   VALIDATION: Batch 117, loss 4.59394645690918\n",
      "   VALIDATION: Batch 118, loss 4.7461724281311035\n",
      "   VALIDATION: Batch 119, loss 3.904346466064453\n",
      "   VALIDATION: Batch 120, loss 3.5173873901367188\n",
      "   VALIDATION: Batch 121, loss 3.8482723236083984\n",
      "   VALIDATION: Batch 122, loss 4.26932430267334\n",
      "   VALIDATION: Batch 123, loss 4.240504264831543\n",
      "   VALIDATION: Batch 124, loss 3.650355815887451\n",
      "   VALIDATION: Batch 125, loss 4.242259502410889\n",
      "   VALIDATION: Batch 126, loss 4.467217445373535\n",
      "   VALIDATION: Batch 127, loss 4.247783184051514\n",
      "   VALIDATION: Batch 128, loss 4.42849063873291\n",
      "   VALIDATION: Batch 129, loss 4.052850246429443\n",
      "   VALIDATION: Batch 130, loss 3.635040760040283\n",
      "   VALIDATION: Batch 131, loss 3.685825824737549\n",
      "   VALIDATION: Batch 132, loss 4.3314642906188965\n",
      "   VALIDATION: Batch 133, loss 4.493426322937012\n",
      "   VALIDATION: Batch 134, loss 4.397885799407959\n",
      "   VALIDATION: Batch 135, loss 4.640625953674316\n",
      "   VALIDATION: Batch 136, loss 4.732433319091797\n",
      "   VALIDATION: Batch 137, loss 4.5428595542907715\n",
      "   VALIDATION: Batch 138, loss 4.254127025604248\n",
      "   VALIDATION: Batch 139, loss 4.675227165222168\n",
      "   VALIDATION: Batch 140, loss 3.7824692726135254\n",
      "   VALIDATION: Batch 141, loss 4.715991020202637\n",
      "   VALIDATION: Batch 142, loss 3.4388701915740967\n",
      "   VALIDATION: Batch 143, loss 4.232043266296387\n",
      "   VALIDATION: Batch 144, loss 4.501235008239746\n",
      "   VALIDATION: Batch 145, loss 4.287631511688232\n",
      "   VALIDATION: Batch 146, loss 4.129734039306641\n",
      "   VALIDATION: Batch 147, loss 4.418795585632324\n",
      "   VALIDATION: Batch 148, loss 4.5543646812438965\n",
      "   VALIDATION: Batch 149, loss 5.036240100860596\n",
      "   VALIDATION: Batch 150, loss 4.615386009216309\n",
      "   VALIDATION: Batch 151, loss 4.9193315505981445\n",
      "   VALIDATION: Batch 152, loss 4.263000011444092\n",
      "   VALIDATION: Batch 153, loss 4.500302791595459\n",
      "   VALIDATION: Batch 154, loss 4.365714073181152\n",
      "   VALIDATION: Batch 155, loss 4.102410316467285\n",
      "   VALIDATION: Batch 156, loss 4.750214576721191\n",
      "   VALIDATION: Batch 157, loss 4.43831729888916\n",
      "   VALIDATION: Batch 158, loss 3.8595938682556152\n",
      "   VALIDATION: Batch 159, loss 4.308524131774902\n",
      "   VALIDATION: Batch 160, loss 4.670753479003906\n",
      "   VALIDATION: Batch 161, loss 4.918461799621582\n",
      "   VALIDATION: Batch 162, loss 4.370851039886475\n",
      "   VALIDATION: Batch 163, loss 3.7471694946289062\n",
      "   VALIDATION: Batch 164, loss 4.283019065856934\n",
      "   VALIDATION: Batch 165, loss 4.7338457107543945\n",
      "   VALIDATION: Batch 166, loss 4.2130231857299805\n",
      "   VALIDATION: Batch 167, loss 4.594622611999512\n",
      "   VALIDATION: Batch 168, loss 3.461214542388916\n",
      "   VALIDATION: Batch 169, loss 4.123246192932129\n",
      "   VALIDATION: Batch 170, loss 4.340431213378906\n",
      "   VALIDATION: Batch 171, loss 4.4914140701293945\n",
      "   VALIDATION: Batch 172, loss 4.290261268615723\n",
      "   VALIDATION: Batch 173, loss 4.151003837585449\n",
      "   VALIDATION: Batch 174, loss 4.597960472106934\n",
      "   VALIDATION: Batch 175, loss 4.396646976470947\n",
      "   VALIDATION: Batch 176, loss 4.199550151824951\n",
      "   VALIDATION: Batch 177, loss 4.161569118499756\n",
      "   VALIDATION: Batch 178, loss 5.24618673324585\n",
      "   VALIDATION: Batch 179, loss 4.502871036529541\n",
      "   VALIDATION: Batch 180, loss 3.984088897705078\n",
      "   VALIDATION: Batch 181, loss 4.209921836853027\n",
      "   VALIDATION: Batch 182, loss 4.451296806335449\n",
      "   VALIDATION: Batch 183, loss 3.4814980030059814\n",
      "   VALIDATION: Batch 184, loss 3.238455295562744\n",
      "   VALIDATION: Batch 185, loss 4.027543544769287\n",
      "   VALIDATION: Batch 186, loss 3.950303316116333\n",
      "   VALIDATION: Batch 187, loss 4.167474746704102\n",
      "   VALIDATION: Batch 188, loss 4.553976535797119\n",
      "   VALIDATION: Batch 189, loss 3.8886523246765137\n",
      "   VALIDATION: Batch 190, loss 3.955587387084961\n",
      "   VALIDATION: Batch 191, loss 4.45974063873291\n",
      "   VALIDATION: Batch 192, loss 4.812469482421875\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 7: |          | 0/? [00:00<?, ?it/s, v_num=30]              TRRAINING: Batch 0, loss 3.9817047119140625\n",
      "Epoch 7: |          | 1/? [00:01<00:00,  0.88it/s, v_num=30]   TRRAINING: Batch 1, loss 3.5961754322052\n",
      "Epoch 7: |          | 2/? [00:01<00:00,  1.00it/s, v_num=30]   TRRAINING: Batch 2, loss 3.6656060218811035\n",
      "Epoch 7: |          | 3/? [00:02<00:00,  1.03it/s, v_num=30]   TRRAINING: Batch 3, loss 3.3677048683166504\n",
      "Epoch 7: |          | 4/? [00:03<00:00,  1.06it/s, v_num=30]   TRRAINING: Batch 4, loss 3.7343344688415527\n",
      "Epoch 7: |          | 5/? [00:04<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 5, loss 4.505362033843994\n",
      "Epoch 7: |          | 6/? [00:05<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 6, loss 3.953803539276123\n",
      "Epoch 7: |          | 7/? [00:06<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 7, loss 3.3204567432403564\n",
      "Epoch 7: |          | 8/? [00:06<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 8, loss 3.434842586517334\n",
      "Epoch 7: |          | 9/? [00:07<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 9, loss 3.719209671020508\n",
      "Epoch 7: |          | 10/? [00:08<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 10, loss 3.978031635284424\n",
      "Epoch 7: |          | 11/? [00:09<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 11, loss 3.850952625274658\n",
      "Epoch 7: |          | 12/? [00:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 12, loss 4.333873271942139\n",
      "Epoch 7: |          | 13/? [00:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 13, loss 3.8592448234558105\n",
      "Epoch 7: |          | 14/? [00:12<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 14, loss 4.018808364868164\n",
      "Epoch 7: |          | 15/? [00:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 15, loss 3.314575672149658\n",
      "Epoch 7: |          | 16/? [00:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 16, loss 3.1420421600341797\n",
      "Epoch 7: |          | 17/? [00:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 17, loss 4.252170562744141\n",
      "Epoch 7: |          | 18/? [00:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 18, loss 3.8022079467773438\n",
      "Epoch 7: |          | 19/? [00:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 19, loss 3.5903072357177734\n",
      "Epoch 7: |          | 20/? [00:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 20, loss 3.841796398162842\n",
      "Epoch 7: |          | 21/? [00:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 21, loss 3.983088254928589\n",
      "Epoch 7: |          | 22/? [00:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 22, loss 3.8567352294921875\n",
      "Epoch 7: |          | 23/? [00:19<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 23, loss 3.26086688041687\n",
      "Epoch 7: |          | 24/? [00:20<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 24, loss 3.8876190185546875\n",
      "Epoch 7: |          | 25/? [00:21<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 25, loss 3.7081761360168457\n",
      "Epoch 7: |          | 26/? [00:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 26, loss 3.420872449874878\n",
      "Epoch 7: |          | 27/? [00:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 27, loss 3.40252423286438\n",
      "Epoch 7: |          | 28/? [00:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 28, loss 4.360568523406982\n",
      "Epoch 7: |          | 29/? [00:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 29, loss 3.8400120735168457\n",
      "Epoch 7: |          | 30/? [00:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 30, loss 3.690289258956909\n",
      "Epoch 7: |          | 31/? [00:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 31, loss 4.25487756729126\n",
      "Epoch 7: |          | 32/? [00:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 32, loss 3.8257641792297363\n",
      "Epoch 7: |          | 33/? [00:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 33, loss 3.633035182952881\n",
      "Epoch 7: |          | 34/? [00:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 34, loss 3.655749797821045\n",
      "Epoch 7: |          | 35/? [00:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 35, loss 3.1321635246276855\n",
      "Epoch 7: |          | 36/? [00:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 36, loss 3.8745174407958984\n",
      "Epoch 7: |          | 37/? [00:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 37, loss 4.057255744934082\n",
      "Epoch 7: |          | 38/? [00:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 38, loss 4.328956127166748\n",
      "Epoch 7: |          | 39/? [00:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 39, loss 4.217450141906738\n",
      "Epoch 7: |          | 40/? [00:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 40, loss 3.630040407180786\n",
      "Epoch 7: |          | 41/? [00:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 41, loss 3.721104383468628\n",
      "Epoch 7: |          | 42/? [00:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 42, loss 3.4907174110412598\n",
      "Epoch 7: |          | 43/? [00:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 43, loss 3.6729016304016113\n",
      "Epoch 7: |          | 44/? [00:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 44, loss 2.8755099773406982\n",
      "Epoch 7: |          | 45/? [00:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 45, loss 2.4203171730041504\n",
      "Epoch 7: |          | 46/? [00:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 46, loss 4.261338710784912\n",
      "Epoch 7: |          | 47/? [00:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 47, loss 3.474452495574951\n",
      "Epoch 7: |          | 48/? [00:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 48, loss 3.3317482471466064\n",
      "Epoch 7: |          | 49/? [00:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 49, loss 3.8055052757263184\n",
      "Epoch 7: |          | 50/? [00:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 50, loss 3.670626163482666\n",
      "Epoch 7: |          | 51/? [00:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 51, loss 3.579577684402466\n",
      "Epoch 7: |          | 52/? [00:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 52, loss 4.303009986877441\n",
      "Epoch 7: |          | 53/? [00:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 53, loss 3.9301719665527344\n",
      "Epoch 7: |          | 54/? [00:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 54, loss 3.7523179054260254\n",
      "Epoch 7: |          | 55/? [00:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 55, loss 3.786938428878784\n",
      "Epoch 7: |          | 56/? [00:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 56, loss 3.909273862838745\n",
      "Epoch 7: |          | 57/? [00:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 57, loss 3.8021492958068848\n",
      "Epoch 7: |          | 58/? [00:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 58, loss 4.9667863845825195\n",
      "Epoch 7: |          | 59/? [00:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 59, loss 3.893540143966675\n",
      "Epoch 7: |          | 60/? [00:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 60, loss 4.060890197753906\n",
      "Epoch 7: |          | 61/? [00:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 61, loss 4.031045436859131\n",
      "Epoch 7: |          | 62/? [00:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 62, loss 3.7059848308563232\n",
      "Epoch 7: |          | 63/? [00:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 63, loss 3.8995773792266846\n",
      "Epoch 7: |          | 64/? [00:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 64, loss 3.702544689178467\n",
      "Epoch 7: |          | 65/? [00:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 65, loss 3.7486324310302734\n",
      "Epoch 7: |          | 66/? [00:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 66, loss 3.1827664375305176\n",
      "Epoch 7: |          | 67/? [00:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 67, loss 3.904989242553711\n",
      "Epoch 7: |          | 68/? [00:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 68, loss 3.8517632484436035\n",
      "Epoch 7: |          | 69/? [00:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 69, loss 3.7942938804626465\n",
      "Epoch 7: |          | 70/? [00:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 70, loss 3.4995293617248535\n",
      "Epoch 7: |          | 71/? [01:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 71, loss 3.4577689170837402\n",
      "Epoch 7: |          | 72/? [01:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 72, loss 3.6435179710388184\n",
      "Epoch 7: |          | 73/? [01:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 73, loss 3.8249804973602295\n",
      "Epoch 7: |          | 74/? [01:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 74, loss 3.5936901569366455\n",
      "Epoch 7: |          | 75/? [01:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 75, loss 3.688880443572998\n",
      "Epoch 7: |          | 76/? [01:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 76, loss 3.7033588886260986\n",
      "Epoch 7: |          | 77/? [01:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 77, loss 3.7218246459960938\n",
      "Epoch 7: |          | 78/? [01:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 78, loss 3.579641342163086\n",
      "Epoch 7: |          | 79/? [01:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 79, loss 3.7499518394470215\n",
      "Epoch 7: |          | 80/? [01:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 80, loss 3.658419370651245\n",
      "Epoch 7: |          | 81/? [01:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 81, loss 3.110626220703125\n",
      "Epoch 7: |          | 82/? [01:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 82, loss 3.9041945934295654\n",
      "Epoch 7: |          | 83/? [01:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 83, loss 3.4240517616271973\n",
      "Epoch 7: |          | 84/? [01:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 84, loss 3.2687594890594482\n",
      "Epoch 7: |          | 85/? [01:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 85, loss 3.137882709503174\n",
      "Epoch 7: |          | 86/? [01:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 86, loss 3.3026020526885986\n",
      "Epoch 7: |          | 87/? [01:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 87, loss 3.462773561477661\n",
      "Epoch 7: |          | 88/? [01:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 88, loss 4.070271968841553\n",
      "Epoch 7: |          | 89/? [01:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 89, loss 4.03579044342041\n",
      "Epoch 7: |          | 90/? [01:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 90, loss 3.8162522315979004\n",
      "Epoch 7: |          | 91/? [01:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 91, loss 3.5286037921905518\n",
      "Epoch 7: |          | 92/? [01:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 92, loss 3.956658124923706\n",
      "Epoch 7: |          | 93/? [01:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 93, loss 4.113459587097168\n",
      "Epoch 7: |          | 94/? [01:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 94, loss 4.012214183807373\n",
      "Epoch 7: |          | 95/? [01:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 95, loss 3.7561278343200684\n",
      "Epoch 7: |          | 96/? [01:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 96, loss 3.5234310626983643\n",
      "Epoch 7: |          | 97/? [01:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 97, loss 3.3208343982696533\n",
      "Epoch 7: |          | 98/? [01:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 98, loss 3.869894504547119\n",
      "Epoch 7: |          | 99/? [01:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 99, loss 4.055505275726318\n",
      "Epoch 7: |          | 100/? [01:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 100, loss 4.001301288604736\n",
      "Epoch 7: |          | 101/? [01:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 101, loss 3.676593780517578\n",
      "Epoch 7: |          | 102/? [01:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 102, loss 3.685514450073242\n",
      "Epoch 7: |          | 103/? [01:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 103, loss 3.509857177734375\n",
      "Epoch 7: |          | 104/? [01:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 104, loss 3.7948384284973145\n",
      "Epoch 7: |          | 105/? [01:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 105, loss 3.772787094116211\n",
      "Epoch 7: |          | 106/? [01:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 106, loss 3.8630542755126953\n",
      "Epoch 7: |          | 107/? [01:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 107, loss 3.880962371826172\n",
      "Epoch 7: |          | 108/? [01:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 108, loss 3.856053113937378\n",
      "Epoch 7: |          | 109/? [01:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 109, loss 3.616049289703369\n",
      "Epoch 7: |          | 110/? [01:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 110, loss 3.7698464393615723\n",
      "Epoch 7: |          | 111/? [01:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 111, loss 4.382376194000244\n",
      "Epoch 7: |          | 112/? [01:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 112, loss 3.1195502281188965\n",
      "Epoch 7: |          | 113/? [01:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 113, loss 2.72198224067688\n",
      "Epoch 7: |          | 114/? [01:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 114, loss 3.9764912128448486\n",
      "Epoch 7: |          | 115/? [01:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 115, loss 4.13089656829834\n",
      "Epoch 7: |          | 116/? [01:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 116, loss 3.285280704498291\n",
      "Epoch 7: |          | 117/? [01:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 117, loss 3.2701926231384277\n",
      "Epoch 7: |          | 118/? [01:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 118, loss 4.03938627243042\n",
      "Epoch 7: |          | 119/? [01:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 119, loss 4.263737678527832\n",
      "Epoch 7: |          | 120/? [01:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 120, loss 4.059934139251709\n",
      "Epoch 7: |          | 121/? [01:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 121, loss 3.773958683013916\n",
      "Epoch 7: |          | 122/? [01:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 122, loss 3.2460780143737793\n",
      "Epoch 7: |          | 123/? [01:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 123, loss 3.7495884895324707\n",
      "Epoch 7: |          | 124/? [01:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 124, loss 3.836017608642578\n",
      "Epoch 7: |          | 125/? [01:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 125, loss 3.5147926807403564\n",
      "Epoch 7: |          | 126/? [01:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 126, loss 4.0161848068237305\n",
      "Epoch 7: |          | 127/? [01:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 127, loss 4.125056266784668\n",
      "Epoch 7: |          | 128/? [01:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 128, loss 3.2754364013671875\n",
      "Epoch 7: |          | 129/? [01:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 129, loss 3.837380886077881\n",
      "Epoch 7: |          | 130/? [01:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 130, loss 3.0144400596618652\n",
      "Epoch 7: |          | 131/? [01:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 131, loss 3.8038291931152344\n",
      "Epoch 7: |          | 132/? [01:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 132, loss 3.859894275665283\n",
      "Epoch 7: |          | 133/? [01:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 133, loss 3.7213032245635986\n",
      "Epoch 7: |          | 134/? [01:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 134, loss 3.914694309234619\n",
      "Epoch 7: |          | 135/? [01:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 135, loss 3.9351189136505127\n",
      "Epoch 7: |          | 136/? [01:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 136, loss 4.006911277770996\n",
      "Epoch 7: |          | 137/? [01:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 137, loss 3.04557728767395\n",
      "Epoch 7: |          | 138/? [01:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 138, loss 3.637732744216919\n",
      "Epoch 7: |          | 139/? [01:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 139, loss 3.996509075164795\n",
      "Epoch 7: |          | 140/? [01:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 140, loss 3.3375325202941895\n",
      "Epoch 7: |          | 141/? [01:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 141, loss 3.4280624389648438\n",
      "Epoch 7: |          | 142/? [02:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 142, loss 4.812628746032715\n",
      "Epoch 7: |          | 143/? [02:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 143, loss 4.466750621795654\n",
      "Epoch 7: |          | 144/? [02:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 144, loss 3.7941722869873047\n",
      "Epoch 7: |          | 145/? [02:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 145, loss 3.2480244636535645\n",
      "Epoch 7: |          | 146/? [02:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 146, loss 3.4328105449676514\n",
      "Epoch 7: |          | 147/? [02:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 147, loss 3.73246693611145\n",
      "Epoch 7: |          | 148/? [02:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 148, loss 3.442966938018799\n",
      "Epoch 7: |          | 149/? [02:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 149, loss 3.0697572231292725\n",
      "Epoch 7: |          | 150/? [02:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 150, loss 3.9228053092956543\n",
      "Epoch 7: |          | 151/? [02:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 151, loss 4.0297698974609375\n",
      "Epoch 7: |          | 152/? [02:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 152, loss 3.9480807781219482\n",
      "Epoch 7: |          | 153/? [02:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 153, loss 3.1024887561798096\n",
      "Epoch 7: |          | 154/? [02:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 154, loss 4.285916328430176\n",
      "Epoch 7: |          | 155/? [02:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 155, loss 3.834054946899414\n",
      "Epoch 7: |          | 156/? [02:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 156, loss 3.211376190185547\n",
      "Epoch 7: |          | 157/? [02:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 157, loss 3.879955291748047\n",
      "Epoch 7: |          | 158/? [02:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 158, loss 3.9778149127960205\n",
      "Epoch 7: |          | 159/? [02:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 159, loss 3.710421085357666\n",
      "Epoch 7: |          | 160/? [02:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 160, loss 3.4912407398223877\n",
      "Epoch 7: |          | 161/? [02:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 161, loss 3.8544654846191406\n",
      "Epoch 7: |          | 162/? [02:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 162, loss 3.8687968254089355\n",
      "Epoch 7: |          | 163/? [02:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 163, loss 3.0261998176574707\n",
      "Epoch 7: |          | 164/? [02:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 164, loss 3.4779555797576904\n",
      "Epoch 7: |          | 165/? [02:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 165, loss 4.257663726806641\n",
      "Epoch 7: |          | 166/? [02:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 166, loss 3.906054735183716\n",
      "Epoch 7: |          | 167/? [02:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 167, loss 4.012507438659668\n",
      "Epoch 7: |          | 168/? [02:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 168, loss 3.5851569175720215\n",
      "Epoch 7: |          | 169/? [02:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 169, loss 3.236798048019409\n",
      "Epoch 7: |          | 170/? [02:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 170, loss 3.483844757080078\n",
      "Epoch 7: |          | 171/? [02:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 171, loss 3.801649570465088\n",
      "Epoch 7: |          | 172/? [02:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 172, loss 3.6238036155700684\n",
      "Epoch 7: |          | 173/? [02:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 173, loss 4.256922721862793\n",
      "Epoch 7: |          | 174/? [02:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 174, loss 3.8618767261505127\n",
      "Epoch 7: |          | 175/? [02:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 175, loss 4.324953556060791\n",
      "Epoch 7: |          | 176/? [02:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 176, loss 3.5763587951660156\n",
      "Epoch 7: |          | 177/? [02:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 177, loss 3.5781593322753906\n",
      "Epoch 7: |          | 178/? [02:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 178, loss 3.4486260414123535\n",
      "Epoch 7: |          | 179/? [02:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 179, loss 4.165335178375244\n",
      "Epoch 7: |          | 180/? [02:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 180, loss 3.6618072986602783\n",
      "Epoch 7: |          | 181/? [02:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 181, loss 3.4820778369903564\n",
      "Epoch 7: |          | 182/? [02:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 182, loss 3.8601927757263184\n",
      "Epoch 7: |          | 183/? [02:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 183, loss 3.3759422302246094\n",
      "Epoch 7: |          | 184/? [02:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 184, loss 3.5752525329589844\n",
      "Epoch 7: |          | 185/? [02:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 185, loss 4.047432899475098\n",
      "Epoch 7: |          | 186/? [02:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 186, loss 3.63966703414917\n",
      "Epoch 7: |          | 187/? [02:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 187, loss 4.112527847290039\n",
      "Epoch 7: |          | 188/? [02:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 188, loss 3.505403518676758\n",
      "Epoch 7: |          | 189/? [02:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 189, loss 4.115013122558594\n",
      "Epoch 7: |          | 190/? [02:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 190, loss 3.6776375770568848\n",
      "Epoch 7: |          | 191/? [02:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 191, loss 4.4087324142456055\n",
      "Epoch 7: |          | 192/? [02:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 192, loss 4.129328727722168\n",
      "Epoch 7: |          | 193/? [02:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 193, loss 3.5703015327453613\n",
      "Epoch 7: |          | 194/? [02:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 194, loss 3.443021774291992\n",
      "Epoch 7: |          | 195/? [02:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 195, loss 4.121977806091309\n",
      "Epoch 7: |          | 196/? [02:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 196, loss 3.9897472858428955\n",
      "Epoch 7: |          | 197/? [02:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 197, loss 3.798922061920166\n",
      "Epoch 7: |          | 198/? [02:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 198, loss 3.154798984527588\n",
      "Epoch 7: |          | 199/? [02:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 199, loss 4.081812381744385\n",
      "Epoch 7: |          | 200/? [02:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 200, loss 3.5479788780212402\n",
      "Epoch 7: |          | 201/? [02:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 201, loss 3.782179355621338\n",
      "Epoch 7: |          | 202/? [02:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 202, loss 3.9438998699188232\n",
      "Epoch 7: |          | 203/? [02:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 203, loss 3.561882734298706\n",
      "Epoch 7: |          | 204/? [02:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 204, loss 3.586731433868408\n",
      "Epoch 7: |          | 205/? [02:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 205, loss 3.501070737838745\n",
      "Epoch 7: |          | 206/? [02:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 206, loss 3.394343852996826\n",
      "Epoch 7: |          | 207/? [02:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 207, loss 3.8837332725524902\n",
      "Epoch 7: |          | 208/? [02:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 208, loss 3.806297779083252\n",
      "Epoch 7: |          | 209/? [02:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 209, loss 3.579133987426758\n",
      "Epoch 7: |          | 210/? [02:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 210, loss 4.268906593322754\n",
      "Epoch 7: |          | 211/? [02:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 211, loss 3.659510374069214\n",
      "Epoch 7: |          | 212/? [02:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 212, loss 3.8827362060546875\n",
      "Epoch 7: |          | 213/? [03:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 213, loss 3.716465711593628\n",
      "Epoch 7: |          | 214/? [03:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 214, loss 3.612696409225464\n",
      "Epoch 7: |          | 215/? [03:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 215, loss 3.2749266624450684\n",
      "Epoch 7: |          | 216/? [03:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 216, loss 3.89250111579895\n",
      "Epoch 7: |          | 217/? [03:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 217, loss 3.862010955810547\n",
      "Epoch 7: |          | 218/? [03:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 218, loss 3.831972122192383\n",
      "Epoch 7: |          | 219/? [03:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 219, loss 3.798555850982666\n",
      "Epoch 7: |          | 220/? [03:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 220, loss 3.8694591522216797\n",
      "Epoch 7: |          | 221/? [03:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 221, loss 3.714986801147461\n",
      "Epoch 7: |          | 222/? [03:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 222, loss 2.952238082885742\n",
      "Epoch 7: |          | 223/? [03:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 223, loss 3.9297919273376465\n",
      "Epoch 7: |          | 224/? [03:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 224, loss 4.014908313751221\n",
      "Epoch 7: |          | 225/? [03:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 225, loss 3.773265838623047\n",
      "Epoch 7: |          | 226/? [03:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 226, loss 3.6858181953430176\n",
      "Epoch 7: |          | 227/? [03:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 227, loss 4.0342793464660645\n",
      "Epoch 7: |          | 228/? [03:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 228, loss 3.7378058433532715\n",
      "Epoch 7: |          | 229/? [03:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 229, loss 3.781203031539917\n",
      "Epoch 7: |          | 230/? [03:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 230, loss 3.7428348064422607\n",
      "Epoch 7: |          | 231/? [03:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 231, loss 3.682016372680664\n",
      "Epoch 7: |          | 232/? [03:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 232, loss 3.4020047187805176\n",
      "Epoch 7: |          | 233/? [03:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 233, loss 4.082608699798584\n",
      "Epoch 7: |          | 234/? [03:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 234, loss 4.0636515617370605\n",
      "Epoch 7: |          | 235/? [03:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 235, loss 4.139583587646484\n",
      "Epoch 7: |          | 236/? [03:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 236, loss 3.598445177078247\n",
      "Epoch 7: |          | 237/? [03:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 237, loss 3.7722370624542236\n",
      "Epoch 7: |          | 238/? [03:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 238, loss 4.027388572692871\n",
      "Epoch 7: |          | 239/? [03:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 239, loss 3.5432934761047363\n",
      "Epoch 7: |          | 240/? [03:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 240, loss 3.1843819618225098\n",
      "Epoch 7: |          | 241/? [03:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 241, loss 3.727473735809326\n",
      "Epoch 7: |          | 242/? [03:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 242, loss 4.131148338317871\n",
      "Epoch 7: |          | 243/? [03:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 243, loss 3.0099451541900635\n",
      "Epoch 7: |          | 244/? [03:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 244, loss 3.441492795944214\n",
      "Epoch 7: |          | 245/? [03:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 245, loss 3.6939804553985596\n",
      "Epoch 7: |          | 246/? [03:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 246, loss 3.823693037033081\n",
      "Epoch 7: |          | 247/? [03:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 247, loss 3.899303913116455\n",
      "Epoch 7: |          | 248/? [03:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 248, loss 3.3928120136260986\n",
      "Epoch 7: |          | 249/? [03:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 249, loss 3.2914810180664062\n",
      "Epoch 7: |          | 250/? [03:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 250, loss 3.9035181999206543\n",
      "Epoch 7: |          | 251/? [03:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 251, loss 3.8391826152801514\n",
      "Epoch 7: |          | 252/? [03:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 252, loss 3.6424338817596436\n",
      "Epoch 7: |          | 253/? [03:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 253, loss 4.440381050109863\n",
      "Epoch 7: |          | 254/? [03:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 254, loss 4.071000099182129\n",
      "Epoch 7: |          | 255/? [03:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 255, loss 3.7362301349639893\n",
      "Epoch 7: |          | 256/? [03:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 256, loss 4.400842189788818\n",
      "Epoch 7: |          | 257/? [03:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 257, loss 3.6120128631591797\n",
      "Epoch 7: |          | 258/? [03:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 258, loss 3.712034225463867\n",
      "Epoch 7: |          | 259/? [03:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 259, loss 3.6106326580047607\n",
      "Epoch 7: |          | 260/? [03:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 260, loss 3.5330958366394043\n",
      "Epoch 7: |          | 261/? [03:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 261, loss 3.3327279090881348\n",
      "Epoch 7: |          | 262/? [03:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 262, loss 4.001997947692871\n",
      "Epoch 7: |          | 263/? [03:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 263, loss 3.702307939529419\n",
      "Epoch 7: |          | 264/? [03:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 264, loss 3.844059705734253\n",
      "Epoch 7: |          | 265/? [03:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 265, loss 3.2819266319274902\n",
      "Epoch 7: |          | 266/? [03:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 266, loss 3.73002290725708\n",
      "Epoch 7: |          | 267/? [03:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 267, loss 3.331305980682373\n",
      "Epoch 7: |          | 268/? [03:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 268, loss 3.6728763580322266\n",
      "Epoch 7: |          | 269/? [03:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 269, loss 4.028685569763184\n",
      "Epoch 7: |          | 270/? [03:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 270, loss 3.6365058422088623\n",
      "Epoch 7: |          | 271/? [03:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 271, loss 3.8831119537353516\n",
      "Epoch 7: |          | 272/? [03:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 272, loss 4.1422882080078125\n",
      "Epoch 7: |          | 273/? [03:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 273, loss 3.3806140422821045\n",
      "Epoch 7: |          | 274/? [03:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 274, loss 4.180347442626953\n",
      "Epoch 7: |          | 275/? [03:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 275, loss 3.908395290374756\n",
      "Epoch 7: |          | 276/? [03:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 276, loss 3.176053047180176\n",
      "Epoch 7: |          | 277/? [03:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 277, loss 3.8862204551696777\n",
      "Epoch 7: |          | 278/? [03:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 278, loss 2.935215950012207\n",
      "Epoch 7: |          | 279/? [03:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 279, loss 3.6010689735412598\n",
      "Epoch 7: |          | 280/? [03:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 280, loss 3.315474271774292\n",
      "Epoch 7: |          | 281/? [03:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 281, loss 3.9609532356262207\n",
      "Epoch 7: |          | 282/? [03:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 282, loss 3.5250906944274902\n",
      "Epoch 7: |          | 283/? [04:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 283, loss 3.597684383392334\n",
      "Epoch 7: |          | 284/? [04:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 284, loss 3.562713146209717\n",
      "Epoch 7: |          | 285/? [04:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 285, loss 3.088972806930542\n",
      "Epoch 7: |          | 286/? [04:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 286, loss 3.655715227127075\n",
      "Epoch 7: |          | 287/? [04:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 287, loss 3.366572618484497\n",
      "Epoch 7: |          | 288/? [04:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 288, loss 3.36445689201355\n",
      "Epoch 7: |          | 289/? [04:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 289, loss 3.161038875579834\n",
      "Epoch 7: |          | 290/? [04:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 290, loss 2.7324719429016113\n",
      "Epoch 7: |          | 291/? [04:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 291, loss 3.8029518127441406\n",
      "Epoch 7: |          | 292/? [04:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 292, loss 3.490694761276245\n",
      "Epoch 7: |          | 293/? [04:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 293, loss 3.77734375\n",
      "Epoch 7: |          | 294/? [04:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 294, loss 3.677518129348755\n",
      "Epoch 7: |          | 295/? [04:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 295, loss 3.9636070728302\n",
      "Epoch 7: |          | 296/? [04:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 296, loss 3.436048984527588\n",
      "Epoch 7: |          | 297/? [04:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 297, loss 4.131281852722168\n",
      "Epoch 7: |          | 298/? [04:19<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 298, loss 3.910642623901367\n",
      "Epoch 7: |          | 299/? [04:20<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 299, loss 4.196804046630859\n",
      "Epoch 7: |          | 300/? [04:20<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 300, loss 3.7221226692199707\n",
      "Epoch 7: |          | 301/? [04:21<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 301, loss 3.5230815410614014\n",
      "Epoch 7: |          | 302/? [04:22<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 302, loss 3.949633836746216\n",
      "Epoch 7: |          | 303/? [04:23<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 303, loss 3.7519898414611816\n",
      "Epoch 7: |          | 304/? [04:24<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 304, loss 3.9401283264160156\n",
      "Epoch 7: |          | 305/? [04:25<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 305, loss 3.9613990783691406\n",
      "Epoch 7: |          | 306/? [04:26<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 306, loss 3.6619906425476074\n",
      "Epoch 7: |          | 307/? [04:27<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 307, loss 3.8807225227355957\n",
      "Epoch 7: |          | 308/? [04:27<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 308, loss 4.035610198974609\n",
      "Epoch 7: |          | 309/? [04:28<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 309, loss 3.6557838916778564\n",
      "Epoch 7: |          | 310/? [04:29<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 310, loss 4.0666184425354\n",
      "Epoch 7: |          | 311/? [04:30<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 311, loss 3.6237499713897705\n",
      "Epoch 7: |          | 312/? [04:31<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 312, loss 3.6738243103027344\n",
      "Epoch 7: |          | 313/? [04:32<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 313, loss 3.4927685260772705\n",
      "Epoch 7: |          | 314/? [04:33<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 314, loss 3.814167022705078\n",
      "Epoch 7: |          | 315/? [04:34<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 315, loss 3.5286712646484375\n",
      "Epoch 7: |          | 316/? [04:34<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 316, loss 3.993483781814575\n",
      "Epoch 7: |          | 317/? [04:35<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 317, loss 3.855694532394409\n",
      "Epoch 7: |          | 318/? [04:36<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 318, loss 3.981529951095581\n",
      "Epoch 7: |          | 319/? [04:37<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 319, loss 3.2455391883850098\n",
      "Epoch 7: |          | 320/? [04:38<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 320, loss 3.7635371685028076\n",
      "Epoch 7: |          | 321/? [04:39<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 321, loss 3.5509159564971924\n",
      "Epoch 7: |          | 322/? [04:40<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 322, loss 4.145371913909912\n",
      "Epoch 7: |          | 323/? [04:41<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 323, loss 4.060479640960693\n",
      "Epoch 7: |          | 324/? [04:42<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 324, loss 3.834012269973755\n",
      "Epoch 7: |          | 325/? [04:42<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 325, loss 4.219291687011719\n",
      "Epoch 7: |          | 326/? [04:43<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 326, loss 3.831228256225586\n",
      "Epoch 7: |          | 327/? [04:44<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 327, loss 3.5407073497772217\n",
      "Epoch 7: |          | 328/? [04:45<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 328, loss 3.3480334281921387\n",
      "Epoch 7: |          | 329/? [04:46<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 329, loss 3.8873260021209717\n",
      "Epoch 7: |          | 330/? [04:47<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 330, loss 4.3949785232543945\n",
      "Epoch 7: |          | 331/? [04:48<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 331, loss 2.72813081741333\n",
      "Epoch 7: |          | 332/? [04:48<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 332, loss 3.75120210647583\n",
      "Epoch 7: |          | 333/? [04:49<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 333, loss 3.6272616386413574\n",
      "Epoch 7: |          | 334/? [04:50<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 334, loss 4.16555118560791\n",
      "Epoch 7: |          | 335/? [04:51<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 335, loss 4.102715492248535\n",
      "Epoch 7: |          | 336/? [04:52<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 336, loss 4.131495952606201\n",
      "Epoch 7: |          | 337/? [04:53<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 337, loss 4.61444091796875\n",
      "Epoch 7: |          | 338/? [04:53<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 338, loss 4.29287576675415\n",
      "Epoch 7: |          | 339/? [04:54<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 339, loss 3.4960174560546875\n",
      "Epoch 7: |          | 340/? [04:55<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 340, loss 3.2680611610412598\n",
      "Epoch 7: |          | 341/? [04:56<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 341, loss 3.2419536113739014\n",
      "Epoch 7: |          | 342/? [04:57<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 342, loss 3.811777114868164\n",
      "Epoch 7: |          | 343/? [04:58<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 343, loss 3.565378189086914\n",
      "Epoch 7: |          | 344/? [04:58<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 344, loss 4.39926290512085\n",
      "Epoch 7: |          | 345/? [04:59<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 345, loss 3.5559990406036377\n",
      "Epoch 7: |          | 346/? [05:00<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 346, loss 3.856661319732666\n",
      "Epoch 7: |          | 347/? [05:01<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 347, loss 3.6539454460144043\n",
      "Epoch 7: |          | 348/? [05:02<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 348, loss 3.052974224090576\n",
      "Epoch 7: |          | 349/? [05:03<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 349, loss 2.9258036613464355\n",
      "Epoch 7: |          | 350/? [05:03<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 350, loss 4.0925493240356445\n",
      "Epoch 7: |          | 351/? [05:04<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 351, loss 4.135622978210449\n",
      "Epoch 7: |          | 352/? [05:05<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 352, loss 3.425943374633789\n",
      "Epoch 7: |          | 353/? [05:06<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 353, loss 3.204939365386963\n",
      "Epoch 7: |          | 354/? [05:07<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 354, loss 3.6047770977020264\n",
      "Epoch 7: |          | 355/? [05:08<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 355, loss 3.932023286819458\n",
      "Epoch 7: |          | 356/? [05:08<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 356, loss 3.9217803478240967\n",
      "Epoch 7: |          | 357/? [05:09<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 357, loss 3.376952648162842\n",
      "Epoch 7: |          | 358/? [05:10<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 358, loss 3.359563112258911\n",
      "Epoch 7: |          | 359/? [05:11<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 359, loss 3.92018461227417\n",
      "Epoch 7: |          | 360/? [05:12<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 360, loss 3.5313620567321777\n",
      "Epoch 7: |          | 361/? [05:13<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 361, loss 3.676715850830078\n",
      "Epoch 7: |          | 362/? [05:14<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 362, loss 3.4385650157928467\n",
      "Epoch 7: |          | 363/? [05:14<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 363, loss 3.3802733421325684\n",
      "Epoch 7: |          | 364/? [05:15<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 364, loss 3.9499943256378174\n",
      "Epoch 7: |          | 365/? [05:16<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 365, loss 3.984386444091797\n",
      "Epoch 7: |          | 366/? [05:17<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 366, loss 3.7995657920837402\n",
      "Epoch 7: |          | 367/? [05:18<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 367, loss 3.7246227264404297\n",
      "Epoch 7: |          | 368/? [05:19<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 368, loss 3.3520312309265137\n",
      "Epoch 7: |          | 369/? [05:20<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 369, loss 3.6181697845458984\n",
      "Epoch 7: |          | 370/? [05:20<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 370, loss 3.3422904014587402\n",
      "Epoch 7: |          | 371/? [05:21<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 371, loss 4.204391956329346\n",
      "Epoch 7: |          | 372/? [05:22<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 372, loss 3.5155444145202637\n",
      "Epoch 7: |          | 373/? [05:23<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 373, loss 3.883787155151367\n",
      "Epoch 7: |          | 374/? [05:24<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 374, loss 3.561579942703247\n",
      "Epoch 7: |          | 375/? [05:25<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 375, loss 4.188385963439941\n",
      "Epoch 7: |          | 376/? [05:25<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 376, loss 3.650313138961792\n",
      "Epoch 7: |          | 377/? [05:26<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 377, loss 3.8414618968963623\n",
      "Epoch 7: |          | 378/? [05:27<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 378, loss 3.976649045944214\n",
      "Epoch 7: |          | 379/? [05:28<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 379, loss 3.7615807056427\n",
      "Epoch 7: |          | 380/? [05:29<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 380, loss 3.8009085655212402\n",
      "Epoch 7: |          | 381/? [05:30<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 381, loss 3.915193557739258\n",
      "Epoch 7: |          | 382/? [05:31<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 382, loss 3.5878729820251465\n",
      "Epoch 7: |          | 383/? [05:32<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 383, loss 3.71917462348938\n",
      "Epoch 7: |          | 384/? [05:33<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 384, loss 4.086171627044678\n",
      "Epoch 7: |          | 385/? [05:33<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 385, loss 3.704841136932373\n",
      "Epoch 7: |          | 386/? [05:34<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 386, loss 2.7451958656311035\n",
      "Epoch 7: |          | 387/? [05:35<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 387, loss 3.5424492359161377\n",
      "Epoch 7: |          | 388/? [05:36<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 388, loss 3.195794105529785\n",
      "Epoch 7: |          | 389/? [05:36<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 389, loss 4.068526744842529\n",
      "Epoch 7: |          | 390/? [05:37<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 390, loss 3.53963041305542\n",
      "Epoch 7: |          | 391/? [05:38<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 391, loss 3.956843614578247\n",
      "Epoch 7: |          | 392/? [05:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 392, loss 4.052948951721191\n",
      "Epoch 7: |          | 393/? [05:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 393, loss 4.070540428161621\n",
      "Epoch 7: |          | 394/? [05:41<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 394, loss 3.7211945056915283\n",
      "Epoch 7: |          | 395/? [05:42<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 395, loss 3.944836378097534\n",
      "Epoch 7: |          | 396/? [05:43<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 396, loss 3.8329555988311768\n",
      "Epoch 7: |          | 397/? [05:43<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 397, loss 3.6268062591552734\n",
      "Epoch 7: |          | 398/? [05:44<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 398, loss 3.548168897628784\n",
      "Epoch 7: |          | 399/? [05:45<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 399, loss 3.641638994216919\n",
      "Epoch 7: |          | 400/? [05:46<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 400, loss 3.632810115814209\n",
      "Epoch 7: |          | 401/? [05:47<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 401, loss 3.5959503650665283\n",
      "Epoch 7: |          | 402/? [05:48<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 402, loss 3.956993818283081\n",
      "Epoch 7: |          | 403/? [05:48<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 403, loss 3.8928303718566895\n",
      "Epoch 7: |          | 404/? [05:49<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 404, loss 3.4189014434814453\n",
      "Epoch 7: |          | 405/? [05:50<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 405, loss 3.4725258350372314\n",
      "Epoch 7: |          | 406/? [05:51<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 406, loss 3.791402816772461\n",
      "Epoch 7: |          | 407/? [05:52<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 407, loss 3.6848487854003906\n",
      "Epoch 7: |          | 408/? [05:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 408, loss 4.118975639343262\n",
      "Epoch 7: |          | 409/? [05:54<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 409, loss 4.033320426940918\n",
      "Epoch 7: |          | 410/? [05:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 410, loss 3.6682116985321045\n",
      "Epoch 7: |          | 411/? [05:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 411, loss 3.60319185256958\n",
      "Epoch 7: |          | 412/? [05:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 412, loss 3.1890578269958496\n",
      "Epoch 7: |          | 413/? [05:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 413, loss 3.8643951416015625\n",
      "Epoch 7: |          | 414/? [05:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 414, loss 3.4407410621643066\n",
      "Epoch 7: |          | 415/? [05:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 415, loss 3.864490509033203\n",
      "Epoch 7: |          | 416/? [05:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 416, loss 4.297544479370117\n",
      "Epoch 7: |          | 417/? [06:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 417, loss 4.259991645812988\n",
      "Epoch 7: |          | 418/? [06:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 418, loss 3.910851001739502\n",
      "Epoch 7: |          | 419/? [06:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 419, loss 3.6632208824157715\n",
      "Epoch 7: |          | 420/? [06:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 420, loss 3.8278987407684326\n",
      "Epoch 7: |          | 421/? [06:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 421, loss 4.330814838409424\n",
      "Epoch 7: |          | 422/? [06:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 422, loss 3.8613905906677246\n",
      "Epoch 7: |          | 423/? [06:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 423, loss 3.490086317062378\n",
      "Epoch 7: |          | 424/? [06:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 424, loss 4.012373924255371\n",
      "Epoch 7: |          | 425/? [06:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 425, loss 3.853574752807617\n",
      "Epoch 7: |          | 426/? [06:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 426, loss 3.4877429008483887\n",
      "Epoch 7: |          | 427/? [06:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 427, loss 3.556912899017334\n",
      "Epoch 7: |          | 428/? [06:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 428, loss 4.282778739929199\n",
      "Epoch 7: |          | 429/? [06:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 429, loss 3.1956393718719482\n",
      "Epoch 7: |          | 430/? [06:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 430, loss 3.8718693256378174\n",
      "Epoch 7: |          | 431/? [06:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 431, loss 3.7351608276367188\n",
      "Epoch 7: |          | 432/? [06:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 432, loss 3.8683040142059326\n",
      "Epoch 7: |          | 433/? [06:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 433, loss 3.82452654838562\n",
      "Epoch 7: |          | 434/? [06:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 434, loss 3.723824977874756\n",
      "Epoch 7: |          | 435/? [06:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 435, loss 3.404818058013916\n",
      "Epoch 7: |          | 436/? [06:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 436, loss 3.7913155555725098\n",
      "Epoch 7: |          | 437/? [06:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 437, loss 4.002995491027832\n",
      "Epoch 7: |          | 438/? [06:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 438, loss 3.618093490600586\n",
      "Epoch 7: |          | 439/? [06:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 439, loss 3.4614949226379395\n",
      "Epoch 7: |          | 440/? [06:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 440, loss 3.3505654335021973\n",
      "Epoch 7: |          | 441/? [06:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 441, loss 3.7788772583007812\n",
      "Epoch 7: |          | 442/? [06:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 442, loss 3.670226573944092\n",
      "Epoch 7: |          | 443/? [06:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 443, loss 3.792980909347534\n",
      "Epoch 7: |          | 444/? [06:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 444, loss 3.801164150238037\n",
      "Epoch 7: |          | 445/? [06:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 445, loss 4.684020042419434\n",
      "Epoch 7: |          | 446/? [06:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 446, loss 3.7410457134246826\n",
      "Epoch 7: |          | 447/? [06:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 447, loss 4.258675575256348\n",
      "Epoch 7: |          | 448/? [06:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 448, loss 3.352573871612549\n",
      "Epoch 7: |          | 449/? [06:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 449, loss 3.7518696784973145\n",
      "Epoch 7: |          | 450/? [06:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 450, loss 4.054373741149902\n",
      "Epoch 7: |          | 451/? [06:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 451, loss 3.761451244354248\n",
      "Epoch 7: |          | 452/? [06:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 452, loss 3.450127124786377\n",
      "Epoch 7: |          | 453/? [06:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 453, loss 4.188994407653809\n",
      "Epoch 7: |          | 454/? [06:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 454, loss 3.5657055377960205\n",
      "Epoch 7: |          | 455/? [06:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 455, loss 3.8899929523468018\n",
      "Epoch 7: |          | 456/? [06:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 456, loss 3.234254837036133\n",
      "Epoch 7: |          | 457/? [06:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 457, loss 3.686716079711914\n",
      "Epoch 7: |          | 458/? [06:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 458, loss 4.102423191070557\n",
      "Epoch 7: |          | 459/? [06:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 459, loss 4.094695091247559\n",
      "Epoch 7: |          | 460/? [06:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 460, loss 3.8468616008758545\n",
      "Epoch 7: |          | 461/? [06:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 461, loss 3.801833391189575\n",
      "Epoch 7: |          | 462/? [06:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 462, loss 3.861992359161377\n",
      "Epoch 7: |          | 463/? [06:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 463, loss 3.7583396434783936\n",
      "Epoch 7: |          | 464/? [06:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 464, loss 3.276812791824341\n",
      "Epoch 7: |          | 465/? [06:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 465, loss 3.5317890644073486\n",
      "Epoch 7: |          | 466/? [06:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 466, loss 3.9969379901885986\n",
      "Epoch 7: |          | 467/? [06:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 467, loss 3.76885986328125\n",
      "Epoch 7: |          | 468/? [06:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 468, loss 3.684370517730713\n",
      "Epoch 7: |          | 469/? [06:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 469, loss 3.8363044261932373\n",
      "Epoch 7: |          | 470/? [06:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 470, loss 3.2749381065368652\n",
      "Epoch 7: |          | 471/? [06:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 471, loss 4.053744792938232\n",
      "Epoch 7: |          | 472/? [06:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 472, loss 3.5256927013397217\n",
      "Epoch 7: |          | 473/? [06:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 473, loss 3.53326678276062\n",
      "Epoch 7: |          | 474/? [06:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 474, loss 3.199702024459839\n",
      "Epoch 7: |          | 475/? [06:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 475, loss 4.399079322814941\n",
      "Epoch 7: |          | 476/? [06:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 476, loss 3.486804485321045\n",
      "Epoch 7: |          | 477/? [06:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 477, loss 3.0342013835906982\n",
      "Epoch 7: |          | 478/? [06:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 478, loss 3.240424394607544\n",
      "Epoch 7: |          | 479/? [06:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 479, loss 3.728604793548584\n",
      "Epoch 7: |          | 480/? [06:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 480, loss 3.5598602294921875\n",
      "Epoch 7: |          | 481/? [06:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 481, loss 3.1954703330993652\n",
      "Epoch 7: |          | 482/? [06:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 482, loss 3.493682384490967\n",
      "Epoch 7: |          | 483/? [06:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 483, loss 3.199857473373413\n",
      "Epoch 7: |          | 484/? [06:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 484, loss 4.058756351470947\n",
      "Epoch 7: |          | 485/? [06:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 485, loss 3.9694983959198\n",
      "Epoch 7: |          | 486/? [06:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 486, loss 3.5215790271759033\n",
      "Epoch 7: |          | 487/? [06:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 487, loss 3.9325404167175293\n",
      "Epoch 7: |          | 488/? [06:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 488, loss 3.6838507652282715\n",
      "Epoch 7: |          | 489/? [07:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 489, loss 3.2202000617980957\n",
      "Epoch 7: |          | 490/? [07:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 490, loss 3.7902705669403076\n",
      "Epoch 7: |          | 491/? [07:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 491, loss 3.6434006690979004\n",
      "Epoch 7: |          | 492/? [07:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 492, loss 3.0149242877960205\n",
      "Epoch 7: |          | 493/? [07:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 493, loss 3.9773635864257812\n",
      "Epoch 7: |          | 494/? [07:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 494, loss 3.7683842182159424\n",
      "Epoch 7: |          | 495/? [07:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 495, loss 3.8661046028137207\n",
      "Epoch 7: |          | 496/? [07:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 496, loss 3.5191237926483154\n",
      "Epoch 7: |          | 497/? [07:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 497, loss 4.051197052001953\n",
      "Epoch 7: |          | 498/? [07:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 498, loss 3.669997453689575\n",
      "Epoch 7: |          | 499/? [07:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 499, loss 3.8237624168395996\n",
      "Epoch 7: |          | 500/? [07:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 500, loss 3.5862388610839844\n",
      "Epoch 7: |          | 501/? [07:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 501, loss 3.3529887199401855\n",
      "Epoch 7: |          | 502/? [07:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 502, loss 3.760962724685669\n",
      "Epoch 7: |          | 503/? [07:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 503, loss 3.6877524852752686\n",
      "Epoch 7: |          | 504/? [07:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 504, loss 3.6411328315734863\n",
      "Epoch 7: |          | 505/? [07:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 505, loss 3.14581561088562\n",
      "Epoch 7: |          | 506/? [07:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 506, loss 3.692986249923706\n",
      "Epoch 7: |          | 507/? [07:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 507, loss 3.714461088180542\n",
      "Epoch 7: |          | 508/? [07:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 508, loss 4.031294822692871\n",
      "Epoch 7: |          | 509/? [07:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 509, loss 3.430626630783081\n",
      "Epoch 7: |          | 510/? [07:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 510, loss 3.8827691078186035\n",
      "Epoch 7: |          | 511/? [07:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 511, loss 3.7388367652893066\n",
      "Epoch 7: |          | 512/? [07:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 512, loss 3.1960504055023193\n",
      "Epoch 7: |          | 513/? [07:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 513, loss 3.4368317127227783\n",
      "Epoch 7: |          | 514/? [07:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 514, loss 3.606916904449463\n",
      "Epoch 7: |          | 515/? [07:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 515, loss 3.2106330394744873\n",
      "Epoch 7: |          | 516/? [07:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 516, loss 3.5681285858154297\n",
      "Epoch 7: |          | 517/? [07:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 517, loss 3.740124464035034\n",
      "Epoch 7: |          | 518/? [07:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 518, loss 3.3491263389587402\n",
      "Epoch 7: |          | 519/? [07:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 519, loss 3.7591183185577393\n",
      "Epoch 7: |          | 520/? [07:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 520, loss 3.618964672088623\n",
      "Epoch 7: |          | 521/? [07:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 521, loss 3.653427839279175\n",
      "Epoch 7: |          | 522/? [07:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 522, loss 4.162507057189941\n",
      "Epoch 7: |          | 523/? [07:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 523, loss 4.2390456199646\n",
      "Epoch 7: |          | 524/? [07:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 524, loss 4.021823406219482\n",
      "Epoch 7: |          | 525/? [07:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 525, loss 3.6218605041503906\n",
      "Epoch 7: |          | 526/? [07:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 526, loss 3.4150021076202393\n",
      "Epoch 7: |          | 527/? [07:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 527, loss 4.099615573883057\n",
      "Epoch 7: |          | 528/? [07:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 528, loss 3.868671417236328\n",
      "Epoch 7: |          | 529/? [07:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 529, loss 3.4463114738464355\n",
      "Epoch 7: |          | 530/? [07:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 530, loss 3.985398769378662\n",
      "Epoch 7: |          | 531/? [07:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 531, loss 3.5068423748016357\n",
      "Epoch 7: |          | 532/? [07:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 532, loss 3.7873809337615967\n",
      "Epoch 7: |          | 533/? [07:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 533, loss 3.4089062213897705\n",
      "Epoch 7: |          | 534/? [07:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 534, loss 3.1301217079162598\n",
      "Epoch 7: |          | 535/? [07:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 535, loss 3.4248390197753906\n",
      "Epoch 7: |          | 536/? [07:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 536, loss 4.049160957336426\n",
      "Epoch 7: |          | 537/? [07:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 537, loss 3.8329505920410156\n",
      "Epoch 7: |          | 538/? [07:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 538, loss 3.476714611053467\n",
      "Epoch 7: |          | 539/? [07:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 539, loss 3.598717212677002\n",
      "Epoch 7: |          | 540/? [07:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 540, loss 3.954012632369995\n",
      "Epoch 7: |          | 541/? [07:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 541, loss 3.737290859222412\n",
      "Epoch 7: |          | 542/? [07:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 542, loss 3.4771485328674316\n",
      "Epoch 7: |          | 543/? [07:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 543, loss 3.872499942779541\n",
      "Epoch 7: |          | 544/? [07:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 544, loss 3.790369749069214\n",
      "Epoch 7: |          | 545/? [07:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 545, loss 3.094151258468628\n",
      "Epoch 7: |          | 546/? [07:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 546, loss 3.8499560356140137\n",
      "Epoch 7: |          | 547/? [07:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 547, loss 4.273549556732178\n",
      "Epoch 7: |          | 548/? [07:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 548, loss 3.932102918624878\n",
      "Epoch 7: |          | 549/? [07:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 549, loss 3.828747510910034\n",
      "Epoch 7: |          | 550/? [07:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 550, loss 4.1351213455200195\n",
      "Epoch 7: |          | 551/? [07:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 551, loss 3.825679063796997\n",
      "Epoch 7: |          | 552/? [07:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 552, loss 3.7648940086364746\n",
      "Epoch 7: |          | 553/? [07:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 553, loss 3.285327911376953\n",
      "Epoch 7: |          | 554/? [07:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 554, loss 3.8324036598205566\n",
      "Epoch 7: |          | 555/? [07:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 555, loss 4.065344333648682\n",
      "Epoch 7: |          | 556/? [07:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 556, loss 3.910151958465576\n",
      "Epoch 7: |          | 557/? [07:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 557, loss 3.3853659629821777\n",
      "Epoch 7: |          | 558/? [07:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 558, loss 3.6295571327209473\n",
      "Epoch 7: |          | 559/? [07:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 559, loss 3.5796971321105957\n",
      "Epoch 7: |          | 560/? [07:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 560, loss 3.1466221809387207\n",
      "Epoch 7: |          | 561/? [08:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 561, loss 2.987677574157715\n",
      "Epoch 7: |          | 562/? [08:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 562, loss 3.9869208335876465\n",
      "Epoch 7: |          | 563/? [08:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 563, loss 3.1155807971954346\n",
      "Epoch 7: |          | 564/? [08:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 564, loss 3.550300121307373\n",
      "Epoch 7: |          | 565/? [08:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 565, loss 3.885140895843506\n",
      "Epoch 7: |          | 566/? [08:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 566, loss 4.005919456481934\n",
      "Epoch 7: |          | 567/? [08:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 567, loss 4.090853214263916\n",
      "Epoch 7: |          | 568/? [08:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 568, loss 3.1865925788879395\n",
      "Epoch 7: |          | 569/? [08:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 569, loss 3.7814762592315674\n",
      "Epoch 7: |          | 570/? [08:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 570, loss 3.888280153274536\n",
      "Epoch 7: |          | 571/? [08:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 571, loss 3.4870896339416504\n",
      "Epoch 7: |          | 572/? [08:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 572, loss 4.423724174499512\n",
      "Epoch 7: |          | 573/? [08:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 573, loss 2.850757122039795\n",
      "Epoch 7: |          | 574/? [08:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 574, loss 3.989861249923706\n",
      "Epoch 7: |          | 575/? [08:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 575, loss 3.34443998336792\n",
      "Epoch 7: |          | 576/? [08:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 576, loss 3.550093173980713\n",
      "Epoch 7: |          | 577/? [08:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 577, loss 3.7398974895477295\n",
      "Epoch 7: |          | 578/? [08:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 578, loss 4.0123796463012695\n",
      "Epoch 7: |          | 579/? [08:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 579, loss 3.1802406311035156\n",
      "Epoch 7: |          | 580/? [08:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 580, loss 3.834043025970459\n",
      "Epoch 7: |          | 581/? [08:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 581, loss 3.8202743530273438\n",
      "Epoch 7: |          | 582/? [08:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 582, loss 3.888841152191162\n",
      "Epoch 7: |          | 583/? [08:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 583, loss 3.6826419830322266\n",
      "Epoch 7: |          | 584/? [08:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 584, loss 3.907083511352539\n",
      "Epoch 7: |          | 585/? [08:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 585, loss 3.80830717086792\n",
      "Epoch 7: |          | 586/? [08:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 586, loss 3.9034647941589355\n",
      "Epoch 7: |          | 587/? [08:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 587, loss 3.9243056774139404\n",
      "Epoch 7: |          | 588/? [08:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 588, loss 3.8036389350891113\n",
      "Epoch 7: |          | 589/? [08:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 589, loss 3.3250892162323\n",
      "Epoch 7: |          | 590/? [08:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 590, loss 3.858847141265869\n",
      "Epoch 7: |          | 591/? [08:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 591, loss 3.7629120349884033\n",
      "Epoch 7: |          | 592/? [08:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 592, loss 3.4149558544158936\n",
      "Epoch 7: |          | 593/? [08:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 593, loss 3.7936272621154785\n",
      "Epoch 7: |          | 594/? [08:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 594, loss 4.52716588973999\n",
      "Epoch 7: |          | 595/? [08:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 595, loss 3.330173969268799\n",
      "Epoch 7: |          | 596/? [08:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 596, loss 3.369206190109253\n",
      "Epoch 7: |          | 597/? [08:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 597, loss 3.5826034545898438\n",
      "Epoch 7: |          | 598/? [08:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 598, loss 4.033606052398682\n",
      "Epoch 7: |          | 599/? [08:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 599, loss 3.723127841949463\n",
      "Epoch 7: |          | 600/? [08:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 600, loss 3.5041351318359375\n",
      "Epoch 7: |          | 601/? [08:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 601, loss 3.7853007316589355\n",
      "Epoch 7: |          | 602/? [08:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 602, loss 3.3370635509490967\n",
      "Epoch 7: |          | 603/? [08:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 603, loss 3.3940041065216064\n",
      "Epoch 7: |          | 604/? [08:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 604, loss 4.898348808288574\n",
      "Epoch 7: |          | 605/? [08:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 605, loss 3.2300331592559814\n",
      "Epoch 7: |          | 606/? [08:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 606, loss 3.5501837730407715\n",
      "Epoch 7: |          | 607/? [08:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 607, loss 3.835615873336792\n",
      "Epoch 7: |          | 608/? [08:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 608, loss 3.6214916706085205\n",
      "Epoch 7: |          | 609/? [08:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 609, loss 3.547548770904541\n",
      "Epoch 7: |          | 610/? [08:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 610, loss 3.6235034465789795\n",
      "Epoch 7: |          | 611/? [08:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 611, loss 3.7392566204071045\n",
      "Epoch 7: |          | 612/? [08:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 612, loss 3.492788791656494\n",
      "Epoch 7: |          | 613/? [08:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 613, loss 3.8130156993865967\n",
      "Epoch 7: |          | 614/? [08:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 614, loss 3.61608624458313\n",
      "Epoch 7: |          | 615/? [08:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 615, loss 4.138144016265869\n",
      "Epoch 7: |          | 616/? [08:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 616, loss 4.275877952575684\n",
      "Epoch 7: |          | 617/? [08:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 617, loss 2.9641549587249756\n",
      "Epoch 7: |          | 618/? [08:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 618, loss 3.84336519241333\n",
      "Epoch 7: |          | 619/? [08:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 619, loss 3.429919719696045\n",
      "Epoch 7: |          | 620/? [08:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 620, loss 3.929719924926758\n",
      "Epoch 7: |          | 621/? [08:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 621, loss 3.4377007484436035\n",
      "Epoch 7: |          | 622/? [08:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 622, loss 3.2255287170410156\n",
      "Epoch 7: |          | 623/? [08:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 623, loss 3.036957263946533\n",
      "Epoch 7: |          | 624/? [08:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 624, loss 2.7609047889709473\n",
      "Epoch 7: |          | 625/? [08:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 625, loss 4.149616241455078\n",
      "Epoch 7: |          | 626/? [08:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 626, loss 3.649787187576294\n",
      "Epoch 7: |          | 627/? [08:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 627, loss 3.555410861968994\n",
      "Epoch 7: |          | 628/? [08:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 628, loss 3.597503662109375\n",
      "Epoch 7: |          | 629/? [08:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 629, loss 3.9458019733428955\n",
      "Epoch 7: |          | 630/? [08:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 630, loss 3.7319107055664062\n",
      "Epoch 7: |          | 631/? [08:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 631, loss 3.834674835205078\n",
      "Epoch 7: |          | 632/? [08:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 632, loss 3.2140090465545654\n",
      "Epoch 7: |          | 633/? [08:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 633, loss 3.9559905529022217\n",
      "Epoch 7: |          | 634/? [09:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 634, loss 3.479532241821289\n",
      "Epoch 7: |          | 635/? [09:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 635, loss 3.3489792346954346\n",
      "Epoch 7: |          | 636/? [09:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 636, loss 3.7332730293273926\n",
      "Epoch 7: |          | 637/? [09:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 637, loss 3.564678907394409\n",
      "Epoch 7: |          | 638/? [09:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 638, loss 3.7777907848358154\n",
      "Epoch 7: |          | 639/? [09:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 639, loss 3.543851375579834\n",
      "Epoch 7: |          | 640/? [09:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 640, loss 4.066893577575684\n",
      "Epoch 7: |          | 641/? [09:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 641, loss 3.1013424396514893\n",
      "Epoch 7: |          | 642/? [09:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 642, loss 3.907700777053833\n",
      "Epoch 7: |          | 643/? [09:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 643, loss 3.7545948028564453\n",
      "Epoch 7: |          | 644/? [09:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 644, loss 3.7547383308410645\n",
      "Epoch 7: |          | 645/? [09:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 645, loss 3.464285373687744\n",
      "Epoch 7: |          | 646/? [09:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 646, loss 3.4842026233673096\n",
      "Epoch 7: |          | 647/? [09:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 647, loss 4.027015686035156\n",
      "Epoch 7: |          | 648/? [09:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 648, loss 3.4342689514160156\n",
      "Epoch 7: |          | 649/? [09:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 649, loss 2.9174811840057373\n",
      "Epoch 7: |          | 650/? [09:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 650, loss 3.9827075004577637\n",
      "Epoch 7: |          | 651/? [09:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 651, loss 4.1114325523376465\n",
      "Epoch 7: |          | 652/? [09:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 652, loss 3.5922558307647705\n",
      "Epoch 7: |          | 653/? [09:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 653, loss 3.759054183959961\n",
      "Epoch 7: |          | 654/? [09:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 654, loss 3.7947800159454346\n",
      "Epoch 7: |          | 655/? [09:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 655, loss 3.593287229537964\n",
      "Epoch 7: |          | 656/? [09:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 656, loss 3.289503574371338\n",
      "Epoch 7: |          | 657/? [09:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 657, loss 5.501234531402588\n",
      "Epoch 7: |          | 658/? [09:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 658, loss 3.155794143676758\n",
      "Epoch 7: |          | 659/? [09:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 659, loss 3.712200880050659\n",
      "Epoch 7: |          | 660/? [09:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 660, loss 4.06686544418335\n",
      "Epoch 7: |          | 661/? [09:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 661, loss 3.995440721511841\n",
      "Epoch 7: |          | 662/? [09:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 662, loss 3.85481333732605\n",
      "Epoch 7: |          | 663/? [09:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 663, loss 3.5820796489715576\n",
      "Epoch 7: |          | 664/? [09:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 664, loss 3.5404090881347656\n",
      "Epoch 7: |          | 665/? [09:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 665, loss 3.832738161087036\n",
      "Epoch 7: |          | 666/? [09:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 666, loss 3.6258533000946045\n",
      "Epoch 7: |          | 667/? [09:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 667, loss 4.432987689971924\n",
      "Epoch 7: |          | 668/? [09:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 668, loss 3.2569031715393066\n",
      "Epoch 7: |          | 669/? [09:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 669, loss 3.4380812644958496\n",
      "Epoch 7: |          | 670/? [09:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 670, loss 4.102337837219238\n",
      "Epoch 7: |          | 671/? [09:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 671, loss 3.8452961444854736\n",
      "Epoch 7: |          | 672/? [09:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 672, loss 3.874598741531372\n",
      "Epoch 7: |          | 673/? [09:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 673, loss 3.7220635414123535\n",
      "Epoch 7: |          | 674/? [09:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 674, loss 2.1599655151367188\n",
      "Epoch 7: |          | 675/? [09:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 675, loss 0.7500529289245605\n",
      "Epoch 7: |          | 676/? [09:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 676, loss 0.6456688642501831\n",
      "Epoch 7: |          | 677/? [09:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 677, loss 0.5520415306091309\n",
      "Epoch 7: |          | 678/? [09:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 678, loss 1.6522420644760132\n",
      "Epoch 7: |          | 679/? [09:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 679, loss 3.1218857765197754\n",
      "Epoch 7: |          | 680/? [09:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 680, loss 3.6142425537109375\n",
      "Epoch 7: |          | 681/? [09:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 681, loss 3.2259697914123535\n",
      "Epoch 7: |          | 682/? [09:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 682, loss 3.499621629714966\n",
      "Epoch 7: |          | 683/? [09:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 683, loss 3.211761474609375\n",
      "Epoch 7: |          | 684/? [09:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 684, loss 4.212216377258301\n",
      "Epoch 7: |          | 685/? [09:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 685, loss 3.805752992630005\n",
      "Epoch 7: |          | 686/? [09:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 686, loss 3.464449405670166\n",
      "Epoch 7: |          | 687/? [09:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 687, loss 3.975041627883911\n",
      "Epoch 7: |          | 688/? [09:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 688, loss 3.406588315963745\n",
      "Epoch 7: |          | 689/? [09:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 689, loss 3.534797191619873\n",
      "Epoch 7: |          | 690/? [09:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 690, loss 4.197052478790283\n",
      "Epoch 7: |          | 691/? [09:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 691, loss 3.6757049560546875\n",
      "Epoch 7: |          | 692/? [09:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 692, loss 3.6895363330841064\n",
      "Epoch 7: |          | 693/? [09:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 693, loss 4.211824417114258\n",
      "Epoch 7: |          | 694/? [09:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 694, loss 3.5683751106262207\n",
      "Epoch 7: |          | 695/? [09:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 695, loss 4.114426136016846\n",
      "Epoch 7: |          | 696/? [09:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 696, loss 3.4502155780792236\n",
      "Epoch 7: |          | 697/? [09:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 697, loss 3.6279475688934326\n",
      "Epoch 7: |          | 698/? [09:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 698, loss 3.1077756881713867\n",
      "Epoch 7: |          | 699/? [09:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 699, loss 3.793445587158203\n",
      "Epoch 7: |          | 700/? [09:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 700, loss 3.862217664718628\n",
      "Epoch 7: |          | 701/? [09:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 701, loss 3.5350089073181152\n",
      "Epoch 7: |          | 702/? [09:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 702, loss 3.799424648284912\n",
      "Epoch 7: |          | 703/? [09:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 703, loss 3.906874895095825\n",
      "Epoch 7: |          | 704/? [09:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 704, loss 3.7544937133789062\n",
      "Epoch 7: |          | 705/? [09:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 705, loss 3.3944029808044434\n",
      "Epoch 7: |          | 706/? [10:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 706, loss 3.452866315841675\n",
      "Epoch 7: |          | 707/? [10:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 707, loss 3.916935682296753\n",
      "Epoch 7: |          | 708/? [10:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 708, loss 3.661533832550049\n",
      "Epoch 7: |          | 709/? [10:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 709, loss 3.5658390522003174\n",
      "Epoch 7: |          | 710/? [10:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 710, loss 4.109494209289551\n",
      "Epoch 7: |          | 711/? [10:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 711, loss 4.107630252838135\n",
      "Epoch 7: |          | 712/? [10:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 712, loss 3.900322675704956\n",
      "Epoch 7: |          | 713/? [10:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 713, loss 3.9905295372009277\n",
      "Epoch 7: |          | 714/? [10:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 714, loss 4.018881797790527\n",
      "Epoch 7: |          | 715/? [10:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 715, loss 3.028494358062744\n",
      "Epoch 7: |          | 716/? [10:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 716, loss 3.689990282058716\n",
      "Epoch 7: |          | 717/? [10:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 717, loss 3.6258578300476074\n",
      "Epoch 7: |          | 718/? [10:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 718, loss 3.1459572315216064\n",
      "Epoch 7: |          | 719/? [10:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 719, loss 3.608372926712036\n",
      "Epoch 7: |          | 720/? [10:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 720, loss 3.3240318298339844\n",
      "Epoch 7: |          | 721/? [10:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 721, loss 3.9993133544921875\n",
      "Epoch 7: |          | 722/? [10:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 722, loss 3.352771759033203\n",
      "Epoch 7: |          | 723/? [10:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 723, loss 3.8466098308563232\n",
      "Epoch 7: |          | 724/? [10:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 724, loss 3.394110918045044\n",
      "Epoch 7: |          | 725/? [10:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 725, loss 3.3751273155212402\n",
      "Epoch 7: |          | 726/? [10:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 726, loss 3.534036159515381\n",
      "Epoch 7: |          | 727/? [10:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 727, loss 3.3445048332214355\n",
      "Epoch 7: |          | 728/? [10:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 728, loss 3.1800646781921387\n",
      "Epoch 7: |          | 729/? [10:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 729, loss 3.6604068279266357\n",
      "Epoch 7: |          | 730/? [10:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 730, loss 3.6326072216033936\n",
      "Epoch 7: |          | 731/? [10:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 731, loss 3.758484363555908\n",
      "Epoch 7: |          | 732/? [10:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 732, loss 3.9662845134735107\n",
      "Epoch 7: |          | 733/? [10:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 733, loss 3.6951332092285156\n",
      "Epoch 7: |          | 734/? [10:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 734, loss 3.814605712890625\n",
      "Epoch 7: |          | 735/? [10:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 735, loss 3.7548835277557373\n",
      "Epoch 7: |          | 736/? [10:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 736, loss 3.3706600666046143\n",
      "Epoch 7: |          | 737/? [10:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 737, loss 4.0954270362854\n",
      "Epoch 7: |          | 738/? [10:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 738, loss 3.322844982147217\n",
      "Epoch 7: |          | 739/? [10:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 739, loss 3.781695604324341\n",
      "Epoch 7: |          | 740/? [10:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 740, loss 3.4961085319519043\n",
      "Epoch 7: |          | 741/? [10:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 741, loss 3.623429775238037\n",
      "Epoch 7: |          | 742/? [10:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 742, loss 3.9744911193847656\n",
      "Epoch 7: |          | 743/? [10:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 743, loss 3.8870151042938232\n",
      "Epoch 7: |          | 744/? [10:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 744, loss 3.8316490650177\n",
      "Epoch 7: |          | 745/? [10:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 745, loss 3.4765477180480957\n",
      "Epoch 7: |          | 746/? [10:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 746, loss 3.7519659996032715\n",
      "Epoch 7: |          | 747/? [10:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 747, loss 3.4697670936584473\n",
      "Epoch 7: |          | 748/? [10:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 748, loss 2.478102207183838\n",
      "Epoch 7: |          | 749/? [10:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 749, loss 3.608858585357666\n",
      "Epoch 7: |          | 750/? [10:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 750, loss 3.8226757049560547\n",
      "Epoch 7: |          | 751/? [10:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 751, loss 2.1701464653015137\n",
      "Epoch 7: |          | 752/? [10:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 752, loss 3.797621965408325\n",
      "Epoch 7: |          | 753/? [10:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 753, loss 2.967885971069336\n",
      "Epoch 7: |          | 754/? [10:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 754, loss 3.4610679149627686\n",
      "Epoch 7: |          | 755/? [10:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 755, loss 3.2727293968200684\n",
      "Epoch 7: |          | 756/? [10:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 756, loss 3.6329421997070312\n",
      "Epoch 7: |          | 757/? [10:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 757, loss 3.695652723312378\n",
      "Epoch 7: |          | 758/? [10:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 758, loss 3.460069179534912\n",
      "Epoch 7: |          | 759/? [10:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 759, loss 3.4091804027557373\n",
      "Epoch 7: |          | 760/? [10:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 760, loss 3.904520034790039\n",
      "Epoch 7: |          | 761/? [10:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 761, loss 3.9180703163146973\n",
      "Epoch 7: |          | 762/? [10:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 762, loss 3.5734188556671143\n",
      "Epoch 7: |          | 763/? [10:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 763, loss 3.6876068115234375\n",
      "Epoch 7: |          | 764/? [10:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 764, loss 3.959101438522339\n",
      "Epoch 7: |          | 765/? [10:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 765, loss 3.769688367843628\n",
      "Epoch 7: |          | 766/? [10:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 766, loss 4.1196794509887695\n",
      "Epoch 7: |          | 767/? [10:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 767, loss 4.18152379989624\n",
      "Epoch 7: |          | 768/? [10:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 768, loss 3.73250150680542\n",
      "Epoch 7: |          | 769/? [10:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 769, loss 2.9843690395355225\n",
      "Epoch 7: |          | 770/? [10:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 770, loss 3.5006814002990723\n",
      "Epoch 7: |          | 771/? [10:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 771, loss 4.202418327331543\n",
      "Epoch 7: |          | 772/? [10:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 772, loss 3.939823865890503\n",
      "Epoch 7: |          | 773/? [10:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 773, loss 3.6468863487243652\n",
      "Epoch 7: |          | 774/? [10:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 774, loss 3.78730845451355\n",
      "Epoch 7: |          | 775/? [10:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 775, loss 4.20590353012085\n",
      "Epoch 7: |          | 776/? [10:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 776, loss 3.679041624069214\n",
      "Epoch 7: |          | 777/? [11:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 777, loss 3.5238895416259766\n",
      "Epoch 7: |          | 778/? [11:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 778, loss 3.917137861251831\n",
      "Epoch 7: |          | 779/? [11:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 779, loss 4.352104187011719\n",
      "Epoch 7: |          | 780/? [11:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 780, loss 3.391679286956787\n",
      "Epoch 7: |          | 781/? [11:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 781, loss 3.4783828258514404\n",
      "Epoch 7: |          | 782/? [11:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 782, loss 3.8233673572540283\n",
      "Epoch 7: |          | 783/? [11:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 783, loss 3.892578125\n",
      "Epoch 7: |          | 784/? [11:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 784, loss 3.487555742263794\n",
      "Epoch 7: |          | 785/? [11:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 785, loss 3.27270245552063\n",
      "Epoch 7: |          | 786/? [11:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 786, loss 4.136336326599121\n",
      "Epoch 7: |          | 787/? [11:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 787, loss 4.062738418579102\n",
      "Epoch 7: |          | 788/? [11:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 788, loss 2.0324442386627197\n",
      "Epoch 7: |          | 789/? [11:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 789, loss 3.579958438873291\n",
      "Epoch 7: |          | 790/? [11:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 790, loss 4.397402286529541\n",
      "Epoch 7: |          | 791/? [11:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 791, loss 4.127371311187744\n",
      "Epoch 7: |          | 792/? [11:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 792, loss 3.3686866760253906\n",
      "Epoch 7: |          | 793/? [11:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 793, loss 3.8283748626708984\n",
      "Epoch 7: |          | 794/? [11:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 794, loss 4.128922462463379\n",
      "Epoch 7: |          | 795/? [11:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 795, loss 3.658637285232544\n",
      "Epoch 7: |          | 796/? [11:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 796, loss 4.009213924407959\n",
      "Epoch 7: |          | 797/? [11:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 797, loss 3.052067995071411\n",
      "Epoch 7: |          | 798/? [11:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 798, loss 3.1488900184631348\n",
      "Epoch 7: |          | 799/? [11:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 799, loss 4.050288200378418\n",
      "Epoch 7: |          | 800/? [11:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 800, loss 3.881157636642456\n",
      "Epoch 7: |          | 801/? [11:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 801, loss 3.4823532104492188\n",
      "Epoch 7: |          | 802/? [11:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 802, loss 3.75954008102417\n",
      "Epoch 7: |          | 803/? [11:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 803, loss 3.6041476726531982\n",
      "Epoch 7: |          | 804/? [11:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 804, loss 3.7566936016082764\n",
      "Epoch 7: |          | 805/? [11:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 805, loss 3.87646222114563\n",
      "Epoch 7: |          | 806/? [11:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 806, loss 4.357365608215332\n",
      "Epoch 7: |          | 807/? [11:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 807, loss 3.7075233459472656\n",
      "Epoch 7: |          | 808/? [11:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 808, loss 3.3818278312683105\n",
      "Epoch 7: |          | 809/? [11:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 809, loss 3.8792552947998047\n",
      "Epoch 7: |          | 810/? [11:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 810, loss 3.672210216522217\n",
      "Epoch 7: |          | 811/? [11:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 811, loss 3.8924083709716797\n",
      "Epoch 7: |          | 812/? [11:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 812, loss 4.486429214477539\n",
      "Epoch 7: |          | 813/? [11:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 813, loss 4.274646759033203\n",
      "Epoch 7: |          | 814/? [11:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 814, loss 3.337268829345703\n",
      "Epoch 7: |          | 815/? [11:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 815, loss 4.044680595397949\n",
      "Epoch 7: |          | 816/? [11:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 816, loss 3.8542392253875732\n",
      "Epoch 7: |          | 817/? [11:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 817, loss 3.187171220779419\n",
      "Epoch 7: |          | 818/? [11:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 818, loss 4.129106521606445\n",
      "Epoch 7: |          | 819/? [11:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 819, loss 3.880831480026245\n",
      "Epoch 7: |          | 820/? [11:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 820, loss 3.692448139190674\n",
      "Epoch 7: |          | 821/? [11:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 821, loss 3.648380756378174\n",
      "Epoch 7: |          | 822/? [11:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 822, loss 3.373807191848755\n",
      "Epoch 7: |          | 823/? [11:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 823, loss 3.407783031463623\n",
      "Epoch 7: |          | 824/? [11:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 824, loss 3.825549364089966\n",
      "Epoch 7: |          | 825/? [11:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 825, loss 3.407433032989502\n",
      "Epoch 7: |          | 826/? [11:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 826, loss 3.9267818927764893\n",
      "Epoch 7: |          | 827/? [11:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 827, loss 3.5594322681427\n",
      "Epoch 7: |          | 828/? [11:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 828, loss 4.0071258544921875\n",
      "Epoch 7: |          | 829/? [11:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 829, loss 3.7122886180877686\n",
      "Epoch 7: |          | 830/? [11:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 830, loss 4.256436347961426\n",
      "Epoch 7: |          | 831/? [11:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 831, loss 2.2382941246032715\n",
      "Epoch 7: |          | 832/? [11:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 832, loss 3.648193836212158\n",
      "Epoch 7: |          | 833/? [11:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 833, loss 3.5532517433166504\n",
      "Epoch 7: |          | 834/? [11:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 834, loss 4.2809529304504395\n",
      "Epoch 7: |          | 835/? [11:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 835, loss 3.6416592597961426\n",
      "Epoch 7: |          | 836/? [11:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 836, loss 4.257388114929199\n",
      "Epoch 7: |          | 837/? [11:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 837, loss 3.7042911052703857\n",
      "Epoch 7: |          | 838/? [11:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 838, loss 3.094472885131836\n",
      "Epoch 7: |          | 839/? [11:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 839, loss 3.4769039154052734\n",
      "Epoch 7: |          | 840/? [12:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 840, loss 3.998105525970459\n",
      "Epoch 7: |          | 841/? [12:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 841, loss 4.024708271026611\n",
      "Epoch 7: |          | 842/? [12:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 842, loss 3.702157974243164\n",
      "Epoch 7: |          | 843/? [12:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 843, loss 4.016317367553711\n",
      "Epoch 7: |          | 844/? [12:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 844, loss 3.394145965576172\n",
      "Epoch 7: |          | 845/? [12:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 845, loss 3.7855281829833984\n",
      "Epoch 7: |          | 846/? [12:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 846, loss 4.2004828453063965\n",
      "Epoch 7: |          | 847/? [12:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 847, loss 3.827678680419922\n",
      "Epoch 7: |          | 848/? [12:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 848, loss 3.411853075027466\n",
      "Epoch 7: |          | 849/? [12:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 849, loss 3.451035737991333\n",
      "Epoch 7: |          | 850/? [12:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 850, loss 3.5648446083068848\n",
      "Epoch 7: |          | 851/? [12:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 851, loss 3.848433017730713\n",
      "Epoch 7: |          | 852/? [12:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 852, loss 4.020501136779785\n",
      "Epoch 7: |          | 853/? [12:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 853, loss 3.8125805854797363\n",
      "Epoch 7: |          | 854/? [12:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 854, loss 3.20097017288208\n",
      "Epoch 7: |          | 855/? [12:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 855, loss 3.4159038066864014\n",
      "Epoch 7: |          | 856/? [12:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 856, loss 3.3777740001678467\n",
      "Epoch 7: |          | 857/? [12:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 857, loss 3.8856284618377686\n",
      "Epoch 7: |          | 858/? [12:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 858, loss 3.8048229217529297\n",
      "Epoch 7: |          | 859/? [12:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 859, loss 3.801867723464966\n",
      "Epoch 7: |          | 860/? [12:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 860, loss 4.160555839538574\n",
      "Epoch 7: |          | 861/? [12:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 861, loss 3.5219483375549316\n",
      "Epoch 7: |          | 862/? [12:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 862, loss 3.8214173316955566\n",
      "Epoch 7: |          | 863/? [12:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 863, loss 3.234722137451172\n",
      "Epoch 7: |          | 864/? [12:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 864, loss 3.765432357788086\n",
      "Epoch 7: |          | 865/? [12:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 865, loss 3.7822182178497314\n",
      "Epoch 7: |          | 866/? [12:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 866, loss 2.8320541381835938\n",
      "Epoch 7: |          | 867/? [12:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 867, loss 2.9614710807800293\n",
      "Epoch 7: |          | 868/? [12:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 868, loss 3.8388848304748535\n",
      "Epoch 7: |          | 869/? [12:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 869, loss 3.910065174102783\n",
      "Epoch 7: |          | 870/? [12:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 870, loss 3.5107626914978027\n",
      "Epoch 7: |          | 871/? [12:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 871, loss 3.8535797595977783\n",
      "Epoch 7: |          | 872/? [12:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 872, loss 3.6486611366271973\n",
      "Epoch 7: |          | 873/? [12:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 873, loss 3.657503843307495\n",
      "Epoch 7: |          | 874/? [12:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 874, loss 3.189479112625122\n",
      "Epoch 7: |          | 875/? [12:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 875, loss 3.899376392364502\n",
      "Epoch 7: |          | 876/? [12:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 876, loss 3.4127719402313232\n",
      "Epoch 7: |          | 877/? [12:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 877, loss 3.841712474822998\n",
      "Epoch 7: |          | 878/? [12:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 878, loss 3.2676079273223877\n",
      "Epoch 7: |          | 879/? [12:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 879, loss 3.3343398571014404\n",
      "Epoch 7: |          | 880/? [12:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 880, loss 4.390347003936768\n",
      "Epoch 7: |          | 881/? [12:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 881, loss 3.8180782794952393\n",
      "Epoch 7: |          | 882/? [12:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 882, loss 3.606679916381836\n",
      "Epoch 7: |          | 883/? [12:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 883, loss 3.7733750343322754\n",
      "Epoch 7: |          | 884/? [12:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 884, loss 3.767526149749756\n",
      "Epoch 7: |          | 885/? [12:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 885, loss 3.5574257373809814\n",
      "Epoch 7: |          | 886/? [12:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 886, loss 4.255383014678955\n",
      "Epoch 7: |          | 887/? [12:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 887, loss 4.203227996826172\n",
      "Epoch 7: |          | 888/? [12:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 888, loss 3.930924892425537\n",
      "Epoch 7: |          | 889/? [12:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 889, loss 3.505565643310547\n",
      "Epoch 7: |          | 890/? [12:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 890, loss 3.694540023803711\n",
      "Epoch 7: |          | 891/? [12:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 891, loss 3.5669760704040527\n",
      "Epoch 7: |          | 892/? [12:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 892, loss 4.135245323181152\n",
      "Epoch 7: |          | 893/? [12:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 893, loss 3.582209348678589\n",
      "Epoch 7: |          | 894/? [12:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 894, loss 3.079115152359009\n",
      "Epoch 7: |          | 895/? [12:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 895, loss 4.216925621032715\n",
      "Epoch 7: |          | 896/? [12:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 896, loss 3.8034634590148926\n",
      "Epoch 7: |          | 897/? [12:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 897, loss 3.842390537261963\n",
      "Epoch 7: |          | 898/? [12:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 898, loss 3.7986271381378174\n",
      "Epoch 7: |          | 899/? [12:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 899, loss 3.587740421295166\n",
      "Epoch 7: |          | 900/? [12:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 900, loss 3.526259183883667\n",
      "Epoch 7: |          | 901/? [12:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 901, loss 3.8865954875946045\n",
      "Epoch 7: |          | 902/? [12:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 902, loss 4.0345964431762695\n",
      "Epoch 7: |          | 903/? [12:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 903, loss 3.3437724113464355\n",
      "Epoch 7: |          | 904/? [12:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 904, loss 3.831066608428955\n",
      "Epoch 7: |          | 905/? [12:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 905, loss 3.982426166534424\n",
      "Epoch 7: |          | 906/? [12:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 906, loss 3.744675397872925\n",
      "Epoch 7: |          | 907/? [12:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 907, loss 3.8150315284729004\n",
      "Epoch 7: |          | 908/? [12:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 908, loss 3.894306182861328\n",
      "Epoch 7: |          | 909/? [12:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 909, loss 3.86051607131958\n",
      "Epoch 7: |          | 910/? [12:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 910, loss 3.619241237640381\n",
      "Epoch 7: |          | 911/? [13:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 911, loss 3.6959404945373535\n",
      "Epoch 7: |          | 912/? [13:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 912, loss 3.65399432182312\n",
      "Epoch 7: |          | 913/? [13:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 913, loss 3.6566321849823\n",
      "Epoch 7: |          | 914/? [13:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 914, loss 3.9242920875549316\n",
      "Epoch 7: |          | 915/? [13:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 915, loss 3.759584426879883\n",
      "Epoch 7: |          | 916/? [13:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 916, loss 3.71745228767395\n",
      "Epoch 7: |          | 917/? [13:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 917, loss 3.629182815551758\n",
      "Epoch 7: |          | 918/? [13:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 918, loss 3.5616397857666016\n",
      "Epoch 7: |          | 919/? [13:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 919, loss 3.5995192527770996\n",
      "Epoch 7: |          | 920/? [13:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 920, loss 3.740283250808716\n",
      "Epoch 7: |          | 921/? [13:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 921, loss 3.554800510406494\n",
      "Epoch 7: |          | 922/? [13:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 922, loss 3.7083849906921387\n",
      "Epoch 7: |          | 923/? [13:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 923, loss 3.574244737625122\n",
      "Epoch 7: |          | 924/? [13:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 924, loss 3.57342791557312\n",
      "Epoch 7: |          | 925/? [13:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 925, loss 3.891298294067383\n",
      "Epoch 7: |          | 926/? [13:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 926, loss 3.647547483444214\n",
      "Epoch 7: |          | 927/? [13:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 927, loss 3.947390079498291\n",
      "Epoch 7: |          | 928/? [13:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 928, loss 3.442387342453003\n",
      "Epoch 7: |          | 929/? [13:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 929, loss 3.533090114593506\n",
      "Epoch 7: |          | 930/? [13:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 930, loss 3.46140718460083\n",
      "Epoch 7: |          | 931/? [13:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 931, loss 3.1731479167938232\n",
      "Epoch 7: |          | 932/? [13:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 932, loss 3.817673444747925\n",
      "Epoch 7: |          | 933/? [13:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 933, loss 3.5706779956817627\n",
      "Epoch 7: |          | 934/? [13:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 934, loss 4.10226583480835\n",
      "Epoch 7: |          | 935/? [13:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 935, loss 4.423879146575928\n",
      "Epoch 7: |          | 936/? [13:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 936, loss 3.660416841506958\n",
      "Epoch 7: |          | 937/? [13:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 937, loss 3.4589664936065674\n",
      "Epoch 7: |          | 938/? [13:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 938, loss 3.603701114654541\n",
      "Epoch 7: |          | 939/? [13:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 939, loss 3.8431286811828613\n",
      "Epoch 7: |          | 940/? [13:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 940, loss 4.015089988708496\n",
      "Epoch 7: |          | 941/? [13:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 941, loss 3.5860543251037598\n",
      "Epoch 7: |          | 942/? [13:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 942, loss 3.06208872795105\n",
      "Epoch 7: |          | 943/? [13:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 943, loss 3.8806958198547363\n",
      "Epoch 7: |          | 944/? [13:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 944, loss 3.006057024002075\n",
      "Epoch 7: |          | 945/? [13:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 945, loss 3.698164463043213\n",
      "Epoch 7: |          | 946/? [13:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 946, loss 3.6242947578430176\n",
      "Epoch 7: |          | 947/? [13:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 947, loss 3.51939058303833\n",
      "Epoch 7: |          | 948/? [13:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 948, loss 3.8034539222717285\n",
      "Epoch 7: |          | 949/? [13:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 949, loss 3.653449535369873\n",
      "Epoch 7: |          | 950/? [13:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 950, loss 3.412304639816284\n",
      "Epoch 7: |          | 951/? [13:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 951, loss 4.065581321716309\n",
      "Epoch 7: |          | 952/? [13:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 952, loss 3.9931864738464355\n",
      "Epoch 7: |          | 953/? [13:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 953, loss 4.535438537597656\n",
      "Epoch 7: |          | 954/? [13:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 954, loss 3.5454230308532715\n",
      "Epoch 7: |          | 955/? [13:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 955, loss 4.144797325134277\n",
      "Epoch 7: |          | 956/? [13:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 956, loss 3.664419651031494\n",
      "Epoch 7: |          | 957/? [13:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 957, loss 3.796877384185791\n",
      "Epoch 7: |          | 958/? [13:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 958, loss 3.8501765727996826\n",
      "Epoch 7: |          | 959/? [13:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 959, loss 3.2498786449432373\n",
      "Epoch 7: |          | 960/? [13:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 960, loss 3.93205189704895\n",
      "Epoch 7: |          | 961/? [13:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 961, loss 4.203811168670654\n",
      "Epoch 7: |          | 962/? [13:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 962, loss 3.7193541526794434\n",
      "Epoch 7: |          | 963/? [13:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 963, loss 3.5314292907714844\n",
      "Epoch 7: |          | 964/? [13:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 964, loss 3.9560420513153076\n",
      "Epoch 7: |          | 965/? [13:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 965, loss 3.4059154987335205\n",
      "Epoch 7: |          | 966/? [13:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 966, loss 3.372412919998169\n",
      "Epoch 7: |          | 967/? [13:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 967, loss 3.5929741859436035\n",
      "Epoch 7: |          | 968/? [13:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 968, loss 3.493508815765381\n",
      "Epoch 7: |          | 969/? [13:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 969, loss 3.422534942626953\n",
      "Epoch 7: |          | 970/? [13:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 970, loss 3.8421027660369873\n",
      "Epoch 7: |          | 971/? [13:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 971, loss 4.042150020599365\n",
      "Epoch 7: |          | 972/? [13:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 972, loss 3.490152359008789\n",
      "Epoch 7: |          | 973/? [13:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 973, loss 3.6727871894836426\n",
      "Epoch 7: |          | 974/? [13:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 974, loss 3.765908718109131\n",
      "Epoch 7: |          | 975/? [13:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 975, loss 3.7782750129699707\n",
      "Epoch 7: |          | 976/? [13:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 976, loss 3.8086133003234863\n",
      "Epoch 7: |          | 977/? [13:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 977, loss 4.3560075759887695\n",
      "Epoch 7: |          | 978/? [13:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 978, loss 3.855100631713867\n",
      "Epoch 7: |          | 979/? [13:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 979, loss 4.081511497497559\n",
      "Epoch 7: |          | 980/? [13:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 980, loss 3.2851510047912598\n",
      "Epoch 7: |          | 981/? [13:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 981, loss 3.141157865524292\n",
      "Epoch 7: |          | 982/? [14:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 982, loss 3.764075517654419\n",
      "Epoch 7: |          | 983/? [14:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 983, loss 4.189898490905762\n",
      "Epoch 7: |          | 984/? [14:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 984, loss 3.286961078643799\n",
      "Epoch 7: |          | 985/? [14:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 985, loss 3.5122406482696533\n",
      "Epoch 7: |          | 986/? [14:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 986, loss 3.5345242023468018\n",
      "Epoch 7: |          | 987/? [14:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 987, loss 3.0751214027404785\n",
      "Epoch 7: |          | 988/? [14:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 988, loss 4.085494041442871\n",
      "Epoch 7: |          | 989/? [14:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 989, loss 3.7011845111846924\n",
      "Epoch 7: |          | 990/? [14:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 990, loss 3.11326265335083\n",
      "Epoch 7: |          | 991/? [14:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 991, loss 3.8127968311309814\n",
      "Epoch 7: |          | 992/? [14:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 992, loss 4.455240249633789\n",
      "Epoch 7: |          | 993/? [14:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 993, loss 3.5667877197265625\n",
      "Epoch 7: |          | 994/? [14:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 994, loss 3.5591092109680176\n",
      "Epoch 7: |          | 995/? [14:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 995, loss 3.9879512786865234\n",
      "Epoch 7: |          | 996/? [14:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 996, loss 3.942263126373291\n",
      "Epoch 7: |          | 997/? [14:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 997, loss 3.6077377796173096\n",
      "Epoch 7: |          | 998/? [14:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 998, loss 3.8294601440429688\n",
      "Epoch 7: |          | 999/? [14:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 999, loss 3.7976202964782715\n",
      "Epoch 7: |          | 1000/? [14:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1000, loss 3.292908191680908\n",
      "Epoch 7: |          | 1001/? [14:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1001, loss 3.977999448776245\n",
      "Epoch 7: |          | 1002/? [14:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1002, loss 3.923196315765381\n",
      "Epoch 7: |          | 1003/? [14:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1003, loss 4.135364055633545\n",
      "Epoch 7: |          | 1004/? [14:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1004, loss 3.194838523864746\n",
      "Epoch 7: |          | 1005/? [14:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1005, loss 3.6847381591796875\n",
      "Epoch 7: |          | 1006/? [14:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1006, loss 3.998171329498291\n",
      "Epoch 7: |          | 1007/? [14:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1007, loss 3.5989246368408203\n",
      "Epoch 7: |          | 1008/? [14:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1008, loss 3.734487533569336\n",
      "Epoch 7: |          | 1009/? [14:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1009, loss 3.9802145957946777\n",
      "Epoch 7: |          | 1010/? [14:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1010, loss 3.1553702354431152\n",
      "Epoch 7: |          | 1011/? [14:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1011, loss 3.7074360847473145\n",
      "Epoch 7: |          | 1012/? [14:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1012, loss 3.5135111808776855\n",
      "Epoch 7: |          | 1013/? [14:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1013, loss 3.6489644050598145\n",
      "Epoch 7: |          | 1014/? [14:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1014, loss 4.117602348327637\n",
      "Epoch 7: |          | 1015/? [14:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1015, loss 3.746415615081787\n",
      "Epoch 7: |          | 1016/? [14:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1016, loss 3.5243499279022217\n",
      "Epoch 7: |          | 1017/? [14:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1017, loss 2.9684925079345703\n",
      "Epoch 7: |          | 1018/? [14:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1018, loss 3.629549741744995\n",
      "Epoch 7: |          | 1019/? [14:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1019, loss 3.7137882709503174\n",
      "Epoch 7: |          | 1020/? [14:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1020, loss 3.2935996055603027\n",
      "Epoch 7: |          | 1021/? [14:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1021, loss 3.5497307777404785\n",
      "Epoch 7: |          | 1022/? [14:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1022, loss 3.3537261486053467\n",
      "Epoch 7: |          | 1023/? [14:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1023, loss 3.1033682823181152\n",
      "Epoch 7: |          | 1024/? [14:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1024, loss 3.5483593940734863\n",
      "Epoch 7: |          | 1025/? [14:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1025, loss 3.457606554031372\n",
      "Epoch 7: |          | 1026/? [14:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1026, loss 2.605451822280884\n",
      "Epoch 7: |          | 1027/? [14:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1027, loss 3.740143299102783\n",
      "Epoch 7: |          | 1028/? [14:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1028, loss 3.619579315185547\n",
      "Epoch 7: |          | 1029/? [14:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1029, loss 3.4942073822021484\n",
      "Epoch 7: |          | 1030/? [14:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1030, loss 3.329899549484253\n",
      "Epoch 7: |          | 1031/? [14:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1031, loss 3.380549907684326\n",
      "Epoch 7: |          | 1032/? [14:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1032, loss 3.8356125354766846\n",
      "Epoch 7: |          | 1033/? [14:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1033, loss 4.054802894592285\n",
      "Epoch 7: |          | 1034/? [14:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1034, loss 3.4708778858184814\n",
      "Epoch 7: |          | 1035/? [14:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1035, loss 3.472777843475342\n",
      "Epoch 7: |          | 1036/? [14:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1036, loss 3.4407970905303955\n",
      "Epoch 7: |          | 1037/? [14:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1037, loss 4.015923023223877\n",
      "Epoch 7: |          | 1038/? [14:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1038, loss 4.175155162811279\n",
      "Epoch 7: |          | 1039/? [14:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1039, loss 4.507597923278809\n",
      "Epoch 7: |          | 1040/? [14:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1040, loss 3.791027069091797\n",
      "Epoch 7: |          | 1041/? [14:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1041, loss 4.088578224182129\n",
      "Epoch 7: |          | 1042/? [14:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1042, loss 3.6747188568115234\n",
      "Epoch 7: |          | 1043/? [14:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1043, loss 4.025972843170166\n",
      "Epoch 7: |          | 1044/? [14:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1044, loss 3.6056246757507324\n",
      "Epoch 7: |          | 1045/? [14:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1045, loss 3.195542812347412\n",
      "Epoch 7: |          | 1046/? [14:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1046, loss 3.07496976852417\n",
      "Epoch 7: |          | 1047/? [14:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1047, loss 4.114209175109863\n",
      "Epoch 7: |          | 1048/? [14:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1048, loss 3.6458213329315186\n",
      "Epoch 7: |          | 1049/? [14:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1049, loss 3.8557841777801514\n",
      "Epoch 7: |          | 1050/? [14:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1050, loss 3.426790952682495\n",
      "Epoch 7: |          | 1051/? [14:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1051, loss 3.3459689617156982\n",
      "Epoch 7: |          | 1052/? [14:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1052, loss 3.9721672534942627\n",
      "Epoch 7: |          | 1053/? [14:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1053, loss 4.144839286804199\n",
      "Epoch 7: |          | 1054/? [15:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1054, loss 3.5679562091827393\n",
      "Epoch 7: |          | 1055/? [15:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1055, loss 3.246447801589966\n",
      "Epoch 7: |          | 1056/? [15:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1056, loss 3.2460455894470215\n",
      "Epoch 7: |          | 1057/? [15:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1057, loss 3.8635971546173096\n",
      "Epoch 7: |          | 1058/? [15:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1058, loss 3.449082136154175\n",
      "Epoch 7: |          | 1059/? [15:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1059, loss 4.0405073165893555\n",
      "Epoch 7: |          | 1060/? [15:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1060, loss 3.889329433441162\n",
      "Epoch 7: |          | 1061/? [15:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1061, loss 2.6837475299835205\n",
      "Epoch 7: |          | 1062/? [15:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1062, loss 3.513129472732544\n",
      "Epoch 7: |          | 1063/? [15:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1063, loss 3.6279633045196533\n",
      "Epoch 7: |          | 1064/? [15:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1064, loss 3.80902099609375\n",
      "Epoch 7: |          | 1065/? [15:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1065, loss 2.4477791786193848\n",
      "Epoch 7: |          | 1066/? [15:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1066, loss 3.7736659049987793\n",
      "Epoch 7: |          | 1067/? [15:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1067, loss 3.2954068183898926\n",
      "Epoch 7: |          | 1068/? [15:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1068, loss 3.4298973083496094\n",
      "Epoch 7: |          | 1069/? [15:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1069, loss 3.7954559326171875\n",
      "Epoch 7: |          | 1070/? [15:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1070, loss 3.574714183807373\n",
      "Epoch 7: |          | 1071/? [15:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1071, loss 3.941028594970703\n",
      "Epoch 7: |          | 1072/? [15:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1072, loss 3.984813690185547\n",
      "Epoch 7: |          | 1073/? [15:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1073, loss 4.124045372009277\n",
      "Epoch 7: |          | 1074/? [15:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1074, loss 3.48644757270813\n",
      "Epoch 7: |          | 1075/? [15:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1075, loss 3.3439013957977295\n",
      "Epoch 7: |          | 1076/? [15:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1076, loss 3.8921256065368652\n",
      "Epoch 7: |          | 1077/? [15:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1077, loss 3.4335741996765137\n",
      "Epoch 7: |          | 1078/? [15:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1078, loss 3.6963462829589844\n",
      "Epoch 7: |          | 1079/? [15:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1079, loss 4.060084342956543\n",
      "Epoch 7: |          | 1080/? [15:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1080, loss 3.6150097846984863\n",
      "Epoch 7: |          | 1081/? [15:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1081, loss 3.9067351818084717\n",
      "Epoch 7: |          | 1082/? [15:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1082, loss 3.53934907913208\n",
      "Epoch 7: |          | 1083/? [15:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1083, loss 3.0535147190093994\n",
      "Epoch 7: |          | 1084/? [15:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1084, loss 2.8970577716827393\n",
      "Epoch 7: |          | 1085/? [15:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1085, loss 3.5696754455566406\n",
      "Epoch 7: |          | 1086/? [15:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1086, loss 3.8353443145751953\n",
      "Epoch 7: |          | 1087/? [15:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1087, loss 4.303934097290039\n",
      "Epoch 7: |          | 1088/? [15:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1088, loss 3.8915321826934814\n",
      "Epoch 7: |          | 1089/? [15:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1089, loss 3.82084584236145\n",
      "Epoch 7: |          | 1090/? [15:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1090, loss 3.6334052085876465\n",
      "Epoch 7: |          | 1091/? [15:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1091, loss 3.4586963653564453\n",
      "Epoch 7: |          | 1092/? [15:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1092, loss 3.76289701461792\n",
      "Epoch 7: |          | 1093/? [15:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1093, loss 3.2560813426971436\n",
      "Epoch 7: |          | 1094/? [15:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1094, loss 3.7620391845703125\n",
      "Epoch 7: |          | 1095/? [15:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1095, loss 3.7385475635528564\n",
      "Epoch 7: |          | 1096/? [15:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1096, loss 4.037421226501465\n",
      "Epoch 7: |          | 1097/? [15:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1097, loss 3.6213138103485107\n",
      "Epoch 7: |          | 1098/? [15:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1098, loss 2.959749698638916\n",
      "Epoch 7: |          | 1099/? [15:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1099, loss 3.5700371265411377\n",
      "Epoch 7: |          | 1100/? [15:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1100, loss 3.7569210529327393\n",
      "Epoch 7: |          | 1101/? [15:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1101, loss 3.4198546409606934\n",
      "Epoch 7: |          | 1102/? [15:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1102, loss 4.163360595703125\n",
      "Epoch 7: |          | 1103/? [15:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1103, loss 4.507624626159668\n",
      "Epoch 7: |          | 1104/? [15:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1104, loss 3.9533848762512207\n",
      "Epoch 7: |          | 1105/? [15:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1105, loss 4.072744369506836\n",
      "Epoch 7: |          | 1106/? [15:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1106, loss 3.5527241230010986\n",
      "Epoch 7: |          | 1107/? [15:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1107, loss 3.7226500511169434\n",
      "Epoch 7: |          | 1108/? [15:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1108, loss 3.712212085723877\n",
      "Epoch 7: |          | 1109/? [15:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1109, loss 3.343698024749756\n",
      "Epoch 7: |          | 1110/? [15:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1110, loss 4.203440189361572\n",
      "Epoch 7: |          | 1111/? [15:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1111, loss 3.902200698852539\n",
      "Epoch 7: |          | 1112/? [15:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1112, loss 3.744029998779297\n",
      "Epoch 7: |          | 1113/? [15:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1113, loss 3.582609176635742\n",
      "Epoch 7: |          | 1114/? [15:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1114, loss 3.051013708114624\n",
      "Epoch 7: |          | 1115/? [15:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1115, loss 2.8780014514923096\n",
      "Epoch 7: |          | 1116/? [15:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1116, loss 3.187796115875244\n",
      "Epoch 7: |          | 1117/? [15:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1117, loss 3.384672164916992\n",
      "Epoch 7: |          | 1118/? [15:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1118, loss 3.5114524364471436\n",
      "Epoch 7: |          | 1119/? [15:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1119, loss 4.104384899139404\n",
      "Epoch 7: |          | 1120/? [15:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1120, loss 3.656890392303467\n",
      "Epoch 7: |          | 1121/? [15:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1121, loss 3.8862247467041016\n",
      "Epoch 7: |          | 1122/? [15:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1122, loss 3.404860734939575\n",
      "Epoch 7: |          | 1123/? [15:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1123, loss 3.681763172149658\n",
      "Epoch 7: |          | 1124/? [16:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1124, loss 4.017622947692871\n",
      "Epoch 7: |          | 1125/? [16:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1125, loss 3.380161762237549\n",
      "Epoch 7: |          | 1126/? [16:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1126, loss 3.2951903343200684\n",
      "Epoch 7: |          | 1127/? [16:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1127, loss 3.566946506500244\n",
      "Epoch 7: |          | 1128/? [16:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1128, loss 3.665314197540283\n",
      "Epoch 7: |          | 1129/? [16:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1129, loss 3.7602076530456543\n",
      "Epoch 7: |          | 1130/? [16:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1130, loss 3.934952974319458\n",
      "Epoch 7: |          | 1131/? [16:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1131, loss 4.010311126708984\n",
      "Epoch 7: |          | 1132/? [16:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1132, loss 2.8526499271392822\n",
      "Epoch 7: |          | 1133/? [16:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1133, loss 3.6189937591552734\n",
      "Epoch 7: |          | 1134/? [16:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1134, loss 3.430528163909912\n",
      "Epoch 7: |          | 1135/? [16:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1135, loss 4.027796268463135\n",
      "Epoch 7: |          | 1136/? [16:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1136, loss 3.6582469940185547\n",
      "Epoch 7: |          | 1137/? [16:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1137, loss 3.7430806159973145\n",
      "Epoch 7: |          | 1138/? [16:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1138, loss 4.130264759063721\n",
      "Epoch 7: |          | 1139/? [16:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1139, loss 3.7817070484161377\n",
      "Epoch 7: |          | 1140/? [16:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1140, loss 3.482381820678711\n",
      "Epoch 7: |          | 1141/? [16:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1141, loss 3.988475799560547\n",
      "Epoch 7: |          | 1142/? [16:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1142, loss 4.1287078857421875\n",
      "Epoch 7: |          | 1143/? [16:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1143, loss 4.116223335266113\n",
      "Epoch 7: |          | 1144/? [16:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1144, loss 3.5153987407684326\n",
      "Epoch 7: |          | 1145/? [16:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1145, loss 3.6905789375305176\n",
      "Epoch 7: |          | 1146/? [16:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1146, loss 3.30944561958313\n",
      "Epoch 7: |          | 1147/? [16:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1147, loss 3.2800421714782715\n",
      "Epoch 7: |          | 1148/? [16:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1148, loss 3.4775383472442627\n",
      "Epoch 7: |          | 1149/? [16:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1149, loss 4.350293159484863\n",
      "Epoch 7: |          | 1150/? [16:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1150, loss 3.8558478355407715\n",
      "Epoch 7: |          | 1151/? [16:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1151, loss 4.092602729797363\n",
      "Epoch 7: |          | 1152/? [16:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1152, loss 3.3940441608428955\n",
      "Epoch 7: |          | 1153/? [16:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1153, loss 3.7652480602264404\n",
      "Epoch 7: |          | 1154/? [16:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1154, loss 3.4434101581573486\n",
      "Epoch 7: |          | 1155/? [16:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1155, loss 3.665172576904297\n",
      "Epoch 7: |          | 1156/? [16:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1156, loss 3.6317946910858154\n",
      "Epoch 7: |          | 1157/? [16:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1157, loss 3.926682710647583\n",
      "Epoch 7: |          | 1158/? [16:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1158, loss 4.080246925354004\n",
      "Epoch 7: |          | 1159/? [16:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1159, loss 2.9411532878875732\n",
      "Epoch 7: |          | 1160/? [16:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1160, loss 4.062981605529785\n",
      "Epoch 7: |          | 1161/? [16:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1161, loss 3.933053970336914\n",
      "Epoch 7: |          | 1162/? [16:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1162, loss 3.8485989570617676\n",
      "Epoch 7: |          | 1163/? [16:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1163, loss 4.435877799987793\n",
      "Epoch 7: |          | 1164/? [16:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1164, loss 4.198837757110596\n",
      "Epoch 7: |          | 1165/? [16:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1165, loss 3.4052834510803223\n",
      "Epoch 7: |          | 1166/? [16:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1166, loss 3.881704330444336\n",
      "Epoch 7: |          | 1167/? [16:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1167, loss 3.913087844848633\n",
      "Epoch 7: |          | 1168/? [16:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1168, loss 4.336483478546143\n",
      "Epoch 7: |          | 1169/? [16:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1169, loss 3.4961235523223877\n",
      "Epoch 7: |          | 1170/? [16:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1170, loss 3.999767303466797\n",
      "Epoch 7: |          | 1171/? [16:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1171, loss 3.390613079071045\n",
      "Epoch 7: |          | 1172/? [16:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1172, loss 3.332202196121216\n",
      "Epoch 7: |          | 1173/? [16:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1173, loss 3.8766188621520996\n",
      "Epoch 7: |          | 1174/? [16:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1174, loss 3.3708243370056152\n",
      "Epoch 7: |          | 1175/? [16:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1175, loss 3.9209818840026855\n",
      "Epoch 7: |          | 1176/? [16:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1176, loss 3.988372325897217\n",
      "Epoch 7: |          | 1177/? [16:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1177, loss 4.138876914978027\n",
      "Epoch 7: |          | 1178/? [16:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1178, loss 3.565139055252075\n",
      "Epoch 7: |          | 1179/? [16:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1179, loss 4.103874683380127\n",
      "Epoch 7: |          | 1180/? [16:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1180, loss 3.952164888381958\n",
      "Epoch 7: |          | 1181/? [16:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1181, loss 3.8470847606658936\n",
      "Epoch 7: |          | 1182/? [16:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1182, loss 3.6841392517089844\n",
      "Epoch 7: |          | 1183/? [16:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1183, loss 3.390469789505005\n",
      "Epoch 7: |          | 1184/? [16:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1184, loss 3.8202223777770996\n",
      "Epoch 7: |          | 1185/? [16:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1185, loss 3.5385665893554688\n",
      "Epoch 7: |          | 1186/? [16:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1186, loss 3.760082960128784\n",
      "Epoch 7: |          | 1187/? [16:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1187, loss 3.6426682472229004\n",
      "Epoch 7: |          | 1188/? [16:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1188, loss 4.032519340515137\n",
      "Epoch 7: |          | 1189/? [16:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1189, loss 4.089588642120361\n",
      "Epoch 7: |          | 1190/? [16:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1190, loss 3.708219528198242\n",
      "Epoch 7: |          | 1191/? [16:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1191, loss 3.7220489978790283\n",
      "Epoch 7: |          | 1192/? [16:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1192, loss 3.9882545471191406\n",
      "Epoch 7: |          | 1193/? [16:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1193, loss 3.488085985183716\n",
      "Epoch 7: |          | 1194/? [16:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1194, loss 3.2430901527404785\n",
      "Epoch 7: |          | 1195/? [17:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1195, loss 3.7033627033233643\n",
      "Epoch 7: |          | 1196/? [17:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1196, loss 3.9445018768310547\n",
      "Epoch 7: |          | 1197/? [17:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1197, loss 3.679870128631592\n",
      "Epoch 7: |          | 1198/? [17:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1198, loss 3.7937209606170654\n",
      "Epoch 7: |          | 1199/? [17:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1199, loss 4.079736709594727\n",
      "Epoch 7: |          | 1200/? [17:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1200, loss 3.271244525909424\n",
      "Epoch 7: |          | 1201/? [17:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1201, loss 3.885424852371216\n",
      "Epoch 7: |          | 1202/? [17:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1202, loss 3.6023952960968018\n",
      "Epoch 7: |          | 1203/? [17:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1203, loss 3.5664191246032715\n",
      "Epoch 7: |          | 1204/? [17:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1204, loss 3.151944160461426\n",
      "Epoch 7: |          | 1205/? [17:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1205, loss 3.7260189056396484\n",
      "Epoch 7: |          | 1206/? [17:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1206, loss 3.7142186164855957\n",
      "Epoch 7: |          | 1207/? [17:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1207, loss 3.9466376304626465\n",
      "Epoch 7: |          | 1208/? [17:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1208, loss 4.150885581970215\n",
      "Epoch 7: |          | 1209/? [17:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1209, loss 3.7303531169891357\n",
      "Epoch 7: |          | 1210/? [17:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1210, loss 4.023044586181641\n",
      "Epoch 7: |          | 1211/? [17:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1211, loss 4.002823352813721\n",
      "Epoch 7: |          | 1212/? [17:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1212, loss 3.7953789234161377\n",
      "Epoch 7: |          | 1213/? [17:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1213, loss 3.5427112579345703\n",
      "Epoch 7: |          | 1214/? [17:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1214, loss 4.066685199737549\n",
      "Epoch 7: |          | 1215/? [17:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1215, loss 3.603933334350586\n",
      "Epoch 7: |          | 1216/? [17:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1216, loss 3.694204330444336\n",
      "Epoch 7: |          | 1217/? [17:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1217, loss 3.745854616165161\n",
      "Epoch 7: |          | 1218/? [17:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1218, loss 3.886796236038208\n",
      "Epoch 7: |          | 1219/? [17:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1219, loss 3.54559326171875\n",
      "Epoch 7: |          | 1220/? [17:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1220, loss 4.204294681549072\n",
      "Epoch 7: |          | 1221/? [17:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1221, loss 3.7594470977783203\n",
      "Epoch 7: |          | 1222/? [17:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1222, loss 2.9485855102539062\n",
      "Epoch 7: |          | 1223/? [17:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1223, loss 3.148806095123291\n",
      "Epoch 7: |          | 1224/? [17:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1224, loss 3.4804816246032715\n",
      "Epoch 7: |          | 1225/? [17:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1225, loss 4.117249488830566\n",
      "Epoch 7: |          | 1226/? [17:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1226, loss 4.070866107940674\n",
      "Epoch 7: |          | 1227/? [17:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1227, loss 3.7483744621276855\n",
      "Epoch 7: |          | 1228/? [17:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1228, loss 3.584080219268799\n",
      "Epoch 7: |          | 1229/? [17:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1229, loss 3.241225481033325\n",
      "Epoch 7: |          | 1230/? [17:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1230, loss 3.870506763458252\n",
      "Epoch 7: |          | 1231/? [17:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1231, loss 3.8875865936279297\n",
      "Epoch 7: |          | 1232/? [17:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1232, loss 4.03684139251709\n",
      "Epoch 7: |          | 1233/? [17:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1233, loss 3.8844528198242188\n",
      "Epoch 7: |          | 1234/? [17:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1234, loss 2.8968505859375\n",
      "Epoch 7: |          | 1235/? [17:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1235, loss 3.9733684062957764\n",
      "Epoch 7: |          | 1236/? [17:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1236, loss 3.3788132667541504\n",
      "Epoch 7: |          | 1237/? [17:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1237, loss 3.655244827270508\n",
      "Epoch 7: |          | 1238/? [17:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1238, loss 3.72607684135437\n",
      "Epoch 7: |          | 1239/? [17:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1239, loss 3.56809663772583\n",
      "Epoch 7: |          | 1240/? [17:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1240, loss 4.132428169250488\n",
      "Epoch 7: |          | 1241/? [17:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1241, loss 3.7368321418762207\n",
      "Epoch 7: |          | 1242/? [17:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1242, loss 3.628607988357544\n",
      "Epoch 7: |          | 1243/? [17:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1243, loss 3.4620070457458496\n",
      "Epoch 7: |          | 1244/? [17:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1244, loss 3.6146388053894043\n",
      "Epoch 7: |          | 1245/? [17:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1245, loss 3.206005096435547\n",
      "Epoch 7: |          | 1246/? [17:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1246, loss 3.8681228160858154\n",
      "Epoch 7: |          | 1247/? [17:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1247, loss 3.8698391914367676\n",
      "Epoch 7: |          | 1248/? [17:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1248, loss 3.449826717376709\n",
      "Epoch 7: |          | 1249/? [17:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1249, loss 3.607591152191162\n",
      "Epoch 7: |          | 1250/? [17:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1250, loss 3.77451753616333\n",
      "Epoch 7: |          | 1251/? [17:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1251, loss 3.5225319862365723\n",
      "Epoch 7: |          | 1252/? [17:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1252, loss 4.282547950744629\n",
      "Epoch 7: |          | 1253/? [17:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1253, loss 3.62160062789917\n",
      "Epoch 7: |          | 1254/? [17:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1254, loss 2.987809896469116\n",
      "Epoch 7: |          | 1255/? [17:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1255, loss 4.292969226837158\n",
      "Epoch 7: |          | 1256/? [17:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1256, loss 3.368567705154419\n",
      "Epoch 7: |          | 1257/? [17:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1257, loss 3.38425874710083\n",
      "Epoch 7: |          | 1258/? [17:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1258, loss 4.047510623931885\n",
      "Epoch 7: |          | 1259/? [17:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1259, loss 3.736984968185425\n",
      "Epoch 7: |          | 1260/? [17:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1260, loss 4.181429386138916\n",
      "Epoch 7: |          | 1261/? [17:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1261, loss 3.5474212169647217\n",
      "Epoch 7: |          | 1262/? [17:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1262, loss 3.5334649085998535\n",
      "Epoch 7: |          | 1263/? [17:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1263, loss 3.8435935974121094\n",
      "Epoch 7: |          | 1264/? [18:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1264, loss 4.086886405944824\n",
      "Epoch 7: |          | 1265/? [18:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1265, loss 3.962357759475708\n",
      "Epoch 7: |          | 1266/? [18:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1266, loss 3.6637394428253174\n",
      "Epoch 7: |          | 1267/? [18:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1267, loss 3.780111789703369\n",
      "Epoch 7: |          | 1268/? [18:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1268, loss 3.632676601409912\n",
      "Epoch 7: |          | 1269/? [18:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1269, loss 3.173635721206665\n",
      "Epoch 7: |          | 1270/? [18:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1270, loss 3.514514446258545\n",
      "Epoch 7: |          | 1271/? [18:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1271, loss 3.692791700363159\n",
      "Epoch 7: |          | 1272/? [18:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1272, loss 3.2867980003356934\n",
      "Epoch 7: |          | 1273/? [18:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1273, loss 3.950329542160034\n",
      "Epoch 7: |          | 1274/? [18:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1274, loss 2.889665126800537\n",
      "Epoch 7: |          | 1275/? [18:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1275, loss 3.4100823402404785\n",
      "Epoch 7: |          | 1276/? [18:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1276, loss 3.689131498336792\n",
      "Epoch 7: |          | 1277/? [18:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1277, loss 3.4237091541290283\n",
      "Epoch 7: |          | 1278/? [18:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1278, loss 3.0855793952941895\n",
      "Epoch 7: |          | 1279/? [18:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1279, loss 3.8016648292541504\n",
      "Epoch 7: |          | 1280/? [18:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1280, loss 3.0742931365966797\n",
      "Epoch 7: |          | 1281/? [18:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1281, loss 3.5557007789611816\n",
      "Epoch 7: |          | 1282/? [18:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1282, loss 3.266028881072998\n",
      "Epoch 7: |          | 1283/? [18:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1283, loss 3.943328380584717\n",
      "Epoch 7: |          | 1284/? [18:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1284, loss 3.1308817863464355\n",
      "Epoch 7: |          | 1285/? [18:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1285, loss 4.107023239135742\n",
      "Epoch 7: |          | 1286/? [18:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1286, loss 2.645329475402832\n",
      "Epoch 7: |          | 1287/? [18:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1287, loss 4.049803733825684\n",
      "Epoch 7: |          | 1288/? [18:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1288, loss 3.84386944770813\n",
      "Epoch 7: |          | 1289/? [18:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1289, loss 3.055267810821533\n",
      "Epoch 7: |          | 1290/? [18:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1290, loss 3.736854076385498\n",
      "Epoch 7: |          | 1291/? [18:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1291, loss 4.614560127258301\n",
      "Epoch 7: |          | 1292/? [18:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1292, loss 3.8887245655059814\n",
      "Epoch 7: |          | 1293/? [18:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1293, loss 3.471418857574463\n",
      "Epoch 7: |          | 1294/? [18:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1294, loss 3.6850380897521973\n",
      "Epoch 7: |          | 1295/? [18:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1295, loss 3.793825149536133\n",
      "Epoch 7: |          | 1296/? [18:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1296, loss 3.050811529159546\n",
      "Epoch 7: |          | 1297/? [18:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1297, loss 3.934603452682495\n",
      "Epoch 7: |          | 1298/? [18:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1298, loss 3.6653456687927246\n",
      "Epoch 7: |          | 1299/? [18:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1299, loss 2.8694612979888916\n",
      "Epoch 7: |          | 1300/? [18:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1300, loss 3.6458725929260254\n",
      "Epoch 7: |          | 1301/? [18:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1301, loss 3.419463634490967\n",
      "Epoch 7: |          | 1302/? [18:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1302, loss 3.546848773956299\n",
      "Epoch 7: |          | 1303/? [18:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1303, loss 3.4817147254943848\n",
      "Epoch 7: |          | 1304/? [18:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1304, loss 4.213281631469727\n",
      "Epoch 7: |          | 1305/? [18:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1305, loss 3.059788465499878\n",
      "Epoch 7: |          | 1306/? [18:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1306, loss 3.652916431427002\n",
      "Epoch 7: |          | 1307/? [18:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1307, loss 3.302212953567505\n",
      "Epoch 7: |          | 1308/? [18:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1308, loss 3.2617805004119873\n",
      "Epoch 7: |          | 1309/? [18:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1309, loss 3.241425037384033\n",
      "Epoch 7: |          | 1310/? [18:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1310, loss 3.8012192249298096\n",
      "Epoch 7: |          | 1311/? [18:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1311, loss 3.296708583831787\n",
      "Epoch 7: |          | 1312/? [18:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1312, loss 3.0357563495635986\n",
      "Epoch 7: |          | 1313/? [18:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1313, loss 4.156721591949463\n",
      "Epoch 7: |          | 1314/? [18:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1314, loss 3.4140853881835938\n",
      "Epoch 7: |          | 1315/? [18:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1315, loss 4.146076202392578\n",
      "Epoch 7: |          | 1316/? [18:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1316, loss 3.908416748046875\n",
      "Epoch 7: |          | 1317/? [18:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1317, loss 3.5714805126190186\n",
      "Epoch 7: |          | 1318/? [18:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1318, loss 3.704680919647217\n",
      "Epoch 7: |          | 1319/? [18:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1319, loss 3.8934130668640137\n",
      "Epoch 7: |          | 1320/? [18:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1320, loss 3.4874043464660645\n",
      "Epoch 7: |          | 1321/? [18:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1321, loss 3.9561450481414795\n",
      "Epoch 7: |          | 1322/? [18:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1322, loss 3.7693381309509277\n",
      "Epoch 7: |          | 1323/? [18:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1323, loss 3.3286235332489014\n",
      "Epoch 7: |          | 1324/? [18:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1324, loss 4.196866989135742\n",
      "Epoch 7: |          | 1325/? [18:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1325, loss 4.336699962615967\n",
      "Epoch 7: |          | 1326/? [18:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1326, loss 3.8087258338928223\n",
      "Epoch 7: |          | 1327/? [18:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1327, loss 3.705566883087158\n",
      "Epoch 7: |          | 1328/? [19:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1328, loss 3.3398823738098145\n",
      "Epoch 7: |          | 1329/? [19:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1329, loss 3.9074416160583496\n",
      "Epoch 7: |          | 1330/? [19:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1330, loss 3.6780662536621094\n",
      "Epoch 7: |          | 1331/? [19:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1331, loss 3.8019652366638184\n",
      "Epoch 7: |          | 1332/? [19:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1332, loss 3.500808000564575\n",
      "Epoch 7: |          | 1333/? [19:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1333, loss 3.471738815307617\n",
      "Epoch 7: |          | 1334/? [19:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1334, loss 3.5956058502197266\n",
      "Epoch 7: |          | 1335/? [19:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1335, loss 3.5711617469787598\n",
      "Epoch 7: |          | 1336/? [19:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1336, loss 3.1454858779907227\n",
      "Epoch 7: |          | 1337/? [19:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1337, loss 3.746981382369995\n",
      "Epoch 7: |          | 1338/? [19:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1338, loss 3.062889575958252\n",
      "Epoch 7: |          | 1339/? [19:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1339, loss 3.661435604095459\n",
      "Epoch 7: |          | 1340/? [19:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1340, loss 3.1218149662017822\n",
      "Epoch 7: |          | 1341/? [19:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1341, loss 3.832087278366089\n",
      "Epoch 7: |          | 1342/? [19:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1342, loss 4.0510687828063965\n",
      "Epoch 7: |          | 1343/? [19:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1343, loss 3.5866870880126953\n",
      "Epoch 7: |          | 1344/? [19:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1344, loss 3.7064342498779297\n",
      "Epoch 7: |          | 1345/? [19:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1345, loss 3.841550827026367\n",
      "Epoch 7: |          | 1346/? [19:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1346, loss 4.8094258308410645\n",
      "Epoch 7: |          | 1347/? [19:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1347, loss 3.83085560798645\n",
      "Epoch 7: |          | 1348/? [19:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1348, loss 3.8696296215057373\n",
      "Epoch 7: |          | 1349/? [19:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1349, loss 3.866359233856201\n",
      "Epoch 7: |          | 1350/? [19:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1350, loss 3.837308168411255\n",
      "Epoch 7: |          | 1351/? [19:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1351, loss 3.847655773162842\n",
      "Epoch 7: |          | 1352/? [19:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1352, loss 3.145232677459717\n",
      "Epoch 7: |          | 1353/? [19:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1353, loss 3.4912381172180176\n",
      "Epoch 7: |          | 1354/? [19:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1354, loss 3.863311290740967\n",
      "Epoch 7: |          | 1355/? [19:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1355, loss 3.978947401046753\n",
      "Epoch 7: |          | 1356/? [19:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1356, loss 3.7591354846954346\n",
      "Epoch 7: |          | 1357/? [19:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1357, loss 3.56181001663208\n",
      "Epoch 7: |          | 1358/? [19:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1358, loss 3.6882824897766113\n",
      "Epoch 7: |          | 1359/? [19:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1359, loss 3.5802626609802246\n",
      "Epoch 7: |          | 1360/? [19:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1360, loss 3.7379746437072754\n",
      "Epoch 7: |          | 1361/? [19:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1361, loss 3.7078068256378174\n",
      "Epoch 7: |          | 1362/? [19:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1362, loss 3.574277400970459\n",
      "Epoch 7: |          | 1363/? [19:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1363, loss 3.02178955078125\n",
      "Epoch 7: |          | 1364/? [19:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1364, loss 3.5114028453826904\n",
      "Epoch 7: |          | 1365/? [19:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1365, loss 3.228853702545166\n",
      "Epoch 7: |          | 1366/? [19:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1366, loss 3.918734312057495\n",
      "Epoch 7: |          | 1367/? [19:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1367, loss 3.2457098960876465\n",
      "Epoch 7: |          | 1368/? [19:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1368, loss 3.0180397033691406\n",
      "Epoch 7: |          | 1369/? [19:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1369, loss 3.713135242462158\n",
      "Epoch 7: |          | 1370/? [19:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1370, loss 3.256481647491455\n",
      "Epoch 7: |          | 1371/? [19:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1371, loss 4.166926383972168\n",
      "Epoch 7: |          | 1372/? [19:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1372, loss 3.643667221069336\n",
      "Epoch 7: |          | 1373/? [19:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1373, loss 3.979710340499878\n",
      "Epoch 7: |          | 1374/? [19:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1374, loss 3.1515791416168213\n",
      "Epoch 7: |          | 1375/? [19:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1375, loss 3.7542529106140137\n",
      "Epoch 7: |          | 1376/? [19:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1376, loss 3.6122164726257324\n",
      "Epoch 7: |          | 1377/? [19:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1377, loss 3.6324639320373535\n",
      "Epoch 7: |          | 1378/? [19:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1378, loss 3.5945181846618652\n",
      "Epoch 7: |          | 1379/? [19:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1379, loss 3.5736701488494873\n",
      "Epoch 7: |          | 1380/? [19:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1380, loss 3.7415575981140137\n",
      "Epoch 7: |          | 1381/? [19:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1381, loss 3.8623099327087402\n",
      "Epoch 7: |          | 1382/? [19:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1382, loss 3.416555881500244\n",
      "Epoch 7: |          | 1383/? [19:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1383, loss 3.634622097015381\n",
      "Epoch 7: |          | 1384/? [19:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1384, loss 3.6257214546203613\n",
      "Epoch 7: |          | 1385/? [19:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1385, loss 3.5939278602600098\n",
      "Epoch 7: |          | 1386/? [19:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1386, loss 3.736300230026245\n",
      "Epoch 7: |          | 1387/? [19:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1387, loss 3.652243137359619\n",
      "Epoch 7: |          | 1388/? [19:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1388, loss 3.2020912170410156\n",
      "Epoch 7: |          | 1389/? [19:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1389, loss 3.858553409576416\n",
      "Epoch 7: |          | 1390/? [19:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1390, loss 4.136229515075684\n",
      "Epoch 7: |          | 1391/? [19:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1391, loss 3.7597644329071045\n",
      "Epoch 7: |          | 1392/? [19:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1392, loss 3.28035044670105\n",
      "Epoch 7: |          | 1393/? [19:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1393, loss 3.438122272491455\n",
      "Epoch 7: |          | 1394/? [19:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1394, loss 3.041975975036621\n",
      "Epoch 7: |          | 1395/? [19:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1395, loss 3.755967617034912\n",
      "Epoch 7: |          | 1396/? [19:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1396, loss 3.5990748405456543\n",
      "Epoch 7: |          | 1397/? [19:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1397, loss 2.9309961795806885\n",
      "Epoch 7: |          | 1398/? [19:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1398, loss 4.110023498535156\n",
      "Epoch 7: |          | 1399/? [20:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1399, loss 4.0932393074035645\n",
      "Epoch 7: |          | 1400/? [20:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1400, loss 3.2711174488067627\n",
      "Epoch 7: |          | 1401/? [20:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1401, loss 3.9251644611358643\n",
      "Epoch 7: |          | 1402/? [20:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1402, loss 3.6210131645202637\n",
      "Epoch 7: |          | 1403/? [20:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1403, loss 3.837484836578369\n",
      "Epoch 7: |          | 1404/? [20:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1404, loss 3.7752585411071777\n",
      "Epoch 7: |          | 1405/? [20:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1405, loss 4.104574680328369\n",
      "Epoch 7: |          | 1406/? [20:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1406, loss 4.026749610900879\n",
      "Epoch 7: |          | 1407/? [20:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1407, loss 4.072043418884277\n",
      "Epoch 7: |          | 1408/? [20:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1408, loss 3.3436508178710938\n",
      "Epoch 7: |          | 1409/? [20:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1409, loss 3.3959670066833496\n",
      "Epoch 7: |          | 1410/? [20:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1410, loss 3.449868679046631\n",
      "Epoch 7: |          | 1411/? [20:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1411, loss 3.7475745677948\n",
      "Epoch 7: |          | 1412/? [20:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1412, loss 3.469665050506592\n",
      "Epoch 7: |          | 1413/? [20:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1413, loss 3.2941670417785645\n",
      "Epoch 7: |          | 1414/? [20:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1414, loss 3.468723773956299\n",
      "Epoch 7: |          | 1415/? [20:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1415, loss 3.6955208778381348\n",
      "Epoch 7: |          | 1416/? [20:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1416, loss 4.048475742340088\n",
      "Epoch 7: |          | 1417/? [20:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1417, loss 3.7530150413513184\n",
      "Epoch 7: |          | 1418/? [20:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1418, loss 3.8488974571228027\n",
      "Epoch 7: |          | 1419/? [20:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1419, loss 3.6011955738067627\n",
      "Epoch 7: |          | 1420/? [20:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1420, loss 3.4644699096679688\n",
      "Epoch 7: |          | 1421/? [20:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1421, loss 3.2725932598114014\n",
      "Epoch 7: |          | 1422/? [20:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1422, loss 3.892094135284424\n",
      "Epoch 7: |          | 1423/? [20:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1423, loss 3.9302399158477783\n",
      "Epoch 7: |          | 1424/? [20:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1424, loss 3.6034443378448486\n",
      "Epoch 7: |          | 1425/? [20:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1425, loss 3.805757999420166\n",
      "Epoch 7: |          | 1426/? [20:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1426, loss 3.372345447540283\n",
      "Epoch 7: |          | 1427/? [20:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1427, loss 3.982243776321411\n",
      "Epoch 7: |          | 1428/? [20:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1428, loss 3.9262547492980957\n",
      "Epoch 7: |          | 1429/? [20:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1429, loss 3.8436203002929688\n",
      "Epoch 7: |          | 1430/? [20:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1430, loss 3.8960251808166504\n",
      "Epoch 7: |          | 1431/? [20:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1431, loss 3.67376971244812\n",
      "Epoch 7: |          | 1432/? [20:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1432, loss 3.7488715648651123\n",
      "Epoch 7: |          | 1433/? [20:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1433, loss 3.6597092151641846\n",
      "Epoch 7: |          | 1434/? [20:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1434, loss 3.7902629375457764\n",
      "Epoch 7: |          | 1435/? [20:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1435, loss 3.39604115486145\n",
      "Epoch 7: |          | 1436/? [20:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1436, loss 3.7562828063964844\n",
      "Epoch 7: |          | 1437/? [20:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1437, loss 3.010493040084839\n",
      "Epoch 7: |          | 1438/? [20:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1438, loss 4.618056297302246\n",
      "Epoch 7: |          | 1439/? [20:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1439, loss 3.6935524940490723\n",
      "Epoch 7: |          | 1440/? [20:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1440, loss 3.9054253101348877\n",
      "Epoch 7: |          | 1441/? [20:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1441, loss 4.182077884674072\n",
      "Epoch 7: |          | 1442/? [20:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1442, loss 4.19370698928833\n",
      "Epoch 7: |          | 1443/? [20:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1443, loss 3.3756966590881348\n",
      "Epoch 7: |          | 1444/? [20:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1444, loss 3.4220848083496094\n",
      "Epoch 7: |          | 1445/? [20:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1445, loss 3.916459560394287\n",
      "Epoch 7: |          | 1446/? [20:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1446, loss 3.4570255279541016\n",
      "Epoch 7: |          | 1447/? [20:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1447, loss 3.619692325592041\n",
      "Epoch 7: |          | 1448/? [20:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1448, loss 3.4668548107147217\n",
      "Epoch 7: |          | 1449/? [20:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1449, loss 3.687431812286377\n",
      "Epoch 7: |          | 1450/? [20:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1450, loss 3.72090220451355\n",
      "Epoch 7: |          | 1451/? [20:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1451, loss 4.055410385131836\n",
      "Epoch 7: |          | 1452/? [20:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1452, loss 3.735471248626709\n",
      "Epoch 7: |          | 1453/? [20:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1453, loss 3.1376144886016846\n",
      "Epoch 7: |          | 1454/? [20:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1454, loss 3.702991008758545\n",
      "Epoch 7: |          | 1455/? [20:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1455, loss 3.787905216217041\n",
      "Epoch 7: |          | 1456/? [20:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1456, loss 3.3701024055480957\n",
      "Epoch 7: |          | 1457/? [20:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1457, loss 3.510408878326416\n",
      "Epoch 7: |          | 1458/? [20:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1458, loss 3.7350025177001953\n",
      "Epoch 7: |          | 1459/? [20:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1459, loss 3.858299970626831\n",
      "Epoch 7: |          | 1460/? [20:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1460, loss 3.8179829120635986\n",
      "Epoch 7: |          | 1461/? [20:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1461, loss 3.71474027633667\n",
      "Epoch 7: |          | 1462/? [20:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1462, loss 4.047576904296875\n",
      "Epoch 7: |          | 1463/? [20:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1463, loss 3.9377894401550293\n",
      "Epoch 7: |          | 1464/? [20:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1464, loss 3.2091064453125\n",
      "Epoch 7: |          | 1465/? [20:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1465, loss 3.6542553901672363\n",
      "Epoch 7: |          | 1466/? [20:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1466, loss 3.3683242797851562\n",
      "Epoch 7: |          | 1467/? [20:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1467, loss 3.9943363666534424\n",
      "Epoch 7: |          | 1468/? [20:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1468, loss 3.512697219848633\n",
      "Epoch 7: |          | 1469/? [21:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1469, loss 3.2918198108673096\n",
      "Epoch 7: |          | 1470/? [21:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1470, loss 3.742804765701294\n",
      "Epoch 7: |          | 1471/? [21:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1471, loss 4.049099922180176\n",
      "Epoch 7: |          | 1472/? [21:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1472, loss 3.808539867401123\n",
      "Epoch 7: |          | 1473/? [21:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1473, loss 3.596526622772217\n",
      "Epoch 7: |          | 1474/? [21:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1474, loss 3.526587724685669\n",
      "Epoch 7: |          | 1475/? [21:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1475, loss 3.106781005859375\n",
      "Epoch 7: |          | 1476/? [21:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1476, loss 3.6910927295684814\n",
      "Epoch 7: |          | 1477/? [21:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1477, loss 3.7287590503692627\n",
      "Epoch 7: |          | 1478/? [21:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1478, loss 3.5943636894226074\n",
      "Epoch 7: |          | 1479/? [21:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1479, loss 4.120150566101074\n",
      "Epoch 7: |          | 1480/? [21:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1480, loss 3.8222122192382812\n",
      "Epoch 7: |          | 1481/? [21:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1481, loss 3.587434768676758\n",
      "Epoch 7: |          | 1482/? [21:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1482, loss 3.7721869945526123\n",
      "Epoch 7: |          | 1483/? [21:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1483, loss 3.3766307830810547\n",
      "Epoch 7: |          | 1484/? [21:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1484, loss 3.5808753967285156\n",
      "Epoch 7: |          | 1485/? [21:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1485, loss 3.7224268913269043\n",
      "Epoch 7: |          | 1486/? [21:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1486, loss 3.7547411918640137\n",
      "Epoch 7: |          | 1487/? [21:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1487, loss 3.163529872894287\n",
      "Epoch 7: |          | 1488/? [21:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1488, loss 3.840001344680786\n",
      "Epoch 7: |          | 1489/? [21:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1489, loss 3.786450147628784\n",
      "Epoch 7: |          | 1490/? [21:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1490, loss 3.602909803390503\n",
      "Epoch 7: |          | 1491/? [21:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1491, loss 2.7202463150024414\n",
      "Epoch 7: |          | 1492/? [21:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1492, loss 3.273991823196411\n",
      "Epoch 7: |          | 1493/? [21:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1493, loss 2.8166513442993164\n",
      "Epoch 7: |          | 1494/? [21:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1494, loss 3.6055564880371094\n",
      "Epoch 7: |          | 1495/? [21:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1495, loss 3.4738006591796875\n",
      "Epoch 7: |          | 1496/? [21:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1496, loss 3.804900646209717\n",
      "Epoch 7: |          | 1497/? [21:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1497, loss 3.0743467807769775\n",
      "Epoch 7: |          | 1498/? [21:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1498, loss 3.397455930709839\n",
      "Epoch 7: |          | 1499/? [21:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1499, loss 3.895983934402466\n",
      "Epoch 7: |          | 1500/? [21:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1500, loss 3.807828903198242\n",
      "Epoch 7: |          | 1501/? [21:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1501, loss 3.697575330734253\n",
      "Epoch 7: |          | 1502/? [21:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1502, loss 3.741406202316284\n",
      "Epoch 7: |          | 1503/? [21:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1503, loss 3.5311665534973145\n",
      "Epoch 7: |          | 1504/? [21:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1504, loss 4.199902534484863\n",
      "Epoch 7: |          | 1505/? [21:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1505, loss 3.9439682960510254\n",
      "Epoch 7: |          | 1506/? [21:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1506, loss 3.597656726837158\n",
      "Epoch 7: |          | 1507/? [21:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1507, loss 3.4916446208953857\n",
      "Epoch 7: |          | 1508/? [21:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1508, loss 3.6514549255371094\n",
      "Epoch 7: |          | 1509/? [21:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1509, loss 3.6206729412078857\n",
      "Epoch 7: |          | 1510/? [21:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1510, loss 3.8940491676330566\n",
      "Epoch 7: |          | 1511/? [21:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1511, loss 3.5044186115264893\n",
      "Epoch 7: |          | 1512/? [21:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1512, loss 4.036428928375244\n",
      "Epoch 7: |          | 1513/? [21:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1513, loss 4.293846607208252\n",
      "Epoch 7: |          | 1514/? [21:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1514, loss 3.3854095935821533\n",
      "Epoch 7: |          | 1515/? [21:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1515, loss 4.139892578125\n",
      "Epoch 7: |          | 1516/? [21:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1516, loss 4.150972843170166\n",
      "Epoch 7: |          | 1517/? [21:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1517, loss 3.5890514850616455\n",
      "Epoch 7: |          | 1518/? [21:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1518, loss 3.4001383781433105\n",
      "Epoch 7: |          | 1519/? [21:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1519, loss 3.9006824493408203\n",
      "Epoch 7: |          | 1520/? [21:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1520, loss 4.137931823730469\n",
      "Epoch 7: |          | 1521/? [21:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1521, loss 3.704089641571045\n",
      "Epoch 7: |          | 1522/? [21:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1522, loss 3.4907851219177246\n",
      "Epoch 7: |          | 1523/? [21:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1523, loss 3.7800421714782715\n",
      "Epoch 7: |          | 1524/? [21:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1524, loss 3.6720588207244873\n",
      "Epoch 7: |          | 1525/? [21:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1525, loss 3.3432350158691406\n",
      "Epoch 7: |          | 1526/? [21:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1526, loss 3.93528413772583\n",
      "Epoch 7: |          | 1527/? [21:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1527, loss 3.963608503341675\n",
      "Epoch 7: |          | 1528/? [21:50<00:00,  1.17it/s, v_num=30]ERROR: Input has inproper shape\n",
      "Epoch 7: |          | 1529/? [21:50<00:00,  1.17it/s, v_num=30]   VALIDATION: Batch 0, loss 4.610540866851807\n",
      "   VALIDATION: Batch 1, loss 3.59112811088562\n",
      "   VALIDATION: Batch 2, loss 4.775651931762695\n",
      "   VALIDATION: Batch 3, loss 4.507336139678955\n",
      "   VALIDATION: Batch 4, loss 4.040890693664551\n",
      "   VALIDATION: Batch 5, loss 3.7151496410369873\n",
      "   VALIDATION: Batch 6, loss 4.033649444580078\n",
      "   VALIDATION: Batch 7, loss 4.680751800537109\n",
      "   VALIDATION: Batch 8, loss 4.516037464141846\n",
      "   VALIDATION: Batch 9, loss 4.6613359451293945\n",
      "   VALIDATION: Batch 10, loss 4.347561836242676\n",
      "   VALIDATION: Batch 11, loss 3.9598586559295654\n",
      "   VALIDATION: Batch 12, loss 4.199609279632568\n",
      "   VALIDATION: Batch 13, loss 4.746371746063232\n",
      "   VALIDATION: Batch 14, loss 4.080191612243652\n",
      "   VALIDATION: Batch 15, loss 3.9718196392059326\n",
      "   VALIDATION: Batch 16, loss 4.582712650299072\n",
      "   VALIDATION: Batch 17, loss 4.197221755981445\n",
      "   VALIDATION: Batch 18, loss 3.544203996658325\n",
      "   VALIDATION: Batch 19, loss 4.4612908363342285\n",
      "   VALIDATION: Batch 20, loss 4.746496677398682\n",
      "   VALIDATION: Batch 21, loss 5.046914577484131\n",
      "   VALIDATION: Batch 22, loss 4.625524044036865\n",
      "   VALIDATION: Batch 23, loss 4.1136393547058105\n",
      "   VALIDATION: Batch 24, loss 3.977179765701294\n",
      "   VALIDATION: Batch 25, loss 4.405182838439941\n",
      "   VALIDATION: Batch 26, loss 4.615822792053223\n",
      "   VALIDATION: Batch 27, loss 4.527915000915527\n",
      "   VALIDATION: Batch 28, loss 4.2675018310546875\n",
      "   VALIDATION: Batch 29, loss 4.482591152191162\n",
      "   VALIDATION: Batch 30, loss 4.04413366317749\n",
      "   VALIDATION: Batch 31, loss 4.3774614334106445\n",
      "   VALIDATION: Batch 32, loss 4.9363884925842285\n",
      "   VALIDATION: Batch 33, loss 3.135758399963379\n",
      "   VALIDATION: Batch 34, loss 4.348568916320801\n",
      "   VALIDATION: Batch 35, loss 4.613141059875488\n",
      "   VALIDATION: Batch 36, loss 3.8593459129333496\n",
      "   VALIDATION: Batch 37, loss 3.8413467407226562\n",
      "   VALIDATION: Batch 38, loss 3.9425578117370605\n",
      "   VALIDATION: Batch 39, loss 4.35490608215332\n",
      "   VALIDATION: Batch 40, loss 4.401859283447266\n",
      "   VALIDATION: Batch 41, loss 3.2096736431121826\n",
      "   VALIDATION: Batch 42, loss 4.481128692626953\n",
      "   VALIDATION: Batch 43, loss 4.581432342529297\n",
      "   VALIDATION: Batch 44, loss 4.17747688293457\n",
      "   VALIDATION: Batch 45, loss 4.577506065368652\n",
      "   VALIDATION: Batch 46, loss 3.6864171028137207\n",
      "   VALIDATION: Batch 47, loss 4.724691867828369\n",
      "   VALIDATION: Batch 48, loss 4.85245418548584\n",
      "   VALIDATION: Batch 49, loss 4.397989749908447\n",
      "   VALIDATION: Batch 50, loss 4.388010025024414\n",
      "   VALIDATION: Batch 51, loss 4.888972282409668\n",
      "   VALIDATION: Batch 52, loss 4.054515361785889\n",
      "   VALIDATION: Batch 53, loss 3.939598560333252\n",
      "   VALIDATION: Batch 54, loss 4.009091854095459\n",
      "   VALIDATION: Batch 55, loss 4.773387908935547\n",
      "   VALIDATION: Batch 56, loss 4.144054412841797\n",
      "   VALIDATION: Batch 57, loss 5.7720046043396\n",
      "   VALIDATION: Batch 58, loss 4.3045830726623535\n",
      "   VALIDATION: Batch 59, loss 3.9293103218078613\n",
      "   VALIDATION: Batch 60, loss 3.477923631668091\n",
      "   VALIDATION: Batch 61, loss 4.3231635093688965\n",
      "   VALIDATION: Batch 62, loss 4.321728706359863\n",
      "   VALIDATION: Batch 63, loss 4.869085311889648\n",
      "   VALIDATION: Batch 64, loss 4.615706920623779\n",
      "   VALIDATION: Batch 65, loss 3.7546868324279785\n",
      "   VALIDATION: Batch 66, loss 4.6516900062561035\n",
      "   VALIDATION: Batch 67, loss 4.0878682136535645\n",
      "   VALIDATION: Batch 68, loss 4.265322685241699\n",
      "   VALIDATION: Batch 69, loss 4.527068138122559\n",
      "   VALIDATION: Batch 70, loss 4.687201976776123\n",
      "   VALIDATION: Batch 71, loss 4.187955379486084\n",
      "   VALIDATION: Batch 72, loss 5.063155174255371\n",
      "   VALIDATION: Batch 73, loss 3.8267123699188232\n",
      "   VALIDATION: Batch 74, loss 4.47456169128418\n",
      "   VALIDATION: Batch 75, loss 4.488744258880615\n",
      "   VALIDATION: Batch 76, loss 4.316318988800049\n",
      "   VALIDATION: Batch 77, loss 4.601735591888428\n",
      "   VALIDATION: Batch 78, loss 4.4139485359191895\n",
      "   VALIDATION: Batch 79, loss 4.385842323303223\n",
      "   VALIDATION: Batch 80, loss 4.465116024017334\n",
      "   VALIDATION: Batch 81, loss 4.230790138244629\n",
      "   VALIDATION: Batch 82, loss 4.627041339874268\n",
      "   VALIDATION: Batch 83, loss 3.823085069656372\n",
      "   VALIDATION: Batch 84, loss 4.564424514770508\n",
      "   VALIDATION: Batch 85, loss 4.183413028717041\n",
      "   VALIDATION: Batch 86, loss 4.254506587982178\n",
      "   VALIDATION: Batch 87, loss 4.14336633682251\n",
      "   VALIDATION: Batch 88, loss 3.734898805618286\n",
      "   VALIDATION: Batch 89, loss 4.016106605529785\n",
      "   VALIDATION: Batch 90, loss 4.3059868812561035\n",
      "   VALIDATION: Batch 91, loss 4.39353609085083\n",
      "   VALIDATION: Batch 92, loss 4.055018901824951\n",
      "   VALIDATION: Batch 93, loss 4.787545204162598\n",
      "   VALIDATION: Batch 94, loss 4.287506580352783\n",
      "   VALIDATION: Batch 95, loss 3.7513434886932373\n",
      "   VALIDATION: Batch 96, loss 4.2209978103637695\n",
      "   VALIDATION: Batch 97, loss 3.9143576622009277\n",
      "   VALIDATION: Batch 98, loss 4.56838321685791\n",
      "   VALIDATION: Batch 99, loss 4.622837066650391\n",
      "   VALIDATION: Batch 100, loss 5.021328926086426\n",
      "   VALIDATION: Batch 101, loss 3.589062452316284\n",
      "   VALIDATION: Batch 102, loss 4.974720478057861\n",
      "   VALIDATION: Batch 103, loss 4.9341912269592285\n",
      "   VALIDATION: Batch 104, loss 3.889209747314453\n",
      "   VALIDATION: Batch 105, loss 4.379957675933838\n",
      "   VALIDATION: Batch 106, loss 4.178822040557861\n",
      "   VALIDATION: Batch 107, loss 4.328754425048828\n",
      "   VALIDATION: Batch 108, loss 4.050253868103027\n",
      "   VALIDATION: Batch 109, loss 4.648346424102783\n",
      "   VALIDATION: Batch 110, loss 4.331008434295654\n",
      "   VALIDATION: Batch 111, loss 4.654841423034668\n",
      "   VALIDATION: Batch 112, loss 5.564416885375977\n",
      "   VALIDATION: Batch 113, loss 4.844159126281738\n",
      "   VALIDATION: Batch 114, loss 4.618561267852783\n",
      "   VALIDATION: Batch 115, loss 4.054714202880859\n",
      "   VALIDATION: Batch 116, loss 3.9099297523498535\n",
      "   VALIDATION: Batch 117, loss 4.587122917175293\n",
      "   VALIDATION: Batch 118, loss 4.754866123199463\n",
      "   VALIDATION: Batch 119, loss 3.8975086212158203\n",
      "   VALIDATION: Batch 120, loss 3.524451732635498\n",
      "   VALIDATION: Batch 121, loss 3.87165904045105\n",
      "   VALIDATION: Batch 122, loss 4.248766899108887\n",
      "   VALIDATION: Batch 123, loss 4.259973526000977\n",
      "   VALIDATION: Batch 124, loss 3.6470019817352295\n",
      "   VALIDATION: Batch 125, loss 4.249125003814697\n",
      "   VALIDATION: Batch 126, loss 4.478071689605713\n",
      "   VALIDATION: Batch 127, loss 4.257319927215576\n",
      "   VALIDATION: Batch 128, loss 4.3981781005859375\n",
      "   VALIDATION: Batch 129, loss 4.054672718048096\n",
      "   VALIDATION: Batch 130, loss 3.6308040618896484\n",
      "   VALIDATION: Batch 131, loss 3.6863365173339844\n",
      "   VALIDATION: Batch 132, loss 4.343510150909424\n",
      "   VALIDATION: Batch 133, loss 4.513507843017578\n",
      "   VALIDATION: Batch 134, loss 4.375284671783447\n",
      "   VALIDATION: Batch 135, loss 4.678393840789795\n",
      "   VALIDATION: Batch 136, loss 4.722273826599121\n",
      "   VALIDATION: Batch 137, loss 4.540677547454834\n",
      "   VALIDATION: Batch 138, loss 4.255424976348877\n",
      "   VALIDATION: Batch 139, loss 4.666695594787598\n",
      "   VALIDATION: Batch 140, loss 3.783612012863159\n",
      "   VALIDATION: Batch 141, loss 4.728813648223877\n",
      "   VALIDATION: Batch 142, loss 3.427631378173828\n",
      "   VALIDATION: Batch 143, loss 4.243650436401367\n",
      "   VALIDATION: Batch 144, loss 4.499960422515869\n",
      "   VALIDATION: Batch 145, loss 4.311631202697754\n",
      "   VALIDATION: Batch 146, loss 4.143134593963623\n",
      "   VALIDATION: Batch 147, loss 4.429736614227295\n",
      "   VALIDATION: Batch 148, loss 4.554465293884277\n",
      "   VALIDATION: Batch 149, loss 5.02973747253418\n",
      "   VALIDATION: Batch 150, loss 4.605044841766357\n",
      "   VALIDATION: Batch 151, loss 4.927966117858887\n",
      "   VALIDATION: Batch 152, loss 4.258983135223389\n",
      "   VALIDATION: Batch 153, loss 4.511471748352051\n",
      "   VALIDATION: Batch 154, loss 4.390255451202393\n",
      "   VALIDATION: Batch 155, loss 4.102022171020508\n",
      "   VALIDATION: Batch 156, loss 4.755412578582764\n",
      "   VALIDATION: Batch 157, loss 4.434426307678223\n",
      "   VALIDATION: Batch 158, loss 3.854893445968628\n",
      "   VALIDATION: Batch 159, loss 4.3056864738464355\n",
      "   VALIDATION: Batch 160, loss 4.648900032043457\n",
      "   VALIDATION: Batch 161, loss 4.904177188873291\n",
      "   VALIDATION: Batch 162, loss 4.374963760375977\n",
      "   VALIDATION: Batch 163, loss 3.758622407913208\n",
      "   VALIDATION: Batch 164, loss 4.301825523376465\n",
      "   VALIDATION: Batch 165, loss 4.733057975769043\n",
      "   VALIDATION: Batch 166, loss 4.2160234451293945\n",
      "   VALIDATION: Batch 167, loss 4.593061923980713\n",
      "   VALIDATION: Batch 168, loss 3.4640743732452393\n",
      "   VALIDATION: Batch 169, loss 4.128836631774902\n",
      "   VALIDATION: Batch 170, loss 4.341165065765381\n",
      "   VALIDATION: Batch 171, loss 4.481232643127441\n",
      "   VALIDATION: Batch 172, loss 4.269832134246826\n",
      "   VALIDATION: Batch 173, loss 4.157084941864014\n",
      "   VALIDATION: Batch 174, loss 4.617561340332031\n",
      "   VALIDATION: Batch 175, loss 4.423605442047119\n",
      "   VALIDATION: Batch 176, loss 4.215151786804199\n",
      "   VALIDATION: Batch 177, loss 4.14530611038208\n",
      "   VALIDATION: Batch 178, loss 5.256963729858398\n",
      "   VALIDATION: Batch 179, loss 4.496932506561279\n",
      "   VALIDATION: Batch 180, loss 3.9941489696502686\n",
      "   VALIDATION: Batch 181, loss 4.169216156005859\n",
      "   VALIDATION: Batch 182, loss 4.46799898147583\n",
      "   VALIDATION: Batch 183, loss 3.4819324016571045\n",
      "   VALIDATION: Batch 184, loss 3.219503879547119\n",
      "   VALIDATION: Batch 185, loss 4.020565986633301\n",
      "   VALIDATION: Batch 186, loss 3.9384586811065674\n",
      "   VALIDATION: Batch 187, loss 4.163255214691162\n",
      "   VALIDATION: Batch 188, loss 4.55226993560791\n",
      "   VALIDATION: Batch 189, loss 3.888049364089966\n",
      "   VALIDATION: Batch 190, loss 3.960710048675537\n",
      "   VALIDATION: Batch 191, loss 4.454992294311523\n",
      "   VALIDATION: Batch 192, loss 4.824637413024902\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 8: |          | 0/? [00:00<?, ?it/s, v_num=30]              TRRAINING: Batch 0, loss 3.912722110748291\n",
      "Epoch 8: |          | 1/? [00:01<00:00,  0.86it/s, v_num=30]   TRRAINING: Batch 1, loss 3.548184633255005\n",
      "Epoch 8: |          | 2/? [00:02<00:00,  0.98it/s, v_num=30]   TRRAINING: Batch 2, loss 3.60090708732605\n",
      "Epoch 8: |          | 3/? [00:02<00:00,  1.01it/s, v_num=30]   TRRAINING: Batch 3, loss 3.3147366046905518\n",
      "Epoch 8: |          | 4/? [00:03<00:00,  1.05it/s, v_num=30]   TRRAINING: Batch 4, loss 3.6721503734588623\n",
      "Epoch 8: |          | 5/? [00:04<00:00,  1.12it/s, v_num=30]   TRRAINING: Batch 5, loss 4.4360761642456055\n",
      "Epoch 8: |          | 6/? [00:05<00:00,  1.12it/s, v_num=30]   TRRAINING: Batch 6, loss 3.869216203689575\n",
      "Epoch 8: |          | 7/? [00:06<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 7, loss 3.2687430381774902\n",
      "Epoch 8: |          | 8/? [00:07<00:00,  1.13it/s, v_num=30]   TRRAINING: Batch 8, loss 3.3821730613708496\n",
      "Epoch 8: |          | 9/? [00:07<00:00,  1.13it/s, v_num=30]   TRRAINING: Batch 9, loss 3.6551384925842285\n",
      "Epoch 8: |          | 10/? [00:08<00:00,  1.13it/s, v_num=30]   TRRAINING: Batch 10, loss 3.9273934364318848\n",
      "Epoch 8: |          | 11/? [00:09<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 11, loss 3.7872402667999268\n",
      "Epoch 8: |          | 12/? [00:10<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 12, loss 4.223676681518555\n",
      "Epoch 8: |          | 13/? [00:11<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 13, loss 3.7746834754943848\n",
      "Epoch 8: |          | 14/? [00:12<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 14, loss 3.9473681449890137\n",
      "Epoch 8: |          | 15/? [00:13<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 15, loss 3.2589688301086426\n",
      "Epoch 8: |          | 16/? [00:13<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 16, loss 3.0946264266967773\n",
      "Epoch 8: |          | 17/? [00:14<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 17, loss 4.1664323806762695\n",
      "Epoch 8: |          | 18/? [00:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 18, loss 3.7279438972473145\n",
      "Epoch 8: |          | 19/? [00:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 19, loss 3.5306060314178467\n",
      "Epoch 8: |          | 20/? [00:17<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 20, loss 3.7760913372039795\n",
      "Epoch 8: |          | 21/? [00:18<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 21, loss 3.9150822162628174\n",
      "Epoch 8: |          | 22/? [00:19<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 22, loss 3.8096985816955566\n",
      "Epoch 8: |          | 23/? [00:20<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 23, loss 3.199000835418701\n",
      "Epoch 8: |          | 24/? [00:21<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 24, loss 3.8230056762695312\n",
      "Epoch 8: |          | 25/? [00:21<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 25, loss 3.6414642333984375\n",
      "Epoch 8: |          | 26/? [00:22<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 26, loss 3.3734641075134277\n",
      "Epoch 8: |          | 27/? [00:23<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 27, loss 3.3540005683898926\n",
      "Epoch 8: |          | 28/? [00:24<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 28, loss 4.227049827575684\n",
      "Epoch 8: |          | 29/? [00:25<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 29, loss 3.779656171798706\n",
      "Epoch 8: |          | 30/? [00:26<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 30, loss 3.6433498859405518\n",
      "Epoch 8: |          | 31/? [00:26<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 31, loss 4.176325798034668\n",
      "Epoch 8: |          | 32/? [00:27<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 32, loss 3.7345738410949707\n",
      "Epoch 8: |          | 33/? [00:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 33, loss 3.5742945671081543\n",
      "Epoch 8: |          | 34/? [00:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 34, loss 3.5829358100891113\n",
      "Epoch 8: |          | 35/? [00:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 35, loss 3.0809216499328613\n",
      "Epoch 8: |          | 36/? [00:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 36, loss 3.8003249168395996\n",
      "Epoch 8: |          | 37/? [00:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 37, loss 3.975512742996216\n",
      "Epoch 8: |          | 38/? [00:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 38, loss 4.274301528930664\n",
      "Epoch 8: |          | 39/? [00:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 39, loss 4.15750789642334\n",
      "Epoch 8: |          | 40/? [00:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 40, loss 3.5790321826934814\n",
      "Epoch 8: |          | 41/? [00:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 41, loss 3.6744868755340576\n",
      "Epoch 8: |          | 42/? [00:36<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 42, loss 3.4153263568878174\n",
      "Epoch 8: |          | 43/? [00:37<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 43, loss 3.613067626953125\n",
      "Epoch 8: |          | 44/? [00:38<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 44, loss 2.8232154846191406\n",
      "Epoch 8: |          | 45/? [00:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 45, loss 2.3874382972717285\n",
      "Epoch 8: |          | 46/? [00:39<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 46, loss 4.197047233581543\n",
      "Epoch 8: |          | 47/? [00:40<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 47, loss 3.4112744331359863\n",
      "Epoch 8: |          | 48/? [00:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 48, loss 3.263059139251709\n",
      "Epoch 8: |          | 49/? [00:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 49, loss 3.7749035358428955\n",
      "Epoch 8: |          | 50/? [00:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 50, loss 3.592172145843506\n",
      "Epoch 8: |          | 51/? [00:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 51, loss 3.5102150440216064\n",
      "Epoch 8: |          | 52/? [00:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 52, loss 4.234158515930176\n",
      "Epoch 8: |          | 53/? [00:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 53, loss 3.878864288330078\n",
      "Epoch 8: |          | 54/? [00:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 54, loss 3.6868107318878174\n",
      "Epoch 8: |          | 55/? [00:47<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 55, loss 3.723984956741333\n",
      "Epoch 8: |          | 56/? [00:48<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 56, loss 3.8573315143585205\n",
      "Epoch 8: |          | 57/? [00:49<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 57, loss 3.7409424781799316\n",
      "Epoch 8: |          | 58/? [00:50<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 58, loss 4.8760576248168945\n",
      "Epoch 8: |          | 59/? [00:51<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 59, loss 3.8224892616271973\n",
      "Epoch 8: |          | 60/? [00:52<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 60, loss 3.9766578674316406\n",
      "Epoch 8: |          | 61/? [00:52<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 61, loss 3.9503612518310547\n",
      "Epoch 8: |          | 62/? [00:53<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 62, loss 3.6504101753234863\n",
      "Epoch 8: |          | 63/? [00:54<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 63, loss 3.847309112548828\n",
      "Epoch 8: |          | 64/? [00:55<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 64, loss 3.632906675338745\n",
      "Epoch 8: |          | 65/? [00:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 65, loss 3.670452833175659\n",
      "Epoch 8: |          | 66/? [00:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 66, loss 3.1126654148101807\n",
      "Epoch 8: |          | 67/? [00:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 67, loss 3.8325095176696777\n",
      "Epoch 8: |          | 68/? [00:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 68, loss 3.7674262523651123\n",
      "Epoch 8: |          | 69/? [00:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 69, loss 3.7089030742645264\n",
      "Epoch 8: |          | 70/? [01:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 70, loss 3.427170515060425\n",
      "Epoch 8: |          | 71/? [01:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 71, loss 3.4064574241638184\n",
      "Epoch 8: |          | 72/? [01:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 72, loss 3.5471997261047363\n",
      "Epoch 8: |          | 73/? [01:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 73, loss 3.7537174224853516\n",
      "Epoch 8: |          | 74/? [01:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 74, loss 3.525963544845581\n",
      "Epoch 8: |          | 75/? [01:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 75, loss 3.6302788257598877\n",
      "Epoch 8: |          | 76/? [01:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 76, loss 3.6513760089874268\n",
      "Epoch 8: |          | 77/? [01:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 77, loss 3.6564133167266846\n",
      "Epoch 8: |          | 78/? [01:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 78, loss 3.5240395069122314\n",
      "Epoch 8: |          | 79/? [01:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 79, loss 3.6781444549560547\n",
      "Epoch 8: |          | 80/? [01:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 80, loss 3.5907254219055176\n",
      "Epoch 8: |          | 81/? [01:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 81, loss 3.0432560443878174\n",
      "Epoch 8: |          | 82/? [01:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 82, loss 3.829198122024536\n",
      "Epoch 8: |          | 83/? [01:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 83, loss 3.3600666522979736\n",
      "Epoch 8: |          | 84/? [01:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 84, loss 3.2123093605041504\n",
      "Epoch 8: |          | 85/? [01:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 85, loss 3.0747079849243164\n",
      "Epoch 8: |          | 86/? [01:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 86, loss 3.244797468185425\n",
      "Epoch 8: |          | 87/? [01:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 87, loss 3.4200713634490967\n",
      "Epoch 8: |          | 88/? [01:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 88, loss 4.00199556350708\n",
      "Epoch 8: |          | 89/? [01:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 89, loss 3.962743043899536\n",
      "Epoch 8: |          | 90/? [01:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 90, loss 3.7524337768554688\n",
      "Epoch 8: |          | 91/? [01:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 91, loss 3.4475700855255127\n",
      "Epoch 8: |          | 92/? [01:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 92, loss 3.8589768409729004\n",
      "Epoch 8: |          | 93/? [01:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 93, loss 4.052666664123535\n",
      "Epoch 8: |          | 94/? [01:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 94, loss 3.948244094848633\n",
      "Epoch 8: |          | 95/? [01:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 95, loss 3.654694080352783\n",
      "Epoch 8: |          | 96/? [01:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 96, loss 3.4572997093200684\n",
      "Epoch 8: |          | 97/? [01:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 97, loss 3.25538969039917\n",
      "Epoch 8: |          | 98/? [01:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 98, loss 3.783568859100342\n",
      "Epoch 8: |          | 99/? [01:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 99, loss 3.991204023361206\n",
      "Epoch 8: |          | 100/? [01:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 100, loss 3.9207286834716797\n",
      "Epoch 8: |          | 101/? [01:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 101, loss 3.590052366256714\n",
      "Epoch 8: |          | 102/? [01:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 102, loss 3.612424850463867\n",
      "Epoch 8: |          | 103/? [01:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 103, loss 3.452470064163208\n",
      "Epoch 8: |          | 104/? [01:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 104, loss 3.724367141723633\n",
      "Epoch 8: |          | 105/? [01:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 105, loss 3.7133147716522217\n",
      "Epoch 8: |          | 106/? [01:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 106, loss 3.809443235397339\n",
      "Epoch 8: |          | 107/? [01:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 107, loss 3.8233063220977783\n",
      "Epoch 8: |          | 108/? [01:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 108, loss 3.785226345062256\n",
      "Epoch 8: |          | 109/? [01:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 109, loss 3.557907819747925\n",
      "Epoch 8: |          | 110/? [01:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 110, loss 3.697335720062256\n",
      "Epoch 8: |          | 111/? [01:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 111, loss 4.319454193115234\n",
      "Epoch 8: |          | 112/? [01:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 112, loss 3.0697834491729736\n",
      "Epoch 8: |          | 113/? [01:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 113, loss 2.6744141578674316\n",
      "Epoch 8: |          | 114/? [01:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 114, loss 3.9013068675994873\n",
      "Epoch 8: |          | 115/? [01:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 115, loss 4.064774513244629\n",
      "Epoch 8: |          | 116/? [01:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 116, loss 3.154261350631714\n",
      "Epoch 8: |          | 117/? [01:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 117, loss 3.1911439895629883\n",
      "Epoch 8: |          | 118/? [01:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 118, loss 3.9529178142547607\n",
      "Epoch 8: |          | 119/? [01:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 119, loss 4.188241958618164\n",
      "Epoch 8: |          | 120/? [01:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 120, loss 3.9873383045196533\n",
      "Epoch 8: |          | 121/? [01:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 121, loss 3.6944384574890137\n",
      "Epoch 8: |          | 122/? [01:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 122, loss 3.1926798820495605\n",
      "Epoch 8: |          | 123/? [01:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 123, loss 3.6957764625549316\n",
      "Epoch 8: |          | 124/? [01:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 124, loss 3.757797956466675\n",
      "Epoch 8: |          | 125/? [01:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 125, loss 3.432300567626953\n",
      "Epoch 8: |          | 126/? [01:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 126, loss 3.9346375465393066\n",
      "Epoch 8: |          | 127/? [01:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 127, loss 4.0442681312561035\n",
      "Epoch 8: |          | 128/? [01:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 128, loss 3.2039763927459717\n",
      "Epoch 8: |          | 129/? [01:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 129, loss 3.771696090698242\n",
      "Epoch 8: |          | 130/? [01:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 130, loss 2.9432218074798584\n",
      "Epoch 8: |          | 131/? [01:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 131, loss 3.731652021408081\n",
      "Epoch 8: |          | 132/? [01:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 132, loss 3.7973484992980957\n",
      "Epoch 8: |          | 133/? [01:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 133, loss 3.6402664184570312\n",
      "Epoch 8: |          | 134/? [01:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 134, loss 3.843419313430786\n",
      "Epoch 8: |          | 135/? [01:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 135, loss 3.8585643768310547\n",
      "Epoch 8: |          | 136/? [01:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 136, loss 3.9251041412353516\n",
      "Epoch 8: |          | 137/? [01:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 137, loss 2.9704110622406006\n",
      "Epoch 8: |          | 138/? [01:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 138, loss 3.574894666671753\n",
      "Epoch 8: |          | 139/? [01:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 139, loss 3.9220237731933594\n",
      "Epoch 8: |          | 140/? [01:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 140, loss 3.277571201324463\n",
      "Epoch 8: |          | 141/? [02:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 141, loss 3.3693225383758545\n",
      "Epoch 8: |          | 142/? [02:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 142, loss 4.692593574523926\n",
      "Epoch 8: |          | 143/? [02:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 143, loss 4.365687370300293\n",
      "Epoch 8: |          | 144/? [02:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 144, loss 3.7395615577697754\n",
      "Epoch 8: |          | 145/? [02:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 145, loss 3.2027690410614014\n",
      "Epoch 8: |          | 146/? [02:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 146, loss 3.338991165161133\n",
      "Epoch 8: |          | 147/? [02:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 147, loss 3.676534652709961\n",
      "Epoch 8: |          | 148/? [02:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 148, loss 3.3676486015319824\n",
      "Epoch 8: |          | 149/? [02:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 149, loss 3.0021140575408936\n",
      "Epoch 8: |          | 150/? [02:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 150, loss 3.8434054851531982\n",
      "Epoch 8: |          | 151/? [02:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 151, loss 3.9413013458251953\n",
      "Epoch 8: |          | 152/? [02:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 152, loss 3.8848159313201904\n",
      "Epoch 8: |          | 153/? [02:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 153, loss 3.0361645221710205\n",
      "Epoch 8: |          | 154/? [02:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 154, loss 4.223330497741699\n",
      "Epoch 8: |          | 155/? [02:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 155, loss 3.753075361251831\n",
      "Epoch 8: |          | 156/? [02:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 156, loss 3.135270595550537\n",
      "Epoch 8: |          | 157/? [02:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 157, loss 3.805478572845459\n",
      "Epoch 8: |          | 158/? [02:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 158, loss 3.8808975219726562\n",
      "Epoch 8: |          | 159/? [02:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 159, loss 3.644108295440674\n",
      "Epoch 8: |          | 160/? [02:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 160, loss 3.4363186359405518\n",
      "Epoch 8: |          | 161/? [02:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 161, loss 3.7708897590637207\n",
      "Epoch 8: |          | 162/? [02:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 162, loss 3.789506435394287\n",
      "Epoch 8: |          | 163/? [02:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 163, loss 2.9614460468292236\n",
      "Epoch 8: |          | 164/? [02:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 164, loss 3.409794569015503\n",
      "Epoch 8: |          | 165/? [02:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 165, loss 4.171904563903809\n",
      "Epoch 8: |          | 166/? [02:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 166, loss 3.8276259899139404\n",
      "Epoch 8: |          | 167/? [02:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 167, loss 3.925687313079834\n",
      "Epoch 8: |          | 168/? [02:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 168, loss 3.5014748573303223\n",
      "Epoch 8: |          | 169/? [02:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 169, loss 3.175853967666626\n",
      "Epoch 8: |          | 170/? [02:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 170, loss 3.4189047813415527\n",
      "Epoch 8: |          | 171/? [02:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 171, loss 3.7380166053771973\n",
      "Epoch 8: |          | 172/? [02:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 172, loss 3.5558090209960938\n",
      "Epoch 8: |          | 173/? [02:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 173, loss 4.133149147033691\n",
      "Epoch 8: |          | 174/? [02:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 174, loss 3.761749744415283\n",
      "Epoch 8: |          | 175/? [02:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 175, loss 4.249279975891113\n",
      "Epoch 8: |          | 176/? [02:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 176, loss 3.5167548656463623\n",
      "Epoch 8: |          | 177/? [02:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 177, loss 3.51936411857605\n",
      "Epoch 8: |          | 178/? [02:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 178, loss 3.364910125732422\n",
      "Epoch 8: |          | 179/? [02:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 179, loss 4.07735013961792\n",
      "Epoch 8: |          | 180/? [02:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 180, loss 3.5947811603546143\n",
      "Epoch 8: |          | 181/? [02:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 181, loss 3.423496961593628\n",
      "Epoch 8: |          | 182/? [02:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 182, loss 3.790327787399292\n",
      "Epoch 8: |          | 183/? [02:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 183, loss 3.310966968536377\n",
      "Epoch 8: |          | 184/? [02:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 184, loss 3.5216147899627686\n",
      "Epoch 8: |          | 185/? [02:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 185, loss 3.9588170051574707\n",
      "Epoch 8: |          | 186/? [02:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 186, loss 3.5652976036071777\n",
      "Epoch 8: |          | 187/? [02:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 187, loss 4.028590202331543\n",
      "Epoch 8: |          | 188/? [02:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 188, loss 3.4446933269500732\n",
      "Epoch 8: |          | 189/? [02:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 189, loss 4.029884338378906\n",
      "Epoch 8: |          | 190/? [02:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 190, loss 3.6227188110351562\n",
      "Epoch 8: |          | 191/? [02:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 191, loss 4.331013202667236\n",
      "Epoch 8: |          | 192/? [02:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 192, loss 4.0396270751953125\n",
      "Epoch 8: |          | 193/? [02:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 193, loss 3.498117446899414\n",
      "Epoch 8: |          | 194/? [02:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 194, loss 3.382357358932495\n",
      "Epoch 8: |          | 195/? [02:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 195, loss 4.087703227996826\n",
      "Epoch 8: |          | 196/? [02:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 196, loss 3.917938709259033\n",
      "Epoch 8: |          | 197/? [02:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 197, loss 3.7207489013671875\n",
      "Epoch 8: |          | 198/? [02:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 198, loss 3.080042600631714\n",
      "Epoch 8: |          | 199/? [02:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 199, loss 4.013519287109375\n",
      "Epoch 8: |          | 200/? [02:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 200, loss 3.4825234413146973\n",
      "Epoch 8: |          | 201/? [02:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 201, loss 3.6908926963806152\n",
      "Epoch 8: |          | 202/? [02:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 202, loss 3.8739593029022217\n",
      "Epoch 8: |          | 203/? [02:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 203, loss 3.4894092082977295\n",
      "Epoch 8: |          | 204/? [02:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 204, loss 3.4841957092285156\n",
      "Epoch 8: |          | 205/? [02:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 205, loss 3.4897067546844482\n",
      "Epoch 8: |          | 206/? [02:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 206, loss 3.3325588703155518\n",
      "Epoch 8: |          | 207/? [02:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 207, loss 3.8100342750549316\n",
      "Epoch 8: |          | 208/? [02:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 208, loss 3.7320475578308105\n",
      "Epoch 8: |          | 209/? [02:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 209, loss 3.499300003051758\n",
      "Epoch 8: |          | 210/? [02:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 210, loss 4.183107852935791\n",
      "Epoch 8: |          | 211/? [02:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 211, loss 3.5878536701202393\n",
      "Epoch 8: |          | 212/? [02:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 212, loss 3.8090224266052246\n",
      "Epoch 8: |          | 213/? [03:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 213, loss 3.644824266433716\n",
      "Epoch 8: |          | 214/? [03:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 214, loss 3.551117420196533\n",
      "Epoch 8: |          | 215/? [03:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 215, loss 3.2093029022216797\n",
      "Epoch 8: |          | 216/? [03:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 216, loss 3.807987689971924\n",
      "Epoch 8: |          | 217/? [03:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 217, loss 3.7823691368103027\n",
      "Epoch 8: |          | 218/? [03:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 218, loss 3.7530555725097656\n",
      "Epoch 8: |          | 219/? [03:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 219, loss 3.7309443950653076\n",
      "Epoch 8: |          | 220/? [03:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 220, loss 3.7885868549346924\n",
      "Epoch 8: |          | 221/? [03:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 221, loss 3.6404318809509277\n",
      "Epoch 8: |          | 222/? [03:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 222, loss 2.905235767364502\n",
      "Epoch 8: |          | 223/? [03:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 223, loss 3.833603620529175\n",
      "Epoch 8: |          | 224/? [03:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 224, loss 3.956404447555542\n",
      "Epoch 8: |          | 225/? [03:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 225, loss 3.707141160964966\n",
      "Epoch 8: |          | 226/? [03:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 226, loss 3.6195907592773438\n",
      "Epoch 8: |          | 227/? [03:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 227, loss 3.968677043914795\n",
      "Epoch 8: |          | 228/? [03:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 228, loss 3.6640141010284424\n",
      "Epoch 8: |          | 229/? [03:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 229, loss 3.7171661853790283\n",
      "Epoch 8: |          | 230/? [03:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 230, loss 3.6731820106506348\n",
      "Epoch 8: |          | 231/? [03:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 231, loss 3.6187567710876465\n",
      "Epoch 8: |          | 232/? [03:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 232, loss 3.3328914642333984\n",
      "Epoch 8: |          | 233/? [03:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 233, loss 3.993776798248291\n",
      "Epoch 8: |          | 234/? [03:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 234, loss 3.973163604736328\n",
      "Epoch 8: |          | 235/? [03:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 235, loss 4.085394859313965\n",
      "Epoch 8: |          | 236/? [03:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 236, loss 3.525916576385498\n",
      "Epoch 8: |          | 237/? [03:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 237, loss 3.6982994079589844\n",
      "Epoch 8: |          | 238/? [03:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 238, loss 3.950650453567505\n",
      "Epoch 8: |          | 239/? [03:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 239, loss 3.471311569213867\n",
      "Epoch 8: |          | 240/? [03:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 240, loss 3.1235742568969727\n",
      "Epoch 8: |          | 241/? [03:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 241, loss 3.665994167327881\n",
      "Epoch 8: |          | 242/? [03:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 242, loss 4.040921211242676\n",
      "Epoch 8: |          | 243/? [03:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 243, loss 2.9538168907165527\n",
      "Epoch 8: |          | 244/? [03:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 244, loss 3.3608295917510986\n",
      "Epoch 8: |          | 245/? [03:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 245, loss 3.6156864166259766\n",
      "Epoch 8: |          | 246/? [03:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 246, loss 3.7598278522491455\n",
      "Epoch 8: |          | 247/? [03:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 247, loss 3.814967632293701\n",
      "Epoch 8: |          | 248/? [03:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 248, loss 3.319584608078003\n",
      "Epoch 8: |          | 249/? [03:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 249, loss 3.2295355796813965\n",
      "Epoch 8: |          | 250/? [03:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 250, loss 3.8298873901367188\n",
      "Epoch 8: |          | 251/? [03:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 251, loss 3.775691509246826\n",
      "Epoch 8: |          | 252/? [03:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 252, loss 3.5731377601623535\n",
      "Epoch 8: |          | 253/? [03:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 253, loss 4.377723693847656\n",
      "Epoch 8: |          | 254/? [03:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 254, loss 3.9889206886291504\n",
      "Epoch 8: |          | 255/? [03:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 255, loss 3.6650142669677734\n",
      "Epoch 8: |          | 256/? [03:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 256, loss 4.234064102172852\n",
      "Epoch 8: |          | 257/? [03:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 257, loss 3.536206007003784\n",
      "Epoch 8: |          | 258/? [03:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 258, loss 3.6340339183807373\n",
      "Epoch 8: |          | 259/? [03:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 259, loss 3.5503978729248047\n",
      "Epoch 8: |          | 260/? [03:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 260, loss 3.4545376300811768\n",
      "Epoch 8: |          | 261/? [03:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 261, loss 3.260303497314453\n",
      "Epoch 8: |          | 262/? [03:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 262, loss 3.927811861038208\n",
      "Epoch 8: |          | 263/? [03:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 263, loss 3.6135330200195312\n",
      "Epoch 8: |          | 264/? [03:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 264, loss 3.765263080596924\n",
      "Epoch 8: |          | 265/? [03:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 265, loss 3.2145984172821045\n",
      "Epoch 8: |          | 266/? [03:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 266, loss 3.659074068069458\n",
      "Epoch 8: |          | 267/? [03:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 267, loss 3.2700653076171875\n",
      "Epoch 8: |          | 268/? [03:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 268, loss 3.58894419670105\n",
      "Epoch 8: |          | 269/? [03:53<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 269, loss 3.956134080886841\n",
      "Epoch 8: |          | 270/? [03:54<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 270, loss 3.5722548961639404\n",
      "Epoch 8: |          | 271/? [03:55<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 271, loss 3.7780728340148926\n",
      "Epoch 8: |          | 272/? [03:56<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 272, loss 4.045114994049072\n",
      "Epoch 8: |          | 273/? [03:56<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 273, loss 3.3169777393341064\n",
      "Epoch 8: |          | 274/? [03:57<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 274, loss 4.081554889678955\n",
      "Epoch 8: |          | 275/? [03:58<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 275, loss 3.82338285446167\n",
      "Epoch 8: |          | 276/? [03:59<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 276, loss 3.1254818439483643\n",
      "Epoch 8: |          | 277/? [04:00<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 277, loss 3.8040740489959717\n",
      "Epoch 8: |          | 278/? [04:01<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 278, loss 2.8711273670196533\n",
      "Epoch 8: |          | 279/? [04:01<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 279, loss 3.515786647796631\n",
      "Epoch 8: |          | 280/? [04:02<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 280, loss 3.2584152221679688\n",
      "Epoch 8: |          | 281/? [04:03<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 281, loss 3.880448579788208\n",
      "Epoch 8: |          | 282/? [04:04<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 282, loss 3.455706834793091\n",
      "Epoch 8: |          | 283/? [04:05<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 283, loss 3.518913745880127\n",
      "Epoch 8: |          | 284/? [04:06<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 284, loss 3.4979209899902344\n",
      "Epoch 8: |          | 285/? [04:07<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 285, loss 3.031642436981201\n",
      "Epoch 8: |          | 286/? [04:08<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 286, loss 3.6005005836486816\n",
      "Epoch 8: |          | 287/? [04:09<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 287, loss 3.301483154296875\n",
      "Epoch 8: |          | 288/? [04:10<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 288, loss 3.305310010910034\n",
      "Epoch 8: |          | 289/? [04:10<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 289, loss 3.071814775466919\n",
      "Epoch 8: |          | 290/? [04:11<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 290, loss 2.674386978149414\n",
      "Epoch 8: |          | 291/? [04:12<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 291, loss 3.728937864303589\n",
      "Epoch 8: |          | 292/? [04:13<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 292, loss 3.4392809867858887\n",
      "Epoch 8: |          | 293/? [04:14<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 293, loss 3.668717861175537\n",
      "Epoch 8: |          | 294/? [04:14<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 294, loss 3.614814281463623\n",
      "Epoch 8: |          | 295/? [04:15<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 295, loss 3.895313262939453\n",
      "Epoch 8: |          | 296/? [04:16<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 296, loss 3.355104923248291\n",
      "Epoch 8: |          | 297/? [04:17<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 297, loss 4.042165279388428\n",
      "Epoch 8: |          | 298/? [04:18<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 298, loss 3.8180019855499268\n",
      "Epoch 8: |          | 299/? [04:19<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 299, loss 4.069995403289795\n",
      "Epoch 8: |          | 300/? [04:19<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 300, loss 3.6416726112365723\n",
      "Epoch 8: |          | 301/? [04:20<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 301, loss 3.4541378021240234\n",
      "Epoch 8: |          | 302/? [04:21<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 302, loss 3.867231845855713\n",
      "Epoch 8: |          | 303/? [04:22<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 303, loss 3.6876041889190674\n",
      "Epoch 8: |          | 304/? [04:23<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 304, loss 3.8703696727752686\n",
      "Epoch 8: |          | 305/? [04:24<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 305, loss 3.8694496154785156\n",
      "Epoch 8: |          | 306/? [04:25<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 306, loss 3.587583065032959\n",
      "Epoch 8: |          | 307/? [04:26<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 307, loss 3.8076682090759277\n",
      "Epoch 8: |          | 308/? [04:26<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 308, loss 3.9473347663879395\n",
      "Epoch 8: |          | 309/? [04:27<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 309, loss 3.5800628662109375\n",
      "Epoch 8: |          | 310/? [04:28<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 310, loss 3.9715678691864014\n",
      "Epoch 8: |          | 311/? [04:29<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 311, loss 3.5340335369110107\n",
      "Epoch 8: |          | 312/? [04:30<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 312, loss 3.597342014312744\n",
      "Epoch 8: |          | 313/? [04:31<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 313, loss 3.4189605712890625\n",
      "Epoch 8: |          | 314/? [04:32<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 314, loss 3.747128963470459\n",
      "Epoch 8: |          | 315/? [04:32<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 315, loss 3.460846424102783\n",
      "Epoch 8: |          | 316/? [04:33<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 316, loss 3.8988037109375\n",
      "Epoch 8: |          | 317/? [04:34<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 317, loss 3.7818970680236816\n",
      "Epoch 8: |          | 318/? [04:35<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 318, loss 3.9000084400177\n",
      "Epoch 8: |          | 319/? [04:36<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 319, loss 3.1816370487213135\n",
      "Epoch 8: |          | 320/? [04:37<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 320, loss 3.6948280334472656\n",
      "Epoch 8: |          | 321/? [04:38<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 321, loss 3.4854674339294434\n",
      "Epoch 8: |          | 322/? [04:39<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 322, loss 4.06247091293335\n",
      "Epoch 8: |          | 323/? [04:40<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 323, loss 3.9721508026123047\n",
      "Epoch 8: |          | 324/? [04:40<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 324, loss 3.7637946605682373\n",
      "Epoch 8: |          | 325/? [04:41<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 325, loss 4.120762348175049\n",
      "Epoch 8: |          | 326/? [04:42<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 326, loss 3.7618279457092285\n",
      "Epoch 8: |          | 327/? [04:43<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 327, loss 3.4564507007598877\n",
      "Epoch 8: |          | 328/? [04:44<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 328, loss 3.2721753120422363\n",
      "Epoch 8: |          | 329/? [04:45<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 329, loss 3.811427593231201\n",
      "Epoch 8: |          | 330/? [04:45<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 330, loss 4.310497760772705\n",
      "Epoch 8: |          | 331/? [04:46<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 331, loss 2.6670398712158203\n",
      "Epoch 8: |          | 332/? [04:47<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 332, loss 3.6789774894714355\n",
      "Epoch 8: |          | 333/? [04:48<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 333, loss 3.5719146728515625\n",
      "Epoch 8: |          | 334/? [04:49<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 334, loss 4.060070991516113\n",
      "Epoch 8: |          | 335/? [04:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 335, loss 3.9907684326171875\n",
      "Epoch 8: |          | 336/? [04:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 336, loss 4.043890476226807\n",
      "Epoch 8: |          | 337/? [04:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 337, loss 4.5082478523254395\n",
      "Epoch 8: |          | 338/? [04:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 338, loss 4.174878120422363\n",
      "Epoch 8: |          | 339/? [04:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 339, loss 3.4209625720977783\n",
      "Epoch 8: |          | 340/? [04:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 340, loss 3.220731735229492\n",
      "Epoch 8: |          | 341/? [04:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 341, loss 3.1857638359069824\n",
      "Epoch 8: |          | 342/? [04:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 342, loss 3.7297279834747314\n",
      "Epoch 8: |          | 343/? [04:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 343, loss 3.497082233428955\n",
      "Epoch 8: |          | 344/? [04:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 344, loss 4.316677093505859\n",
      "Epoch 8: |          | 345/? [04:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 345, loss 3.477003812789917\n",
      "Epoch 8: |          | 346/? [04:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 346, loss 3.7725472450256348\n",
      "Epoch 8: |          | 347/? [04:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 347, loss 3.5896522998809814\n",
      "Epoch 8: |          | 348/? [05:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 348, loss 3.007791042327881\n",
      "Epoch 8: |          | 349/? [05:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 349, loss 2.8715853691101074\n",
      "Epoch 8: |          | 350/? [05:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 350, loss 4.011273384094238\n",
      "Epoch 8: |          | 351/? [05:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 351, loss 4.051150321960449\n",
      "Epoch 8: |          | 352/? [05:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 352, loss 3.3626484870910645\n",
      "Epoch 8: |          | 353/? [05:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 353, loss 3.1488513946533203\n",
      "Epoch 8: |          | 354/? [05:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 354, loss 3.5634162425994873\n",
      "Epoch 8: |          | 355/? [05:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 355, loss 3.8608412742614746\n",
      "Epoch 8: |          | 356/? [05:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 356, loss 3.8649494647979736\n",
      "Epoch 8: |          | 357/? [05:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 357, loss 3.3182387351989746\n",
      "Epoch 8: |          | 358/? [05:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 358, loss 3.2881340980529785\n",
      "Epoch 8: |          | 359/? [05:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 359, loss 3.8263041973114014\n",
      "Epoch 8: |          | 360/? [05:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 360, loss 3.457505702972412\n",
      "Epoch 8: |          | 361/? [05:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 361, loss 3.599123477935791\n",
      "Epoch 8: |          | 362/? [05:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 362, loss 3.3697590827941895\n",
      "Epoch 8: |          | 363/? [05:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 363, loss 3.2912585735321045\n",
      "Epoch 8: |          | 364/? [05:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 364, loss 3.879431962966919\n",
      "Epoch 8: |          | 365/? [05:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 365, loss 3.902031660079956\n",
      "Epoch 8: |          | 366/? [05:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 366, loss 3.7364418506622314\n",
      "Epoch 8: |          | 367/? [05:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 367, loss 3.6737067699432373\n",
      "Epoch 8: |          | 368/? [05:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 368, loss 3.2745540142059326\n",
      "Epoch 8: |          | 369/? [05:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 369, loss 3.5615649223327637\n",
      "Epoch 8: |          | 370/? [05:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 370, loss 3.2961621284484863\n",
      "Epoch 8: |          | 371/? [05:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 371, loss 4.116916656494141\n",
      "Epoch 8: |          | 372/? [05:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 372, loss 3.4491050243377686\n",
      "Epoch 8: |          | 373/? [05:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 373, loss 3.830437421798706\n",
      "Epoch 8: |          | 374/? [05:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 374, loss 3.5076231956481934\n",
      "Epoch 8: |          | 375/? [05:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 375, loss 4.112093448638916\n",
      "Epoch 8: |          | 376/? [05:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 376, loss 3.584895610809326\n",
      "Epoch 8: |          | 377/? [05:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 377, loss 3.764677047729492\n",
      "Epoch 8: |          | 378/? [05:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 378, loss 3.9041316509246826\n",
      "Epoch 8: |          | 379/? [05:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 379, loss 3.698265790939331\n",
      "Epoch 8: |          | 380/? [05:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 380, loss 3.7239131927490234\n",
      "Epoch 8: |          | 381/? [05:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 381, loss 3.837799072265625\n",
      "Epoch 8: |          | 382/? [05:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 382, loss 3.5368893146514893\n",
      "Epoch 8: |          | 383/? [05:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 383, loss 3.649944305419922\n",
      "Epoch 8: |          | 384/? [05:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 384, loss 3.998082399368286\n",
      "Epoch 8: |          | 385/? [05:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 385, loss 3.6407501697540283\n",
      "Epoch 8: |          | 386/? [05:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 386, loss 2.6979856491088867\n",
      "Epoch 8: |          | 387/? [05:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 387, loss 3.4685001373291016\n",
      "Epoch 8: |          | 388/? [05:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 388, loss 3.077162742614746\n",
      "Epoch 8: |          | 389/? [05:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 389, loss 3.981637477874756\n",
      "Epoch 8: |          | 390/? [05:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 390, loss 3.4594199657440186\n",
      "Epoch 8: |          | 391/? [05:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 391, loss 3.869417905807495\n",
      "Epoch 8: |          | 392/? [05:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 392, loss 3.9837028980255127\n",
      "Epoch 8: |          | 393/? [05:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 393, loss 3.9942054748535156\n",
      "Epoch 8: |          | 394/? [05:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 394, loss 3.636082410812378\n",
      "Epoch 8: |          | 395/? [05:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 395, loss 3.8631930351257324\n",
      "Epoch 8: |          | 396/? [05:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 396, loss 3.7509219646453857\n",
      "Epoch 8: |          | 397/? [05:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 397, loss 3.5870769023895264\n",
      "Epoch 8: |          | 398/? [05:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 398, loss 3.482830762863159\n",
      "Epoch 8: |          | 399/? [05:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 399, loss 3.5669682025909424\n",
      "Epoch 8: |          | 400/? [05:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 400, loss 3.5505499839782715\n",
      "Epoch 8: |          | 401/? [05:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 401, loss 3.5218348503112793\n",
      "Epoch 8: |          | 402/? [05:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 402, loss 3.896867036819458\n",
      "Epoch 8: |          | 403/? [05:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 403, loss 3.8017749786376953\n",
      "Epoch 8: |          | 404/? [05:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 404, loss 3.3462777137756348\n",
      "Epoch 8: |          | 405/? [05:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 405, loss 3.405993700027466\n",
      "Epoch 8: |          | 406/? [05:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 406, loss 3.722853422164917\n",
      "Epoch 8: |          | 407/? [05:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 407, loss 3.5980377197265625\n",
      "Epoch 8: |          | 408/? [05:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 408, loss 4.043079853057861\n",
      "Epoch 8: |          | 409/? [05:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 409, loss 3.9298508167266846\n",
      "Epoch 8: |          | 410/? [05:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 410, loss 3.595162868499756\n",
      "Epoch 8: |          | 411/? [05:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 411, loss 3.541382312774658\n",
      "Epoch 8: |          | 412/? [05:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 412, loss 3.1252684593200684\n",
      "Epoch 8: |          | 413/? [05:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 413, loss 3.791660785675049\n",
      "Epoch 8: |          | 414/? [05:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 414, loss 3.3790879249572754\n",
      "Epoch 8: |          | 415/? [05:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 415, loss 3.785489559173584\n",
      "Epoch 8: |          | 416/? [05:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 416, loss 4.213503360748291\n",
      "Epoch 8: |          | 417/? [05:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 417, loss 4.159520149230957\n",
      "Epoch 8: |          | 418/? [05:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 418, loss 3.818255662918091\n",
      "Epoch 8: |          | 419/? [06:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 419, loss 3.5557518005371094\n",
      "Epoch 8: |          | 420/? [06:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 420, loss 3.7574734687805176\n",
      "Epoch 8: |          | 421/? [06:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 421, loss 4.240216255187988\n",
      "Epoch 8: |          | 422/? [06:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 422, loss 3.7893223762512207\n",
      "Epoch 8: |          | 423/? [06:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 423, loss 3.4263839721679688\n",
      "Epoch 8: |          | 424/? [06:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 424, loss 4.033044815063477\n",
      "Epoch 8: |          | 425/? [06:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 425, loss 3.7884745597839355\n",
      "Epoch 8: |          | 426/? [06:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 426, loss 3.420433521270752\n",
      "Epoch 8: |          | 427/? [06:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 427, loss 3.4855079650878906\n",
      "Epoch 8: |          | 428/? [06:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 428, loss 4.209742546081543\n",
      "Epoch 8: |          | 429/? [06:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 429, loss 3.139450788497925\n",
      "Epoch 8: |          | 430/? [06:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 430, loss 3.793891429901123\n",
      "Epoch 8: |          | 431/? [06:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 431, loss 3.6743216514587402\n",
      "Epoch 8: |          | 432/? [06:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 432, loss 3.819645404815674\n",
      "Epoch 8: |          | 433/? [06:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 433, loss 3.76072359085083\n",
      "Epoch 8: |          | 434/? [06:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 434, loss 3.6700310707092285\n",
      "Epoch 8: |          | 435/? [06:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 435, loss 3.350492000579834\n",
      "Epoch 8: |          | 436/? [06:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 436, loss 3.7551681995391846\n",
      "Epoch 8: |          | 437/? [06:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 437, loss 3.9498019218444824\n",
      "Epoch 8: |          | 438/? [06:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 438, loss 3.5635647773742676\n",
      "Epoch 8: |          | 439/? [06:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 439, loss 3.3790507316589355\n",
      "Epoch 8: |          | 440/? [06:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 440, loss 3.2659778594970703\n",
      "Epoch 8: |          | 441/? [06:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 441, loss 3.703789234161377\n",
      "Epoch 8: |          | 442/? [06:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 442, loss 3.611225128173828\n",
      "Epoch 8: |          | 443/? [06:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 443, loss 3.7267215251922607\n",
      "Epoch 8: |          | 444/? [06:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 444, loss 3.7535240650177\n",
      "Epoch 8: |          | 445/? [06:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 445, loss 4.610999584197998\n",
      "Epoch 8: |          | 446/? [06:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 446, loss 3.6802620887756348\n",
      "Epoch 8: |          | 447/? [06:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 447, loss 4.181753158569336\n",
      "Epoch 8: |          | 448/? [06:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 448, loss 3.2982242107391357\n",
      "Epoch 8: |          | 449/? [06:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 449, loss 3.7026195526123047\n",
      "Epoch 8: |          | 450/? [06:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 450, loss 3.9759554862976074\n",
      "Epoch 8: |          | 451/? [06:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 451, loss 3.6807796955108643\n",
      "Epoch 8: |          | 452/? [06:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 452, loss 3.3835957050323486\n",
      "Epoch 8: |          | 453/? [06:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 453, loss 4.126806735992432\n",
      "Epoch 8: |          | 454/? [06:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 454, loss 3.4919166564941406\n",
      "Epoch 8: |          | 455/? [06:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 455, loss 3.8161933422088623\n",
      "Epoch 8: |          | 456/? [06:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 456, loss 3.17181396484375\n",
      "Epoch 8: |          | 457/? [06:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 457, loss 3.6287224292755127\n",
      "Epoch 8: |          | 458/? [06:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 458, loss 4.034054279327393\n",
      "Epoch 8: |          | 459/? [06:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 459, loss 4.024605751037598\n",
      "Epoch 8: |          | 460/? [06:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 460, loss 3.773477077484131\n",
      "Epoch 8: |          | 461/? [06:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 461, loss 3.741298198699951\n",
      "Epoch 8: |          | 462/? [06:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 462, loss 3.8045966625213623\n",
      "Epoch 8: |          | 463/? [06:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 463, loss 3.6764163970947266\n",
      "Epoch 8: |          | 464/? [06:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 464, loss 3.221555709838867\n",
      "Epoch 8: |          | 465/? [06:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 465, loss 3.4658138751983643\n",
      "Epoch 8: |          | 466/? [06:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 466, loss 3.9299235343933105\n",
      "Epoch 8: |          | 467/? [06:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 467, loss 3.7239537239074707\n",
      "Epoch 8: |          | 468/? [06:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 468, loss 3.6331844329833984\n",
      "Epoch 8: |          | 469/? [06:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 469, loss 3.7576146125793457\n",
      "Epoch 8: |          | 470/? [06:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 470, loss 3.2265751361846924\n",
      "Epoch 8: |          | 471/? [06:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 471, loss 4.004362106323242\n",
      "Epoch 8: |          | 472/? [06:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 472, loss 3.4531402587890625\n",
      "Epoch 8: |          | 473/? [06:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 473, loss 3.4692931175231934\n",
      "Epoch 8: |          | 474/? [06:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 474, loss 3.156238555908203\n",
      "Epoch 8: |          | 475/? [06:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 475, loss 4.309779167175293\n",
      "Epoch 8: |          | 476/? [06:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 476, loss 3.436072587966919\n",
      "Epoch 8: |          | 477/? [06:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 477, loss 2.969985008239746\n",
      "Epoch 8: |          | 478/? [06:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 478, loss 3.1904196739196777\n",
      "Epoch 8: |          | 479/? [06:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 479, loss 3.641043186187744\n",
      "Epoch 8: |          | 480/? [06:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 480, loss 3.4909005165100098\n",
      "Epoch 8: |          | 481/? [06:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 481, loss 3.1598525047302246\n",
      "Epoch 8: |          | 482/? [06:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 482, loss 3.4390015602111816\n",
      "Epoch 8: |          | 483/? [06:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 483, loss 3.1370487213134766\n",
      "Epoch 8: |          | 484/? [06:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 484, loss 3.9875996112823486\n",
      "Epoch 8: |          | 485/? [06:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 485, loss 3.900618076324463\n",
      "Epoch 8: |          | 486/? [06:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 486, loss 3.4404654502868652\n",
      "Epoch 8: |          | 487/? [06:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 487, loss 3.862807512283325\n",
      "Epoch 8: |          | 488/? [06:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 488, loss 3.611708164215088\n",
      "Epoch 8: |          | 489/? [06:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 489, loss 3.1771035194396973\n",
      "Epoch 8: |          | 490/? [06:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 490, loss 3.714951276779175\n",
      "Epoch 8: |          | 491/? [07:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 491, loss 3.5935420989990234\n",
      "Epoch 8: |          | 492/? [07:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 492, loss 2.974454879760742\n",
      "Epoch 8: |          | 493/? [07:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 493, loss 3.9143028259277344\n",
      "Epoch 8: |          | 494/? [07:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 494, loss 3.7132091522216797\n",
      "Epoch 8: |          | 495/? [07:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 495, loss 3.7904562950134277\n",
      "Epoch 8: |          | 496/? [07:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 496, loss 3.4518120288848877\n",
      "Epoch 8: |          | 497/? [07:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 497, loss 3.9833598136901855\n",
      "Epoch 8: |          | 498/? [07:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 498, loss 3.607959032058716\n",
      "Epoch 8: |          | 499/? [07:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 499, loss 3.768296003341675\n",
      "Epoch 8: |          | 500/? [07:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 500, loss 3.503075361251831\n",
      "Epoch 8: |          | 501/? [07:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 501, loss 3.2750847339630127\n",
      "Epoch 8: |          | 502/? [07:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 502, loss 3.6918556690216064\n",
      "Epoch 8: |          | 503/? [07:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 503, loss 3.6412136554718018\n",
      "Epoch 8: |          | 504/? [07:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 504, loss 3.557103395462036\n",
      "Epoch 8: |          | 505/? [07:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 505, loss 3.0838048458099365\n",
      "Epoch 8: |          | 506/? [07:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 506, loss 3.6223907470703125\n",
      "Epoch 8: |          | 507/? [07:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 507, loss 3.6523609161376953\n",
      "Epoch 8: |          | 508/? [07:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 508, loss 3.9745826721191406\n",
      "Epoch 8: |          | 509/? [07:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 509, loss 3.39574933052063\n",
      "Epoch 8: |          | 510/? [07:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 510, loss 3.814971923828125\n",
      "Epoch 8: |          | 511/? [07:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 511, loss 3.670426845550537\n",
      "Epoch 8: |          | 512/? [07:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 512, loss 3.132660388946533\n",
      "Epoch 8: |          | 513/? [07:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 513, loss 3.3906807899475098\n",
      "Epoch 8: |          | 514/? [07:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 514, loss 3.5410914421081543\n",
      "Epoch 8: |          | 515/? [07:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 515, loss 3.1465799808502197\n",
      "Epoch 8: |          | 516/? [07:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 516, loss 3.5161795616149902\n",
      "Epoch 8: |          | 517/? [07:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 517, loss 3.6573009490966797\n",
      "Epoch 8: |          | 518/? [07:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 518, loss 3.284137010574341\n",
      "Epoch 8: |          | 519/? [07:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 519, loss 3.6854679584503174\n",
      "Epoch 8: |          | 520/? [07:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 520, loss 3.554933547973633\n",
      "Epoch 8: |          | 521/? [07:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 521, loss 3.5886433124542236\n",
      "Epoch 8: |          | 522/? [07:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 522, loss 4.096765041351318\n",
      "Epoch 8: |          | 523/? [07:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 523, loss 4.154784202575684\n",
      "Epoch 8: |          | 524/? [07:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 524, loss 3.9693045616149902\n",
      "Epoch 8: |          | 525/? [07:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 525, loss 3.556523084640503\n",
      "Epoch 8: |          | 526/? [07:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 526, loss 3.3619513511657715\n",
      "Epoch 8: |          | 527/? [07:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 527, loss 4.032846927642822\n",
      "Epoch 8: |          | 528/? [07:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 528, loss 3.782060146331787\n",
      "Epoch 8: |          | 529/? [07:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 529, loss 3.3897297382354736\n",
      "Epoch 8: |          | 530/? [07:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 530, loss 3.9080169200897217\n",
      "Epoch 8: |          | 531/? [07:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 531, loss 3.435032367706299\n",
      "Epoch 8: |          | 532/? [07:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 532, loss 3.7238869667053223\n",
      "Epoch 8: |          | 533/? [07:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 533, loss 3.347721815109253\n",
      "Epoch 8: |          | 534/? [07:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 534, loss 3.0593743324279785\n",
      "Epoch 8: |          | 535/? [07:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 535, loss 3.3569602966308594\n",
      "Epoch 8: |          | 536/? [07:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 536, loss 3.970282793045044\n",
      "Epoch 8: |          | 537/? [07:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 537, loss 3.773524045944214\n",
      "Epoch 8: |          | 538/? [07:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 538, loss 3.4141993522644043\n",
      "Epoch 8: |          | 539/? [07:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 539, loss 3.546820878982544\n",
      "Epoch 8: |          | 540/? [07:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 540, loss 3.88372802734375\n",
      "Epoch 8: |          | 541/? [07:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 541, loss 3.680147171020508\n",
      "Epoch 8: |          | 542/? [07:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 542, loss 3.4293055534362793\n",
      "Epoch 8: |          | 543/? [07:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 543, loss 3.7903590202331543\n",
      "Epoch 8: |          | 544/? [07:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 544, loss 3.7381222248077393\n",
      "Epoch 8: |          | 545/? [07:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 545, loss 3.056384325027466\n",
      "Epoch 8: |          | 546/? [07:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 546, loss 3.8068575859069824\n",
      "Epoch 8: |          | 547/? [07:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 547, loss 4.225808620452881\n",
      "Epoch 8: |          | 548/? [07:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 548, loss 3.854172468185425\n",
      "Epoch 8: |          | 549/? [07:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 549, loss 3.7531089782714844\n",
      "Epoch 8: |          | 550/? [07:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 550, loss 4.07034969329834\n",
      "Epoch 8: |          | 551/? [07:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 551, loss 3.760267734527588\n",
      "Epoch 8: |          | 552/? [07:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 552, loss 3.7086939811706543\n",
      "Epoch 8: |          | 553/? [07:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 553, loss 3.2169902324676514\n",
      "Epoch 8: |          | 554/? [07:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 554, loss 3.76051664352417\n",
      "Epoch 8: |          | 555/? [07:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 555, loss 3.980590343475342\n",
      "Epoch 8: |          | 556/? [07:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 556, loss 3.84516978263855\n",
      "Epoch 8: |          | 557/? [07:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 557, loss 3.309871196746826\n",
      "Epoch 8: |          | 558/? [07:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 558, loss 3.557018995285034\n",
      "Epoch 8: |          | 559/? [07:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 559, loss 3.5106754302978516\n",
      "Epoch 8: |          | 560/? [07:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 560, loss 3.088469982147217\n",
      "Epoch 8: |          | 561/? [07:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 561, loss 2.878141403198242\n",
      "Epoch 8: |          | 562/? [07:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 562, loss 3.9007961750030518\n",
      "Epoch 8: |          | 563/? [08:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 563, loss 3.054957866668701\n",
      "Epoch 8: |          | 564/? [08:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 564, loss 3.4846153259277344\n",
      "Epoch 8: |          | 565/? [08:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 565, loss 3.785123825073242\n",
      "Epoch 8: |          | 566/? [08:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 566, loss 3.9482414722442627\n",
      "Epoch 8: |          | 567/? [08:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 567, loss 4.018551826477051\n",
      "Epoch 8: |          | 568/? [08:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 568, loss 3.102311849594116\n",
      "Epoch 8: |          | 569/? [08:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 569, loss 3.692164659500122\n",
      "Epoch 8: |          | 570/? [08:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 570, loss 3.8143253326416016\n",
      "Epoch 8: |          | 571/? [08:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 571, loss 3.4308669567108154\n",
      "Epoch 8: |          | 572/? [08:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 572, loss 4.345663070678711\n",
      "Epoch 8: |          | 573/? [08:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 573, loss 2.809105396270752\n",
      "Epoch 8: |          | 574/? [08:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 574, loss 3.9537177085876465\n",
      "Epoch 8: |          | 575/? [08:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 575, loss 3.2946574687957764\n",
      "Epoch 8: |          | 576/? [08:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 576, loss 3.4859766960144043\n",
      "Epoch 8: |          | 577/? [08:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 577, loss 3.6731438636779785\n",
      "Epoch 8: |          | 578/? [08:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 578, loss 3.954742908477783\n",
      "Epoch 8: |          | 579/? [08:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 579, loss 3.135396957397461\n",
      "Epoch 8: |          | 580/? [08:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 580, loss 3.7728183269500732\n",
      "Epoch 8: |          | 581/? [08:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 581, loss 3.764568328857422\n",
      "Epoch 8: |          | 582/? [08:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 582, loss 3.8249316215515137\n",
      "Epoch 8: |          | 583/? [08:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 583, loss 3.6267940998077393\n",
      "Epoch 8: |          | 584/? [08:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 584, loss 3.846736431121826\n",
      "Epoch 8: |          | 585/? [08:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 585, loss 3.749243974685669\n",
      "Epoch 8: |          | 586/? [08:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 586, loss 3.8451759815216064\n",
      "Epoch 8: |          | 587/? [08:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 587, loss 3.8628172874450684\n",
      "Epoch 8: |          | 588/? [08:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 588, loss 3.7283425331115723\n",
      "Epoch 8: |          | 589/? [08:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 589, loss 3.2748305797576904\n",
      "Epoch 8: |          | 590/? [08:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 590, loss 3.768951892852783\n",
      "Epoch 8: |          | 591/? [08:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 591, loss 3.7019283771514893\n",
      "Epoch 8: |          | 592/? [08:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 592, loss 3.3654632568359375\n",
      "Epoch 8: |          | 593/? [08:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 593, loss 3.7396323680877686\n",
      "Epoch 8: |          | 594/? [08:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 594, loss 4.456192970275879\n",
      "Epoch 8: |          | 595/? [08:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 595, loss 3.2812752723693848\n",
      "Epoch 8: |          | 596/? [08:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 596, loss 3.3336310386657715\n",
      "Epoch 8: |          | 597/? [08:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 597, loss 3.533254623413086\n",
      "Epoch 8: |          | 598/? [08:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 598, loss 3.9480319023132324\n",
      "Epoch 8: |          | 599/? [08:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 599, loss 3.667938709259033\n",
      "Epoch 8: |          | 600/? [08:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 600, loss 3.439509630203247\n",
      "Epoch 8: |          | 601/? [08:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 601, loss 3.718621015548706\n",
      "Epoch 8: |          | 602/? [08:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 602, loss 3.286970615386963\n",
      "Epoch 8: |          | 603/? [08:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 603, loss 3.374001979827881\n",
      "Epoch 8: |          | 604/? [08:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 604, loss 4.671896457672119\n",
      "Epoch 8: |          | 605/? [08:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 605, loss 3.1757490634918213\n",
      "Epoch 8: |          | 606/? [08:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 606, loss 3.481299877166748\n",
      "Epoch 8: |          | 607/? [08:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 607, loss 3.771416187286377\n",
      "Epoch 8: |          | 608/? [08:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 608, loss 3.5438785552978516\n",
      "Epoch 8: |          | 609/? [08:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 609, loss 3.511298418045044\n",
      "Epoch 8: |          | 610/? [08:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 610, loss 3.574017286300659\n",
      "Epoch 8: |          | 611/? [08:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 611, loss 3.683424711227417\n",
      "Epoch 8: |          | 612/? [08:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 612, loss 3.4309089183807373\n",
      "Epoch 8: |          | 613/? [08:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 613, loss 3.7453079223632812\n",
      "Epoch 8: |          | 614/? [08:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 614, loss 3.542931318283081\n",
      "Epoch 8: |          | 615/? [08:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 615, loss 4.0659942626953125\n",
      "Epoch 8: |          | 616/? [08:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 616, loss 4.186111927032471\n",
      "Epoch 8: |          | 617/? [08:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 617, loss 2.8294763565063477\n",
      "Epoch 8: |          | 618/? [08:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 618, loss 3.7570815086364746\n",
      "Epoch 8: |          | 619/? [08:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 619, loss 3.3243918418884277\n",
      "Epoch 8: |          | 620/? [08:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 620, loss 3.847105026245117\n",
      "Epoch 8: |          | 621/? [08:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 621, loss 3.3495521545410156\n",
      "Epoch 8: |          | 622/? [08:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 622, loss 3.156000852584839\n",
      "Epoch 8: |          | 623/? [08:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 623, loss 2.9803466796875\n",
      "Epoch 8: |          | 624/? [08:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 624, loss 2.710495948791504\n",
      "Epoch 8: |          | 625/? [08:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 625, loss 4.089526176452637\n",
      "Epoch 8: |          | 626/? [08:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 626, loss 3.5586636066436768\n",
      "Epoch 8: |          | 627/? [08:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 627, loss 3.4857468605041504\n",
      "Epoch 8: |          | 628/? [08:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 628, loss 3.5261027812957764\n",
      "Epoch 8: |          | 629/? [08:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 629, loss 3.874411106109619\n",
      "Epoch 8: |          | 630/? [08:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 630, loss 3.649829864501953\n",
      "Epoch 8: |          | 631/? [08:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 631, loss 3.745692014694214\n",
      "Epoch 8: |          | 632/? [08:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 632, loss 3.1469414234161377\n",
      "Epoch 8: |          | 633/? [08:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 633, loss 3.864964723587036\n",
      "Epoch 8: |          | 634/? [08:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 634, loss 3.4167656898498535\n",
      "Epoch 8: |          | 635/? [09:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 635, loss 3.2730231285095215\n",
      "Epoch 8: |          | 636/? [09:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 636, loss 3.6461024284362793\n",
      "Epoch 8: |          | 637/? [09:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 637, loss 3.4956459999084473\n",
      "Epoch 8: |          | 638/? [09:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 638, loss 3.7007675170898438\n",
      "Epoch 8: |          | 639/? [09:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 639, loss 3.456472873687744\n",
      "Epoch 8: |          | 640/? [09:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 640, loss 3.9867935180664062\n",
      "Epoch 8: |          | 641/? [09:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 641, loss 3.0220513343811035\n",
      "Epoch 8: |          | 642/? [09:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 642, loss 3.8402323722839355\n",
      "Epoch 8: |          | 643/? [09:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 643, loss 3.671672821044922\n",
      "Epoch 8: |          | 644/? [09:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 644, loss 3.688678741455078\n",
      "Epoch 8: |          | 645/? [09:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 645, loss 3.394620418548584\n",
      "Epoch 8: |          | 646/? [09:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 646, loss 3.3983139991760254\n",
      "Epoch 8: |          | 647/? [09:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 647, loss 3.948580503463745\n",
      "Epoch 8: |          | 648/? [09:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 648, loss 3.3545403480529785\n",
      "Epoch 8: |          | 649/? [09:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 649, loss 2.8036556243896484\n",
      "Epoch 8: |          | 650/? [09:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 650, loss 3.9070301055908203\n",
      "Epoch 8: |          | 651/? [09:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 651, loss 4.046030044555664\n",
      "Epoch 8: |          | 652/? [09:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 652, loss 3.5317184925079346\n",
      "Epoch 8: |          | 653/? [09:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 653, loss 3.6864733695983887\n",
      "Epoch 8: |          | 654/? [09:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 654, loss 3.7258498668670654\n",
      "Epoch 8: |          | 655/? [09:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 655, loss 3.51050066947937\n",
      "Epoch 8: |          | 656/? [09:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 656, loss 3.2093334197998047\n",
      "Epoch 8: |          | 657/? [09:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 657, loss 5.424750328063965\n",
      "Epoch 8: |          | 658/? [09:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 658, loss 3.0694680213928223\n",
      "Epoch 8: |          | 659/? [09:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 659, loss 3.6368823051452637\n",
      "Epoch 8: |          | 660/? [09:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 660, loss 3.9786765575408936\n",
      "Epoch 8: |          | 661/? [09:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 661, loss 3.918567180633545\n",
      "Epoch 8: |          | 662/? [09:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 662, loss 3.7557168006896973\n",
      "Epoch 8: |          | 663/? [09:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 663, loss 3.524620771408081\n",
      "Epoch 8: |          | 664/? [09:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 664, loss 3.489304780960083\n",
      "Epoch 8: |          | 665/? [09:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 665, loss 3.764051914215088\n",
      "Epoch 8: |          | 666/? [09:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 666, loss 3.5763840675354004\n",
      "Epoch 8: |          | 667/? [09:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 667, loss 4.352677822113037\n",
      "Epoch 8: |          | 668/? [09:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 668, loss 3.193845748901367\n",
      "Epoch 8: |          | 669/? [09:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 669, loss 3.3725876808166504\n",
      "Epoch 8: |          | 670/? [09:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 670, loss 4.022834300994873\n",
      "Epoch 8: |          | 671/? [09:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 671, loss 3.7837882041931152\n",
      "Epoch 8: |          | 672/? [09:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 672, loss 3.802703857421875\n",
      "Epoch 8: |          | 673/? [09:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 673, loss 3.6576061248779297\n",
      "Epoch 8: |          | 674/? [09:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 674, loss 2.1021339893341064\n",
      "Epoch 8: |          | 675/? [09:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 675, loss 0.7197347283363342\n",
      "Epoch 8: |          | 676/? [09:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 676, loss 0.6688944101333618\n",
      "Epoch 8: |          | 677/? [09:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 677, loss 0.5345574617385864\n",
      "Epoch 8: |          | 678/? [09:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 678, loss 1.6363332271575928\n",
      "Epoch 8: |          | 679/? [09:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 679, loss 3.0504558086395264\n",
      "Epoch 8: |          | 680/? [09:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 680, loss 3.5417284965515137\n",
      "Epoch 8: |          | 681/? [09:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 681, loss 3.1747260093688965\n",
      "Epoch 8: |          | 682/? [09:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 682, loss 3.4311916828155518\n",
      "Epoch 8: |          | 683/? [09:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 683, loss 3.1516950130462646\n",
      "Epoch 8: |          | 684/? [09:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 684, loss 4.131053447723389\n",
      "Epoch 8: |          | 685/? [09:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 685, loss 3.7140426635742188\n",
      "Epoch 8: |          | 686/? [09:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 686, loss 3.3903450965881348\n",
      "Epoch 8: |          | 687/? [09:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 687, loss 3.8702495098114014\n",
      "Epoch 8: |          | 688/? [09:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 688, loss 3.321847915649414\n",
      "Epoch 8: |          | 689/? [09:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 689, loss 3.468898296356201\n",
      "Epoch 8: |          | 690/? [09:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 690, loss 4.125417709350586\n",
      "Epoch 8: |          | 691/? [09:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 691, loss 3.6107583045959473\n",
      "Epoch 8: |          | 692/? [09:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 692, loss 3.6214680671691895\n",
      "Epoch 8: |          | 693/? [09:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 693, loss 4.143706798553467\n",
      "Epoch 8: |          | 694/? [09:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 694, loss 3.5075531005859375\n",
      "Epoch 8: |          | 695/? [09:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 695, loss 4.037989139556885\n",
      "Epoch 8: |          | 696/? [09:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 696, loss 3.4024009704589844\n",
      "Epoch 8: |          | 697/? [09:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 697, loss 3.5504207611083984\n",
      "Epoch 8: |          | 698/? [09:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 698, loss 3.052776575088501\n",
      "Epoch 8: |          | 699/? [09:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 699, loss 3.740837574005127\n",
      "Epoch 8: |          | 700/? [09:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 700, loss 3.796520709991455\n",
      "Epoch 8: |          | 701/? [09:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 701, loss 3.4670212268829346\n",
      "Epoch 8: |          | 702/? [09:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 702, loss 3.720332622528076\n",
      "Epoch 8: |          | 703/? [09:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 703, loss 3.813448429107666\n",
      "Epoch 8: |          | 704/? [09:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 704, loss 3.6750831604003906\n",
      "Epoch 8: |          | 705/? [09:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 705, loss 3.335319995880127\n",
      "Epoch 8: |          | 706/? [09:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 706, loss 3.3959693908691406\n",
      "Epoch 8: |          | 707/? [09:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 707, loss 3.8460090160369873\n",
      "Epoch 8: |          | 708/? [10:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 708, loss 3.5899524688720703\n",
      "Epoch 8: |          | 709/? [10:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 709, loss 3.486527919769287\n",
      "Epoch 8: |          | 710/? [10:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 710, loss 4.011523723602295\n",
      "Epoch 8: |          | 711/? [10:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 711, loss 4.00942850112915\n",
      "Epoch 8: |          | 712/? [10:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 712, loss 3.8054890632629395\n",
      "Epoch 8: |          | 713/? [10:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 713, loss 3.927628755569458\n",
      "Epoch 8: |          | 714/? [10:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 714, loss 3.9257149696350098\n",
      "Epoch 8: |          | 715/? [10:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 715, loss 2.9773361682891846\n",
      "Epoch 8: |          | 716/? [10:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 716, loss 3.6230640411376953\n",
      "Epoch 8: |          | 717/? [10:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 717, loss 3.5609593391418457\n",
      "Epoch 8: |          | 718/? [10:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 718, loss 3.0797784328460693\n",
      "Epoch 8: |          | 719/? [10:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 719, loss 3.5339431762695312\n",
      "Epoch 8: |          | 720/? [10:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 720, loss 3.248633623123169\n",
      "Epoch 8: |          | 721/? [10:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 721, loss 3.912184238433838\n",
      "Epoch 8: |          | 722/? [10:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 722, loss 3.2966041564941406\n",
      "Epoch 8: |          | 723/? [10:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 723, loss 3.7704226970672607\n",
      "Epoch 8: |          | 724/? [10:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 724, loss 3.331019639968872\n",
      "Epoch 8: |          | 725/? [10:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 725, loss 3.316786527633667\n",
      "Epoch 8: |          | 726/? [10:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 726, loss 3.468987226486206\n",
      "Epoch 8: |          | 727/? [10:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 727, loss 3.2837166786193848\n",
      "Epoch 8: |          | 728/? [10:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 728, loss 3.102573871612549\n",
      "Epoch 8: |          | 729/? [10:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 729, loss 3.6046195030212402\n",
      "Epoch 8: |          | 730/? [10:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 730, loss 3.581207275390625\n",
      "Epoch 8: |          | 731/? [10:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 731, loss 3.6794915199279785\n",
      "Epoch 8: |          | 732/? [10:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 732, loss 3.8884735107421875\n",
      "Epoch 8: |          | 733/? [10:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 733, loss 3.6219544410705566\n",
      "Epoch 8: |          | 734/? [10:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 734, loss 3.7216827869415283\n",
      "Epoch 8: |          | 735/? [10:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 735, loss 3.6740810871124268\n",
      "Epoch 8: |          | 736/? [10:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 736, loss 3.3094871044158936\n",
      "Epoch 8: |          | 737/? [10:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 737, loss 4.026205539703369\n",
      "Epoch 8: |          | 738/? [10:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 738, loss 3.2590184211730957\n",
      "Epoch 8: |          | 739/? [10:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 739, loss 3.6857800483703613\n",
      "Epoch 8: |          | 740/? [10:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 740, loss 3.4312186241149902\n",
      "Epoch 8: |          | 741/? [10:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 741, loss 3.556781768798828\n",
      "Epoch 8: |          | 742/? [10:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 742, loss 3.917428493499756\n",
      "Epoch 8: |          | 743/? [10:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 743, loss 3.814180850982666\n",
      "Epoch 8: |          | 744/? [10:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 744, loss 3.7676262855529785\n",
      "Epoch 8: |          | 745/? [10:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 745, loss 3.4174110889434814\n",
      "Epoch 8: |          | 746/? [10:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 746, loss 3.6728732585906982\n",
      "Epoch 8: |          | 747/? [10:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 747, loss 3.4022510051727295\n",
      "Epoch 8: |          | 748/? [10:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 748, loss 2.407186985015869\n",
      "Epoch 8: |          | 749/? [10:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 749, loss 3.5285754203796387\n",
      "Epoch 8: |          | 750/? [10:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 750, loss 3.725705623626709\n",
      "Epoch 8: |          | 751/? [10:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 751, loss 2.112790822982788\n",
      "Epoch 8: |          | 752/? [10:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 752, loss 3.701707124710083\n",
      "Epoch 8: |          | 753/? [10:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 753, loss 2.88916015625\n",
      "Epoch 8: |          | 754/? [10:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 754, loss 3.3850700855255127\n",
      "Epoch 8: |          | 755/? [10:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 755, loss 3.215961456298828\n",
      "Epoch 8: |          | 756/? [10:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 756, loss 3.567142963409424\n",
      "Epoch 8: |          | 757/? [10:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 757, loss 3.619853973388672\n",
      "Epoch 8: |          | 758/? [10:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 758, loss 3.38811993598938\n",
      "Epoch 8: |          | 759/? [10:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 759, loss 3.3157875537872314\n",
      "Epoch 8: |          | 760/? [10:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 760, loss 3.8285727500915527\n",
      "Epoch 8: |          | 761/? [10:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 761, loss 3.8370983600616455\n",
      "Epoch 8: |          | 762/? [10:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 762, loss 3.493140697479248\n",
      "Epoch 8: |          | 763/? [10:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 763, loss 3.5499229431152344\n",
      "Epoch 8: |          | 764/? [10:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 764, loss 3.877089262008667\n",
      "Epoch 8: |          | 765/? [10:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 765, loss 3.686569929122925\n",
      "Epoch 8: |          | 766/? [10:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 766, loss 4.041055202484131\n",
      "Epoch 8: |          | 767/? [10:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 767, loss 4.081247329711914\n",
      "Epoch 8: |          | 768/? [10:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 768, loss 3.6587822437286377\n",
      "Epoch 8: |          | 769/? [10:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 769, loss 2.90195894241333\n",
      "Epoch 8: |          | 770/? [10:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 770, loss 3.4047598838806152\n",
      "Epoch 8: |          | 771/? [10:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 771, loss 4.099700927734375\n",
      "Epoch 8: |          | 772/? [11:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 772, loss 3.8796486854553223\n",
      "Epoch 8: |          | 773/? [11:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 773, loss 3.5875377655029297\n",
      "Epoch 8: |          | 774/? [11:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 774, loss 3.7123591899871826\n",
      "Epoch 8: |          | 775/? [11:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 775, loss 4.12318229675293\n",
      "Epoch 8: |          | 776/? [11:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 776, loss 3.5937423706054688\n",
      "Epoch 8: |          | 777/? [11:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 777, loss 3.455361843109131\n",
      "Epoch 8: |          | 778/? [11:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 778, loss 3.847306728363037\n",
      "Epoch 8: |          | 779/? [11:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 779, loss 4.274650573730469\n",
      "Epoch 8: |          | 780/? [11:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 780, loss 3.316063404083252\n",
      "Epoch 8: |          | 781/? [11:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 781, loss 3.4144985675811768\n",
      "Epoch 8: |          | 782/? [11:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 782, loss 3.752180814743042\n",
      "Epoch 8: |          | 783/? [11:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 783, loss 3.80070424079895\n",
      "Epoch 8: |          | 784/? [11:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 784, loss 3.4355320930480957\n",
      "Epoch 8: |          | 785/? [11:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 785, loss 3.196402072906494\n",
      "Epoch 8: |          | 786/? [11:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 786, loss 4.055300235748291\n",
      "Epoch 8: |          | 787/? [11:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 787, loss 3.9726505279541016\n",
      "Epoch 8: |          | 788/? [11:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 788, loss 1.8551428318023682\n",
      "Epoch 8: |          | 789/? [11:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 789, loss 3.509115695953369\n",
      "Epoch 8: |          | 790/? [11:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 790, loss 4.321275234222412\n",
      "Epoch 8: |          | 791/? [11:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 791, loss 4.020719051361084\n",
      "Epoch 8: |          | 792/? [11:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 792, loss 3.3173210620880127\n",
      "Epoch 8: |          | 793/? [11:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 793, loss 3.757664442062378\n",
      "Epoch 8: |          | 794/? [11:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 794, loss 4.044490814208984\n",
      "Epoch 8: |          | 795/? [11:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 795, loss 3.5749993324279785\n",
      "Epoch 8: |          | 796/? [11:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 796, loss 3.9276251792907715\n",
      "Epoch 8: |          | 797/? [11:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 797, loss 2.9852700233459473\n",
      "Epoch 8: |          | 798/? [11:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 798, loss 3.087836742401123\n",
      "Epoch 8: |          | 799/? [11:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 799, loss 3.973041534423828\n",
      "Epoch 8: |          | 800/? [11:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 800, loss 3.8120474815368652\n",
      "Epoch 8: |          | 801/? [11:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 801, loss 3.4191386699676514\n",
      "Epoch 8: |          | 802/? [11:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 802, loss 3.6858184337615967\n",
      "Epoch 8: |          | 803/? [11:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 803, loss 3.5216777324676514\n",
      "Epoch 8: |          | 804/? [11:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 804, loss 3.690004348754883\n",
      "Epoch 8: |          | 805/? [11:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 805, loss 3.770684003829956\n",
      "Epoch 8: |          | 806/? [11:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 806, loss 4.311450004577637\n",
      "Epoch 8: |          | 807/? [11:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 807, loss 3.6340889930725098\n",
      "Epoch 8: |          | 808/? [11:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 808, loss 3.3242619037628174\n",
      "Epoch 8: |          | 809/? [11:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 809, loss 3.7993171215057373\n",
      "Epoch 8: |          | 810/? [11:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 810, loss 3.5796477794647217\n",
      "Epoch 8: |          | 811/? [11:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 811, loss 3.8173327445983887\n",
      "Epoch 8: |          | 812/? [11:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 812, loss 4.385964870452881\n",
      "Epoch 8: |          | 813/? [11:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 813, loss 4.210595607757568\n",
      "Epoch 8: |          | 814/? [11:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 814, loss 3.284738063812256\n",
      "Epoch 8: |          | 815/? [11:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 815, loss 3.9759628772735596\n",
      "Epoch 8: |          | 816/? [11:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 816, loss 3.7681145668029785\n",
      "Epoch 8: |          | 817/? [11:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 817, loss 3.1410155296325684\n",
      "Epoch 8: |          | 818/? [11:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 818, loss 4.049524784088135\n",
      "Epoch 8: |          | 819/? [11:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 819, loss 3.7967123985290527\n",
      "Epoch 8: |          | 820/? [11:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 820, loss 3.626479387283325\n",
      "Epoch 8: |          | 821/? [11:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 821, loss 3.5744423866271973\n",
      "Epoch 8: |          | 822/? [11:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 822, loss 3.303170680999756\n",
      "Epoch 8: |          | 823/? [11:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 823, loss 3.338925838470459\n",
      "Epoch 8: |          | 824/? [11:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 824, loss 3.758985996246338\n",
      "Epoch 8: |          | 825/? [11:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 825, loss 3.3476932048797607\n",
      "Epoch 8: |          | 826/? [11:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 826, loss 3.855095624923706\n",
      "Epoch 8: |          | 827/? [11:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 827, loss 3.5036628246307373\n",
      "Epoch 8: |          | 828/? [11:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 828, loss 3.916651964187622\n",
      "Epoch 8: |          | 829/? [11:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 829, loss 3.649878740310669\n",
      "Epoch 8: |          | 830/? [11:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 830, loss 4.182936191558838\n",
      "Epoch 8: |          | 831/? [11:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 831, loss 2.185117244720459\n",
      "Epoch 8: |          | 832/? [11:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 832, loss 3.5708961486816406\n",
      "Epoch 8: |          | 833/? [11:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 833, loss 3.4905624389648438\n",
      "Epoch 8: |          | 834/? [11:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 834, loss 4.203293800354004\n",
      "Epoch 8: |          | 835/? [11:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 835, loss 3.5551536083221436\n",
      "Epoch 8: |          | 836/? [11:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 836, loss 4.1781206130981445\n",
      "Epoch 8: |          | 837/? [11:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 837, loss 3.623053789138794\n",
      "Epoch 8: |          | 838/? [11:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 838, loss 3.034846067428589\n",
      "Epoch 8: |          | 839/? [11:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 839, loss 3.399148464202881\n",
      "Epoch 8: |          | 840/? [11:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 840, loss 3.918895721435547\n",
      "Epoch 8: |          | 841/? [12:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 841, loss 3.93280029296875\n",
      "Epoch 8: |          | 842/? [12:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 842, loss 3.630169630050659\n",
      "Epoch 8: |          | 843/? [12:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 843, loss 3.9331603050231934\n",
      "Epoch 8: |          | 844/? [12:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 844, loss 3.3205084800720215\n",
      "Epoch 8: |          | 845/? [12:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 845, loss 3.7024567127227783\n",
      "Epoch 8: |          | 846/? [12:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 846, loss 4.114743709564209\n",
      "Epoch 8: |          | 847/? [12:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 847, loss 3.7511463165283203\n",
      "Epoch 8: |          | 848/? [12:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 848, loss 3.3333728313446045\n",
      "Epoch 8: |          | 849/? [12:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 849, loss 3.3822951316833496\n",
      "Epoch 8: |          | 850/? [12:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 850, loss 3.49446177482605\n",
      "Epoch 8: |          | 851/? [12:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 851, loss 3.773205518722534\n",
      "Epoch 8: |          | 852/? [12:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 852, loss 3.9546756744384766\n",
      "Epoch 8: |          | 853/? [12:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 853, loss 3.7145683765411377\n",
      "Epoch 8: |          | 854/? [12:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 854, loss 3.1483449935913086\n",
      "Epoch 8: |          | 855/? [12:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 855, loss 3.3447108268737793\n",
      "Epoch 8: |          | 856/? [12:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 856, loss 3.307767391204834\n",
      "Epoch 8: |          | 857/? [12:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 857, loss 3.8206474781036377\n",
      "Epoch 8: |          | 858/? [12:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 858, loss 3.738171339035034\n",
      "Epoch 8: |          | 859/? [12:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 859, loss 3.729344606399536\n",
      "Epoch 8: |          | 860/? [12:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 860, loss 4.079678058624268\n",
      "Epoch 8: |          | 861/? [12:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 861, loss 3.4550673961639404\n",
      "Epoch 8: |          | 862/? [12:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 862, loss 3.7414145469665527\n",
      "Epoch 8: |          | 863/? [12:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 863, loss 3.1666502952575684\n",
      "Epoch 8: |          | 864/? [12:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 864, loss 3.6819934844970703\n",
      "Epoch 8: |          | 865/? [12:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 865, loss 3.7153327465057373\n",
      "Epoch 8: |          | 866/? [12:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 866, loss 2.7623844146728516\n",
      "Epoch 8: |          | 867/? [12:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 867, loss 2.887709140777588\n",
      "Epoch 8: |          | 868/? [12:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 868, loss 3.7573070526123047\n",
      "Epoch 8: |          | 869/? [12:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 869, loss 3.841958999633789\n",
      "Epoch 8: |          | 870/? [12:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 870, loss 3.461202621459961\n",
      "Epoch 8: |          | 871/? [12:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 871, loss 3.776808977127075\n",
      "Epoch 8: |          | 872/? [12:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 872, loss 3.603567600250244\n",
      "Epoch 8: |          | 873/? [12:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 873, loss 3.58984637260437\n",
      "Epoch 8: |          | 874/? [12:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 874, loss 3.1293766498565674\n",
      "Epoch 8: |          | 875/? [12:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 875, loss 3.810539960861206\n",
      "Epoch 8: |          | 876/? [12:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 876, loss 3.3382809162139893\n",
      "Epoch 8: |          | 877/? [12:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 877, loss 3.786560535430908\n",
      "Epoch 8: |          | 878/? [12:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 878, loss 3.2103095054626465\n",
      "Epoch 8: |          | 879/? [12:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 879, loss 3.2674362659454346\n",
      "Epoch 8: |          | 880/? [12:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 880, loss 4.321424961090088\n",
      "Epoch 8: |          | 881/? [12:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 881, loss 3.74236798286438\n",
      "Epoch 8: |          | 882/? [12:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 882, loss 3.558781385421753\n",
      "Epoch 8: |          | 883/? [12:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 883, loss 3.7183685302734375\n",
      "Epoch 8: |          | 884/? [12:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 884, loss 3.6775550842285156\n",
      "Epoch 8: |          | 885/? [12:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 885, loss 3.4836831092834473\n",
      "Epoch 8: |          | 886/? [12:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 886, loss 4.1846442222595215\n",
      "Epoch 8: |          | 887/? [12:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 887, loss 4.135619163513184\n",
      "Epoch 8: |          | 888/? [12:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 888, loss 3.85363507270813\n",
      "Epoch 8: |          | 889/? [12:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 889, loss 3.425060749053955\n",
      "Epoch 8: |          | 890/? [12:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 890, loss 3.6081531047821045\n",
      "Epoch 8: |          | 891/? [12:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 891, loss 3.4860618114471436\n",
      "Epoch 8: |          | 892/? [12:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 892, loss 4.046956539154053\n",
      "Epoch 8: |          | 893/? [12:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 893, loss 3.5135180950164795\n",
      "Epoch 8: |          | 894/? [12:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 894, loss 3.0217177867889404\n",
      "Epoch 8: |          | 895/? [12:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 895, loss 4.158188343048096\n",
      "Epoch 8: |          | 896/? [12:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 896, loss 3.7389800548553467\n",
      "Epoch 8: |          | 897/? [12:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 897, loss 3.7524313926696777\n",
      "Epoch 8: |          | 898/? [12:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 898, loss 3.7132275104522705\n",
      "Epoch 8: |          | 899/? [12:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 899, loss 3.535292387008667\n",
      "Epoch 8: |          | 900/? [12:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 900, loss 3.459324359893799\n",
      "Epoch 8: |          | 901/? [12:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 901, loss 3.816702365875244\n",
      "Epoch 8: |          | 902/? [12:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 902, loss 3.9553444385528564\n",
      "Epoch 8: |          | 903/? [12:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 903, loss 3.273651599884033\n",
      "Epoch 8: |          | 904/? [12:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 904, loss 3.7574474811553955\n",
      "Epoch 8: |          | 905/? [12:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 905, loss 3.908482789993286\n",
      "Epoch 8: |          | 906/? [12:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 906, loss 3.6866981983184814\n",
      "Epoch 8: |          | 907/? [12:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 907, loss 3.742689847946167\n",
      "Epoch 8: |          | 908/? [12:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 908, loss 3.853503704071045\n",
      "Epoch 8: |          | 909/? [12:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 909, loss 3.787797451019287\n",
      "Epoch 8: |          | 910/? [12:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 910, loss 3.5162551403045654\n",
      "Epoch 8: |          | 911/? [12:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 911, loss 3.6292006969451904\n",
      "Epoch 8: |          | 912/? [13:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 912, loss 3.5903191566467285\n",
      "Epoch 8: |          | 913/? [13:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 913, loss 3.589496612548828\n",
      "Epoch 8: |          | 914/? [13:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 914, loss 3.8471150398254395\n",
      "Epoch 8: |          | 915/? [13:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 915, loss 3.6884474754333496\n",
      "Epoch 8: |          | 916/? [13:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 916, loss 3.660735607147217\n",
      "Epoch 8: |          | 917/? [13:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 917, loss 3.592233180999756\n",
      "Epoch 8: |          | 918/? [13:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 918, loss 3.505908250808716\n",
      "Epoch 8: |          | 919/? [13:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 919, loss 3.541469097137451\n",
      "Epoch 8: |          | 920/? [13:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 920, loss 3.686868190765381\n",
      "Epoch 8: |          | 921/? [13:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 921, loss 3.4914817810058594\n",
      "Epoch 8: |          | 922/? [13:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 922, loss 3.6350860595703125\n",
      "Epoch 8: |          | 923/? [13:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 923, loss 3.500347852706909\n",
      "Epoch 8: |          | 924/? [13:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 924, loss 3.5323987007141113\n",
      "Epoch 8: |          | 925/? [13:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 925, loss 3.8236522674560547\n",
      "Epoch 8: |          | 926/? [13:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 926, loss 3.5894055366516113\n",
      "Epoch 8: |          | 927/? [13:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 927, loss 3.8788809776306152\n",
      "Epoch 8: |          | 928/? [13:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 928, loss 3.3760650157928467\n",
      "Epoch 8: |          | 929/? [13:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 929, loss 3.470200777053833\n",
      "Epoch 8: |          | 930/? [13:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 930, loss 3.393909454345703\n",
      "Epoch 8: |          | 931/? [13:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 931, loss 3.1197569370269775\n",
      "Epoch 8: |          | 932/? [13:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 932, loss 3.73160982131958\n",
      "Epoch 8: |          | 933/? [13:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 933, loss 3.5061278343200684\n",
      "Epoch 8: |          | 934/? [13:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 934, loss 4.013842582702637\n",
      "Epoch 8: |          | 935/? [13:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 935, loss 4.346435070037842\n",
      "Epoch 8: |          | 936/? [13:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 936, loss 3.6083285808563232\n",
      "Epoch 8: |          | 937/? [13:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 937, loss 3.3517658710479736\n",
      "Epoch 8: |          | 938/? [13:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 938, loss 3.5534865856170654\n",
      "Epoch 8: |          | 939/? [13:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 939, loss 3.773535966873169\n",
      "Epoch 8: |          | 940/? [13:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 940, loss 3.942958116531372\n",
      "Epoch 8: |          | 941/? [13:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 941, loss 3.5080726146698\n",
      "Epoch 8: |          | 942/? [13:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 942, loss 2.9833731651306152\n",
      "Epoch 8: |          | 943/? [13:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 943, loss 3.805575132369995\n",
      "Epoch 8: |          | 944/? [13:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 944, loss 2.944685459136963\n",
      "Epoch 8: |          | 945/? [13:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 945, loss 3.6423492431640625\n",
      "Epoch 8: |          | 946/? [13:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 946, loss 3.5433459281921387\n",
      "Epoch 8: |          | 947/? [13:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 947, loss 3.4545960426330566\n",
      "Epoch 8: |          | 948/? [13:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 948, loss 3.736466884613037\n",
      "Epoch 8: |          | 949/? [13:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 949, loss 3.5723469257354736\n",
      "Epoch 8: |          | 950/? [13:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 950, loss 3.3598008155822754\n",
      "Epoch 8: |          | 951/? [13:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 951, loss 3.9892590045928955\n",
      "Epoch 8: |          | 952/? [13:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 952, loss 3.9095816612243652\n",
      "Epoch 8: |          | 953/? [13:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 953, loss 4.446610450744629\n",
      "Epoch 8: |          | 954/? [13:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 954, loss 3.4633727073669434\n",
      "Epoch 8: |          | 955/? [13:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 955, loss 4.062338829040527\n",
      "Epoch 8: |          | 956/? [13:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 956, loss 3.5975375175476074\n",
      "Epoch 8: |          | 957/? [13:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 957, loss 3.7269511222839355\n",
      "Epoch 8: |          | 958/? [13:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 958, loss 3.7550530433654785\n",
      "Epoch 8: |          | 959/? [13:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 959, loss 3.162264585494995\n",
      "Epoch 8: |          | 960/? [13:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 960, loss 3.8440120220184326\n",
      "Epoch 8: |          | 961/? [13:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 961, loss 4.128006935119629\n",
      "Epoch 8: |          | 962/? [13:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 962, loss 3.6679370403289795\n",
      "Epoch 8: |          | 963/? [13:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 963, loss 3.4613311290740967\n",
      "Epoch 8: |          | 964/? [13:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 964, loss 3.8767552375793457\n",
      "Epoch 8: |          | 965/? [13:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 965, loss 3.3401806354522705\n",
      "Epoch 8: |          | 966/? [13:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 966, loss 3.301162004470825\n",
      "Epoch 8: |          | 967/? [13:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 967, loss 3.5235679149627686\n",
      "Epoch 8: |          | 968/? [13:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 968, loss 3.431516647338867\n",
      "Epoch 8: |          | 969/? [13:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 969, loss 3.3691420555114746\n",
      "Epoch 8: |          | 970/? [13:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 970, loss 3.7707619667053223\n",
      "Epoch 8: |          | 971/? [13:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 971, loss 3.9428744316101074\n",
      "Epoch 8: |          | 972/? [13:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 972, loss 3.4202656745910645\n",
      "Epoch 8: |          | 973/? [13:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 973, loss 3.600991725921631\n",
      "Epoch 8: |          | 974/? [13:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 974, loss 3.691737651824951\n",
      "Epoch 8: |          | 975/? [13:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 975, loss 3.720886707305908\n",
      "Epoch 8: |          | 976/? [13:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 976, loss 3.725362777709961\n",
      "Epoch 8: |          | 977/? [13:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 977, loss 4.281434059143066\n",
      "Epoch 8: |          | 978/? [13:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 978, loss 3.7818362712860107\n",
      "Epoch 8: |          | 979/? [13:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 979, loss 3.9848875999450684\n",
      "Epoch 8: |          | 980/? [13:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 980, loss 3.193647861480713\n",
      "Epoch 8: |          | 981/? [13:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 981, loss 3.108555555343628\n",
      "Epoch 8: |          | 982/? [13:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 982, loss 3.6919357776641846\n",
      "Epoch 8: |          | 983/? [13:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 983, loss 4.13021993637085\n",
      "Epoch 8: |          | 984/? [14:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 984, loss 3.2182681560516357\n",
      "Epoch 8: |          | 985/? [14:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 985, loss 3.438079833984375\n",
      "Epoch 8: |          | 986/? [14:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 986, loss 3.4797539710998535\n",
      "Epoch 8: |          | 987/? [14:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 987, loss 2.990902900695801\n",
      "Epoch 8: |          | 988/? [14:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 988, loss 4.023777961730957\n",
      "Epoch 8: |          | 989/? [14:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 989, loss 3.6571877002716064\n",
      "Epoch 8: |          | 990/? [14:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 990, loss 3.0752720832824707\n",
      "Epoch 8: |          | 991/? [14:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 991, loss 3.7693276405334473\n",
      "Epoch 8: |          | 992/? [14:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 992, loss 4.370013236999512\n",
      "Epoch 8: |          | 993/? [14:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 993, loss 3.508831739425659\n",
      "Epoch 8: |          | 994/? [14:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 994, loss 3.5078399181365967\n",
      "Epoch 8: |          | 995/? [14:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 995, loss 3.938288450241089\n",
      "Epoch 8: |          | 996/? [14:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 996, loss 3.8774025440216064\n",
      "Epoch 8: |          | 997/? [14:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 997, loss 3.548978090286255\n",
      "Epoch 8: |          | 998/? [14:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 998, loss 3.7574706077575684\n",
      "Epoch 8: |          | 999/? [14:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 999, loss 3.7170207500457764\n",
      "Epoch 8: |          | 1000/? [14:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1000, loss 3.242793321609497\n",
      "Epoch 8: |          | 1001/? [14:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1001, loss 3.911402940750122\n",
      "Epoch 8: |          | 1002/? [14:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1002, loss 3.851961851119995\n",
      "Epoch 8: |          | 1003/? [14:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1003, loss 4.0698771476745605\n",
      "Epoch 8: |          | 1004/? [14:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1004, loss 3.1587181091308594\n",
      "Epoch 8: |          | 1005/? [14:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1005, loss 3.630760908126831\n",
      "Epoch 8: |          | 1006/? [14:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1006, loss 3.9280409812927246\n",
      "Epoch 8: |          | 1007/? [14:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1007, loss 3.5129222869873047\n",
      "Epoch 8: |          | 1008/? [14:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1008, loss 3.6619250774383545\n",
      "Epoch 8: |          | 1009/? [14:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1009, loss 3.9005675315856934\n",
      "Epoch 8: |          | 1010/? [14:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1010, loss 3.095951795578003\n",
      "Epoch 8: |          | 1011/? [14:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1011, loss 3.6379902362823486\n",
      "Epoch 8: |          | 1012/? [14:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1012, loss 3.4662086963653564\n",
      "Epoch 8: |          | 1013/? [14:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1013, loss 3.5636162757873535\n",
      "Epoch 8: |          | 1014/? [14:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1014, loss 4.039750099182129\n",
      "Epoch 8: |          | 1015/? [14:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1015, loss 3.6879334449768066\n",
      "Epoch 8: |          | 1016/? [14:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1016, loss 3.469465970993042\n",
      "Epoch 8: |          | 1017/? [14:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1017, loss 2.9397969245910645\n",
      "Epoch 8: |          | 1018/? [14:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1018, loss 3.564096450805664\n",
      "Epoch 8: |          | 1019/? [14:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1019, loss 3.640989303588867\n",
      "Epoch 8: |          | 1020/? [14:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1020, loss 3.2259700298309326\n",
      "Epoch 8: |          | 1021/? [14:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1021, loss 3.5011329650878906\n",
      "Epoch 8: |          | 1022/? [14:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1022, loss 3.2989985942840576\n",
      "Epoch 8: |          | 1023/? [14:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1023, loss 3.036353826522827\n",
      "Epoch 8: |          | 1024/? [14:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1024, loss 3.4655299186706543\n",
      "Epoch 8: |          | 1025/? [14:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1025, loss 3.374547243118286\n",
      "Epoch 8: |          | 1026/? [14:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1026, loss 2.571321964263916\n",
      "Epoch 8: |          | 1027/? [14:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1027, loss 3.7023048400878906\n",
      "Epoch 8: |          | 1028/? [14:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1028, loss 3.5422768592834473\n",
      "Epoch 8: |          | 1029/? [14:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1029, loss 3.4178566932678223\n",
      "Epoch 8: |          | 1030/? [14:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1030, loss 3.2743515968322754\n",
      "Epoch 8: |          | 1031/? [14:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1031, loss 3.3249919414520264\n",
      "Epoch 8: |          | 1032/? [14:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1032, loss 3.7751288414001465\n",
      "Epoch 8: |          | 1033/? [14:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1033, loss 3.9921493530273438\n",
      "Epoch 8: |          | 1034/? [14:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1034, loss 3.403609037399292\n",
      "Epoch 8: |          | 1035/? [14:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1035, loss 3.403740406036377\n",
      "Epoch 8: |          | 1036/? [14:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1036, loss 3.396909236907959\n",
      "Epoch 8: |          | 1037/? [14:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1037, loss 3.957915782928467\n",
      "Epoch 8: |          | 1038/? [14:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1038, loss 4.123358726501465\n",
      "Epoch 8: |          | 1039/? [14:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1039, loss 4.438471794128418\n",
      "Epoch 8: |          | 1040/? [14:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1040, loss 3.7401041984558105\n",
      "Epoch 8: |          | 1041/? [14:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1041, loss 4.0067620277404785\n",
      "Epoch 8: |          | 1042/? [14:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1042, loss 3.6105265617370605\n",
      "Epoch 8: |          | 1043/? [14:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1043, loss 3.9453330039978027\n",
      "Epoch 8: |          | 1044/? [14:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1044, loss 3.5266788005828857\n",
      "Epoch 8: |          | 1045/? [14:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1045, loss 3.131352186203003\n",
      "Epoch 8: |          | 1046/? [14:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1046, loss 3.0170416831970215\n",
      "Epoch 8: |          | 1047/? [14:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1047, loss 4.034857749938965\n",
      "Epoch 8: |          | 1048/? [14:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1048, loss 3.587947130203247\n",
      "Epoch 8: |          | 1049/? [14:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1049, loss 3.797506332397461\n",
      "Epoch 8: |          | 1050/? [14:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1050, loss 3.365370988845825\n",
      "Epoch 8: |          | 1051/? [14:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1051, loss 3.2729389667510986\n",
      "Epoch 8: |          | 1052/? [14:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1052, loss 3.8898208141326904\n",
      "Epoch 8: |          | 1053/? [14:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1053, loss 4.074973106384277\n",
      "Epoch 8: |          | 1054/? [14:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1054, loss 3.5206406116485596\n",
      "Epoch 8: |          | 1055/? [15:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1055, loss 3.1860766410827637\n",
      "Epoch 8: |          | 1056/? [15:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1056, loss 3.176640033721924\n",
      "Epoch 8: |          | 1057/? [15:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1057, loss 3.7943332195281982\n",
      "Epoch 8: |          | 1058/? [15:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1058, loss 3.3875155448913574\n",
      "Epoch 8: |          | 1059/? [15:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1059, loss 3.9716389179229736\n",
      "Epoch 8: |          | 1060/? [15:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1060, loss 3.8074240684509277\n",
      "Epoch 8: |          | 1061/? [15:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1061, loss 2.616603374481201\n",
      "Epoch 8: |          | 1062/? [15:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1062, loss 3.44132924079895\n",
      "Epoch 8: |          | 1063/? [15:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1063, loss 3.5496463775634766\n",
      "Epoch 8: |          | 1064/? [15:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1064, loss 3.7240734100341797\n",
      "Epoch 8: |          | 1065/? [15:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1065, loss 2.391810894012451\n",
      "Epoch 8: |          | 1066/? [15:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1066, loss 3.6943206787109375\n",
      "Epoch 8: |          | 1067/? [15:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1067, loss 3.2264811992645264\n",
      "Epoch 8: |          | 1068/? [15:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1068, loss 3.357740879058838\n",
      "Epoch 8: |          | 1069/? [15:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1069, loss 3.7200393676757812\n",
      "Epoch 8: |          | 1070/? [15:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1070, loss 3.515601634979248\n",
      "Epoch 8: |          | 1071/? [15:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1071, loss 3.873729705810547\n",
      "Epoch 8: |          | 1072/? [15:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1072, loss 3.9202914237976074\n",
      "Epoch 8: |          | 1073/? [15:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1073, loss 4.024770259857178\n",
      "Epoch 8: |          | 1074/? [15:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1074, loss 3.4043750762939453\n",
      "Epoch 8: |          | 1075/? [15:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1075, loss 3.258650302886963\n",
      "Epoch 8: |          | 1076/? [15:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1076, loss 3.8154773712158203\n",
      "Epoch 8: |          | 1077/? [15:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1077, loss 3.3515841960906982\n",
      "Epoch 8: |          | 1078/? [15:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1078, loss 3.612637758255005\n",
      "Epoch 8: |          | 1079/? [15:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1079, loss 3.929932117462158\n",
      "Epoch 8: |          | 1080/? [15:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1080, loss 3.5381863117218018\n",
      "Epoch 8: |          | 1081/? [15:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1081, loss 3.8259644508361816\n",
      "Epoch 8: |          | 1082/? [15:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1082, loss 3.480620861053467\n",
      "Epoch 8: |          | 1083/? [15:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1083, loss 2.974682092666626\n",
      "Epoch 8: |          | 1084/? [15:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1084, loss 2.8610289096832275\n",
      "Epoch 8: |          | 1085/? [15:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1085, loss 3.497187852859497\n",
      "Epoch 8: |          | 1086/? [15:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1086, loss 3.7563724517822266\n",
      "Epoch 8: |          | 1087/? [15:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1087, loss 4.195807456970215\n",
      "Epoch 8: |          | 1088/? [15:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1088, loss 3.826003313064575\n",
      "Epoch 8: |          | 1089/? [15:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1089, loss 3.701005458831787\n",
      "Epoch 8: |          | 1090/? [15:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1090, loss 3.5572898387908936\n",
      "Epoch 8: |          | 1091/? [15:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1091, loss 3.388597011566162\n",
      "Epoch 8: |          | 1092/? [15:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1092, loss 3.673617124557495\n",
      "Epoch 8: |          | 1093/? [15:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1093, loss 3.189586639404297\n",
      "Epoch 8: |          | 1094/? [15:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1094, loss 3.685631513595581\n",
      "Epoch 8: |          | 1095/? [15:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1095, loss 3.6835403442382812\n",
      "Epoch 8: |          | 1096/? [15:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1096, loss 3.954554319381714\n",
      "Epoch 8: |          | 1097/? [15:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1097, loss 3.5610878467559814\n",
      "Epoch 8: |          | 1098/? [15:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1098, loss 2.9128646850585938\n",
      "Epoch 8: |          | 1099/? [15:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1099, loss 3.4882941246032715\n",
      "Epoch 8: |          | 1100/? [15:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1100, loss 3.6958682537078857\n",
      "Epoch 8: |          | 1101/? [15:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1101, loss 3.3489041328430176\n",
      "Epoch 8: |          | 1102/? [15:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1102, loss 4.057051181793213\n",
      "Epoch 8: |          | 1103/? [15:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1103, loss 4.3955841064453125\n",
      "Epoch 8: |          | 1104/? [15:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1104, loss 3.8696041107177734\n",
      "Epoch 8: |          | 1105/? [15:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1105, loss 3.9837520122528076\n",
      "Epoch 8: |          | 1106/? [15:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1106, loss 3.4884448051452637\n",
      "Epoch 8: |          | 1107/? [15:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1107, loss 3.6557469367980957\n",
      "Epoch 8: |          | 1108/? [15:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1108, loss 3.6531028747558594\n",
      "Epoch 8: |          | 1109/? [15:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1109, loss 3.276738405227661\n",
      "Epoch 8: |          | 1110/? [15:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1110, loss 4.142432689666748\n",
      "Epoch 8: |          | 1111/? [15:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1111, loss 3.8154845237731934\n",
      "Epoch 8: |          | 1112/? [15:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1112, loss 3.677748441696167\n",
      "Epoch 8: |          | 1113/? [15:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1113, loss 3.5113937854766846\n",
      "Epoch 8: |          | 1114/? [15:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1114, loss 2.998661994934082\n",
      "Epoch 8: |          | 1115/? [15:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1115, loss 2.8257200717926025\n",
      "Epoch 8: |          | 1116/? [15:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1116, loss 3.1304311752319336\n",
      "Epoch 8: |          | 1117/? [15:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1117, loss 3.318458080291748\n",
      "Epoch 8: |          | 1118/? [15:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1118, loss 3.4543843269348145\n",
      "Epoch 8: |          | 1119/? [15:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1119, loss 4.033910751342773\n",
      "Epoch 8: |          | 1120/? [15:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1120, loss 3.5871810913085938\n",
      "Epoch 8: |          | 1121/? [15:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1121, loss 3.8168625831604004\n",
      "Epoch 8: |          | 1122/? [15:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1122, loss 3.306241273880005\n",
      "Epoch 8: |          | 1123/? [15:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1123, loss 3.602247714996338\n",
      "Epoch 8: |          | 1124/? [15:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1124, loss 3.9642345905303955\n",
      "Epoch 8: |          | 1125/? [15:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1125, loss 3.316411256790161\n",
      "Epoch 8: |          | 1126/? [16:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1126, loss 3.2337920665740967\n",
      "Epoch 8: |          | 1127/? [16:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1127, loss 3.4738521575927734\n",
      "Epoch 8: |          | 1128/? [16:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1128, loss 3.5906147956848145\n",
      "Epoch 8: |          | 1129/? [16:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1129, loss 3.7025558948516846\n",
      "Epoch 8: |          | 1130/? [16:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1130, loss 3.8733718395233154\n",
      "Epoch 8: |          | 1131/? [16:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1131, loss 3.91443133354187\n",
      "Epoch 8: |          | 1132/? [16:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1132, loss 2.8109068870544434\n",
      "Epoch 8: |          | 1133/? [16:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1133, loss 3.543219804763794\n",
      "Epoch 8: |          | 1134/? [16:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1134, loss 3.374917984008789\n",
      "Epoch 8: |          | 1135/? [16:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1135, loss 3.962191104888916\n",
      "Epoch 8: |          | 1136/? [16:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1136, loss 3.6013665199279785\n",
      "Epoch 8: |          | 1137/? [16:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1137, loss 3.68219256401062\n",
      "Epoch 8: |          | 1138/? [16:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1138, loss 4.041687965393066\n",
      "Epoch 8: |          | 1139/? [16:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1139, loss 3.7862460613250732\n",
      "Epoch 8: |          | 1140/? [16:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1140, loss 3.4509308338165283\n",
      "Epoch 8: |          | 1141/? [16:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1141, loss 3.906445264816284\n",
      "Epoch 8: |          | 1142/? [16:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1142, loss 4.051152229309082\n",
      "Epoch 8: |          | 1143/? [16:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1143, loss 4.024839878082275\n",
      "Epoch 8: |          | 1144/? [16:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1144, loss 3.4409027099609375\n",
      "Epoch 8: |          | 1145/? [16:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1145, loss 3.6132400035858154\n",
      "Epoch 8: |          | 1146/? [16:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1146, loss 3.1965012550354004\n",
      "Epoch 8: |          | 1147/? [16:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1147, loss 3.226914644241333\n",
      "Epoch 8: |          | 1148/? [16:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1148, loss 3.4103381633758545\n",
      "Epoch 8: |          | 1149/? [16:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1149, loss 4.427969455718994\n",
      "Epoch 8: |          | 1150/? [16:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1150, loss 3.7895073890686035\n",
      "Epoch 8: |          | 1151/? [16:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1151, loss 4.0183329582214355\n",
      "Epoch 8: |          | 1152/? [16:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1152, loss 3.337219715118408\n",
      "Epoch 8: |          | 1153/? [16:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1153, loss 3.7079989910125732\n",
      "Epoch 8: |          | 1154/? [16:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1154, loss 3.392754077911377\n",
      "Epoch 8: |          | 1155/? [16:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1155, loss 3.6027207374572754\n",
      "Epoch 8: |          | 1156/? [16:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1156, loss 3.575315475463867\n",
      "Epoch 8: |          | 1157/? [16:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1157, loss 3.8657143115997314\n",
      "Epoch 8: |          | 1158/? [16:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1158, loss 3.982921600341797\n",
      "Epoch 8: |          | 1159/? [16:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1159, loss 2.9002621173858643\n",
      "Epoch 8: |          | 1160/? [16:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1160, loss 4.014009952545166\n",
      "Epoch 8: |          | 1161/? [16:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1161, loss 3.853468418121338\n",
      "Epoch 8: |          | 1162/? [16:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1162, loss 3.773808717727661\n",
      "Epoch 8: |          | 1163/? [16:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1163, loss 4.3681230545043945\n",
      "Epoch 8: |          | 1164/? [16:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1164, loss 4.142006874084473\n",
      "Epoch 8: |          | 1165/? [16:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1165, loss 3.3545467853546143\n",
      "Epoch 8: |          | 1166/? [16:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1166, loss 3.8129220008850098\n",
      "Epoch 8: |          | 1167/? [16:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1167, loss 3.8291282653808594\n",
      "Epoch 8: |          | 1168/? [16:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1168, loss 4.2487993240356445\n",
      "Epoch 8: |          | 1169/? [16:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1169, loss 3.462939500808716\n",
      "Epoch 8: |          | 1170/? [16:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1170, loss 3.9334232807159424\n",
      "Epoch 8: |          | 1171/? [16:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1171, loss 3.2989959716796875\n",
      "Epoch 8: |          | 1172/? [16:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1172, loss 3.246161937713623\n",
      "Epoch 8: |          | 1173/? [16:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1173, loss 3.8088576793670654\n",
      "Epoch 8: |          | 1174/? [16:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1174, loss 3.3302223682403564\n",
      "Epoch 8: |          | 1175/? [16:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1175, loss 3.8547351360321045\n",
      "Epoch 8: |          | 1176/? [16:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1176, loss 3.922049045562744\n",
      "Epoch 8: |          | 1177/? [16:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1177, loss 4.084604263305664\n",
      "Epoch 8: |          | 1178/? [16:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1178, loss 3.4985835552215576\n",
      "Epoch 8: |          | 1179/? [16:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1179, loss 4.046154975891113\n",
      "Epoch 8: |          | 1180/? [16:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1180, loss 3.885434627532959\n",
      "Epoch 8: |          | 1181/? [16:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1181, loss 3.783032178878784\n",
      "Epoch 8: |          | 1182/? [16:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1182, loss 3.6356914043426514\n",
      "Epoch 8: |          | 1183/? [16:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1183, loss 3.328151226043701\n",
      "Epoch 8: |          | 1184/? [16:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1184, loss 3.7598934173583984\n",
      "Epoch 8: |          | 1185/? [16:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1185, loss 3.466428756713867\n",
      "Epoch 8: |          | 1186/? [16:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1186, loss 3.6896731853485107\n",
      "Epoch 8: |          | 1187/? [16:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1187, loss 3.586876392364502\n",
      "Epoch 8: |          | 1188/? [16:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1188, loss 3.977144241333008\n",
      "Epoch 8: |          | 1189/? [16:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1189, loss 4.034756660461426\n",
      "Epoch 8: |          | 1190/? [16:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1190, loss 3.6385700702667236\n",
      "Epoch 8: |          | 1191/? [16:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1191, loss 3.6662585735321045\n",
      "Epoch 8: |          | 1192/? [16:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1192, loss 3.9175076484680176\n",
      "Epoch 8: |          | 1193/? [16:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1193, loss 3.4261984825134277\n",
      "Epoch 8: |          | 1194/? [16:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1194, loss 3.1739563941955566\n",
      "Epoch 8: |          | 1195/? [16:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1195, loss 3.647205352783203\n",
      "Epoch 8: |          | 1196/? [17:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1196, loss 3.865755081176758\n",
      "Epoch 8: |          | 1197/? [17:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1197, loss 3.597026824951172\n",
      "Epoch 8: |          | 1198/? [17:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1198, loss 3.725337505340576\n",
      "Epoch 8: |          | 1199/? [17:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1199, loss 4.024949550628662\n",
      "Epoch 8: |          | 1200/? [17:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1200, loss 3.2066712379455566\n",
      "Epoch 8: |          | 1201/? [17:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1201, loss 3.8136284351348877\n",
      "Epoch 8: |          | 1202/? [17:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1202, loss 3.5348587036132812\n",
      "Epoch 8: |          | 1203/? [17:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1203, loss 3.495697498321533\n",
      "Epoch 8: |          | 1204/? [17:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1204, loss 3.0963521003723145\n",
      "Epoch 8: |          | 1205/? [17:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1205, loss 3.660560131072998\n",
      "Epoch 8: |          | 1206/? [17:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1206, loss 3.6772549152374268\n",
      "Epoch 8: |          | 1207/? [17:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1207, loss 3.863264560699463\n",
      "Epoch 8: |          | 1208/? [17:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1208, loss 4.075122833251953\n",
      "Epoch 8: |          | 1209/? [17:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1209, loss 3.6695187091827393\n",
      "Epoch 8: |          | 1210/? [17:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1210, loss 3.9426567554473877\n",
      "Epoch 8: |          | 1211/? [17:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1211, loss 3.925507068634033\n",
      "Epoch 8: |          | 1212/? [17:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1212, loss 3.725938081741333\n",
      "Epoch 8: |          | 1213/? [17:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1213, loss 3.4683101177215576\n",
      "Epoch 8: |          | 1214/? [17:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1214, loss 3.985383987426758\n",
      "Epoch 8: |          | 1215/? [17:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1215, loss 3.5497374534606934\n",
      "Epoch 8: |          | 1216/? [17:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1216, loss 3.640500545501709\n",
      "Epoch 8: |          | 1217/? [17:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1217, loss 3.675377368927002\n",
      "Epoch 8: |          | 1218/? [17:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1218, loss 3.802320957183838\n",
      "Epoch 8: |          | 1219/? [17:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1219, loss 3.4822325706481934\n",
      "Epoch 8: |          | 1220/? [17:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1220, loss 4.1559929847717285\n",
      "Epoch 8: |          | 1221/? [17:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1221, loss 3.6520302295684814\n",
      "Epoch 8: |          | 1222/? [17:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1222, loss 2.910656452178955\n",
      "Epoch 8: |          | 1223/? [17:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1223, loss 3.1081366539001465\n",
      "Epoch 8: |          | 1224/? [17:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1224, loss 3.4227237701416016\n",
      "Epoch 8: |          | 1225/? [17:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1225, loss 4.065919876098633\n",
      "Epoch 8: |          | 1226/? [17:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1226, loss 3.98591947555542\n",
      "Epoch 8: |          | 1227/? [17:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1227, loss 3.6777443885803223\n",
      "Epoch 8: |          | 1228/? [17:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1228, loss 3.5146281719207764\n",
      "Epoch 8: |          | 1229/? [17:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1229, loss 3.1950745582580566\n",
      "Epoch 8: |          | 1230/? [17:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1230, loss 3.816096067428589\n",
      "Epoch 8: |          | 1231/? [17:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1231, loss 3.815030336380005\n",
      "Epoch 8: |          | 1232/? [17:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1232, loss 3.977325439453125\n",
      "Epoch 8: |          | 1233/? [17:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1233, loss 3.8039441108703613\n",
      "Epoch 8: |          | 1234/? [17:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1234, loss 2.842014789581299\n",
      "Epoch 8: |          | 1235/? [17:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1235, loss 3.9153075218200684\n",
      "Epoch 8: |          | 1236/? [17:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1236, loss 3.324841022491455\n",
      "Epoch 8: |          | 1237/? [17:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1237, loss 3.6017940044403076\n",
      "Epoch 8: |          | 1238/? [17:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1238, loss 3.666919708251953\n",
      "Epoch 8: |          | 1239/? [17:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1239, loss 3.502476930618286\n",
      "Epoch 8: |          | 1240/? [17:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1240, loss 4.051565647125244\n",
      "Epoch 8: |          | 1241/? [17:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1241, loss 3.659473419189453\n",
      "Epoch 8: |          | 1242/? [17:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1242, loss 3.486983060836792\n",
      "Epoch 8: |          | 1243/? [17:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1243, loss 3.408618450164795\n",
      "Epoch 8: |          | 1244/? [17:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1244, loss 3.5459320545196533\n",
      "Epoch 8: |          | 1245/? [17:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1245, loss 3.148317813873291\n",
      "Epoch 8: |          | 1246/? [17:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1246, loss 3.766986846923828\n",
      "Epoch 8: |          | 1247/? [17:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1247, loss 3.7974414825439453\n",
      "Epoch 8: |          | 1248/? [17:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1248, loss 3.394091844558716\n",
      "Epoch 8: |          | 1249/? [17:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1249, loss 3.5581459999084473\n",
      "Epoch 8: |          | 1250/? [17:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1250, loss 3.6891684532165527\n",
      "Epoch 8: |          | 1251/? [17:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1251, loss 3.4556846618652344\n",
      "Epoch 8: |          | 1252/? [17:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1252, loss 4.1978020668029785\n",
      "Epoch 8: |          | 1253/? [17:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1253, loss 3.542712688446045\n",
      "Epoch 8: |          | 1254/? [17:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1254, loss 2.9255478382110596\n",
      "Epoch 8: |          | 1255/? [17:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1255, loss 4.227219581604004\n",
      "Epoch 8: |          | 1256/? [17:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1256, loss 3.2952346801757812\n",
      "Epoch 8: |          | 1257/? [17:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1257, loss 3.319826126098633\n",
      "Epoch 8: |          | 1258/? [17:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1258, loss 3.9860165119171143\n",
      "Epoch 8: |          | 1259/? [17:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1259, loss 3.662696361541748\n",
      "Epoch 8: |          | 1260/? [17:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1260, loss 4.104382514953613\n",
      "Epoch 8: |          | 1261/? [17:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1261, loss 3.473004102706909\n",
      "Epoch 8: |          | 1262/? [17:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1262, loss 3.4740443229675293\n",
      "Epoch 8: |          | 1263/? [17:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1263, loss 3.7653679847717285\n",
      "Epoch 8: |          | 1264/? [18:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1264, loss 4.000828742980957\n",
      "Epoch 8: |          | 1265/? [18:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1265, loss 3.8878676891326904\n",
      "Epoch 8: |          | 1266/? [18:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1266, loss 3.6026835441589355\n",
      "Epoch 8: |          | 1267/? [18:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1267, loss 3.71724271774292\n",
      "Epoch 8: |          | 1268/? [18:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1268, loss 3.5426673889160156\n",
      "Epoch 8: |          | 1269/? [18:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1269, loss 3.1196205615997314\n",
      "Epoch 8: |          | 1270/? [18:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1270, loss 3.466265916824341\n",
      "Epoch 8: |          | 1271/? [18:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1271, loss 3.6299877166748047\n",
      "Epoch 8: |          | 1272/? [18:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1272, loss 3.2229740619659424\n",
      "Epoch 8: |          | 1273/? [18:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1273, loss 3.8808960914611816\n",
      "Epoch 8: |          | 1274/? [18:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1274, loss 2.8386340141296387\n",
      "Epoch 8: |          | 1275/? [18:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1275, loss 3.3418164253234863\n",
      "Epoch 8: |          | 1276/? [18:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1276, loss 3.618870496749878\n",
      "Epoch 8: |          | 1277/? [18:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1277, loss 3.3775343894958496\n",
      "Epoch 8: |          | 1278/? [18:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1278, loss 3.0387630462646484\n",
      "Epoch 8: |          | 1279/? [18:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1279, loss 3.7254550457000732\n",
      "Epoch 8: |          | 1280/? [18:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1280, loss 3.0182132720947266\n",
      "Epoch 8: |          | 1281/? [18:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1281, loss 3.5087032318115234\n",
      "Epoch 8: |          | 1282/? [18:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1282, loss 3.2042641639709473\n",
      "Epoch 8: |          | 1283/? [18:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1283, loss 3.852205991744995\n",
      "Epoch 8: |          | 1284/? [18:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1284, loss 3.070493698120117\n",
      "Epoch 8: |          | 1285/? [18:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1285, loss 4.03865909576416\n",
      "Epoch 8: |          | 1286/? [18:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1286, loss 2.6017165184020996\n",
      "Epoch 8: |          | 1287/? [18:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1287, loss 3.9879539012908936\n",
      "Epoch 8: |          | 1288/? [18:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1288, loss 3.762570858001709\n",
      "Epoch 8: |          | 1289/? [18:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1289, loss 3.0196337699890137\n",
      "Epoch 8: |          | 1290/? [18:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1290, loss 3.6702792644500732\n",
      "Epoch 8: |          | 1291/? [18:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1291, loss 4.5156168937683105\n",
      "Epoch 8: |          | 1292/? [18:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1292, loss 3.8109183311462402\n",
      "Epoch 8: |          | 1293/? [18:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1293, loss 3.4035353660583496\n",
      "Epoch 8: |          | 1294/? [18:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1294, loss 3.6086859703063965\n",
      "Epoch 8: |          | 1295/? [18:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1295, loss 3.7320218086242676\n",
      "Epoch 8: |          | 1296/? [18:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1296, loss 3.0109140872955322\n",
      "Epoch 8: |          | 1297/? [18:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1297, loss 3.8622055053710938\n",
      "Epoch 8: |          | 1298/? [18:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1298, loss 3.591517686843872\n",
      "Epoch 8: |          | 1299/? [18:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1299, loss 2.8269712924957275\n",
      "Epoch 8: |          | 1300/? [18:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1300, loss 3.5848803520202637\n",
      "Epoch 8: |          | 1301/? [18:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1301, loss 3.3469607830047607\n",
      "Epoch 8: |          | 1302/? [18:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1302, loss 3.4709506034851074\n",
      "Epoch 8: |          | 1303/? [18:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1303, loss 3.4301822185516357\n",
      "Epoch 8: |          | 1304/? [18:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1304, loss 4.1316118240356445\n",
      "Epoch 8: |          | 1305/? [18:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1305, loss 3.007746934890747\n",
      "Epoch 8: |          | 1306/? [18:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1306, loss 3.5865845680236816\n",
      "Epoch 8: |          | 1307/? [18:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1307, loss 3.249204635620117\n",
      "Epoch 8: |          | 1308/? [18:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1308, loss 3.197432518005371\n",
      "Epoch 8: |          | 1309/? [18:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1309, loss 3.169874429702759\n",
      "Epoch 8: |          | 1310/? [18:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1310, loss 3.726548671722412\n",
      "Epoch 8: |          | 1311/? [18:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1311, loss 3.249952793121338\n",
      "Epoch 8: |          | 1312/? [18:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1312, loss 2.970719814300537\n",
      "Epoch 8: |          | 1313/? [18:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1313, loss 4.095314025878906\n",
      "Epoch 8: |          | 1314/? [18:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1314, loss 3.3514583110809326\n",
      "Epoch 8: |          | 1315/? [18:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1315, loss 4.063579082489014\n",
      "Epoch 8: |          | 1316/? [18:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1316, loss 3.8359553813934326\n",
      "Epoch 8: |          | 1317/? [18:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1317, loss 3.5121676921844482\n",
      "Epoch 8: |          | 1318/? [18:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1318, loss 3.647047758102417\n",
      "Epoch 8: |          | 1319/? [18:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1319, loss 3.8248419761657715\n",
      "Epoch 8: |          | 1320/? [18:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1320, loss 3.4181933403015137\n",
      "Epoch 8: |          | 1321/? [18:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1321, loss 3.842769145965576\n",
      "Epoch 8: |          | 1322/? [18:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1322, loss 3.6901748180389404\n",
      "Epoch 8: |          | 1323/? [18:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1323, loss 3.2615456581115723\n",
      "Epoch 8: |          | 1324/? [18:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1324, loss 4.110806465148926\n",
      "Epoch 8: |          | 1325/? [18:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1325, loss 4.237679958343506\n",
      "Epoch 8: |          | 1326/? [19:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1326, loss 3.725792646408081\n",
      "Epoch 8: |          | 1327/? [19:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1327, loss 3.614386796951294\n",
      "Epoch 8: |          | 1328/? [19:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1328, loss 3.2837624549865723\n",
      "Epoch 8: |          | 1329/? [19:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1329, loss 3.823019504547119\n",
      "Epoch 8: |          | 1330/? [19:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1330, loss 3.582984209060669\n",
      "Epoch 8: |          | 1331/? [19:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1331, loss 3.7160239219665527\n",
      "Epoch 8: |          | 1332/? [19:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1332, loss 3.4286487102508545\n",
      "Epoch 8: |          | 1333/? [19:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1333, loss 3.4051105976104736\n",
      "Epoch 8: |          | 1334/? [19:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1334, loss 3.520968198776245\n",
      "Epoch 8: |          | 1335/? [19:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1335, loss 3.5150692462921143\n",
      "Epoch 8: |          | 1336/? [19:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1336, loss 3.0931668281555176\n",
      "Epoch 8: |          | 1337/? [19:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1337, loss 3.6723694801330566\n",
      "Epoch 8: |          | 1338/? [19:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1338, loss 3.003814220428467\n",
      "Epoch 8: |          | 1339/? [19:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1339, loss 3.5893502235412598\n",
      "Epoch 8: |          | 1340/? [19:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1340, loss 3.058098316192627\n",
      "Epoch 8: |          | 1341/? [19:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1341, loss 3.7624354362487793\n",
      "Epoch 8: |          | 1342/? [19:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1342, loss 3.9833240509033203\n",
      "Epoch 8: |          | 1343/? [19:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1343, loss 3.5023560523986816\n",
      "Epoch 8: |          | 1344/? [19:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1344, loss 3.6283862590789795\n",
      "Epoch 8: |          | 1345/? [19:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1345, loss 3.7764275074005127\n",
      "Epoch 8: |          | 1346/? [19:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1346, loss 4.699573516845703\n",
      "Epoch 8: |          | 1347/? [19:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1347, loss 3.7423815727233887\n",
      "Epoch 8: |          | 1348/? [19:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1348, loss 3.7399654388427734\n",
      "Epoch 8: |          | 1349/? [19:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1349, loss 3.770217180252075\n",
      "Epoch 8: |          | 1350/? [19:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1350, loss 3.7383644580841064\n",
      "Epoch 8: |          | 1351/? [19:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1351, loss 3.760565996170044\n",
      "Epoch 8: |          | 1352/? [19:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1352, loss 3.080724000930786\n",
      "Epoch 8: |          | 1353/? [19:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1353, loss 3.4243323802948\n",
      "Epoch 8: |          | 1354/? [19:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1354, loss 3.7685012817382812\n",
      "Epoch 8: |          | 1355/? [19:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1355, loss 3.878868818283081\n",
      "Epoch 8: |          | 1356/? [19:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1356, loss 3.6854960918426514\n",
      "Epoch 8: |          | 1357/? [19:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1357, loss 3.494729995727539\n",
      "Epoch 8: |          | 1358/? [19:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1358, loss 3.611262798309326\n",
      "Epoch 8: |          | 1359/? [19:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1359, loss 3.511706590652466\n",
      "Epoch 8: |          | 1360/? [19:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1360, loss 3.653437376022339\n",
      "Epoch 8: |          | 1361/? [19:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1361, loss 3.638427257537842\n",
      "Epoch 8: |          | 1362/? [19:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1362, loss 3.5218238830566406\n",
      "Epoch 8: |          | 1363/? [19:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1363, loss 2.9752156734466553\n",
      "Epoch 8: |          | 1364/? [19:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1364, loss 3.42859148979187\n",
      "Epoch 8: |          | 1365/? [19:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1365, loss 3.1589431762695312\n",
      "Epoch 8: |          | 1366/? [19:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1366, loss 3.8448996543884277\n",
      "Epoch 8: |          | 1367/? [19:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1367, loss 3.1922154426574707\n",
      "Epoch 8: |          | 1368/? [19:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1368, loss 2.9712700843811035\n",
      "Epoch 8: |          | 1369/? [19:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1369, loss 3.652914524078369\n",
      "Epoch 8: |          | 1370/? [19:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1370, loss 3.2069358825683594\n",
      "Epoch 8: |          | 1371/? [19:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1371, loss 4.09043550491333\n",
      "Epoch 8: |          | 1372/? [19:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1372, loss 3.5586884021759033\n",
      "Epoch 8: |          | 1373/? [19:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1373, loss 3.8937268257141113\n",
      "Epoch 8: |          | 1374/? [19:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1374, loss 3.0797791481018066\n",
      "Epoch 8: |          | 1375/? [19:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1375, loss 3.6828300952911377\n",
      "Epoch 8: |          | 1376/? [19:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1376, loss 3.5474624633789062\n",
      "Epoch 8: |          | 1377/? [19:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1377, loss 3.5496788024902344\n",
      "Epoch 8: |          | 1378/? [19:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1378, loss 3.496351957321167\n",
      "Epoch 8: |          | 1379/? [19:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1379, loss 3.489777088165283\n",
      "Epoch 8: |          | 1380/? [19:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1380, loss 3.6842093467712402\n",
      "Epoch 8: |          | 1381/? [19:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1381, loss 3.7865447998046875\n",
      "Epoch 8: |          | 1382/? [19:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1382, loss 3.352353572845459\n",
      "Epoch 8: |          | 1383/? [19:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1383, loss 3.5476233959198\n",
      "Epoch 8: |          | 1384/? [19:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1384, loss 3.560973644256592\n",
      "Epoch 8: |          | 1385/? [19:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1385, loss 3.5296332836151123\n",
      "Epoch 8: |          | 1386/? [19:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1386, loss 3.6656010150909424\n",
      "Epoch 8: |          | 1387/? [19:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1387, loss 3.582714080810547\n",
      "Epoch 8: |          | 1388/? [19:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1388, loss 3.134880304336548\n",
      "Epoch 8: |          | 1389/? [19:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1389, loss 3.7786717414855957\n",
      "Epoch 8: |          | 1390/? [19:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1390, loss 4.0539116859436035\n",
      "Epoch 8: |          | 1391/? [19:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1391, loss 3.708944320678711\n",
      "Epoch 8: |          | 1392/? [19:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1392, loss 3.209977388381958\n",
      "Epoch 8: |          | 1393/? [19:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1393, loss 3.372499942779541\n",
      "Epoch 8: |          | 1394/? [19:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1394, loss 2.983630657196045\n",
      "Epoch 8: |          | 1395/? [19:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1395, loss 3.6987361907958984\n",
      "Epoch 8: |          | 1396/? [20:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1396, loss 3.533426284790039\n",
      "Epoch 8: |          | 1397/? [20:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1397, loss 2.8712146282196045\n",
      "Epoch 8: |          | 1398/? [20:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1398, loss 4.046316146850586\n",
      "Epoch 8: |          | 1399/? [20:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1399, loss 4.019409656524658\n",
      "Epoch 8: |          | 1400/? [20:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1400, loss 3.2050843238830566\n",
      "Epoch 8: |          | 1401/? [20:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1401, loss 3.8233642578125\n",
      "Epoch 8: |          | 1402/? [20:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1402, loss 3.563361406326294\n",
      "Epoch 8: |          | 1403/? [20:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1403, loss 3.7784512042999268\n",
      "Epoch 8: |          | 1404/? [20:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1404, loss 3.7090911865234375\n",
      "Epoch 8: |          | 1405/? [20:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1405, loss 4.049937725067139\n",
      "Epoch 8: |          | 1406/? [20:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1406, loss 3.958805799484253\n",
      "Epoch 8: |          | 1407/? [20:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1407, loss 4.020203113555908\n",
      "Epoch 8: |          | 1408/? [20:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1408, loss 3.2840309143066406\n",
      "Epoch 8: |          | 1409/? [20:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1409, loss 3.333920955657959\n",
      "Epoch 8: |          | 1410/? [20:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1410, loss 3.365828275680542\n",
      "Epoch 8: |          | 1411/? [20:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1411, loss 3.6612160205841064\n",
      "Epoch 8: |          | 1412/? [20:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1412, loss 3.390476942062378\n",
      "Epoch 8: |          | 1413/? [20:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1413, loss 3.2324957847595215\n",
      "Epoch 8: |          | 1414/? [20:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1414, loss 3.405855894088745\n",
      "Epoch 8: |          | 1415/? [20:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1415, loss 3.619837999343872\n",
      "Epoch 8: |          | 1416/? [20:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1416, loss 3.996577739715576\n",
      "Epoch 8: |          | 1417/? [20:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1417, loss 3.6874358654022217\n",
      "Epoch 8: |          | 1418/? [20:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1418, loss 3.7648227214813232\n",
      "Epoch 8: |          | 1419/? [20:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1419, loss 3.533536911010742\n",
      "Epoch 8: |          | 1420/? [20:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1420, loss 3.411240339279175\n",
      "Epoch 8: |          | 1421/? [20:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1421, loss 3.2095885276794434\n",
      "Epoch 8: |          | 1422/? [20:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1422, loss 3.8118197917938232\n",
      "Epoch 8: |          | 1423/? [20:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1423, loss 3.8493809700012207\n",
      "Epoch 8: |          | 1424/? [20:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1424, loss 3.5411784648895264\n",
      "Epoch 8: |          | 1425/? [20:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1425, loss 3.735902786254883\n",
      "Epoch 8: |          | 1426/? [20:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1426, loss 3.3182132244110107\n",
      "Epoch 8: |          | 1427/? [20:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1427, loss 3.9128575325012207\n",
      "Epoch 8: |          | 1428/? [20:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1428, loss 3.8618686199188232\n",
      "Epoch 8: |          | 1429/? [20:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1429, loss 3.7725894451141357\n",
      "Epoch 8: |          | 1430/? [20:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1430, loss 3.8130531311035156\n",
      "Epoch 8: |          | 1431/? [20:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1431, loss 3.6124114990234375\n",
      "Epoch 8: |          | 1432/? [20:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1432, loss 3.672687530517578\n",
      "Epoch 8: |          | 1433/? [20:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1433, loss 3.5813517570495605\n",
      "Epoch 8: |          | 1434/? [20:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1434, loss 3.7176673412323\n",
      "Epoch 8: |          | 1435/? [20:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1435, loss 3.336069107055664\n",
      "Epoch 8: |          | 1436/? [20:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1436, loss 3.691321611404419\n",
      "Epoch 8: |          | 1437/? [20:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1437, loss 2.9384491443634033\n",
      "Epoch 8: |          | 1438/? [20:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1438, loss 4.546802043914795\n",
      "Epoch 8: |          | 1439/? [20:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1439, loss 3.588658094406128\n",
      "Epoch 8: |          | 1440/? [20:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1440, loss 3.825080156326294\n",
      "Epoch 8: |          | 1441/? [20:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1441, loss 4.114485263824463\n",
      "Epoch 8: |          | 1442/? [20:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1442, loss 4.127726078033447\n",
      "Epoch 8: |          | 1443/? [20:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1443, loss 3.324596881866455\n",
      "Epoch 8: |          | 1444/? [20:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1444, loss 3.34405779838562\n",
      "Epoch 8: |          | 1445/? [20:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1445, loss 3.8309624195098877\n",
      "Epoch 8: |          | 1446/? [20:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1446, loss 3.3816428184509277\n",
      "Epoch 8: |          | 1447/? [20:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1447, loss 3.5307164192199707\n",
      "Epoch 8: |          | 1448/? [20:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1448, loss 3.3972058296203613\n",
      "Epoch 8: |          | 1449/? [20:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1449, loss 3.6088719367980957\n",
      "Epoch 8: |          | 1450/? [20:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1450, loss 3.6388156414031982\n",
      "Epoch 8: |          | 1451/? [20:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1451, loss 3.9898104667663574\n",
      "Epoch 8: |          | 1452/? [20:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1452, loss 3.657888412475586\n",
      "Epoch 8: |          | 1453/? [20:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1453, loss 3.0883665084838867\n",
      "Epoch 8: |          | 1454/? [20:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1454, loss 3.6486763954162598\n",
      "Epoch 8: |          | 1455/? [20:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1455, loss 3.739635944366455\n",
      "Epoch 8: |          | 1456/? [20:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1456, loss 3.315324306488037\n",
      "Epoch 8: |          | 1457/? [20:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1457, loss 3.4319355487823486\n",
      "Epoch 8: |          | 1458/? [20:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1458, loss 3.6863410472869873\n",
      "Epoch 8: |          | 1459/? [20:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1459, loss 3.7797863483428955\n",
      "Epoch 8: |          | 1460/? [20:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1460, loss 3.7428581714630127\n",
      "Epoch 8: |          | 1461/? [20:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1461, loss 3.6670634746551514\n",
      "Epoch 8: |          | 1462/? [20:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1462, loss 3.9700310230255127\n",
      "Epoch 8: |          | 1463/? [20:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1463, loss 3.8805625438690186\n",
      "Epoch 8: |          | 1464/? [20:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1464, loss 3.1492977142333984\n",
      "Epoch 8: |          | 1465/? [20:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1465, loss 3.5946381092071533\n",
      "Epoch 8: |          | 1466/? [21:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1466, loss 3.2929000854492188\n",
      "Epoch 8: |          | 1467/? [21:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1467, loss 3.916114091873169\n",
      "Epoch 8: |          | 1468/? [21:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1468, loss 3.44024658203125\n",
      "Epoch 8: |          | 1469/? [21:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1469, loss 3.238349199295044\n",
      "Epoch 8: |          | 1470/? [21:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1470, loss 3.6755378246307373\n",
      "Epoch 8: |          | 1471/? [21:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1471, loss 3.9661712646484375\n",
      "Epoch 8: |          | 1472/? [21:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1472, loss 3.743818998336792\n",
      "Epoch 8: |          | 1473/? [21:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1473, loss 3.5438742637634277\n",
      "Epoch 8: |          | 1474/? [21:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1474, loss 3.4612228870391846\n",
      "Epoch 8: |          | 1475/? [21:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1475, loss 3.039182662963867\n",
      "Epoch 8: |          | 1476/? [21:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1476, loss 3.6141178607940674\n",
      "Epoch 8: |          | 1477/? [21:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1477, loss 3.6530601978302\n",
      "Epoch 8: |          | 1478/? [21:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1478, loss 3.528485059738159\n",
      "Epoch 8: |          | 1479/? [21:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1479, loss 4.04280948638916\n",
      "Epoch 8: |          | 1480/? [21:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1480, loss 3.7539379596710205\n",
      "Epoch 8: |          | 1481/? [21:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1481, loss 3.525766372680664\n",
      "Epoch 8: |          | 1482/? [21:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1482, loss 3.7091784477233887\n",
      "Epoch 8: |          | 1483/? [21:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1483, loss 3.2974276542663574\n",
      "Epoch 8: |          | 1484/? [21:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1484, loss 3.5259621143341064\n",
      "Epoch 8: |          | 1485/? [21:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1485, loss 3.6589088439941406\n",
      "Epoch 8: |          | 1486/? [21:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1486, loss 3.705418825149536\n",
      "Epoch 8: |          | 1487/? [21:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1487, loss 3.096421480178833\n",
      "Epoch 8: |          | 1488/? [21:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1488, loss 3.7604269981384277\n",
      "Epoch 8: |          | 1489/? [21:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1489, loss 3.7290408611297607\n",
      "Epoch 8: |          | 1490/? [21:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1490, loss 3.590503692626953\n",
      "Epoch 8: |          | 1491/? [21:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1491, loss 2.6790082454681396\n",
      "Epoch 8: |          | 1492/? [21:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1492, loss 3.2180209159851074\n",
      "Epoch 8: |          | 1493/? [21:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1493, loss 2.7186083793640137\n",
      "Epoch 8: |          | 1494/? [21:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1494, loss 3.5427279472351074\n",
      "Epoch 8: |          | 1495/? [21:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1495, loss 3.4197494983673096\n",
      "Epoch 8: |          | 1496/? [21:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1496, loss 3.7754948139190674\n",
      "Epoch 8: |          | 1497/? [21:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1497, loss 3.0121378898620605\n",
      "Epoch 8: |          | 1498/? [21:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1498, loss 3.3328964710235596\n",
      "Epoch 8: |          | 1499/? [21:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1499, loss 3.8289828300476074\n",
      "Epoch 8: |          | 1500/? [21:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1500, loss 3.743408203125\n",
      "Epoch 8: |          | 1501/? [21:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1501, loss 3.633836030960083\n",
      "Epoch 8: |          | 1502/? [21:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1502, loss 3.679042100906372\n",
      "Epoch 8: |          | 1503/? [21:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1503, loss 3.4746861457824707\n",
      "Epoch 8: |          | 1504/? [21:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1504, loss 4.134514808654785\n",
      "Epoch 8: |          | 1505/? [21:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1505, loss 3.882737398147583\n",
      "Epoch 8: |          | 1506/? [21:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1506, loss 3.5332603454589844\n",
      "Epoch 8: |          | 1507/? [21:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1507, loss 3.445561647415161\n",
      "Epoch 8: |          | 1508/? [21:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1508, loss 3.58842396736145\n",
      "Epoch 8: |          | 1509/? [21:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1509, loss 3.5782103538513184\n",
      "Epoch 8: |          | 1510/? [21:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1510, loss 3.8267409801483154\n",
      "Epoch 8: |          | 1511/? [21:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1511, loss 3.459951400756836\n",
      "Epoch 8: |          | 1512/? [21:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1512, loss 3.9670448303222656\n",
      "Epoch 8: |          | 1513/? [21:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1513, loss 4.246529579162598\n",
      "Epoch 8: |          | 1514/? [21:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1514, loss 3.3317253589630127\n",
      "Epoch 8: |          | 1515/? [21:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1515, loss 4.058963775634766\n",
      "Epoch 8: |          | 1516/? [21:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1516, loss 4.100356578826904\n",
      "Epoch 8: |          | 1517/? [21:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1517, loss 3.5400664806365967\n",
      "Epoch 8: |          | 1518/? [21:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1518, loss 3.360888719558716\n",
      "Epoch 8: |          | 1519/? [21:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1519, loss 3.8321304321289062\n",
      "Epoch 8: |          | 1520/? [21:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1520, loss 3.841858386993408\n",
      "Epoch 8: |          | 1521/? [21:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1521, loss 3.6277599334716797\n",
      "Epoch 8: |          | 1522/? [21:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1522, loss 3.424276828765869\n",
      "Epoch 8: |          | 1523/? [21:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1523, loss 3.715373992919922\n",
      "Epoch 8: |          | 1524/? [21:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1524, loss 3.578975200653076\n",
      "Epoch 8: |          | 1525/? [21:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1525, loss 3.2660624980926514\n",
      "Epoch 8: |          | 1526/? [21:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1526, loss 3.8497066497802734\n",
      "Epoch 8: |          | 1527/? [21:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1527, loss 3.8637969493865967\n",
      "Epoch 8: |          | 1528/? [21:52<00:00,  1.16it/s, v_num=30]ERROR: Input has inproper shape\n",
      "Epoch 8: |          | 1529/? [21:52<00:00,  1.17it/s, v_num=30]   VALIDATION: Batch 0, loss 4.630031585693359\n",
      "   VALIDATION: Batch 1, loss 3.5840671062469482\n",
      "   VALIDATION: Batch 2, loss 4.759005546569824\n",
      "   VALIDATION: Batch 3, loss 4.514618873596191\n",
      "   VALIDATION: Batch 4, loss 4.022961616516113\n",
      "   VALIDATION: Batch 5, loss 3.700899839401245\n",
      "   VALIDATION: Batch 6, loss 4.014251232147217\n",
      "   VALIDATION: Batch 7, loss 4.672242641448975\n",
      "   VALIDATION: Batch 8, loss 4.538369655609131\n",
      "   VALIDATION: Batch 9, loss 4.664111614227295\n",
      "   VALIDATION: Batch 10, loss 4.348252296447754\n",
      "   VALIDATION: Batch 11, loss 3.9495511054992676\n",
      "   VALIDATION: Batch 12, loss 4.194581985473633\n",
      "   VALIDATION: Batch 13, loss 4.747664451599121\n",
      "   VALIDATION: Batch 14, loss 4.084897518157959\n",
      "   VALIDATION: Batch 15, loss 3.971640110015869\n",
      "   VALIDATION: Batch 16, loss 4.563706398010254\n",
      "   VALIDATION: Batch 17, loss 4.283908843994141\n",
      "   VALIDATION: Batch 18, loss 3.5379385948181152\n",
      "   VALIDATION: Batch 19, loss 4.478915214538574\n",
      "   VALIDATION: Batch 20, loss 4.752678871154785\n",
      "   VALIDATION: Batch 21, loss 5.076249599456787\n",
      "   VALIDATION: Batch 22, loss 4.620311737060547\n",
      "   VALIDATION: Batch 23, loss 4.125766754150391\n",
      "   VALIDATION: Batch 24, loss 3.979282855987549\n",
      "   VALIDATION: Batch 25, loss 4.412888050079346\n",
      "   VALIDATION: Batch 26, loss 4.608981132507324\n",
      "   VALIDATION: Batch 27, loss 4.538332939147949\n",
      "   VALIDATION: Batch 28, loss 4.268692970275879\n",
      "   VALIDATION: Batch 29, loss 4.46926212310791\n",
      "   VALIDATION: Batch 30, loss 4.033931732177734\n",
      "   VALIDATION: Batch 31, loss 4.384697914123535\n",
      "   VALIDATION: Batch 32, loss 4.946648597717285\n",
      "   VALIDATION: Batch 33, loss 3.1317436695098877\n",
      "   VALIDATION: Batch 34, loss 4.363138675689697\n",
      "   VALIDATION: Batch 35, loss 4.607706069946289\n",
      "   VALIDATION: Batch 36, loss 3.854910373687744\n",
      "   VALIDATION: Batch 37, loss 3.8422939777374268\n",
      "   VALIDATION: Batch 38, loss 3.965466260910034\n",
      "   VALIDATION: Batch 39, loss 4.373563289642334\n",
      "   VALIDATION: Batch 40, loss 4.4062724113464355\n",
      "   VALIDATION: Batch 41, loss 3.211109161376953\n",
      "   VALIDATION: Batch 42, loss 4.482623100280762\n",
      "   VALIDATION: Batch 43, loss 4.586494445800781\n",
      "   VALIDATION: Batch 44, loss 4.182116985321045\n",
      "   VALIDATION: Batch 45, loss 4.581545829772949\n",
      "   VALIDATION: Batch 46, loss 3.6744258403778076\n",
      "   VALIDATION: Batch 47, loss 4.723426818847656\n",
      "   VALIDATION: Batch 48, loss 4.860764026641846\n",
      "   VALIDATION: Batch 49, loss 4.407943248748779\n",
      "   VALIDATION: Batch 50, loss 4.389062404632568\n",
      "   VALIDATION: Batch 51, loss 4.885916709899902\n",
      "   VALIDATION: Batch 52, loss 4.06486701965332\n",
      "   VALIDATION: Batch 53, loss 3.938443422317505\n",
      "   VALIDATION: Batch 54, loss 4.013108730316162\n",
      "   VALIDATION: Batch 55, loss 4.769981861114502\n",
      "   VALIDATION: Batch 56, loss 4.142022132873535\n",
      "   VALIDATION: Batch 57, loss 5.769225120544434\n",
      "   VALIDATION: Batch 58, loss 4.306163787841797\n",
      "   VALIDATION: Batch 59, loss 3.9392330646514893\n",
      "   VALIDATION: Batch 60, loss 3.4635016918182373\n",
      "   VALIDATION: Batch 61, loss 4.320940971374512\n",
      "   VALIDATION: Batch 62, loss 4.323977470397949\n",
      "   VALIDATION: Batch 63, loss 4.869102954864502\n",
      "   VALIDATION: Batch 64, loss 4.634235858917236\n",
      "   VALIDATION: Batch 65, loss 3.7617759704589844\n",
      "   VALIDATION: Batch 66, loss 4.661719799041748\n",
      "   VALIDATION: Batch 67, loss 4.099312782287598\n",
      "   VALIDATION: Batch 68, loss 4.2691264152526855\n",
      "   VALIDATION: Batch 69, loss 4.5240583419799805\n",
      "   VALIDATION: Batch 70, loss 4.70200252532959\n",
      "   VALIDATION: Batch 71, loss 4.194962501525879\n",
      "   VALIDATION: Batch 72, loss 5.089352130889893\n",
      "   VALIDATION: Batch 73, loss 3.824991226196289\n",
      "   VALIDATION: Batch 74, loss 4.481795787811279\n",
      "   VALIDATION: Batch 75, loss 4.4909348487854\n",
      "   VALIDATION: Batch 76, loss 4.325958728790283\n",
      "   VALIDATION: Batch 77, loss 4.600821495056152\n",
      "   VALIDATION: Batch 78, loss 4.430271148681641\n",
      "   VALIDATION: Batch 79, loss 4.39417028427124\n",
      "   VALIDATION: Batch 80, loss 4.483592510223389\n",
      "   VALIDATION: Batch 81, loss 4.231662273406982\n",
      "   VALIDATION: Batch 82, loss 4.615371227264404\n",
      "   VALIDATION: Batch 83, loss 3.820673704147339\n",
      "   VALIDATION: Batch 84, loss 4.572323799133301\n",
      "   VALIDATION: Batch 85, loss 4.199763774871826\n",
      "   VALIDATION: Batch 86, loss 4.258608818054199\n",
      "   VALIDATION: Batch 87, loss 4.135584831237793\n",
      "   VALIDATION: Batch 88, loss 3.7378334999084473\n",
      "   VALIDATION: Batch 89, loss 4.00157356262207\n",
      "   VALIDATION: Batch 90, loss 4.302168846130371\n",
      "   VALIDATION: Batch 91, loss 4.3391923904418945\n",
      "   VALIDATION: Batch 92, loss 4.049126625061035\n",
      "   VALIDATION: Batch 93, loss 4.772158145904541\n",
      "   VALIDATION: Batch 94, loss 4.271270751953125\n",
      "   VALIDATION: Batch 95, loss 3.7479045391082764\n",
      "   VALIDATION: Batch 96, loss 4.2339887619018555\n",
      "   VALIDATION: Batch 97, loss 3.9085566997528076\n",
      "   VALIDATION: Batch 98, loss 4.576807498931885\n",
      "   VALIDATION: Batch 99, loss 4.6386213302612305\n",
      "   VALIDATION: Batch 100, loss 5.03706693649292\n",
      "   VALIDATION: Batch 101, loss 3.5970001220703125\n",
      "   VALIDATION: Batch 102, loss 5.014003753662109\n",
      "   VALIDATION: Batch 103, loss 4.946164131164551\n",
      "   VALIDATION: Batch 104, loss 3.8711681365966797\n",
      "   VALIDATION: Batch 105, loss 4.3833723068237305\n",
      "   VALIDATION: Batch 106, loss 4.167490482330322\n",
      "   VALIDATION: Batch 107, loss 4.3330159187316895\n",
      "   VALIDATION: Batch 108, loss 4.047834873199463\n",
      "   VALIDATION: Batch 109, loss 4.6624836921691895\n",
      "   VALIDATION: Batch 110, loss 4.361448287963867\n",
      "   VALIDATION: Batch 111, loss 4.672541618347168\n",
      "   VALIDATION: Batch 112, loss 5.585949420928955\n",
      "   VALIDATION: Batch 113, loss 4.873353004455566\n",
      "   VALIDATION: Batch 114, loss 4.632379055023193\n",
      "   VALIDATION: Batch 115, loss 4.055048942565918\n",
      "   VALIDATION: Batch 116, loss 3.9136569499969482\n",
      "   VALIDATION: Batch 117, loss 4.589669227600098\n",
      "   VALIDATION: Batch 118, loss 4.770562171936035\n",
      "   VALIDATION: Batch 119, loss 3.9044411182403564\n",
      "   VALIDATION: Batch 120, loss 3.5151333808898926\n",
      "   VALIDATION: Batch 121, loss 3.8455023765563965\n",
      "   VALIDATION: Batch 122, loss 4.297767162322998\n",
      "   VALIDATION: Batch 123, loss 4.245249271392822\n",
      "   VALIDATION: Batch 124, loss 3.6544711589813232\n",
      "   VALIDATION: Batch 125, loss 4.245923042297363\n",
      "   VALIDATION: Batch 126, loss 4.471671104431152\n",
      "   VALIDATION: Batch 127, loss 4.274240016937256\n",
      "   VALIDATION: Batch 128, loss 4.41818904876709\n",
      "   VALIDATION: Batch 129, loss 4.049894332885742\n",
      "   VALIDATION: Batch 130, loss 3.6176064014434814\n",
      "   VALIDATION: Batch 131, loss 3.683018445968628\n",
      "   VALIDATION: Batch 132, loss 4.34854793548584\n",
      "   VALIDATION: Batch 133, loss 4.498729705810547\n",
      "   VALIDATION: Batch 134, loss 4.371401309967041\n",
      "   VALIDATION: Batch 135, loss 4.669858932495117\n",
      "   VALIDATION: Batch 136, loss 4.726521015167236\n",
      "   VALIDATION: Batch 137, loss 4.533566951751709\n",
      "   VALIDATION: Batch 138, loss 4.246492385864258\n",
      "   VALIDATION: Batch 139, loss 4.670130729675293\n",
      "   VALIDATION: Batch 140, loss 3.789419174194336\n",
      "   VALIDATION: Batch 141, loss 4.730210304260254\n",
      "   VALIDATION: Batch 142, loss 3.426312208175659\n",
      "   VALIDATION: Batch 143, loss 4.248655319213867\n",
      "   VALIDATION: Batch 144, loss 4.514848232269287\n",
      "   VALIDATION: Batch 145, loss 4.306541919708252\n",
      "   VALIDATION: Batch 146, loss 4.13714599609375\n",
      "   VALIDATION: Batch 147, loss 4.445002555847168\n",
      "   VALIDATION: Batch 148, loss 4.539605617523193\n",
      "   VALIDATION: Batch 149, loss 5.059090614318848\n",
      "   VALIDATION: Batch 150, loss 4.615250587463379\n",
      "   VALIDATION: Batch 151, loss 4.929765701293945\n",
      "   VALIDATION: Batch 152, loss 4.273489475250244\n",
      "   VALIDATION: Batch 153, loss 4.51731014251709\n",
      "   VALIDATION: Batch 154, loss 4.376355171203613\n",
      "   VALIDATION: Batch 155, loss 4.113142967224121\n",
      "   VALIDATION: Batch 156, loss 4.762889385223389\n",
      "   VALIDATION: Batch 157, loss 4.440670967102051\n",
      "   VALIDATION: Batch 158, loss 3.858609676361084\n",
      "   VALIDATION: Batch 159, loss 4.304880619049072\n",
      "   VALIDATION: Batch 160, loss 4.683078765869141\n",
      "   VALIDATION: Batch 161, loss 4.9056315422058105\n",
      "   VALIDATION: Batch 162, loss 4.388041973114014\n",
      "   VALIDATION: Batch 163, loss 3.754328489303589\n",
      "   VALIDATION: Batch 164, loss 4.296346664428711\n",
      "   VALIDATION: Batch 165, loss 4.746187686920166\n",
      "   VALIDATION: Batch 166, loss 4.223425388336182\n",
      "   VALIDATION: Batch 167, loss 4.578972339630127\n",
      "   VALIDATION: Batch 168, loss 3.455775737762451\n",
      "   VALIDATION: Batch 169, loss 4.136824131011963\n",
      "   VALIDATION: Batch 170, loss 4.347044467926025\n",
      "   VALIDATION: Batch 171, loss 4.484941005706787\n",
      "   VALIDATION: Batch 172, loss 4.285084247589111\n",
      "   VALIDATION: Batch 173, loss 4.151984214782715\n",
      "   VALIDATION: Batch 174, loss 4.625455856323242\n",
      "   VALIDATION: Batch 175, loss 4.414586067199707\n",
      "   VALIDATION: Batch 176, loss 4.212618827819824\n",
      "   VALIDATION: Batch 177, loss 4.12993049621582\n",
      "   VALIDATION: Batch 178, loss 5.269646644592285\n",
      "   VALIDATION: Batch 179, loss 4.513861179351807\n",
      "   VALIDATION: Batch 180, loss 3.987987518310547\n",
      "   VALIDATION: Batch 181, loss 4.1889472007751465\n",
      "   VALIDATION: Batch 182, loss 4.467047214508057\n",
      "   VALIDATION: Batch 183, loss 3.4658210277557373\n",
      "   VALIDATION: Batch 184, loss 3.204880952835083\n",
      "   VALIDATION: Batch 185, loss 4.02121639251709\n",
      "   VALIDATION: Batch 186, loss 3.9692702293395996\n",
      "   VALIDATION: Batch 187, loss 4.154203414916992\n",
      "   VALIDATION: Batch 188, loss 4.562901496887207\n",
      "   VALIDATION: Batch 189, loss 3.8852334022521973\n",
      "   VALIDATION: Batch 190, loss 3.956390857696533\n",
      "   VALIDATION: Batch 191, loss 4.462569236755371\n",
      "   VALIDATION: Batch 192, loss 4.846646785736084\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 9: |          | 0/? [00:00<?, ?it/s, v_num=30]              TRRAINING: Batch 0, loss 3.8431899547576904\n",
      "Epoch 9: |          | 1/? [00:01<00:00,  0.89it/s, v_num=30]   TRRAINING: Batch 1, loss 3.47477650642395\n",
      "Epoch 9: |          | 2/? [00:01<00:00,  1.00it/s, v_num=30]   TRRAINING: Batch 2, loss 3.5286803245544434\n",
      "Epoch 9: |          | 3/? [00:02<00:00,  1.04it/s, v_num=30]   TRRAINING: Batch 3, loss 3.239959239959717\n",
      "Epoch 9: |          | 4/? [00:03<00:00,  1.07it/s, v_num=30]   TRRAINING: Batch 4, loss 3.6058971881866455\n",
      "Epoch 9: |          | 5/? [00:04<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 5, loss 4.348067283630371\n",
      "Epoch 9: |          | 6/? [00:05<00:00,  1.14it/s, v_num=30]   TRRAINING: Batch 6, loss 3.777930736541748\n",
      "Epoch 9: |          | 7/? [00:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 7, loss 3.1964640617370605\n",
      "Epoch 9: |          | 8/? [00:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 8, loss 3.3026206493377686\n",
      "Epoch 9: |          | 9/? [00:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 9, loss 3.582408905029297\n",
      "Epoch 9: |          | 10/? [00:08<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 10, loss 3.845973253250122\n",
      "Epoch 9: |          | 11/? [00:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 11, loss 3.6879658699035645\n",
      "Epoch 9: |          | 12/? [00:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 12, loss 4.020340919494629\n",
      "Epoch 9: |          | 13/? [00:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 13, loss 3.7110867500305176\n",
      "Epoch 9: |          | 14/? [00:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 14, loss 3.856658458709717\n",
      "Epoch 9: |          | 15/? [00:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 15, loss 3.179114818572998\n",
      "Epoch 9: |          | 16/? [00:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 16, loss 3.0344889163970947\n",
      "Epoch 9: |          | 17/? [00:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 17, loss 4.093894004821777\n",
      "Epoch 9: |          | 18/? [00:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 18, loss 3.6554527282714844\n",
      "Epoch 9: |          | 19/? [00:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 19, loss 3.460739850997925\n",
      "Epoch 9: |          | 20/? [00:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 20, loss 3.689755916595459\n",
      "Epoch 9: |          | 21/? [00:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 21, loss 3.8356590270996094\n",
      "Epoch 9: |          | 22/? [00:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 22, loss 3.7332916259765625\n",
      "Epoch 9: |          | 23/? [00:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 23, loss 3.135507583618164\n",
      "Epoch 9: |          | 24/? [00:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 24, loss 3.742144823074341\n",
      "Epoch 9: |          | 25/? [00:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 25, loss 3.5709033012390137\n",
      "Epoch 9: |          | 26/? [00:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 26, loss 3.2903709411621094\n",
      "Epoch 9: |          | 27/? [00:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 27, loss 3.2850289344787598\n",
      "Epoch 9: |          | 28/? [00:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 28, loss 4.164384841918945\n",
      "Epoch 9: |          | 29/? [00:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 29, loss 3.684150218963623\n",
      "Epoch 9: |          | 30/? [00:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 30, loss 3.5637993812561035\n",
      "Epoch 9: |          | 31/? [00:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 31, loss 4.091726303100586\n",
      "Epoch 9: |          | 32/? [00:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 32, loss 3.6725380420684814\n",
      "Epoch 9: |          | 33/? [00:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 33, loss 3.5064473152160645\n",
      "Epoch 9: |          | 34/? [00:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 34, loss 3.5096099376678467\n",
      "Epoch 9: |          | 35/? [00:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 35, loss 3.025452136993408\n",
      "Epoch 9: |          | 36/? [00:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 36, loss 3.7329463958740234\n",
      "Epoch 9: |          | 37/? [00:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 37, loss 3.8948073387145996\n",
      "Epoch 9: |          | 38/? [00:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 38, loss 4.188506603240967\n",
      "Epoch 9: |          | 39/? [00:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 39, loss 4.076425552368164\n",
      "Epoch 9: |          | 40/? [00:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 40, loss 3.493352174758911\n",
      "Epoch 9: |          | 41/? [00:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 41, loss 3.5947227478027344\n",
      "Epoch 9: |          | 42/? [00:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 42, loss 3.3383355140686035\n",
      "Epoch 9: |          | 43/? [00:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 43, loss 3.5461573600769043\n",
      "Epoch 9: |          | 44/? [00:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 44, loss 2.743673801422119\n",
      "Epoch 9: |          | 45/? [00:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 45, loss 2.301046133041382\n",
      "Epoch 9: |          | 46/? [00:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 46, loss 4.099981784820557\n",
      "Epoch 9: |          | 47/? [00:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 47, loss 3.34319806098938\n",
      "Epoch 9: |          | 48/? [00:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 48, loss 3.1947388648986816\n",
      "Epoch 9: |          | 49/? [00:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 49, loss 3.6692798137664795\n",
      "Epoch 9: |          | 50/? [00:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 50, loss 3.527559280395508\n",
      "Epoch 9: |          | 51/? [00:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 51, loss 3.441575288772583\n",
      "Epoch 9: |          | 52/? [00:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 52, loss 4.17198371887207\n",
      "Epoch 9: |          | 53/? [00:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 53, loss 3.803910732269287\n",
      "Epoch 9: |          | 54/? [00:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 54, loss 3.621476650238037\n",
      "Epoch 9: |          | 55/? [00:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 55, loss 3.6472506523132324\n",
      "Epoch 9: |          | 56/? [00:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 56, loss 3.7780349254608154\n",
      "Epoch 9: |          | 57/? [00:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 57, loss 3.6855366230010986\n",
      "Epoch 9: |          | 58/? [00:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 58, loss 4.78695011138916\n",
      "Epoch 9: |          | 59/? [00:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 59, loss 3.7402336597442627\n",
      "Epoch 9: |          | 60/? [00:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 60, loss 3.8996872901916504\n",
      "Epoch 9: |          | 61/? [00:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 61, loss 3.8810722827911377\n",
      "Epoch 9: |          | 62/? [00:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 62, loss 3.5776500701904297\n",
      "Epoch 9: |          | 63/? [00:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 63, loss 3.7805137634277344\n",
      "Epoch 9: |          | 64/? [00:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 64, loss 3.56013560295105\n",
      "Epoch 9: |          | 65/? [00:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 65, loss 3.590946912765503\n",
      "Epoch 9: |          | 66/? [00:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 66, loss 3.048234462738037\n",
      "Epoch 9: |          | 67/? [00:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 67, loss 3.783923387527466\n",
      "Epoch 9: |          | 68/? [00:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 68, loss 3.6702582836151123\n",
      "Epoch 9: |          | 69/? [00:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 69, loss 3.644570827484131\n",
      "Epoch 9: |          | 70/? [00:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 70, loss 3.372716188430786\n",
      "Epoch 9: |          | 71/? [01:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 71, loss 3.3305270671844482\n",
      "Epoch 9: |          | 72/? [01:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 72, loss 3.4720263481140137\n",
      "Epoch 9: |          | 73/? [01:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 73, loss 3.672219753265381\n",
      "Epoch 9: |          | 74/? [01:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 74, loss 3.461705446243286\n",
      "Epoch 9: |          | 75/? [01:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 75, loss 3.5485916137695312\n",
      "Epoch 9: |          | 76/? [01:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 76, loss 3.5676047801971436\n",
      "Epoch 9: |          | 77/? [01:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 77, loss 3.572373151779175\n",
      "Epoch 9: |          | 78/? [01:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 78, loss 3.465628147125244\n",
      "Epoch 9: |          | 79/? [01:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 79, loss 3.6278891563415527\n",
      "Epoch 9: |          | 80/? [01:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 80, loss 3.5426876544952393\n",
      "Epoch 9: |          | 81/? [01:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 81, loss 2.9676437377929688\n",
      "Epoch 9: |          | 82/? [01:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 82, loss 3.7521049976348877\n",
      "Epoch 9: |          | 83/? [01:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 83, loss 3.299973964691162\n",
      "Epoch 9: |          | 84/? [01:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 84, loss 3.166262149810791\n",
      "Epoch 9: |          | 85/? [01:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 85, loss 3.0092973709106445\n",
      "Epoch 9: |          | 86/? [01:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 86, loss 3.1932435035705566\n",
      "Epoch 9: |          | 87/? [01:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 87, loss 3.3584046363830566\n",
      "Epoch 9: |          | 88/? [01:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 88, loss 3.926283359527588\n",
      "Epoch 9: |          | 89/? [01:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 89, loss 3.8931884765625\n",
      "Epoch 9: |          | 90/? [01:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 90, loss 3.679567337036133\n",
      "Epoch 9: |          | 91/? [01:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 91, loss 3.366154193878174\n",
      "Epoch 9: |          | 92/? [01:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 92, loss 3.7858612537384033\n",
      "Epoch 9: |          | 93/? [01:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 93, loss 3.9873626232147217\n",
      "Epoch 9: |          | 94/? [01:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 94, loss 3.8589768409729004\n",
      "Epoch 9: |          | 95/? [01:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 95, loss 3.661442518234253\n",
      "Epoch 9: |          | 96/? [01:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 96, loss 3.3918960094451904\n",
      "Epoch 9: |          | 97/? [01:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 97, loss 3.2010676860809326\n",
      "Epoch 9: |          | 98/? [01:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 98, loss 3.7236030101776123\n",
      "Epoch 9: |          | 99/? [01:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 99, loss 3.913344144821167\n",
      "Epoch 9: |          | 100/? [01:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 100, loss 3.845259428024292\n",
      "Epoch 9: |          | 101/? [01:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 101, loss 3.519641876220703\n",
      "Epoch 9: |          | 102/? [01:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 102, loss 3.527510166168213\n",
      "Epoch 9: |          | 103/? [01:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 103, loss 3.397108554840088\n",
      "Epoch 9: |          | 104/? [01:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 104, loss 3.6380341053009033\n",
      "Epoch 9: |          | 105/? [01:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 105, loss 3.658615827560425\n",
      "Epoch 9: |          | 106/? [01:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 106, loss 3.729051113128662\n",
      "Epoch 9: |          | 107/? [01:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 107, loss 3.7497172355651855\n",
      "Epoch 9: |          | 108/? [01:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 108, loss 3.7130424976348877\n",
      "Epoch 9: |          | 109/? [01:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 109, loss 3.5150439739227295\n",
      "Epoch 9: |          | 110/? [01:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 110, loss 3.6232662200927734\n",
      "Epoch 9: |          | 111/? [01:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 111, loss 4.25529670715332\n",
      "Epoch 9: |          | 112/? [01:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 112, loss 3.000020980834961\n",
      "Epoch 9: |          | 113/? [01:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 113, loss 2.6216392517089844\n",
      "Epoch 9: |          | 114/? [01:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 114, loss 3.8315834999084473\n",
      "Epoch 9: |          | 115/? [01:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 115, loss 4.016241550445557\n",
      "Epoch 9: |          | 116/? [01:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 116, loss 3.0753719806671143\n",
      "Epoch 9: |          | 117/? [01:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 117, loss 3.1473546028137207\n",
      "Epoch 9: |          | 118/? [01:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 118, loss 3.879796266555786\n",
      "Epoch 9: |          | 119/? [01:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 119, loss 4.119933128356934\n",
      "Epoch 9: |          | 120/? [01:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 120, loss 3.8920178413391113\n",
      "Epoch 9: |          | 121/? [01:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 121, loss 3.6290364265441895\n",
      "Epoch 9: |          | 122/? [01:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 122, loss 3.1174514293670654\n",
      "Epoch 9: |          | 123/? [01:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 123, loss 3.6323390007019043\n",
      "Epoch 9: |          | 124/? [01:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 124, loss 3.6840755939483643\n",
      "Epoch 9: |          | 125/? [01:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 125, loss 3.368917942047119\n",
      "Epoch 9: |          | 126/? [01:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 126, loss 3.8695430755615234\n",
      "Epoch 9: |          | 127/? [01:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 127, loss 3.97040057182312\n",
      "Epoch 9: |          | 128/? [01:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 128, loss 3.1508853435516357\n",
      "Epoch 9: |          | 129/? [01:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 129, loss 3.6997580528259277\n",
      "Epoch 9: |          | 130/? [01:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 130, loss 2.8991479873657227\n",
      "Epoch 9: |          | 131/? [01:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 131, loss 3.669694423675537\n",
      "Epoch 9: |          | 132/? [01:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 132, loss 3.744752883911133\n",
      "Epoch 9: |          | 133/? [01:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 133, loss 3.563502550125122\n",
      "Epoch 9: |          | 134/? [01:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 134, loss 3.7771220207214355\n",
      "Epoch 9: |          | 135/? [01:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 135, loss 3.7982845306396484\n",
      "Epoch 9: |          | 136/? [01:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 136, loss 3.8715903759002686\n",
      "Epoch 9: |          | 137/? [01:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 137, loss 2.925316333770752\n",
      "Epoch 9: |          | 138/? [01:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 138, loss 3.494152545928955\n",
      "Epoch 9: |          | 139/? [01:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 139, loss 3.8613994121551514\n",
      "Epoch 9: |          | 140/? [01:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 140, loss 3.2185120582580566\n",
      "Epoch 9: |          | 141/? [01:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 141, loss 3.3077545166015625\n",
      "Epoch 9: |          | 142/? [01:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 142, loss 4.558431148529053\n",
      "Epoch 9: |          | 143/? [02:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 143, loss 4.295979022979736\n",
      "Epoch 9: |          | 144/? [02:01<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 144, loss 3.672147750854492\n",
      "Epoch 9: |          | 145/? [02:02<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 145, loss 3.1030542850494385\n",
      "Epoch 9: |          | 146/? [02:03<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 146, loss 3.278810977935791\n",
      "Epoch 9: |          | 147/? [02:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 147, loss 3.5845954418182373\n",
      "Epoch 9: |          | 148/? [02:04<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 148, loss 3.2956604957580566\n",
      "Epoch 9: |          | 149/? [02:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 149, loss 2.9431991577148438\n",
      "Epoch 9: |          | 150/? [02:05<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 150, loss 3.7659060955047607\n",
      "Epoch 9: |          | 151/? [02:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 151, loss 3.863680362701416\n",
      "Epoch 9: |          | 152/? [02:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 152, loss 3.7894554138183594\n",
      "Epoch 9: |          | 153/? [02:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 153, loss 2.9909396171569824\n",
      "Epoch 9: |          | 154/? [02:09<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 154, loss 4.146038055419922\n",
      "Epoch 9: |          | 155/? [02:10<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 155, loss 3.6670773029327393\n",
      "Epoch 9: |          | 156/? [02:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 156, loss 3.082322120666504\n",
      "Epoch 9: |          | 157/? [02:11<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 157, loss 3.7423462867736816\n",
      "Epoch 9: |          | 158/? [02:12<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 158, loss 3.8210253715515137\n",
      "Epoch 9: |          | 159/? [02:13<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 159, loss 3.574704647064209\n",
      "Epoch 9: |          | 160/? [02:14<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 160, loss 3.3755111694335938\n",
      "Epoch 9: |          | 161/? [02:15<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 161, loss 3.707923173904419\n",
      "Epoch 9: |          | 162/? [02:16<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 162, loss 3.720837116241455\n",
      "Epoch 9: |          | 163/? [02:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 163, loss 2.913090229034424\n",
      "Epoch 9: |          | 164/? [02:17<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 164, loss 3.3483898639678955\n",
      "Epoch 9: |          | 165/? [02:18<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 165, loss 4.092259883880615\n",
      "Epoch 9: |          | 166/? [02:19<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 166, loss 3.7680840492248535\n",
      "Epoch 9: |          | 167/? [02:20<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 167, loss 3.8677055835723877\n",
      "Epoch 9: |          | 168/? [02:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 168, loss 3.449897289276123\n",
      "Epoch 9: |          | 169/? [02:21<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 169, loss 3.119386911392212\n",
      "Epoch 9: |          | 170/? [02:22<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 170, loss 3.3573155403137207\n",
      "Epoch 9: |          | 171/? [02:23<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 171, loss 3.6785266399383545\n",
      "Epoch 9: |          | 172/? [02:24<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 172, loss 3.4746367931365967\n",
      "Epoch 9: |          | 173/? [02:25<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 173, loss 4.045039176940918\n",
      "Epoch 9: |          | 174/? [02:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 174, loss 3.700496196746826\n",
      "Epoch 9: |          | 175/? [02:26<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 175, loss 4.1648054122924805\n",
      "Epoch 9: |          | 176/? [02:27<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 176, loss 3.444296360015869\n",
      "Epoch 9: |          | 177/? [02:28<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 177, loss 3.4412944316864014\n",
      "Epoch 9: |          | 178/? [02:29<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 178, loss 3.311166763305664\n",
      "Epoch 9: |          | 179/? [02:30<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 179, loss 3.9995205402374268\n",
      "Epoch 9: |          | 180/? [02:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 180, loss 3.5271804332733154\n",
      "Epoch 9: |          | 181/? [02:31<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 181, loss 3.3431313037872314\n",
      "Epoch 9: |          | 182/? [02:32<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 182, loss 3.7122814655303955\n",
      "Epoch 9: |          | 183/? [02:33<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 183, loss 3.244417667388916\n",
      "Epoch 9: |          | 184/? [02:34<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 184, loss 3.464073896408081\n",
      "Epoch 9: |          | 185/? [02:35<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 185, loss 3.889866590499878\n",
      "Epoch 9: |          | 186/? [02:36<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 186, loss 3.5020930767059326\n",
      "Epoch 9: |          | 187/? [02:37<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 187, loss 3.962712526321411\n",
      "Epoch 9: |          | 188/? [02:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 188, loss 3.381985902786255\n",
      "Epoch 9: |          | 189/? [02:38<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 189, loss 3.943434238433838\n",
      "Epoch 9: |          | 190/? [02:39<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 190, loss 3.53913950920105\n",
      "Epoch 9: |          | 191/? [02:40<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 191, loss 4.2724809646606445\n",
      "Epoch 9: |          | 192/? [02:41<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 192, loss 3.944115400314331\n",
      "Epoch 9: |          | 193/? [02:42<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 193, loss 3.4315857887268066\n",
      "Epoch 9: |          | 194/? [02:43<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 194, loss 3.314842939376831\n",
      "Epoch 9: |          | 195/? [02:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 195, loss 3.9868369102478027\n",
      "Epoch 9: |          | 196/? [02:44<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 196, loss 3.8600807189941406\n",
      "Epoch 9: |          | 197/? [02:45<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 197, loss 3.6437861919403076\n",
      "Epoch 9: |          | 198/? [02:46<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 198, loss 3.010005474090576\n",
      "Epoch 9: |          | 199/? [02:47<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 199, loss 3.9637062549591064\n",
      "Epoch 9: |          | 200/? [02:48<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 200, loss 3.4084060192108154\n",
      "Epoch 9: |          | 201/? [02:49<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 201, loss 3.6017508506774902\n",
      "Epoch 9: |          | 202/? [02:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 202, loss 3.7949306964874268\n",
      "Epoch 9: |          | 203/? [02:50<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 203, loss 3.4024341106414795\n",
      "Epoch 9: |          | 204/? [02:51<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 204, loss 3.422840118408203\n",
      "Epoch 9: |          | 205/? [02:52<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 205, loss 3.3825736045837402\n",
      "Epoch 9: |          | 206/? [02:53<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 206, loss 3.2644717693328857\n",
      "Epoch 9: |          | 207/? [02:54<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 207, loss 3.738807201385498\n",
      "Epoch 9: |          | 208/? [02:55<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 208, loss 3.6747899055480957\n",
      "Epoch 9: |          | 209/? [02:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 209, loss 3.439185619354248\n",
      "Epoch 9: |          | 210/? [02:56<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 210, loss 4.122564792633057\n",
      "Epoch 9: |          | 211/? [02:57<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 211, loss 3.5173144340515137\n",
      "Epoch 9: |          | 212/? [02:58<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 212, loss 3.746285915374756\n",
      "Epoch 9: |          | 213/? [02:59<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 213, loss 3.589923143386841\n",
      "Epoch 9: |          | 214/? [03:00<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 214, loss 3.470242977142334\n",
      "Epoch 9: |          | 215/? [03:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 215, loss 3.1315114498138428\n",
      "Epoch 9: |          | 216/? [03:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 216, loss 3.727808713912964\n",
      "Epoch 9: |          | 217/? [03:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 217, loss 3.721569776535034\n",
      "Epoch 9: |          | 218/? [03:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 218, loss 3.697205066680908\n",
      "Epoch 9: |          | 219/? [03:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 219, loss 3.6606640815734863\n",
      "Epoch 9: |          | 220/? [03:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 220, loss 3.726261854171753\n",
      "Epoch 9: |          | 221/? [03:06<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 221, loss 3.5773513317108154\n",
      "Epoch 9: |          | 222/? [03:07<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 222, loss 2.8445639610290527\n",
      "Epoch 9: |          | 223/? [03:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 223, loss 3.7556426525115967\n",
      "Epoch 9: |          | 224/? [03:08<00:00,  1.19it/s, v_num=30]   TRRAINING: Batch 224, loss 3.8804633617401123\n",
      "Epoch 9: |          | 225/? [03:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 225, loss 3.6316771507263184\n",
      "Epoch 9: |          | 226/? [03:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 226, loss 3.562990188598633\n",
      "Epoch 9: |          | 227/? [03:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 227, loss 3.9033493995666504\n",
      "Epoch 9: |          | 228/? [03:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 228, loss 3.5888538360595703\n",
      "Epoch 9: |          | 229/? [03:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 229, loss 3.6542792320251465\n",
      "Epoch 9: |          | 230/? [03:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 230, loss 3.588245391845703\n",
      "Epoch 9: |          | 231/? [03:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 231, loss 3.5609097480773926\n",
      "Epoch 9: |          | 232/? [03:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 232, loss 3.2693119049072266\n",
      "Epoch 9: |          | 233/? [03:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 233, loss 3.9071860313415527\n",
      "Epoch 9: |          | 234/? [03:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 234, loss 3.9116287231445312\n",
      "Epoch 9: |          | 235/? [03:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 235, loss 4.01327657699585\n",
      "Epoch 9: |          | 236/? [03:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 236, loss 3.4523606300354004\n",
      "Epoch 9: |          | 237/? [03:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 237, loss 3.6227822303771973\n",
      "Epoch 9: |          | 238/? [03:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 238, loss 3.8768486976623535\n",
      "Epoch 9: |          | 239/? [03:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 239, loss 3.3753974437713623\n",
      "Epoch 9: |          | 240/? [03:28<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 240, loss 3.0718390941619873\n",
      "Epoch 9: |          | 241/? [03:28<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 241, loss 3.5660316944122314\n",
      "Epoch 9: |          | 242/? [03:29<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 242, loss 3.959716320037842\n",
      "Epoch 9: |          | 243/? [03:30<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 243, loss 2.8930270671844482\n",
      "Epoch 9: |          | 244/? [03:31<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 244, loss 3.2945122718811035\n",
      "Epoch 9: |          | 245/? [03:32<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 245, loss 3.5536792278289795\n",
      "Epoch 9: |          | 246/? [03:33<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 246, loss 3.676445722579956\n",
      "Epoch 9: |          | 247/? [03:34<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 247, loss 3.7530007362365723\n",
      "Epoch 9: |          | 248/? [03:34<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 248, loss 3.2546401023864746\n",
      "Epoch 9: |          | 249/? [03:35<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 249, loss 3.176872730255127\n",
      "Epoch 9: |          | 250/? [03:36<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 250, loss 3.7527668476104736\n",
      "Epoch 9: |          | 251/? [03:37<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 251, loss 3.716602325439453\n",
      "Epoch 9: |          | 252/? [03:38<00:00,  1.15it/s, v_num=30]   TRRAINING: Batch 252, loss 3.515242099761963\n",
      "Epoch 9: |          | 253/? [03:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 253, loss 4.301181793212891\n",
      "Epoch 9: |          | 254/? [03:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 254, loss 3.9137845039367676\n",
      "Epoch 9: |          | 255/? [03:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 255, loss 3.6202609539031982\n",
      "Epoch 9: |          | 256/? [03:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 256, loss 4.086724281311035\n",
      "Epoch 9: |          | 257/? [03:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 257, loss 3.498180389404297\n",
      "Epoch 9: |          | 258/? [03:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 258, loss 3.582876682281494\n",
      "Epoch 9: |          | 259/? [03:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 259, loss 3.486428737640381\n",
      "Epoch 9: |          | 260/? [03:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 260, loss 3.407465696334839\n",
      "Epoch 9: |          | 261/? [03:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 261, loss 3.181342840194702\n",
      "Epoch 9: |          | 262/? [03:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 262, loss 3.8558387756347656\n",
      "Epoch 9: |          | 263/? [03:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 263, loss 3.551814556121826\n",
      "Epoch 9: |          | 264/? [03:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 264, loss 3.7085087299346924\n",
      "Epoch 9: |          | 265/? [03:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 265, loss 3.1673781871795654\n",
      "Epoch 9: |          | 266/? [03:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 266, loss 3.613076686859131\n",
      "Epoch 9: |          | 267/? [03:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 267, loss 3.205256938934326\n",
      "Epoch 9: |          | 268/? [03:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 268, loss 3.523597240447998\n",
      "Epoch 9: |          | 269/? [03:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 269, loss 3.884058713912964\n",
      "Epoch 9: |          | 270/? [03:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 270, loss 3.496098756790161\n",
      "Epoch 9: |          | 271/? [03:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 271, loss 3.6752090454101562\n",
      "Epoch 9: |          | 272/? [03:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 272, loss 3.9653637409210205\n",
      "Epoch 9: |          | 273/? [03:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 273, loss 3.255533218383789\n",
      "Epoch 9: |          | 274/? [03:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 274, loss 4.009669303894043\n",
      "Epoch 9: |          | 275/? [03:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 275, loss 3.7699761390686035\n",
      "Epoch 9: |          | 276/? [03:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 276, loss 3.0429718494415283\n",
      "Epoch 9: |          | 277/? [03:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 277, loss 3.750532865524292\n",
      "Epoch 9: |          | 278/? [03:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 278, loss 2.829561710357666\n",
      "Epoch 9: |          | 279/? [04:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 279, loss 3.4693245887756348\n",
      "Epoch 9: |          | 280/? [04:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 280, loss 3.2020716667175293\n",
      "Epoch 9: |          | 281/? [04:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 281, loss 3.817014217376709\n",
      "Epoch 9: |          | 282/? [04:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 282, loss 3.387847900390625\n",
      "Epoch 9: |          | 283/? [04:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 283, loss 3.4425132274627686\n",
      "Epoch 9: |          | 284/? [04:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 284, loss 3.430455446243286\n",
      "Epoch 9: |          | 285/? [04:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 285, loss 2.9759504795074463\n",
      "Epoch 9: |          | 286/? [04:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 286, loss 3.531464099884033\n",
      "Epoch 9: |          | 287/? [04:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 287, loss 3.2355990409851074\n",
      "Epoch 9: |          | 288/? [04:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 288, loss 3.2450649738311768\n",
      "Epoch 9: |          | 289/? [04:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 289, loss 3.009408712387085\n",
      "Epoch 9: |          | 290/? [04:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 290, loss 2.60353946685791\n",
      "Epoch 9: |          | 291/? [04:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 291, loss 3.6709632873535156\n",
      "Epoch 9: |          | 292/? [04:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 292, loss 3.373107433319092\n",
      "Epoch 9: |          | 293/? [04:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 293, loss 3.6173202991485596\n",
      "Epoch 9: |          | 294/? [04:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 294, loss 3.5565261840820312\n",
      "Epoch 9: |          | 295/? [04:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 295, loss 3.837069272994995\n",
      "Epoch 9: |          | 296/? [04:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 296, loss 3.301116466522217\n",
      "Epoch 9: |          | 297/? [04:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 297, loss 3.9706122875213623\n",
      "Epoch 9: |          | 298/? [04:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 298, loss 3.756582736968994\n",
      "Epoch 9: |          | 299/? [04:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 299, loss 3.9805450439453125\n",
      "Epoch 9: |          | 300/? [04:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 300, loss 3.581267833709717\n",
      "Epoch 9: |          | 301/? [04:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 301, loss 3.387974500656128\n",
      "Epoch 9: |          | 302/? [04:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 302, loss 3.808682680130005\n",
      "Epoch 9: |          | 303/? [04:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 303, loss 3.6132118701934814\n",
      "Epoch 9: |          | 304/? [04:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 304, loss 3.7852885723114014\n",
      "Epoch 9: |          | 305/? [04:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 305, loss 3.7999210357666016\n",
      "Epoch 9: |          | 306/? [04:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 306, loss 3.5189318656921387\n",
      "Epoch 9: |          | 307/? [04:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 307, loss 3.7425589561462402\n",
      "Epoch 9: |          | 308/? [04:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 308, loss 3.8931121826171875\n",
      "Epoch 9: |          | 309/? [04:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 309, loss 3.4994475841522217\n",
      "Epoch 9: |          | 310/? [04:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 310, loss 3.882363796234131\n",
      "Epoch 9: |          | 311/? [04:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 311, loss 3.4658751487731934\n",
      "Epoch 9: |          | 312/? [04:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 312, loss 3.5164217948913574\n",
      "Epoch 9: |          | 313/? [04:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 313, loss 3.3545989990234375\n",
      "Epoch 9: |          | 314/? [04:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 314, loss 3.674572706222534\n",
      "Epoch 9: |          | 315/? [04:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 315, loss 3.401930332183838\n",
      "Epoch 9: |          | 316/? [04:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 316, loss 3.8158154487609863\n",
      "Epoch 9: |          | 317/? [04:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 317, loss 3.7037513256073\n",
      "Epoch 9: |          | 318/? [04:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 318, loss 3.826591968536377\n",
      "Epoch 9: |          | 319/? [04:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 319, loss 3.098870277404785\n",
      "Epoch 9: |          | 320/? [04:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 320, loss 3.629126787185669\n",
      "Epoch 9: |          | 321/? [04:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 321, loss 3.4154956340789795\n",
      "Epoch 9: |          | 322/? [04:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 322, loss 3.971120834350586\n",
      "Epoch 9: |          | 323/? [04:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 323, loss 3.8995888233184814\n",
      "Epoch 9: |          | 324/? [04:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 324, loss 3.698132038116455\n",
      "Epoch 9: |          | 325/? [04:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 325, loss 4.044488906860352\n",
      "Epoch 9: |          | 326/? [04:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 326, loss 3.6927273273468018\n",
      "Epoch 9: |          | 327/? [04:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 327, loss 3.3958022594451904\n",
      "Epoch 9: |          | 328/? [04:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 328, loss 3.2215237617492676\n",
      "Epoch 9: |          | 329/? [04:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 329, loss 3.721456527709961\n",
      "Epoch 9: |          | 330/? [04:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 330, loss 4.249995231628418\n",
      "Epoch 9: |          | 331/? [04:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 331, loss 2.6093292236328125\n",
      "Epoch 9: |          | 332/? [04:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 332, loss 3.5926411151885986\n",
      "Epoch 9: |          | 333/? [04:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 333, loss 3.518104076385498\n",
      "Epoch 9: |          | 334/? [04:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 334, loss 3.9927830696105957\n",
      "Epoch 9: |          | 335/? [04:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 335, loss 3.9246208667755127\n",
      "Epoch 9: |          | 336/? [04:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 336, loss 3.9705119132995605\n",
      "Epoch 9: |          | 337/? [04:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 337, loss 4.399796962738037\n",
      "Epoch 9: |          | 338/? [04:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 338, loss 4.063138008117676\n",
      "Epoch 9: |          | 339/? [04:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 339, loss 3.3690123558044434\n",
      "Epoch 9: |          | 340/? [04:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 340, loss 3.107062816619873\n",
      "Epoch 9: |          | 341/? [04:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 341, loss 3.121443271636963\n",
      "Epoch 9: |          | 342/? [04:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 342, loss 3.6669960021972656\n",
      "Epoch 9: |          | 343/? [04:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 343, loss 3.437798261642456\n",
      "Epoch 9: |          | 344/? [04:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 344, loss 4.257207870483398\n",
      "Epoch 9: |          | 345/? [04:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 345, loss 3.4099178314208984\n",
      "Epoch 9: |          | 346/? [04:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 346, loss 3.701751708984375\n",
      "Epoch 9: |          | 347/? [04:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 347, loss 3.5308384895324707\n",
      "Epoch 9: |          | 348/? [04:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 348, loss 2.95196533203125\n",
      "Epoch 9: |          | 349/? [05:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 349, loss 2.82383394241333\n",
      "Epoch 9: |          | 350/? [05:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 350, loss 3.955634593963623\n",
      "Epoch 9: |          | 351/? [05:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 351, loss 3.9969654083251953\n",
      "Epoch 9: |          | 352/? [05:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 352, loss 3.308657169342041\n",
      "Epoch 9: |          | 353/? [05:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 353, loss 3.0965006351470947\n",
      "Epoch 9: |          | 354/? [05:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 354, loss 3.5179734230041504\n",
      "Epoch 9: |          | 355/? [05:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 355, loss 3.7987818717956543\n",
      "Epoch 9: |          | 356/? [05:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 356, loss 3.810328960418701\n",
      "Epoch 9: |          | 357/? [05:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 357, loss 3.279542922973633\n",
      "Epoch 9: |          | 358/? [05:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 358, loss 3.24257230758667\n",
      "Epoch 9: |          | 359/? [05:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 359, loss 3.762535810470581\n",
      "Epoch 9: |          | 360/? [05:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 360, loss 3.3929476737976074\n",
      "Epoch 9: |          | 361/? [05:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 361, loss 3.5534896850585938\n",
      "Epoch 9: |          | 362/? [05:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 362, loss 3.312218427658081\n",
      "Epoch 9: |          | 363/? [05:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 363, loss 3.241610050201416\n",
      "Epoch 9: |          | 364/? [05:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 364, loss 3.809291362762451\n",
      "Epoch 9: |          | 365/? [05:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 365, loss 3.8228042125701904\n",
      "Epoch 9: |          | 366/? [05:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 366, loss 3.683462619781494\n",
      "Epoch 9: |          | 367/? [05:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 367, loss 3.5822227001190186\n",
      "Epoch 9: |          | 368/? [05:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 368, loss 3.2234930992126465\n",
      "Epoch 9: |          | 369/? [05:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 369, loss 3.479400157928467\n",
      "Epoch 9: |          | 370/? [05:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 370, loss 3.236679792404175\n",
      "Epoch 9: |          | 371/? [05:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 371, loss 4.051379203796387\n",
      "Epoch 9: |          | 372/? [05:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 372, loss 3.368993043899536\n",
      "Epoch 9: |          | 373/? [05:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 373, loss 3.7714219093322754\n",
      "Epoch 9: |          | 374/? [05:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 374, loss 3.461066722869873\n",
      "Epoch 9: |          | 375/? [05:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 375, loss 4.047336101531982\n",
      "Epoch 9: |          | 376/? [05:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 376, loss 3.5221900939941406\n",
      "Epoch 9: |          | 377/? [05:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 377, loss 3.707096576690674\n",
      "Epoch 9: |          | 378/? [05:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 378, loss 3.8332176208496094\n",
      "Epoch 9: |          | 379/? [05:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 379, loss 3.64445424079895\n",
      "Epoch 9: |          | 380/? [05:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 380, loss 3.642986297607422\n",
      "Epoch 9: |          | 381/? [05:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 381, loss 3.7740230560302734\n",
      "Epoch 9: |          | 382/? [05:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 382, loss 3.5041115283966064\n",
      "Epoch 9: |          | 383/? [05:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 383, loss 3.5850830078125\n",
      "Epoch 9: |          | 384/? [05:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 384, loss 3.9300389289855957\n",
      "Epoch 9: |          | 385/? [05:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 385, loss 3.5681004524230957\n",
      "Epoch 9: |          | 386/? [05:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 386, loss 2.6408944129943848\n",
      "Epoch 9: |          | 387/? [05:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 387, loss 3.4111149311065674\n",
      "Epoch 9: |          | 388/? [05:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 388, loss 2.974921703338623\n",
      "Epoch 9: |          | 389/? [05:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 389, loss 3.910836696624756\n",
      "Epoch 9: |          | 390/? [05:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 390, loss 3.385826826095581\n",
      "Epoch 9: |          | 391/? [05:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 391, loss 3.774021625518799\n",
      "Epoch 9: |          | 392/? [05:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 392, loss 3.920440673828125\n",
      "Epoch 9: |          | 393/? [05:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 393, loss 3.932288408279419\n",
      "Epoch 9: |          | 394/? [05:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 394, loss 3.5922722816467285\n",
      "Epoch 9: |          | 395/? [05:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 395, loss 3.8072280883789062\n",
      "Epoch 9: |          | 396/? [05:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 396, loss 3.663321018218994\n",
      "Epoch 9: |          | 397/? [05:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 397, loss 3.4879791736602783\n",
      "Epoch 9: |          | 398/? [05:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 398, loss 3.4048304557800293\n",
      "Epoch 9: |          | 399/? [05:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 399, loss 3.492095470428467\n",
      "Epoch 9: |          | 400/? [05:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 400, loss 3.4955546855926514\n",
      "Epoch 9: |          | 401/? [05:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 401, loss 3.469041347503662\n",
      "Epoch 9: |          | 402/? [05:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 402, loss 3.834327220916748\n",
      "Epoch 9: |          | 403/? [05:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 403, loss 3.7517402172088623\n",
      "Epoch 9: |          | 404/? [05:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 404, loss 3.295325517654419\n",
      "Epoch 9: |          | 405/? [05:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 405, loss 3.3635544776916504\n",
      "Epoch 9: |          | 406/? [05:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 406, loss 3.647859573364258\n",
      "Epoch 9: |          | 407/? [05:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 407, loss 3.5357823371887207\n",
      "Epoch 9: |          | 408/? [05:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 408, loss 3.9771041870117188\n",
      "Epoch 9: |          | 409/? [05:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 409, loss 3.875934600830078\n",
      "Epoch 9: |          | 410/? [05:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 410, loss 3.5163979530334473\n",
      "Epoch 9: |          | 411/? [05:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 411, loss 3.464564085006714\n",
      "Epoch 9: |          | 412/? [05:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 412, loss 3.0799756050109863\n",
      "Epoch 9: |          | 413/? [05:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 413, loss 3.7255330085754395\n",
      "Epoch 9: |          | 414/? [05:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 414, loss 3.3371987342834473\n",
      "Epoch 9: |          | 415/? [05:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 415, loss 3.7306182384490967\n",
      "Epoch 9: |          | 416/? [05:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 416, loss 4.162501335144043\n",
      "Epoch 9: |          | 417/? [05:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 417, loss 4.085846900939941\n",
      "Epoch 9: |          | 418/? [05:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 418, loss 3.7501723766326904\n",
      "Epoch 9: |          | 419/? [05:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 419, loss 3.49041748046875\n",
      "Epoch 9: |          | 420/? [05:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 420, loss 3.6932361125946045\n",
      "Epoch 9: |          | 421/? [06:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 421, loss 4.160726070404053\n",
      "Epoch 9: |          | 422/? [06:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 422, loss 3.713031053543091\n",
      "Epoch 9: |          | 423/? [06:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 423, loss 3.366668701171875\n",
      "Epoch 9: |          | 424/? [06:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 424, loss 3.8913676738739014\n",
      "Epoch 9: |          | 425/? [06:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 425, loss 3.5878214836120605\n",
      "Epoch 9: |          | 426/? [06:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 426, loss 3.3679606914520264\n",
      "Epoch 9: |          | 427/? [06:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 427, loss 3.4188945293426514\n",
      "Epoch 9: |          | 428/? [06:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 428, loss 4.111424922943115\n",
      "Epoch 9: |          | 429/? [06:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 429, loss 3.067579746246338\n",
      "Epoch 9: |          | 430/? [06:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 430, loss 3.701982021331787\n",
      "Epoch 9: |          | 431/? [06:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 431, loss 3.6013126373291016\n",
      "Epoch 9: |          | 432/? [06:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 432, loss 3.723717451095581\n",
      "Epoch 9: |          | 433/? [06:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 433, loss 3.6890931129455566\n",
      "Epoch 9: |          | 434/? [06:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 434, loss 3.5753257274627686\n",
      "Epoch 9: |          | 435/? [06:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 435, loss 3.2828497886657715\n",
      "Epoch 9: |          | 436/? [06:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 436, loss 3.6556522846221924\n",
      "Epoch 9: |          | 437/? [06:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 437, loss 3.8899428844451904\n",
      "Epoch 9: |          | 438/? [06:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 438, loss 3.5044143199920654\n",
      "Epoch 9: |          | 439/? [06:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 439, loss 3.3107352256774902\n",
      "Epoch 9: |          | 440/? [06:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 440, loss 3.166598320007324\n",
      "Epoch 9: |          | 441/? [06:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 441, loss 3.6255390644073486\n",
      "Epoch 9: |          | 442/? [06:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 442, loss 3.515946865081787\n",
      "Epoch 9: |          | 443/? [06:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 443, loss 3.6580138206481934\n",
      "Epoch 9: |          | 444/? [06:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 444, loss 3.6841137409210205\n",
      "Epoch 9: |          | 445/? [06:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 445, loss 4.538922309875488\n",
      "Epoch 9: |          | 446/? [06:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 446, loss 3.6050477027893066\n",
      "Epoch 9: |          | 447/? [06:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 447, loss 4.112478256225586\n",
      "Epoch 9: |          | 448/? [06:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 448, loss 3.2090396881103516\n",
      "Epoch 9: |          | 449/? [06:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 449, loss 3.641913890838623\n",
      "Epoch 9: |          | 450/? [06:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 450, loss 3.9027562141418457\n",
      "Epoch 9: |          | 451/? [06:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 451, loss 3.62817645072937\n",
      "Epoch 9: |          | 452/? [06:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 452, loss 3.3128275871276855\n",
      "Epoch 9: |          | 453/? [06:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 453, loss 4.0451579093933105\n",
      "Epoch 9: |          | 454/? [06:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 454, loss 3.4295878410339355\n",
      "Epoch 9: |          | 455/? [06:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 455, loss 3.7233200073242188\n",
      "Epoch 9: |          | 456/? [06:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 456, loss 3.1138482093811035\n",
      "Epoch 9: |          | 457/? [06:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 457, loss 3.552304744720459\n",
      "Epoch 9: |          | 458/? [06:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 458, loss 3.9657275676727295\n",
      "Epoch 9: |          | 459/? [06:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 459, loss 3.958153247833252\n",
      "Epoch 9: |          | 460/? [06:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 460, loss 3.6967990398406982\n",
      "Epoch 9: |          | 461/? [06:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 461, loss 3.6703171730041504\n",
      "Epoch 9: |          | 462/? [06:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 462, loss 3.7321739196777344\n",
      "Epoch 9: |          | 463/? [06:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 463, loss 3.611408233642578\n",
      "Epoch 9: |          | 464/? [06:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 464, loss 3.1574912071228027\n",
      "Epoch 9: |          | 465/? [06:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 465, loss 3.399369478225708\n",
      "Epoch 9: |          | 466/? [06:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 466, loss 3.881570816040039\n",
      "Epoch 9: |          | 467/? [06:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 467, loss 3.639739513397217\n",
      "Epoch 9: |          | 468/? [06:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 468, loss 3.5576095581054688\n",
      "Epoch 9: |          | 469/? [06:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 469, loss 3.695890426635742\n",
      "Epoch 9: |          | 470/? [06:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 470, loss 3.1743485927581787\n",
      "Epoch 9: |          | 471/? [06:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 471, loss 3.943758726119995\n",
      "Epoch 9: |          | 472/? [06:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 472, loss 3.38679838180542\n",
      "Epoch 9: |          | 473/? [06:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 473, loss 3.392885208129883\n",
      "Epoch 9: |          | 474/? [06:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 474, loss 3.0842626094818115\n",
      "Epoch 9: |          | 475/? [06:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 475, loss 4.206623554229736\n",
      "Epoch 9: |          | 476/? [06:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 476, loss 3.373251438140869\n",
      "Epoch 9: |          | 477/? [06:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 477, loss 2.923954486846924\n",
      "Epoch 9: |          | 478/? [06:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 478, loss 3.1212379932403564\n",
      "Epoch 9: |          | 479/? [06:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 479, loss 3.563143491744995\n",
      "Epoch 9: |          | 480/? [06:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 480, loss 3.3895156383514404\n",
      "Epoch 9: |          | 481/? [06:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 481, loss 3.07017183303833\n",
      "Epoch 9: |          | 482/? [06:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 482, loss 3.360006809234619\n",
      "Epoch 9: |          | 483/? [06:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 483, loss 3.0786962509155273\n",
      "Epoch 9: |          | 484/? [06:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 484, loss 3.9125475883483887\n",
      "Epoch 9: |          | 485/? [06:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 485, loss 3.833698272705078\n",
      "Epoch 9: |          | 486/? [06:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 486, loss 3.381319046020508\n",
      "Epoch 9: |          | 487/? [06:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 487, loss 3.800579786300659\n",
      "Epoch 9: |          | 488/? [06:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 488, loss 3.5569260120391846\n",
      "Epoch 9: |          | 489/? [06:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 489, loss 3.1191067695617676\n",
      "Epoch 9: |          | 490/? [06:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 490, loss 3.645402431488037\n",
      "Epoch 9: |          | 491/? [06:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 491, loss 3.5250065326690674\n",
      "Epoch 9: |          | 492/? [06:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 492, loss 2.898754358291626\n",
      "Epoch 9: |          | 493/? [07:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 493, loss 3.849823474884033\n",
      "Epoch 9: |          | 494/? [07:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 494, loss 3.636606216430664\n",
      "Epoch 9: |          | 495/? [07:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 495, loss 3.742199659347534\n",
      "Epoch 9: |          | 496/? [07:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 496, loss 3.3861632347106934\n",
      "Epoch 9: |          | 497/? [07:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 497, loss 3.9173858165740967\n",
      "Epoch 9: |          | 498/? [07:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 498, loss 3.5389842987060547\n",
      "Epoch 9: |          | 499/? [07:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 499, loss 3.690873622894287\n",
      "Epoch 9: |          | 500/? [07:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 500, loss 3.4464097023010254\n",
      "Epoch 9: |          | 501/? [07:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 501, loss 3.2297019958496094\n",
      "Epoch 9: |          | 502/? [07:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 502, loss 3.6419262886047363\n",
      "Epoch 9: |          | 503/? [07:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 503, loss 3.5403857231140137\n",
      "Epoch 9: |          | 504/? [07:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 504, loss 3.515211582183838\n",
      "Epoch 9: |          | 505/? [07:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 505, loss 3.0328001976013184\n",
      "Epoch 9: |          | 506/? [07:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 506, loss 3.5445713996887207\n",
      "Epoch 9: |          | 507/? [07:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 507, loss 3.564486265182495\n",
      "Epoch 9: |          | 508/? [07:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 508, loss 3.9000296592712402\n",
      "Epoch 9: |          | 509/? [07:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 509, loss 3.3301143646240234\n",
      "Epoch 9: |          | 510/? [07:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 510, loss 3.734980821609497\n",
      "Epoch 9: |          | 511/? [07:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 511, loss 3.6168270111083984\n",
      "Epoch 9: |          | 512/? [07:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 512, loss 3.0794780254364014\n",
      "Epoch 9: |          | 513/? [07:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 513, loss 3.311171293258667\n",
      "Epoch 9: |          | 514/? [07:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 514, loss 3.4832634925842285\n",
      "Epoch 9: |          | 515/? [07:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 515, loss 3.0883522033691406\n",
      "Epoch 9: |          | 516/? [07:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 516, loss 3.462279796600342\n",
      "Epoch 9: |          | 517/? [07:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 517, loss 3.5915684700012207\n",
      "Epoch 9: |          | 518/? [07:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 518, loss 3.213547468185425\n",
      "Epoch 9: |          | 519/? [07:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 519, loss 3.619025707244873\n",
      "Epoch 9: |          | 520/? [07:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 520, loss 3.5033631324768066\n",
      "Epoch 9: |          | 521/? [07:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 521, loss 3.52162504196167\n",
      "Epoch 9: |          | 522/? [07:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 522, loss 4.050885200500488\n",
      "Epoch 9: |          | 523/? [07:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 523, loss 4.086150169372559\n",
      "Epoch 9: |          | 524/? [07:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 524, loss 3.9044597148895264\n",
      "Epoch 9: |          | 525/? [07:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 525, loss 3.5008888244628906\n",
      "Epoch 9: |          | 526/? [07:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 526, loss 3.296351909637451\n",
      "Epoch 9: |          | 527/? [07:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 527, loss 3.9701104164123535\n",
      "Epoch 9: |          | 528/? [07:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 528, loss 3.7278003692626953\n",
      "Epoch 9: |          | 529/? [07:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 529, loss 3.3247368335723877\n",
      "Epoch 9: |          | 530/? [07:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 530, loss 3.8359742164611816\n",
      "Epoch 9: |          | 531/? [07:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 531, loss 3.3671584129333496\n",
      "Epoch 9: |          | 532/? [07:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 532, loss 3.6403496265411377\n",
      "Epoch 9: |          | 533/? [07:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 533, loss 3.298168897628784\n",
      "Epoch 9: |          | 534/? [07:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 534, loss 2.9966049194335938\n",
      "Epoch 9: |          | 535/? [07:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 535, loss 3.2449240684509277\n",
      "Epoch 9: |          | 536/? [07:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 536, loss 3.9093871116638184\n",
      "Epoch 9: |          | 537/? [07:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 537, loss 3.7022414207458496\n",
      "Epoch 9: |          | 538/? [07:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 538, loss 3.3450584411621094\n",
      "Epoch 9: |          | 539/? [07:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 539, loss 3.47221040725708\n",
      "Epoch 9: |          | 540/? [07:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 540, loss 3.806354522705078\n",
      "Epoch 9: |          | 541/? [07:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 541, loss 3.603093385696411\n",
      "Epoch 9: |          | 542/? [07:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 542, loss 3.3671791553497314\n",
      "Epoch 9: |          | 543/? [07:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 543, loss 3.7395713329315186\n",
      "Epoch 9: |          | 544/? [07:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 544, loss 3.6588950157165527\n",
      "Epoch 9: |          | 545/? [07:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 545, loss 3.001159906387329\n",
      "Epoch 9: |          | 546/? [07:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 546, loss 3.7208313941955566\n",
      "Epoch 9: |          | 547/? [07:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 547, loss 4.173096179962158\n",
      "Epoch 9: |          | 548/? [07:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 548, loss 3.7931923866271973\n",
      "Epoch 9: |          | 549/? [07:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 549, loss 3.7001190185546875\n",
      "Epoch 9: |          | 550/? [07:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 550, loss 3.9978318214416504\n",
      "Epoch 9: |          | 551/? [07:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 551, loss 3.710442304611206\n",
      "Epoch 9: |          | 552/? [07:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 552, loss 3.633864641189575\n",
      "Epoch 9: |          | 553/? [07:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 553, loss 3.1662490367889404\n",
      "Epoch 9: |          | 554/? [07:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 554, loss 3.700587749481201\n",
      "Epoch 9: |          | 555/? [07:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 555, loss 3.9156997203826904\n",
      "Epoch 9: |          | 556/? [07:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 556, loss 3.7940833568573\n",
      "Epoch 9: |          | 557/? [07:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 557, loss 3.2677993774414062\n",
      "Epoch 9: |          | 558/? [07:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 558, loss 3.517671585083008\n",
      "Epoch 9: |          | 559/? [07:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 559, loss 3.4465839862823486\n",
      "Epoch 9: |          | 560/? [07:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 560, loss 3.036734104156494\n",
      "Epoch 9: |          | 561/? [07:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 561, loss 2.871793746948242\n",
      "Epoch 9: |          | 562/? [07:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 562, loss 3.8440537452697754\n",
      "Epoch 9: |          | 563/? [07:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 563, loss 2.9952614307403564\n",
      "Epoch 9: |          | 564/? [07:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 564, loss 3.418816089630127\n",
      "Epoch 9: |          | 565/? [08:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 565, loss 3.7133336067199707\n",
      "Epoch 9: |          | 566/? [08:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 566, loss 3.8764870166778564\n",
      "Epoch 9: |          | 567/? [08:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 567, loss 3.9318923950195312\n",
      "Epoch 9: |          | 568/? [08:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 568, loss 3.0568575859069824\n",
      "Epoch 9: |          | 569/? [08:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 569, loss 3.6164023876190186\n",
      "Epoch 9: |          | 570/? [08:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 570, loss 3.7519733905792236\n",
      "Epoch 9: |          | 571/? [08:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 571, loss 3.3756091594696045\n",
      "Epoch 9: |          | 572/? [08:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 572, loss 4.2947893142700195\n",
      "Epoch 9: |          | 573/? [08:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 573, loss 2.767411470413208\n",
      "Epoch 9: |          | 574/? [08:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 574, loss 3.8801894187927246\n",
      "Epoch 9: |          | 575/? [08:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 575, loss 3.244868755340576\n",
      "Epoch 9: |          | 576/? [08:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 576, loss 3.440523862838745\n",
      "Epoch 9: |          | 577/? [08:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 577, loss 3.6170411109924316\n",
      "Epoch 9: |          | 578/? [08:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 578, loss 3.89483380317688\n",
      "Epoch 9: |          | 579/? [08:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 579, loss 3.0795044898986816\n",
      "Epoch 9: |          | 580/? [08:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 580, loss 3.706653118133545\n",
      "Epoch 9: |          | 581/? [08:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 581, loss 3.6960549354553223\n",
      "Epoch 9: |          | 582/? [08:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 582, loss 3.758497714996338\n",
      "Epoch 9: |          | 583/? [08:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 583, loss 3.5634193420410156\n",
      "Epoch 9: |          | 584/? [08:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 584, loss 3.786479949951172\n",
      "Epoch 9: |          | 585/? [08:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 585, loss 3.6720147132873535\n",
      "Epoch 9: |          | 586/? [08:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 586, loss 3.7670562267303467\n",
      "Epoch 9: |          | 587/? [08:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 587, loss 3.7989954948425293\n",
      "Epoch 9: |          | 588/? [08:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 588, loss 3.6481494903564453\n",
      "Epoch 9: |          | 589/? [08:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 589, loss 3.215263843536377\n",
      "Epoch 9: |          | 590/? [08:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 590, loss 3.685594081878662\n",
      "Epoch 9: |          | 591/? [08:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 591, loss 3.6231770515441895\n",
      "Epoch 9: |          | 592/? [08:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 592, loss 3.2946178913116455\n",
      "Epoch 9: |          | 593/? [08:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 593, loss 3.6807713508605957\n",
      "Epoch 9: |          | 594/? [08:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 594, loss 4.38344669342041\n",
      "Epoch 9: |          | 595/? [08:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 595, loss 3.2262561321258545\n",
      "Epoch 9: |          | 596/? [08:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 596, loss 3.2767996788024902\n",
      "Epoch 9: |          | 597/? [08:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 597, loss 3.4702372550964355\n",
      "Epoch 9: |          | 598/? [08:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 598, loss 3.880580425262451\n",
      "Epoch 9: |          | 599/? [08:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 599, loss 3.578200101852417\n",
      "Epoch 9: |          | 600/? [08:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 600, loss 3.3671183586120605\n",
      "Epoch 9: |          | 601/? [08:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 601, loss 3.6597275733947754\n",
      "Epoch 9: |          | 602/? [08:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 602, loss 3.2167694568634033\n",
      "Epoch 9: |          | 603/? [08:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 603, loss 3.288644313812256\n",
      "Epoch 9: |          | 604/? [08:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 604, loss 4.377089023590088\n",
      "Epoch 9: |          | 605/? [08:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 605, loss 3.092076301574707\n",
      "Epoch 9: |          | 606/? [08:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 606, loss 3.4228217601776123\n",
      "Epoch 9: |          | 607/? [08:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 607, loss 3.676596164703369\n",
      "Epoch 9: |          | 608/? [08:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 608, loss 3.4750239849090576\n",
      "Epoch 9: |          | 609/? [08:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 609, loss 3.407414674758911\n",
      "Epoch 9: |          | 610/? [08:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 610, loss 3.499697208404541\n",
      "Epoch 9: |          | 611/? [08:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 611, loss 3.6120429039001465\n",
      "Epoch 9: |          | 612/? [08:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 612, loss 3.372962236404419\n",
      "Epoch 9: |          | 613/? [08:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 613, loss 3.6795754432678223\n",
      "Epoch 9: |          | 614/? [08:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 614, loss 3.46040415763855\n",
      "Epoch 9: |          | 615/? [08:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 615, loss 3.9962210655212402\n",
      "Epoch 9: |          | 616/? [08:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 616, loss 4.084675312042236\n",
      "Epoch 9: |          | 617/? [08:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 617, loss 2.7793195247650146\n",
      "Epoch 9: |          | 618/? [08:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 618, loss 3.6922500133514404\n",
      "Epoch 9: |          | 619/? [08:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 619, loss 3.2421417236328125\n",
      "Epoch 9: |          | 620/? [08:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 620, loss 3.770936965942383\n",
      "Epoch 9: |          | 621/? [08:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 621, loss 3.2990593910217285\n",
      "Epoch 9: |          | 622/? [08:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 622, loss 3.07903790473938\n",
      "Epoch 9: |          | 623/? [08:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 623, loss 2.9347660541534424\n",
      "Epoch 9: |          | 624/? [08:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 624, loss 2.6699302196502686\n",
      "Epoch 9: |          | 625/? [08:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 625, loss 4.039055824279785\n",
      "Epoch 9: |          | 626/? [08:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 626, loss 3.482283353805542\n",
      "Epoch 9: |          | 627/? [08:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 627, loss 3.4357330799102783\n",
      "Epoch 9: |          | 628/? [08:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 628, loss 3.4611144065856934\n",
      "Epoch 9: |          | 629/? [08:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 629, loss 3.7934932708740234\n",
      "Epoch 9: |          | 630/? [08:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 630, loss 3.591257095336914\n",
      "Epoch 9: |          | 631/? [08:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 631, loss 3.6716151237487793\n",
      "Epoch 9: |          | 632/? [08:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 632, loss 3.100228786468506\n",
      "Epoch 9: |          | 633/? [08:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 633, loss 3.7862401008605957\n",
      "Epoch 9: |          | 634/? [08:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 634, loss 3.35957407951355\n",
      "Epoch 9: |          | 635/? [08:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 635, loss 3.2264556884765625\n",
      "Epoch 9: |          | 636/? [08:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 636, loss 3.602560520172119\n",
      "Epoch 9: |          | 637/? [09:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 637, loss 3.4218437671661377\n",
      "Epoch 9: |          | 638/? [09:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 638, loss 3.6455254554748535\n",
      "Epoch 9: |          | 639/? [09:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 639, loss 3.408057451248169\n",
      "Epoch 9: |          | 640/? [09:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 640, loss 3.9067482948303223\n",
      "Epoch 9: |          | 641/? [09:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 641, loss 2.962797164916992\n",
      "Epoch 9: |          | 642/? [09:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 642, loss 3.7826857566833496\n",
      "Epoch 9: |          | 643/? [09:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 643, loss 3.5956997871398926\n",
      "Epoch 9: |          | 644/? [09:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 644, loss 3.6317882537841797\n",
      "Epoch 9: |          | 645/? [09:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 645, loss 3.3442389965057373\n",
      "Epoch 9: |          | 646/? [09:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 646, loss 3.3247909545898438\n",
      "Epoch 9: |          | 647/? [09:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 647, loss 3.8590996265411377\n",
      "Epoch 9: |          | 648/? [09:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 648, loss 3.2824454307556152\n",
      "Epoch 9: |          | 649/? [09:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 649, loss 2.724236249923706\n",
      "Epoch 9: |          | 650/? [09:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 650, loss 3.8303115367889404\n",
      "Epoch 9: |          | 651/? [09:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 651, loss 3.9902732372283936\n",
      "Epoch 9: |          | 652/? [09:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 652, loss 3.4664695262908936\n",
      "Epoch 9: |          | 653/? [09:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 653, loss 3.6234054565429688\n",
      "Epoch 9: |          | 654/? [09:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 654, loss 3.6482625007629395\n",
      "Epoch 9: |          | 655/? [09:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 655, loss 3.4391398429870605\n",
      "Epoch 9: |          | 656/? [09:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 656, loss 3.1648993492126465\n",
      "Epoch 9: |          | 657/? [09:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 657, loss 5.196437358856201\n",
      "Epoch 9: |          | 658/? [09:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 658, loss 2.9881906509399414\n",
      "Epoch 9: |          | 659/? [09:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 659, loss 3.578568935394287\n",
      "Epoch 9: |          | 660/? [09:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 660, loss 3.886181354522705\n",
      "Epoch 9: |          | 661/? [09:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 661, loss 3.8596558570861816\n",
      "Epoch 9: |          | 662/? [09:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 662, loss 3.698375701904297\n",
      "Epoch 9: |          | 663/? [09:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 663, loss 3.4445674419403076\n",
      "Epoch 9: |          | 664/? [09:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 664, loss 3.4178473949432373\n",
      "Epoch 9: |          | 665/? [09:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 665, loss 3.6941750049591064\n",
      "Epoch 9: |          | 666/? [09:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 666, loss 3.4953620433807373\n",
      "Epoch 9: |          | 667/? [09:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 667, loss 4.2718048095703125\n",
      "Epoch 9: |          | 668/? [09:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 668, loss 3.1295595169067383\n",
      "Epoch 9: |          | 669/? [09:26<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 669, loss 3.3009986877441406\n",
      "Epoch 9: |          | 670/? [09:27<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 670, loss 3.950411558151245\n",
      "Epoch 9: |          | 671/? [09:28<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 671, loss 3.7145347595214844\n",
      "Epoch 9: |          | 672/? [09:29<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 672, loss 3.7317981719970703\n",
      "Epoch 9: |          | 673/? [09:30<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 673, loss 3.587506055831909\n",
      "Epoch 9: |          | 674/? [09:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 674, loss 2.054110288619995\n",
      "Epoch 9: |          | 675/? [09:31<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 675, loss 0.7350850701332092\n",
      "Epoch 9: |          | 676/? [09:32<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 676, loss 0.6004208326339722\n",
      "Epoch 9: |          | 677/? [09:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 677, loss 0.4983089566230774\n",
      "Epoch 9: |          | 678/? [09:33<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 678, loss 1.6657826900482178\n",
      "Epoch 9: |          | 679/? [09:34<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 679, loss 3.0067453384399414\n",
      "Epoch 9: |          | 680/? [09:35<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 680, loss 3.4658164978027344\n",
      "Epoch 9: |          | 681/? [09:36<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 681, loss 3.1183083057403564\n",
      "Epoch 9: |          | 682/? [09:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 682, loss 3.349956512451172\n",
      "Epoch 9: |          | 683/? [09:37<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 683, loss 3.0843892097473145\n",
      "Epoch 9: |          | 684/? [09:38<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 684, loss 4.048823356628418\n",
      "Epoch 9: |          | 685/? [09:39<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 685, loss 3.6495003700256348\n",
      "Epoch 9: |          | 686/? [09:40<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 686, loss 3.3485805988311768\n",
      "Epoch 9: |          | 687/? [09:41<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 687, loss 3.797450542449951\n",
      "Epoch 9: |          | 688/? [09:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 688, loss 3.262619733810425\n",
      "Epoch 9: |          | 689/? [09:42<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 689, loss 3.3888003826141357\n",
      "Epoch 9: |          | 690/? [09:43<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 690, loss 4.0453691482543945\n",
      "Epoch 9: |          | 691/? [09:44<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 691, loss 3.536250352859497\n",
      "Epoch 9: |          | 692/? [09:45<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 692, loss 3.562067747116089\n",
      "Epoch 9: |          | 693/? [09:46<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 693, loss 4.079232215881348\n",
      "Epoch 9: |          | 694/? [09:47<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 694, loss 3.44199800491333\n",
      "Epoch 9: |          | 695/? [09:48<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 695, loss 3.964477062225342\n",
      "Epoch 9: |          | 696/? [09:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 696, loss 3.347294330596924\n",
      "Epoch 9: |          | 697/? [09:49<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 697, loss 3.49128794670105\n",
      "Epoch 9: |          | 698/? [09:50<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 698, loss 3.011265993118286\n",
      "Epoch 9: |          | 699/? [09:51<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 699, loss 3.692579984664917\n",
      "Epoch 9: |          | 700/? [09:52<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 700, loss 3.719508409500122\n",
      "Epoch 9: |          | 701/? [09:53<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 701, loss 3.417678117752075\n",
      "Epoch 9: |          | 702/? [09:54<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 702, loss 3.6659419536590576\n",
      "Epoch 9: |          | 703/? [09:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 703, loss 3.762615203857422\n",
      "Epoch 9: |          | 704/? [09:55<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 704, loss 3.610325574874878\n",
      "Epoch 9: |          | 705/? [09:56<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 705, loss 3.275479793548584\n",
      "Epoch 9: |          | 706/? [09:57<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 706, loss 3.330606460571289\n",
      "Epoch 9: |          | 707/? [09:58<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 707, loss 3.777130603790283\n",
      "Epoch 9: |          | 708/? [09:59<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 708, loss 3.5372300148010254\n",
      "Epoch 9: |          | 709/? [10:00<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 709, loss 3.4337611198425293\n",
      "Epoch 9: |          | 710/? [10:01<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 710, loss 3.9441699981689453\n",
      "Epoch 9: |          | 711/? [10:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 711, loss 3.9297587871551514\n",
      "Epoch 9: |          | 712/? [10:02<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 712, loss 3.7592644691467285\n",
      "Epoch 9: |          | 713/? [10:03<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 713, loss 3.861232042312622\n",
      "Epoch 9: |          | 714/? [10:04<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 714, loss 3.858424663543701\n",
      "Epoch 9: |          | 715/? [10:05<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 715, loss 2.920100212097168\n",
      "Epoch 9: |          | 716/? [10:06<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 716, loss 3.5681278705596924\n",
      "Epoch 9: |          | 717/? [10:07<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 717, loss 3.5251851081848145\n",
      "Epoch 9: |          | 718/? [10:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 718, loss 3.0280585289001465\n",
      "Epoch 9: |          | 719/? [10:08<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 719, loss 3.4587695598602295\n",
      "Epoch 9: |          | 720/? [10:09<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 720, loss 3.1875908374786377\n",
      "Epoch 9: |          | 721/? [10:10<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 721, loss 3.8463871479034424\n",
      "Epoch 9: |          | 722/? [10:11<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 722, loss 3.2475368976593018\n",
      "Epoch 9: |          | 723/? [10:12<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 723, loss 3.7053637504577637\n",
      "Epoch 9: |          | 724/? [10:13<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 724, loss 3.247488021850586\n",
      "Epoch 9: |          | 725/? [10:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 725, loss 3.249556303024292\n",
      "Epoch 9: |          | 726/? [10:14<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 726, loss 3.400036334991455\n",
      "Epoch 9: |          | 727/? [10:15<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 727, loss 3.213291883468628\n",
      "Epoch 9: |          | 728/? [10:16<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 728, loss 3.056053876876831\n",
      "Epoch 9: |          | 729/? [10:17<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 729, loss 3.5441489219665527\n",
      "Epoch 9: |          | 730/? [10:18<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 730, loss 3.5257010459899902\n",
      "Epoch 9: |          | 731/? [10:19<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 731, loss 3.6149330139160156\n",
      "Epoch 9: |          | 732/? [10:20<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 732, loss 3.8178791999816895\n",
      "Epoch 9: |          | 733/? [10:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 733, loss 3.5646519660949707\n",
      "Epoch 9: |          | 734/? [10:21<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 734, loss 3.6428158283233643\n",
      "Epoch 9: |          | 735/? [10:22<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 735, loss 3.605839490890503\n",
      "Epoch 9: |          | 736/? [10:23<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 736, loss 3.2434375286102295\n",
      "Epoch 9: |          | 737/? [10:24<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 737, loss 3.9645538330078125\n",
      "Epoch 9: |          | 738/? [10:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 738, loss 3.222670078277588\n",
      "Epoch 9: |          | 739/? [10:25<00:00,  1.18it/s, v_num=30]   TRRAINING: Batch 739, loss 3.6166584491729736\n",
      "Epoch 9: |          | 740/? [10:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 740, loss 3.394085645675659\n",
      "Epoch 9: |          | 741/? [10:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 741, loss 3.4957573413848877\n",
      "Epoch 9: |          | 742/? [10:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 742, loss 3.8494277000427246\n",
      "Epoch 9: |          | 743/? [10:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 743, loss 3.756887912750244\n",
      "Epoch 9: |          | 744/? [10:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 744, loss 3.6924140453338623\n",
      "Epoch 9: |          | 745/? [10:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 745, loss 3.353407382965088\n",
      "Epoch 9: |          | 746/? [10:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 746, loss 3.605332136154175\n",
      "Epoch 9: |          | 747/? [10:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 747, loss 3.3401806354522705\n",
      "Epoch 9: |          | 748/? [10:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 748, loss 2.4152586460113525\n",
      "Epoch 9: |          | 749/? [10:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 749, loss 3.4653942584991455\n",
      "Epoch 9: |          | 750/? [10:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 750, loss 3.691972255706787\n",
      "Epoch 9: |          | 751/? [10:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 751, loss 2.0607199668884277\n",
      "Epoch 9: |          | 752/? [10:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 752, loss 3.656217098236084\n",
      "Epoch 9: |          | 753/? [10:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 753, loss 2.8236095905303955\n",
      "Epoch 9: |          | 754/? [10:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 754, loss 3.3338699340820312\n",
      "Epoch 9: |          | 755/? [10:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 755, loss 3.167004346847534\n",
      "Epoch 9: |          | 756/? [10:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 756, loss 3.5046660900115967\n",
      "Epoch 9: |          | 757/? [10:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 757, loss 3.55985951423645\n",
      "Epoch 9: |          | 758/? [10:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 758, loss 3.352062940597534\n",
      "Epoch 9: |          | 759/? [10:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 759, loss 3.2680420875549316\n",
      "Epoch 9: |          | 760/? [10:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 760, loss 3.767596483230591\n",
      "Epoch 9: |          | 761/? [10:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 761, loss 3.777827024459839\n",
      "Epoch 9: |          | 762/? [10:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 762, loss 3.426457643508911\n",
      "Epoch 9: |          | 763/? [10:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 763, loss 3.4765942096710205\n",
      "Epoch 9: |          | 764/? [10:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 764, loss 3.822599411010742\n",
      "Epoch 9: |          | 765/? [10:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 765, loss 3.6389644145965576\n",
      "Epoch 9: |          | 766/? [10:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 766, loss 3.9692587852478027\n",
      "Epoch 9: |          | 767/? [10:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 767, loss 4.0026092529296875\n",
      "Epoch 9: |          | 768/? [10:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 768, loss 3.591294527053833\n",
      "Epoch 9: |          | 769/? [10:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 769, loss 2.878321647644043\n",
      "Epoch 9: |          | 770/? [10:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 770, loss 3.373060941696167\n",
      "Epoch 9: |          | 771/? [10:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 771, loss 4.033430576324463\n",
      "Epoch 9: |          | 772/? [10:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 772, loss 3.816631317138672\n",
      "Epoch 9: |          | 773/? [11:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 773, loss 3.5153708457946777\n",
      "Epoch 9: |          | 774/? [11:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 774, loss 3.650257110595703\n",
      "Epoch 9: |          | 775/? [11:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 775, loss 4.0609540939331055\n",
      "Epoch 9: |          | 776/? [11:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 776, loss 3.543099880218506\n",
      "Epoch 9: |          | 777/? [11:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 777, loss 3.3790314197540283\n",
      "Epoch 9: |          | 778/? [11:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 778, loss 3.7988028526306152\n",
      "Epoch 9: |          | 779/? [11:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 779, loss 4.208835124969482\n",
      "Epoch 9: |          | 780/? [11:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 780, loss 3.2645556926727295\n",
      "Epoch 9: |          | 781/? [11:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 781, loss 3.3699920177459717\n",
      "Epoch 9: |          | 782/? [11:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 782, loss 3.692333936691284\n",
      "Epoch 9: |          | 783/? [11:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 783, loss 3.750206470489502\n",
      "Epoch 9: |          | 784/? [11:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 784, loss 3.3726296424865723\n",
      "Epoch 9: |          | 785/? [11:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 785, loss 3.139204502105713\n",
      "Epoch 9: |          | 786/? [11:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 786, loss 3.992321014404297\n",
      "Epoch 9: |          | 787/? [11:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 787, loss 3.8962173461914062\n",
      "Epoch 9: |          | 788/? [11:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 788, loss 1.8639438152313232\n",
      "Epoch 9: |          | 789/? [11:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 789, loss 3.450962543487549\n",
      "Epoch 9: |          | 790/? [11:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 790, loss 4.2361016273498535\n",
      "Epoch 9: |          | 791/? [11:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 791, loss 3.9721693992614746\n",
      "Epoch 9: |          | 792/? [11:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 792, loss 3.257129669189453\n",
      "Epoch 9: |          | 793/? [11:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 793, loss 3.699939727783203\n",
      "Epoch 9: |          | 794/? [11:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 794, loss 3.997415065765381\n",
      "Epoch 9: |          | 795/? [11:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 795, loss 3.5250613689422607\n",
      "Epoch 9: |          | 796/? [11:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 796, loss 3.859337568283081\n",
      "Epoch 9: |          | 797/? [11:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 797, loss 2.932948112487793\n",
      "Epoch 9: |          | 798/? [11:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 798, loss 3.026871681213379\n",
      "Epoch 9: |          | 799/? [11:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 799, loss 3.9169578552246094\n",
      "Epoch 9: |          | 800/? [11:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 800, loss 3.731187343597412\n",
      "Epoch 9: |          | 801/? [11:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 801, loss 3.3814995288848877\n",
      "Epoch 9: |          | 802/? [11:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 802, loss 3.6352458000183105\n",
      "Epoch 9: |          | 803/? [11:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 803, loss 3.447890043258667\n",
      "Epoch 9: |          | 804/? [11:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 804, loss 3.649345874786377\n",
      "Epoch 9: |          | 805/? [11:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 805, loss 3.712034225463867\n",
      "Epoch 9: |          | 806/? [11:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 806, loss 4.22267484664917\n",
      "Epoch 9: |          | 807/? [11:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 807, loss 3.5633997917175293\n",
      "Epoch 9: |          | 808/? [11:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 808, loss 3.280966281890869\n",
      "Epoch 9: |          | 809/? [11:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 809, loss 3.744370698928833\n",
      "Epoch 9: |          | 810/? [11:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 810, loss 3.530604839324951\n",
      "Epoch 9: |          | 811/? [11:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 811, loss 3.748070478439331\n",
      "Epoch 9: |          | 812/? [11:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 812, loss 4.3004655838012695\n",
      "Epoch 9: |          | 813/? [11:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 813, loss 4.152531623840332\n",
      "Epoch 9: |          | 814/? [11:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 814, loss 3.2302098274230957\n",
      "Epoch 9: |          | 815/? [11:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 815, loss 3.900468349456787\n",
      "Epoch 9: |          | 816/? [11:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 816, loss 3.710374116897583\n",
      "Epoch 9: |          | 817/? [11:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 817, loss 3.074672222137451\n",
      "Epoch 9: |          | 818/? [11:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 818, loss 3.9744651317596436\n",
      "Epoch 9: |          | 819/? [11:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 819, loss 3.7295982837677\n",
      "Epoch 9: |          | 820/? [11:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 820, loss 3.5617001056671143\n",
      "Epoch 9: |          | 821/? [11:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 821, loss 3.506024122238159\n",
      "Epoch 9: |          | 822/? [11:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 822, loss 3.2611911296844482\n",
      "Epoch 9: |          | 823/? [11:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 823, loss 3.292196750640869\n",
      "Epoch 9: |          | 824/? [11:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 824, loss 3.6907222270965576\n",
      "Epoch 9: |          | 825/? [11:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 825, loss 3.296544313430786\n",
      "Epoch 9: |          | 826/? [11:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 826, loss 3.8074862957000732\n",
      "Epoch 9: |          | 827/? [11:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 827, loss 3.4513378143310547\n",
      "Epoch 9: |          | 828/? [11:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 828, loss 3.8464443683624268\n",
      "Epoch 9: |          | 829/? [11:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 829, loss 3.617295742034912\n",
      "Epoch 9: |          | 830/? [11:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 830, loss 4.1085662841796875\n",
      "Epoch 9: |          | 831/? [11:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 831, loss 2.1447885036468506\n",
      "Epoch 9: |          | 832/? [11:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 832, loss 3.5090994834899902\n",
      "Epoch 9: |          | 833/? [11:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 833, loss 3.433896541595459\n",
      "Epoch 9: |          | 834/? [11:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 834, loss 4.139540672302246\n",
      "Epoch 9: |          | 835/? [11:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 835, loss 3.492436170578003\n",
      "Epoch 9: |          | 836/? [11:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 836, loss 4.106484413146973\n",
      "Epoch 9: |          | 837/? [11:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 837, loss 3.536449432373047\n",
      "Epoch 9: |          | 838/? [11:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 838, loss 2.989957094192505\n",
      "Epoch 9: |          | 839/? [11:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 839, loss 3.3568973541259766\n",
      "Epoch 9: |          | 840/? [11:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 840, loss 3.873115062713623\n",
      "Epoch 9: |          | 841/? [11:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 841, loss 3.8912715911865234\n",
      "Epoch 9: |          | 842/? [11:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 842, loss 3.5771896839141846\n",
      "Epoch 9: |          | 843/? [12:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 843, loss 3.871849775314331\n",
      "Epoch 9: |          | 844/? [12:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 844, loss 3.274632215499878\n",
      "Epoch 9: |          | 845/? [12:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 845, loss 3.6332459449768066\n",
      "Epoch 9: |          | 846/? [12:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 846, loss 4.02302885055542\n",
      "Epoch 9: |          | 847/? [12:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 847, loss 3.6800014972686768\n",
      "Epoch 9: |          | 848/? [12:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 848, loss 3.2867653369903564\n",
      "Epoch 9: |          | 849/? [12:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 849, loss 3.3145835399627686\n",
      "Epoch 9: |          | 850/? [12:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 850, loss 3.4444212913513184\n",
      "Epoch 9: |          | 851/? [12:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 851, loss 3.6995582580566406\n",
      "Epoch 9: |          | 852/? [12:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 852, loss 3.899244785308838\n",
      "Epoch 9: |          | 853/? [12:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 853, loss 3.6514382362365723\n",
      "Epoch 9: |          | 854/? [12:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 854, loss 3.1113476753234863\n",
      "Epoch 9: |          | 855/? [12:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 855, loss 3.2816290855407715\n",
      "Epoch 9: |          | 856/? [12:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 856, loss 3.258934497833252\n",
      "Epoch 9: |          | 857/? [12:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 857, loss 3.736621856689453\n",
      "Epoch 9: |          | 858/? [12:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 858, loss 3.691340208053589\n",
      "Epoch 9: |          | 859/? [12:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 859, loss 3.673144817352295\n",
      "Epoch 9: |          | 860/? [12:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 860, loss 4.010223388671875\n",
      "Epoch 9: |          | 861/? [12:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 861, loss 3.4022343158721924\n",
      "Epoch 9: |          | 862/? [12:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 862, loss 3.6653645038604736\n",
      "Epoch 9: |          | 863/? [12:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 863, loss 3.1239285469055176\n",
      "Epoch 9: |          | 864/? [12:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 864, loss 3.609339475631714\n",
      "Epoch 9: |          | 865/? [12:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 865, loss 3.6558609008789062\n",
      "Epoch 9: |          | 866/? [12:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 866, loss 2.718510627746582\n",
      "Epoch 9: |          | 867/? [12:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 867, loss 2.83134126663208\n",
      "Epoch 9: |          | 868/? [12:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 868, loss 3.695266008377075\n",
      "Epoch 9: |          | 869/? [12:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 869, loss 3.775496006011963\n",
      "Epoch 9: |          | 870/? [12:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 870, loss 3.405374050140381\n",
      "Epoch 9: |          | 871/? [12:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 871, loss 3.7218856811523438\n",
      "Epoch 9: |          | 872/? [12:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 872, loss 3.5457763671875\n",
      "Epoch 9: |          | 873/? [12:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 873, loss 3.542672634124756\n",
      "Epoch 9: |          | 874/? [12:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 874, loss 3.080460786819458\n",
      "Epoch 9: |          | 875/? [12:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 875, loss 3.755542039871216\n",
      "Epoch 9: |          | 876/? [12:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 876, loss 3.2694091796875\n",
      "Epoch 9: |          | 877/? [12:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 877, loss 3.7339470386505127\n",
      "Epoch 9: |          | 878/? [12:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 878, loss 3.141087532043457\n",
      "Epoch 9: |          | 879/? [12:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 879, loss 3.228121519088745\n",
      "Epoch 9: |          | 880/? [12:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 880, loss 4.251292705535889\n",
      "Epoch 9: |          | 881/? [12:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 881, loss 3.6835238933563232\n",
      "Epoch 9: |          | 882/? [12:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 882, loss 3.4932072162628174\n",
      "Epoch 9: |          | 883/? [12:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 883, loss 3.671988010406494\n",
      "Epoch 9: |          | 884/? [12:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 884, loss 3.6356606483459473\n",
      "Epoch 9: |          | 885/? [12:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 885, loss 3.4267234802246094\n",
      "Epoch 9: |          | 886/? [12:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 886, loss 4.120509147644043\n",
      "Epoch 9: |          | 887/? [12:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 887, loss 4.075308322906494\n",
      "Epoch 9: |          | 888/? [12:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 888, loss 3.7793021202087402\n",
      "Epoch 9: |          | 889/? [12:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 889, loss 3.3442904949188232\n",
      "Epoch 9: |          | 890/? [12:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 890, loss 3.525883913040161\n",
      "Epoch 9: |          | 891/? [12:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 891, loss 3.4563064575195312\n",
      "Epoch 9: |          | 892/? [12:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 892, loss 3.9870009422302246\n",
      "Epoch 9: |          | 893/? [12:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 893, loss 3.4579319953918457\n",
      "Epoch 9: |          | 894/? [12:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 894, loss 2.9631552696228027\n",
      "Epoch 9: |          | 895/? [12:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 895, loss 4.092999458312988\n",
      "Epoch 9: |          | 896/? [12:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 896, loss 3.686689853668213\n",
      "Epoch 9: |          | 897/? [12:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 897, loss 3.691631317138672\n",
      "Epoch 9: |          | 898/? [12:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 898, loss 3.668114185333252\n",
      "Epoch 9: |          | 899/? [12:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 899, loss 3.469327211380005\n",
      "Epoch 9: |          | 900/? [12:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 900, loss 3.4078774452209473\n",
      "Epoch 9: |          | 901/? [12:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 901, loss 3.7624995708465576\n",
      "Epoch 9: |          | 902/? [12:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 902, loss 3.8944544792175293\n",
      "Epoch 9: |          | 903/? [12:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 903, loss 3.216681718826294\n",
      "Epoch 9: |          | 904/? [12:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 904, loss 3.708836078643799\n",
      "Epoch 9: |          | 905/? [12:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 905, loss 3.852839946746826\n",
      "Epoch 9: |          | 906/? [12:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 906, loss 3.6275062561035156\n",
      "Epoch 9: |          | 907/? [12:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 907, loss 3.682382583618164\n",
      "Epoch 9: |          | 908/? [12:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 908, loss 3.7412936687469482\n",
      "Epoch 9: |          | 909/? [12:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 909, loss 3.7316765785217285\n",
      "Epoch 9: |          | 910/? [12:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 910, loss 3.4501869678497314\n",
      "Epoch 9: |          | 911/? [12:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 911, loss 3.572619676589966\n",
      "Epoch 9: |          | 912/? [12:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 912, loss 3.5468831062316895\n",
      "Epoch 9: |          | 913/? [13:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 913, loss 3.5273780822753906\n",
      "Epoch 9: |          | 914/? [13:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 914, loss 3.795337677001953\n",
      "Epoch 9: |          | 915/? [13:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 915, loss 3.6235923767089844\n",
      "Epoch 9: |          | 916/? [13:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 916, loss 3.5836658477783203\n",
      "Epoch 9: |          | 917/? [13:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 917, loss 3.500375270843506\n",
      "Epoch 9: |          | 918/? [13:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 918, loss 3.443784236907959\n",
      "Epoch 9: |          | 919/? [13:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 919, loss 3.486290454864502\n",
      "Epoch 9: |          | 920/? [13:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 920, loss 3.6232166290283203\n",
      "Epoch 9: |          | 921/? [13:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 921, loss 3.4370365142822266\n",
      "Epoch 9: |          | 922/? [13:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 922, loss 3.5854294300079346\n",
      "Epoch 9: |          | 923/? [13:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 923, loss 3.466033935546875\n",
      "Epoch 9: |          | 924/? [13:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 924, loss 3.456233501434326\n",
      "Epoch 9: |          | 925/? [13:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 925, loss 3.7706661224365234\n",
      "Epoch 9: |          | 926/? [13:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 926, loss 3.518249988555908\n",
      "Epoch 9: |          | 927/? [13:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 927, loss 3.8169751167297363\n",
      "Epoch 9: |          | 928/? [13:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 928, loss 3.3493494987487793\n",
      "Epoch 9: |          | 929/? [13:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 929, loss 3.402179718017578\n",
      "Epoch 9: |          | 930/? [13:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 930, loss 3.3302528858184814\n",
      "Epoch 9: |          | 931/? [13:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 931, loss 3.060206651687622\n",
      "Epoch 9: |          | 932/? [13:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 932, loss 3.6654534339904785\n",
      "Epoch 9: |          | 933/? [13:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 933, loss 3.44413685798645\n",
      "Epoch 9: |          | 934/? [13:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 934, loss 3.948244571685791\n",
      "Epoch 9: |          | 935/? [13:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 935, loss 4.276767253875732\n",
      "Epoch 9: |          | 936/? [13:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 936, loss 3.515751600265503\n",
      "Epoch 9: |          | 937/? [13:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 937, loss 3.268932342529297\n",
      "Epoch 9: |          | 938/? [13:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 938, loss 3.501913547515869\n",
      "Epoch 9: |          | 939/? [13:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 939, loss 3.711317777633667\n",
      "Epoch 9: |          | 940/? [13:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 940, loss 3.87988018989563\n",
      "Epoch 9: |          | 941/? [13:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 941, loss 3.462616443634033\n",
      "Epoch 9: |          | 942/? [13:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 942, loss 2.94096040725708\n",
      "Epoch 9: |          | 943/? [13:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 943, loss 3.737433671951294\n",
      "Epoch 9: |          | 944/? [13:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 944, loss 2.8946919441223145\n",
      "Epoch 9: |          | 945/? [13:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 945, loss 3.5826339721679688\n",
      "Epoch 9: |          | 946/? [13:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 946, loss 3.4663524627685547\n",
      "Epoch 9: |          | 947/? [13:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 947, loss 3.3852756023406982\n",
      "Epoch 9: |          | 948/? [13:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 948, loss 3.6806132793426514\n",
      "Epoch 9: |          | 949/? [13:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 949, loss 3.5259621143341064\n",
      "Epoch 9: |          | 950/? [13:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 950, loss 3.2972099781036377\n",
      "Epoch 9: |          | 951/? [13:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 951, loss 3.946547031402588\n",
      "Epoch 9: |          | 952/? [13:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 952, loss 3.867121458053589\n",
      "Epoch 9: |          | 953/? [13:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 953, loss 4.386717796325684\n",
      "Epoch 9: |          | 954/? [13:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 954, loss 3.4115097522735596\n",
      "Epoch 9: |          | 955/? [13:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 955, loss 4.0122270584106445\n",
      "Epoch 9: |          | 956/? [13:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 956, loss 3.536468982696533\n",
      "Epoch 9: |          | 957/? [13:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 957, loss 3.649440288543701\n",
      "Epoch 9: |          | 958/? [13:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 958, loss 3.6996490955352783\n",
      "Epoch 9: |          | 959/? [13:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 959, loss 3.100924491882324\n",
      "Epoch 9: |          | 960/? [13:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 960, loss 3.7767651081085205\n",
      "Epoch 9: |          | 961/? [13:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 961, loss 4.054465293884277\n",
      "Epoch 9: |          | 962/? [13:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 962, loss 3.5663819313049316\n",
      "Epoch 9: |          | 963/? [13:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 963, loss 3.409367322921753\n",
      "Epoch 9: |          | 964/? [13:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 964, loss 3.8185553550720215\n",
      "Epoch 9: |          | 965/? [13:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 965, loss 3.2825820446014404\n",
      "Epoch 9: |          | 966/? [13:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 966, loss 3.2409005165100098\n",
      "Epoch 9: |          | 967/? [13:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 967, loss 3.4735336303710938\n",
      "Epoch 9: |          | 968/? [13:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 968, loss 3.377751111984253\n",
      "Epoch 9: |          | 969/? [13:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 969, loss 3.316692352294922\n",
      "Epoch 9: |          | 970/? [13:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 970, loss 3.712421417236328\n",
      "Epoch 9: |          | 971/? [13:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 971, loss 3.869178295135498\n",
      "Epoch 9: |          | 972/? [13:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 972, loss 3.369640827178955\n",
      "Epoch 9: |          | 973/? [13:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 973, loss 3.5347790718078613\n",
      "Epoch 9: |          | 974/? [13:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 974, loss 3.6329872608184814\n",
      "Epoch 9: |          | 975/? [13:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 975, loss 3.6572391986846924\n",
      "Epoch 9: |          | 976/? [13:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 976, loss 3.676612138748169\n",
      "Epoch 9: |          | 977/? [13:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 977, loss 4.210526466369629\n",
      "Epoch 9: |          | 978/? [13:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 978, loss 3.696054458618164\n",
      "Epoch 9: |          | 979/? [13:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 979, loss 3.9177863597869873\n",
      "Epoch 9: |          | 980/? [13:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 980, loss 3.1284730434417725\n",
      "Epoch 9: |          | 981/? [13:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 981, loss 3.044835090637207\n",
      "Epoch 9: |          | 982/? [13:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 982, loss 3.6522185802459717\n",
      "Epoch 9: |          | 983/? [14:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 983, loss 4.053073883056641\n",
      "Epoch 9: |          | 984/? [14:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 984, loss 3.1782796382904053\n",
      "Epoch 9: |          | 985/? [14:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 985, loss 3.3922512531280518\n",
      "Epoch 9: |          | 986/? [14:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 986, loss 3.4176273345947266\n",
      "Epoch 9: |          | 987/? [14:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 987, loss 2.931156635284424\n",
      "Epoch 9: |          | 988/? [14:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 988, loss 3.9581761360168457\n",
      "Epoch 9: |          | 989/? [14:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 989, loss 3.576216220855713\n",
      "Epoch 9: |          | 990/? [14:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 990, loss 3.0077908039093018\n",
      "Epoch 9: |          | 991/? [14:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 991, loss 3.730149030685425\n",
      "Epoch 9: |          | 992/? [14:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 992, loss 4.32413387298584\n",
      "Epoch 9: |          | 993/? [14:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 993, loss 3.449324131011963\n",
      "Epoch 9: |          | 994/? [14:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 994, loss 3.4392216205596924\n",
      "Epoch 9: |          | 995/? [14:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 995, loss 3.861356019973755\n",
      "Epoch 9: |          | 996/? [14:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 996, loss 3.8119289875030518\n",
      "Epoch 9: |          | 997/? [14:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 997, loss 3.5120882987976074\n",
      "Epoch 9: |          | 998/? [14:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 998, loss 3.7109692096710205\n",
      "Epoch 9: |          | 999/? [14:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 999, loss 3.6466031074523926\n",
      "Epoch 9: |          | 1000/? [14:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1000, loss 3.1650547981262207\n",
      "Epoch 9: |          | 1001/? [14:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1001, loss 3.8395378589630127\n",
      "Epoch 9: |          | 1002/? [14:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1002, loss 3.7998180389404297\n",
      "Epoch 9: |          | 1003/? [14:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1003, loss 4.022928714752197\n",
      "Epoch 9: |          | 1004/? [14:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1004, loss 3.1014652252197266\n",
      "Epoch 9: |          | 1005/? [14:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1005, loss 3.5719685554504395\n",
      "Epoch 9: |          | 1006/? [14:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1006, loss 3.845412015914917\n",
      "Epoch 9: |          | 1007/? [14:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1007, loss 3.44553804397583\n",
      "Epoch 9: |          | 1008/? [14:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1008, loss 3.6106598377227783\n",
      "Epoch 9: |          | 1009/? [14:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1009, loss 3.8230526447296143\n",
      "Epoch 9: |          | 1010/? [14:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1010, loss 3.044267177581787\n",
      "Epoch 9: |          | 1011/? [14:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1011, loss 3.564605236053467\n",
      "Epoch 9: |          | 1012/? [14:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1012, loss 3.4007885456085205\n",
      "Epoch 9: |          | 1013/? [14:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1013, loss 3.4946036338806152\n",
      "Epoch 9: |          | 1014/? [14:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1014, loss 3.9892449378967285\n",
      "Epoch 9: |          | 1015/? [14:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1015, loss 3.617011308670044\n",
      "Epoch 9: |          | 1016/? [14:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1016, loss 3.408766984939575\n",
      "Epoch 9: |          | 1017/? [14:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1017, loss 2.902434825897217\n",
      "Epoch 9: |          | 1018/? [14:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1018, loss 3.5051937103271484\n",
      "Epoch 9: |          | 1019/? [14:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1019, loss 3.5781028270721436\n",
      "Epoch 9: |          | 1020/? [14:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1020, loss 3.1702027320861816\n",
      "Epoch 9: |          | 1021/? [14:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1021, loss 3.4515480995178223\n",
      "Epoch 9: |          | 1022/? [14:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1022, loss 3.2525475025177\n",
      "Epoch 9: |          | 1023/? [14:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1023, loss 2.99190092086792\n",
      "Epoch 9: |          | 1024/? [14:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1024, loss 3.4229018688201904\n",
      "Epoch 9: |          | 1025/? [14:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1025, loss 3.324349880218506\n",
      "Epoch 9: |          | 1026/? [14:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1026, loss 2.5278756618499756\n",
      "Epoch 9: |          | 1027/? [14:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1027, loss 3.634368419647217\n",
      "Epoch 9: |          | 1028/? [14:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1028, loss 3.484394073486328\n",
      "Epoch 9: |          | 1029/? [14:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1029, loss 3.356433153152466\n",
      "Epoch 9: |          | 1030/? [14:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1030, loss 3.2164978981018066\n",
      "Epoch 9: |          | 1031/? [14:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1031, loss 3.2705256938934326\n",
      "Epoch 9: |          | 1032/? [14:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1032, loss 3.6640400886535645\n",
      "Epoch 9: |          | 1033/? [14:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1033, loss 3.9203078746795654\n",
      "Epoch 9: |          | 1034/? [14:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1034, loss 3.3460171222686768\n",
      "Epoch 9: |          | 1035/? [14:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1035, loss 3.352787494659424\n",
      "Epoch 9: |          | 1036/? [14:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1036, loss 3.347660541534424\n",
      "Epoch 9: |          | 1037/? [14:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1037, loss 3.876682758331299\n",
      "Epoch 9: |          | 1038/? [14:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1038, loss 4.0519609451293945\n",
      "Epoch 9: |          | 1039/? [14:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1039, loss 4.363891124725342\n",
      "Epoch 9: |          | 1040/? [14:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1040, loss 3.683645248413086\n",
      "Epoch 9: |          | 1041/? [14:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1041, loss 3.952066421508789\n",
      "Epoch 9: |          | 1042/? [14:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1042, loss 3.533418655395508\n",
      "Epoch 9: |          | 1043/? [14:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1043, loss 3.893852710723877\n",
      "Epoch 9: |          | 1044/? [14:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1044, loss 3.4819366931915283\n",
      "Epoch 9: |          | 1045/? [14:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1045, loss 3.090407371520996\n",
      "Epoch 9: |          | 1046/? [14:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1046, loss 2.9718518257141113\n",
      "Epoch 9: |          | 1047/? [14:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1047, loss 3.9576027393341064\n",
      "Epoch 9: |          | 1048/? [14:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1048, loss 3.544656276702881\n",
      "Epoch 9: |          | 1049/? [14:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1049, loss 3.7314453125\n",
      "Epoch 9: |          | 1050/? [14:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1050, loss 3.307673215866089\n",
      "Epoch 9: |          | 1051/? [14:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1051, loss 3.2089295387268066\n",
      "Epoch 9: |          | 1052/? [14:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1052, loss 3.823719024658203\n",
      "Epoch 9: |          | 1053/? [14:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1053, loss 4.021512508392334\n",
      "Epoch 9: |          | 1054/? [15:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1054, loss 3.4752039909362793\n",
      "Epoch 9: |          | 1055/? [15:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1055, loss 3.126551628112793\n",
      "Epoch 9: |          | 1056/? [15:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1056, loss 3.1226162910461426\n",
      "Epoch 9: |          | 1057/? [15:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1057, loss 3.731299877166748\n",
      "Epoch 9: |          | 1058/? [15:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1058, loss 3.336142063140869\n",
      "Epoch 9: |          | 1059/? [15:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1059, loss 3.906843900680542\n",
      "Epoch 9: |          | 1060/? [15:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1060, loss 3.747986316680908\n",
      "Epoch 9: |          | 1061/? [15:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1061, loss 2.5961813926696777\n",
      "Epoch 9: |          | 1062/? [15:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1062, loss 3.4303042888641357\n",
      "Epoch 9: |          | 1063/? [15:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1063, loss 3.494051456451416\n",
      "Epoch 9: |          | 1064/? [15:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1064, loss 3.6711223125457764\n",
      "Epoch 9: |          | 1065/? [15:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1065, loss 2.363516330718994\n",
      "Epoch 9: |          | 1066/? [15:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1066, loss 3.6093506813049316\n",
      "Epoch 9: |          | 1067/? [15:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1067, loss 3.175689220428467\n",
      "Epoch 9: |          | 1068/? [15:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1068, loss 3.3209667205810547\n",
      "Epoch 9: |          | 1069/? [15:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1069, loss 3.6758904457092285\n",
      "Epoch 9: |          | 1070/? [15:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1070, loss 3.4592292308807373\n",
      "Epoch 9: |          | 1071/? [15:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1071, loss 3.7974934577941895\n",
      "Epoch 9: |          | 1072/? [15:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1072, loss 3.8531317710876465\n",
      "Epoch 9: |          | 1073/? [15:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1073, loss 3.966540575027466\n",
      "Epoch 9: |          | 1074/? [15:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1074, loss 3.350581407546997\n",
      "Epoch 9: |          | 1075/? [15:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1075, loss 3.216923236846924\n",
      "Epoch 9: |          | 1076/? [15:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1076, loss 3.7562966346740723\n",
      "Epoch 9: |          | 1077/? [15:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1077, loss 3.3014304637908936\n",
      "Epoch 9: |          | 1078/? [15:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1078, loss 3.5518155097961426\n",
      "Epoch 9: |          | 1079/? [15:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1079, loss 3.8812623023986816\n",
      "Epoch 9: |          | 1080/? [15:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1080, loss 3.4614968299865723\n",
      "Epoch 9: |          | 1081/? [15:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1081, loss 3.7767932415008545\n",
      "Epoch 9: |          | 1082/? [15:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1082, loss 3.4177703857421875\n",
      "Epoch 9: |          | 1083/? [15:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1083, loss 2.9106037616729736\n",
      "Epoch 9: |          | 1084/? [15:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1084, loss 2.8167595863342285\n",
      "Epoch 9: |          | 1085/? [15:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1085, loss 3.4419751167297363\n",
      "Epoch 9: |          | 1086/? [15:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1086, loss 3.691781997680664\n",
      "Epoch 9: |          | 1087/? [15:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1087, loss 4.145328521728516\n",
      "Epoch 9: |          | 1088/? [15:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1088, loss 3.7512047290802\n",
      "Epoch 9: |          | 1089/? [15:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1089, loss 3.635270595550537\n",
      "Epoch 9: |          | 1090/? [15:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1090, loss 3.4885971546173096\n",
      "Epoch 9: |          | 1091/? [15:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1091, loss 3.3319644927978516\n",
      "Epoch 9: |          | 1092/? [15:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1092, loss 3.6348483562469482\n",
      "Epoch 9: |          | 1093/? [15:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1093, loss 3.1519386768341064\n",
      "Epoch 9: |          | 1094/? [15:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1094, loss 3.618856906890869\n",
      "Epoch 9: |          | 1095/? [15:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1095, loss 3.598832607269287\n",
      "Epoch 9: |          | 1096/? [15:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1096, loss 3.883192777633667\n",
      "Epoch 9: |          | 1097/? [15:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1097, loss 3.497579574584961\n",
      "Epoch 9: |          | 1098/? [15:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1098, loss 2.8683717250823975\n",
      "Epoch 9: |          | 1099/? [15:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1099, loss 3.425377368927002\n",
      "Epoch 9: |          | 1100/? [15:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1100, loss 3.6268322467803955\n",
      "Epoch 9: |          | 1101/? [15:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1101, loss 3.290220260620117\n",
      "Epoch 9: |          | 1102/? [15:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1102, loss 3.9748663902282715\n",
      "Epoch 9: |          | 1103/? [15:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1103, loss 4.266633033752441\n",
      "Epoch 9: |          | 1104/? [15:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1104, loss 3.7936413288116455\n",
      "Epoch 9: |          | 1105/? [15:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1105, loss 3.9217514991760254\n",
      "Epoch 9: |          | 1106/? [15:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1106, loss 3.4160308837890625\n",
      "Epoch 9: |          | 1107/? [15:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1107, loss 3.589029312133789\n",
      "Epoch 9: |          | 1108/? [15:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1108, loss 3.587231397628784\n",
      "Epoch 9: |          | 1109/? [15:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1109, loss 3.223886489868164\n",
      "Epoch 9: |          | 1110/? [15:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1110, loss 4.0688982009887695\n",
      "Epoch 9: |          | 1111/? [15:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1111, loss 3.7468104362487793\n",
      "Epoch 9: |          | 1112/? [15:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1112, loss 3.6177680492401123\n",
      "Epoch 9: |          | 1113/? [15:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1113, loss 3.462254285812378\n",
      "Epoch 9: |          | 1114/? [15:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1114, loss 2.9418015480041504\n",
      "Epoch 9: |          | 1115/? [15:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1115, loss 2.7693655490875244\n",
      "Epoch 9: |          | 1116/? [15:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1116, loss 3.0767409801483154\n",
      "Epoch 9: |          | 1117/? [15:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1117, loss 3.2382373809814453\n",
      "Epoch 9: |          | 1118/? [15:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1118, loss 3.4032890796661377\n",
      "Epoch 9: |          | 1119/? [15:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1119, loss 3.99421763420105\n",
      "Epoch 9: |          | 1120/? [15:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1120, loss 3.5274016857147217\n",
      "Epoch 9: |          | 1121/? [15:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1121, loss 3.7695536613464355\n",
      "Epoch 9: |          | 1122/? [15:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1122, loss 3.265460968017578\n",
      "Epoch 9: |          | 1123/? [16:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1123, loss 3.5476481914520264\n",
      "Epoch 9: |          | 1124/? [16:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1124, loss 3.9107518196105957\n",
      "Epoch 9: |          | 1125/? [16:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1125, loss 3.2699902057647705\n",
      "Epoch 9: |          | 1126/? [16:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1126, loss 3.1749653816223145\n",
      "Epoch 9: |          | 1127/? [16:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1127, loss 3.4120566844940186\n",
      "Epoch 9: |          | 1128/? [16:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1128, loss 3.5566482543945312\n",
      "Epoch 9: |          | 1129/? [16:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1129, loss 3.653893232345581\n",
      "Epoch 9: |          | 1130/? [16:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1130, loss 3.8081135749816895\n",
      "Epoch 9: |          | 1131/? [16:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1131, loss 3.857199192047119\n",
      "Epoch 9: |          | 1132/? [16:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1132, loss 2.769512891769409\n",
      "Epoch 9: |          | 1133/? [16:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1133, loss 3.4811930656433105\n",
      "Epoch 9: |          | 1134/? [16:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1134, loss 3.3226382732391357\n",
      "Epoch 9: |          | 1135/? [16:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1135, loss 3.8932278156280518\n",
      "Epoch 9: |          | 1136/? [16:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1136, loss 3.5292885303497314\n",
      "Epoch 9: |          | 1137/? [16:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1137, loss 3.6131179332733154\n",
      "Epoch 9: |          | 1138/? [16:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1138, loss 3.963671922683716\n",
      "Epoch 9: |          | 1139/? [16:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1139, loss 3.670180082321167\n",
      "Epoch 9: |          | 1140/? [16:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1140, loss 3.386500597000122\n",
      "Epoch 9: |          | 1141/? [16:15<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1141, loss 3.877192258834839\n",
      "Epoch 9: |          | 1142/? [16:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1142, loss 3.986558198928833\n",
      "Epoch 9: |          | 1143/? [16:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1143, loss 3.9730587005615234\n",
      "Epoch 9: |          | 1144/? [16:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1144, loss 3.393285036087036\n",
      "Epoch 9: |          | 1145/? [16:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1145, loss 3.559309482574463\n",
      "Epoch 9: |          | 1146/? [16:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1146, loss 3.1277456283569336\n",
      "Epoch 9: |          | 1147/? [16:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1147, loss 3.1840662956237793\n",
      "Epoch 9: |          | 1148/? [16:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1148, loss 3.3601632118225098\n",
      "Epoch 9: |          | 1149/? [16:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1149, loss 4.179715156555176\n",
      "Epoch 9: |          | 1150/? [16:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1150, loss 3.7352633476257324\n",
      "Epoch 9: |          | 1151/? [16:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1151, loss 3.93804669380188\n",
      "Epoch 9: |          | 1152/? [16:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1152, loss 3.272928237915039\n",
      "Epoch 9: |          | 1153/? [16:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1153, loss 3.622967481613159\n",
      "Epoch 9: |          | 1154/? [16:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1154, loss 3.323760986328125\n",
      "Epoch 9: |          | 1155/? [16:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1155, loss 3.538775682449341\n",
      "Epoch 9: |          | 1156/? [16:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1156, loss 3.5028653144836426\n",
      "Epoch 9: |          | 1157/? [16:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1157, loss 3.782106399536133\n",
      "Epoch 9: |          | 1158/? [16:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1158, loss 3.889507293701172\n",
      "Epoch 9: |          | 1159/? [16:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1159, loss 2.836106538772583\n",
      "Epoch 9: |          | 1160/? [16:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1160, loss 3.9182372093200684\n",
      "Epoch 9: |          | 1161/? [16:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1161, loss 3.7621777057647705\n",
      "Epoch 9: |          | 1162/? [16:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1162, loss 3.7083992958068848\n",
      "Epoch 9: |          | 1163/? [16:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1163, loss 4.314030170440674\n",
      "Epoch 9: |          | 1164/? [16:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1164, loss 4.0760087966918945\n",
      "Epoch 9: |          | 1165/? [16:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1165, loss 3.3060765266418457\n",
      "Epoch 9: |          | 1166/? [16:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1166, loss 3.7599406242370605\n",
      "Epoch 9: |          | 1167/? [16:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1167, loss 3.7635421752929688\n",
      "Epoch 9: |          | 1168/? [16:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1168, loss 4.192997455596924\n",
      "Epoch 9: |          | 1169/? [16:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1169, loss 3.411015033721924\n",
      "Epoch 9: |          | 1170/? [16:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1170, loss 3.8648929595947266\n",
      "Epoch 9: |          | 1171/? [16:40<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1171, loss 3.2428200244903564\n",
      "Epoch 9: |          | 1172/? [16:41<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1172, loss 3.2231335639953613\n",
      "Epoch 9: |          | 1173/? [16:42<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1173, loss 3.7407100200653076\n",
      "Epoch 9: |          | 1174/? [16:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1174, loss 3.267411470413208\n",
      "Epoch 9: |          | 1175/? [16:43<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1175, loss 3.783433198928833\n",
      "Epoch 9: |          | 1176/? [16:44<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1176, loss 3.8495306968688965\n",
      "Epoch 9: |          | 1177/? [16:45<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1177, loss 4.007115840911865\n",
      "Epoch 9: |          | 1178/? [16:46<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1178, loss 3.433248996734619\n",
      "Epoch 9: |          | 1179/? [16:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1179, loss 3.996702194213867\n",
      "Epoch 9: |          | 1180/? [16:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1180, loss 3.8500213623046875\n",
      "Epoch 9: |          | 1181/? [16:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1181, loss 3.7195448875427246\n",
      "Epoch 9: |          | 1182/? [16:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1182, loss 3.5645592212677\n",
      "Epoch 9: |          | 1183/? [16:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1183, loss 3.2877020835876465\n",
      "Epoch 9: |          | 1184/? [16:51<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1184, loss 3.706533908843994\n",
      "Epoch 9: |          | 1185/? [16:52<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1185, loss 3.4137775897979736\n",
      "Epoch 9: |          | 1186/? [16:53<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1186, loss 3.6396517753601074\n",
      "Epoch 9: |          | 1187/? [16:54<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1187, loss 3.5389513969421387\n",
      "Epoch 9: |          | 1188/? [16:55<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1188, loss 3.9025444984436035\n",
      "Epoch 9: |          | 1189/? [16:56<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1189, loss 3.9403953552246094\n",
      "Epoch 9: |          | 1190/? [16:57<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1190, loss 3.579348087310791\n",
      "Epoch 9: |          | 1191/? [16:58<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1191, loss 3.6048717498779297\n",
      "Epoch 9: |          | 1192/? [16:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1192, loss 3.8564987182617188\n",
      "Epoch 9: |          | 1193/? [16:59<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1193, loss 3.3513450622558594\n",
      "Epoch 9: |          | 1194/? [17:00<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1194, loss 3.139252185821533\n",
      "Epoch 9: |          | 1195/? [17:01<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1195, loss 3.5958378314971924\n",
      "Epoch 9: |          | 1196/? [17:02<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1196, loss 3.811309814453125\n",
      "Epoch 9: |          | 1197/? [17:03<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1197, loss 3.559119701385498\n",
      "Epoch 9: |          | 1198/? [17:04<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1198, loss 3.6662654876708984\n",
      "Epoch 9: |          | 1199/? [17:05<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1199, loss 3.965920925140381\n",
      "Epoch 9: |          | 1200/? [17:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1200, loss 3.1498236656188965\n",
      "Epoch 9: |          | 1201/? [17:06<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1201, loss 3.7510173320770264\n",
      "Epoch 9: |          | 1202/? [17:07<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1202, loss 3.465137481689453\n",
      "Epoch 9: |          | 1203/? [17:08<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1203, loss 3.4365172386169434\n",
      "Epoch 9: |          | 1204/? [17:09<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1204, loss 3.048027515411377\n",
      "Epoch 9: |          | 1205/? [17:10<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1205, loss 3.6077041625976562\n",
      "Epoch 9: |          | 1206/? [17:11<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1206, loss 3.5964534282684326\n",
      "Epoch 9: |          | 1207/? [17:12<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1207, loss 3.7823119163513184\n",
      "Epoch 9: |          | 1208/? [17:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1208, loss 4.014547824859619\n",
      "Epoch 9: |          | 1209/? [17:13<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1209, loss 3.594578266143799\n",
      "Epoch 9: |          | 1210/? [17:14<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1210, loss 3.88771390914917\n",
      "Epoch 9: |          | 1211/? [17:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1211, loss 3.8592071533203125\n",
      "Epoch 9: |          | 1212/? [17:16<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1212, loss 3.673989772796631\n",
      "Epoch 9: |          | 1213/? [17:17<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1213, loss 3.4178664684295654\n",
      "Epoch 9: |          | 1214/? [17:18<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1214, loss 3.9118621349334717\n",
      "Epoch 9: |          | 1215/? [17:19<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1215, loss 3.5003249645233154\n",
      "Epoch 9: |          | 1216/? [17:20<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1216, loss 3.558264970779419\n",
      "Epoch 9: |          | 1217/? [17:21<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1217, loss 3.598392963409424\n",
      "Epoch 9: |          | 1218/? [17:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1218, loss 3.751572847366333\n",
      "Epoch 9: |          | 1219/? [17:22<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1219, loss 3.409848690032959\n",
      "Epoch 9: |          | 1220/? [17:23<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1220, loss 4.07147741317749\n",
      "Epoch 9: |          | 1221/? [17:24<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1221, loss 3.5825562477111816\n",
      "Epoch 9: |          | 1222/? [17:25<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1222, loss 2.8578381538391113\n",
      "Epoch 9: |          | 1223/? [17:26<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1223, loss 3.0563244819641113\n",
      "Epoch 9: |          | 1224/? [17:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1224, loss 3.363112211227417\n",
      "Epoch 9: |          | 1225/? [17:27<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1225, loss 4.008914947509766\n",
      "Epoch 9: |          | 1226/? [17:28<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1226, loss 3.93469500541687\n",
      "Epoch 9: |          | 1227/? [17:29<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1227, loss 3.63519287109375\n",
      "Epoch 9: |          | 1228/? [17:30<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1228, loss 3.4755797386169434\n",
      "Epoch 9: |          | 1229/? [17:31<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1229, loss 3.133119821548462\n",
      "Epoch 9: |          | 1230/? [17:32<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1230, loss 3.738523483276367\n",
      "Epoch 9: |          | 1231/? [17:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1231, loss 3.7546260356903076\n",
      "Epoch 9: |          | 1232/? [17:33<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1232, loss 3.926635265350342\n",
      "Epoch 9: |          | 1233/? [17:34<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1233, loss 3.747406005859375\n",
      "Epoch 9: |          | 1234/? [17:35<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1234, loss 2.7879366874694824\n",
      "Epoch 9: |          | 1235/? [17:36<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1235, loss 3.863948345184326\n",
      "Epoch 9: |          | 1236/? [17:37<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1236, loss 3.2760326862335205\n",
      "Epoch 9: |          | 1237/? [17:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1237, loss 3.533869981765747\n",
      "Epoch 9: |          | 1238/? [17:38<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1238, loss 3.6096692085266113\n",
      "Epoch 9: |          | 1239/? [17:39<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1239, loss 3.4308555126190186\n",
      "Epoch 9: |          | 1240/? [17:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1240, loss 3.995124101638794\n",
      "Epoch 9: |          | 1241/? [17:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1241, loss 3.5871481895446777\n",
      "Epoch 9: |          | 1242/? [17:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1242, loss 3.4159576892852783\n",
      "Epoch 9: |          | 1243/? [17:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1243, loss 3.3533999919891357\n",
      "Epoch 9: |          | 1244/? [17:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1244, loss 3.4857494831085205\n",
      "Epoch 9: |          | 1245/? [17:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1245, loss 3.105760097503662\n",
      "Epoch 9: |          | 1246/? [17:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1246, loss 3.707538604736328\n",
      "Epoch 9: |          | 1247/? [17:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1247, loss 3.7346012592315674\n",
      "Epoch 9: |          | 1248/? [17:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1248, loss 3.3406131267547607\n",
      "Epoch 9: |          | 1249/? [17:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1249, loss 3.5081634521484375\n",
      "Epoch 9: |          | 1250/? [17:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1250, loss 3.6435272693634033\n",
      "Epoch 9: |          | 1251/? [17:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1251, loss 3.414177417755127\n",
      "Epoch 9: |          | 1252/? [17:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1252, loss 4.126911640167236\n",
      "Epoch 9: |          | 1253/? [17:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1253, loss 3.4793598651885986\n",
      "Epoch 9: |          | 1254/? [17:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1254, loss 2.8639421463012695\n",
      "Epoch 9: |          | 1255/? [17:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1255, loss 4.160813331604004\n",
      "Epoch 9: |          | 1256/? [18:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1256, loss 3.2417056560516357\n",
      "Epoch 9: |          | 1257/? [18:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1257, loss 3.2635486125946045\n",
      "Epoch 9: |          | 1258/? [18:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1258, loss 3.9208762645721436\n",
      "Epoch 9: |          | 1259/? [18:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1259, loss 3.593059539794922\n",
      "Epoch 9: |          | 1260/? [18:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1260, loss 4.029461860656738\n",
      "Epoch 9: |          | 1261/? [18:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1261, loss 3.3900413513183594\n",
      "Epoch 9: |          | 1262/? [18:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1262, loss 3.4125847816467285\n",
      "Epoch 9: |          | 1263/? [18:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1263, loss 3.7015624046325684\n",
      "Epoch 9: |          | 1264/? [18:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1264, loss 3.9287590980529785\n",
      "Epoch 9: |          | 1265/? [18:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1265, loss 3.822526216506958\n",
      "Epoch 9: |          | 1266/? [18:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1266, loss 3.547293186187744\n",
      "Epoch 9: |          | 1267/? [18:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1267, loss 3.6542861461639404\n",
      "Epoch 9: |          | 1268/? [18:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1268, loss 3.4869656562805176\n",
      "Epoch 9: |          | 1269/? [18:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1269, loss 3.0742931365966797\n",
      "Epoch 9: |          | 1270/? [18:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1270, loss 3.4026458263397217\n",
      "Epoch 9: |          | 1271/? [18:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1271, loss 3.5658164024353027\n",
      "Epoch 9: |          | 1272/? [18:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1272, loss 3.159630537033081\n",
      "Epoch 9: |          | 1273/? [18:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1273, loss 3.813615322113037\n",
      "Epoch 9: |          | 1274/? [18:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1274, loss 2.784276247024536\n",
      "Epoch 9: |          | 1275/? [18:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1275, loss 3.2950351238250732\n",
      "Epoch 9: |          | 1276/? [18:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1276, loss 3.5624306201934814\n",
      "Epoch 9: |          | 1277/? [18:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1277, loss 3.302706480026245\n",
      "Epoch 9: |          | 1278/? [18:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1278, loss 2.9763131141662598\n",
      "Epoch 9: |          | 1279/? [18:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1279, loss 3.6708290576934814\n",
      "Epoch 9: |          | 1280/? [18:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1280, loss 2.9633591175079346\n",
      "Epoch 9: |          | 1281/? [18:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1281, loss 3.452859401702881\n",
      "Epoch 9: |          | 1282/? [18:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1282, loss 3.1568753719329834\n",
      "Epoch 9: |          | 1283/? [18:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1283, loss 3.80118989944458\n",
      "Epoch 9: |          | 1284/? [18:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1284, loss 3.0252864360809326\n",
      "Epoch 9: |          | 1285/? [18:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1285, loss 3.9787838459014893\n",
      "Epoch 9: |          | 1286/? [18:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1286, loss 2.5481083393096924\n",
      "Epoch 9: |          | 1287/? [18:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1287, loss 3.9219303131103516\n",
      "Epoch 9: |          | 1288/? [18:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1288, loss 3.713824510574341\n",
      "Epoch 9: |          | 1289/? [18:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1289, loss 2.96565842628479\n",
      "Epoch 9: |          | 1290/? [18:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1290, loss 3.600609540939331\n",
      "Epoch 9: |          | 1291/? [18:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1291, loss 4.4588446617126465\n",
      "Epoch 9: |          | 1292/? [18:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1292, loss 3.7354793548583984\n",
      "Epoch 9: |          | 1293/? [18:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1293, loss 3.3333706855773926\n",
      "Epoch 9: |          | 1294/? [18:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1294, loss 3.5430800914764404\n",
      "Epoch 9: |          | 1295/? [18:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1295, loss 3.6707611083984375\n",
      "Epoch 9: |          | 1296/? [18:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1296, loss 2.950286865234375\n",
      "Epoch 9: |          | 1297/? [18:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1297, loss 3.8098082542419434\n",
      "Epoch 9: |          | 1298/? [18:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1298, loss 3.534520387649536\n",
      "Epoch 9: |          | 1299/? [18:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1299, loss 2.783886432647705\n",
      "Epoch 9: |          | 1300/? [18:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1300, loss 3.5411362648010254\n",
      "Epoch 9: |          | 1301/? [18:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1301, loss 3.2919375896453857\n",
      "Epoch 9: |          | 1302/? [18:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1302, loss 3.4165046215057373\n",
      "Epoch 9: |          | 1303/? [18:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1303, loss 3.37298846244812\n",
      "Epoch 9: |          | 1304/? [18:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1304, loss 4.07167911529541\n",
      "Epoch 9: |          | 1305/? [18:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1305, loss 2.9498109817504883\n",
      "Epoch 9: |          | 1306/? [18:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1306, loss 3.5258402824401855\n",
      "Epoch 9: |          | 1307/? [18:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1307, loss 3.1951072216033936\n",
      "Epoch 9: |          | 1308/? [18:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1308, loss 3.16627836227417\n",
      "Epoch 9: |          | 1309/? [18:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1309, loss 3.1022801399230957\n",
      "Epoch 9: |          | 1310/? [18:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1310, loss 3.6616222858428955\n",
      "Epoch 9: |          | 1311/? [18:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1311, loss 3.2041213512420654\n",
      "Epoch 9: |          | 1312/? [18:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1312, loss 2.914449453353882\n",
      "Epoch 9: |          | 1313/? [18:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1313, loss 4.02523136138916\n",
      "Epoch 9: |          | 1314/? [18:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1314, loss 3.2917983531951904\n",
      "Epoch 9: |          | 1315/? [18:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1315, loss 4.00211238861084\n",
      "Epoch 9: |          | 1316/? [18:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1316, loss 3.7866241931915283\n",
      "Epoch 9: |          | 1317/? [18:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1317, loss 3.4609227180480957\n",
      "Epoch 9: |          | 1318/? [18:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1318, loss 3.5877578258514404\n",
      "Epoch 9: |          | 1319/? [18:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1319, loss 3.768432140350342\n",
      "Epoch 9: |          | 1320/? [18:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1320, loss 3.355128526687622\n",
      "Epoch 9: |          | 1321/? [18:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1321, loss 3.7616589069366455\n",
      "Epoch 9: |          | 1322/? [18:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1322, loss 3.6191070079803467\n",
      "Epoch 9: |          | 1323/? [18:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1323, loss 3.2122044563293457\n",
      "Epoch 9: |          | 1324/? [18:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1324, loss 4.042926788330078\n",
      "Epoch 9: |          | 1325/? [18:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1325, loss 4.159818172454834\n",
      "Epoch 9: |          | 1326/? [19:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1326, loss 3.674511432647705\n",
      "Epoch 9: |          | 1327/? [19:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1327, loss 3.558668851852417\n",
      "Epoch 9: |          | 1328/? [19:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1328, loss 3.22774076461792\n",
      "Epoch 9: |          | 1329/? [19:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1329, loss 3.759157657623291\n",
      "Epoch 9: |          | 1330/? [19:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1330, loss 3.520895004272461\n",
      "Epoch 9: |          | 1331/? [19:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1331, loss 3.659034252166748\n",
      "Epoch 9: |          | 1332/? [19:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1332, loss 3.3522980213165283\n",
      "Epoch 9: |          | 1333/? [19:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1333, loss 3.3531482219696045\n",
      "Epoch 9: |          | 1334/? [19:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1334, loss 3.4594039916992188\n",
      "Epoch 9: |          | 1335/? [19:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1335, loss 3.442857265472412\n",
      "Epoch 9: |          | 1336/? [19:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1336, loss 3.039628505706787\n",
      "Epoch 9: |          | 1337/? [19:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1337, loss 3.59039306640625\n",
      "Epoch 9: |          | 1338/? [19:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1338, loss 2.961383819580078\n",
      "Epoch 9: |          | 1339/? [19:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1339, loss 3.513984203338623\n",
      "Epoch 9: |          | 1340/? [19:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1340, loss 3.0005788803100586\n",
      "Epoch 9: |          | 1341/? [19:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1341, loss 3.6906325817108154\n",
      "Epoch 9: |          | 1342/? [19:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1342, loss 3.9226088523864746\n",
      "Epoch 9: |          | 1343/? [19:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1343, loss 3.4422607421875\n",
      "Epoch 9: |          | 1344/? [19:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1344, loss 3.564253330230713\n",
      "Epoch 9: |          | 1345/? [19:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1345, loss 3.7087581157684326\n",
      "Epoch 9: |          | 1346/? [19:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1346, loss 4.603151798248291\n",
      "Epoch 9: |          | 1347/? [19:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1347, loss 3.664048433303833\n",
      "Epoch 9: |          | 1348/? [19:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1348, loss 3.678004741668701\n",
      "Epoch 9: |          | 1349/? [19:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1349, loss 3.7261531352996826\n",
      "Epoch 9: |          | 1350/? [19:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1350, loss 3.6439476013183594\n",
      "Epoch 9: |          | 1351/? [19:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1351, loss 3.696634292602539\n",
      "Epoch 9: |          | 1352/? [19:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1352, loss 3.0194942951202393\n",
      "Epoch 9: |          | 1353/? [19:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1353, loss 3.381877899169922\n",
      "Epoch 9: |          | 1354/? [19:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1354, loss 3.6989941596984863\n",
      "Epoch 9: |          | 1355/? [19:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1355, loss 3.832289218902588\n",
      "Epoch 9: |          | 1356/? [19:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1356, loss 3.6273436546325684\n",
      "Epoch 9: |          | 1357/? [19:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1357, loss 3.4449374675750732\n",
      "Epoch 9: |          | 1358/? [19:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1358, loss 3.567981004714966\n",
      "Epoch 9: |          | 1359/? [19:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1359, loss 3.4517006874084473\n",
      "Epoch 9: |          | 1360/? [19:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1360, loss 3.602665662765503\n",
      "Epoch 9: |          | 1361/? [19:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1361, loss 3.578411102294922\n",
      "Epoch 9: |          | 1362/? [19:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1362, loss 3.4610252380371094\n",
      "Epoch 9: |          | 1363/? [19:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1363, loss 2.908451795578003\n",
      "Epoch 9: |          | 1364/? [19:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1364, loss 3.366201400756836\n",
      "Epoch 9: |          | 1365/? [19:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1365, loss 3.112555503845215\n",
      "Epoch 9: |          | 1366/? [19:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1366, loss 3.785900831222534\n",
      "Epoch 9: |          | 1367/? [19:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1367, loss 3.120112180709839\n",
      "Epoch 9: |          | 1368/? [19:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1368, loss 2.9115514755249023\n",
      "Epoch 9: |          | 1369/? [19:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1369, loss 3.6124866008758545\n",
      "Epoch 9: |          | 1370/? [19:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1370, loss 3.1590325832366943\n",
      "Epoch 9: |          | 1371/? [19:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1371, loss 4.010429382324219\n",
      "Epoch 9: |          | 1372/? [19:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1372, loss 3.509631633758545\n",
      "Epoch 9: |          | 1373/? [19:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1373, loss 3.8325366973876953\n",
      "Epoch 9: |          | 1374/? [19:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1374, loss 3.0238699913024902\n",
      "Epoch 9: |          | 1375/? [19:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1375, loss 3.614671230316162\n",
      "Epoch 9: |          | 1376/? [19:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1376, loss 3.4842638969421387\n",
      "Epoch 9: |          | 1377/? [19:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1377, loss 3.4993138313293457\n",
      "Epoch 9: |          | 1378/? [19:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1378, loss 3.4382576942443848\n",
      "Epoch 9: |          | 1379/? [19:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1379, loss 3.430325746536255\n",
      "Epoch 9: |          | 1380/? [19:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1380, loss 3.60913348197937\n",
      "Epoch 9: |          | 1381/? [19:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1381, loss 3.7249579429626465\n",
      "Epoch 9: |          | 1382/? [19:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1382, loss 3.2910284996032715\n",
      "Epoch 9: |          | 1383/? [19:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1383, loss 3.4807376861572266\n",
      "Epoch 9: |          | 1384/? [19:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1384, loss 3.4942405223846436\n",
      "Epoch 9: |          | 1385/? [19:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1385, loss 3.4705746173858643\n",
      "Epoch 9: |          | 1386/? [19:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1386, loss 3.622162342071533\n",
      "Epoch 9: |          | 1387/? [19:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1387, loss 3.5273818969726562\n",
      "Epoch 9: |          | 1388/? [19:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1388, loss 3.0964131355285645\n",
      "Epoch 9: |          | 1389/? [19:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1389, loss 3.7175402641296387\n",
      "Epoch 9: |          | 1390/? [19:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1390, loss 3.988636016845703\n",
      "Epoch 9: |          | 1391/? [19:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1391, loss 3.634634494781494\n",
      "Epoch 9: |          | 1392/? [19:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1392, loss 3.1488277912139893\n",
      "Epoch 9: |          | 1393/? [19:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1393, loss 3.3197529315948486\n",
      "Epoch 9: |          | 1394/? [19:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1394, loss 2.935762643814087\n",
      "Epoch 9: |          | 1395/? [19:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1395, loss 3.637427806854248\n",
      "Epoch 9: |          | 1396/? [19:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1396, loss 3.474325180053711\n",
      "Epoch 9: |          | 1397/? [20:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1397, loss 2.827650785446167\n",
      "Epoch 9: |          | 1398/? [20:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1398, loss 3.964487075805664\n",
      "Epoch 9: |          | 1399/? [20:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1399, loss 3.9408316612243652\n",
      "Epoch 9: |          | 1400/? [20:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1400, loss 3.159167766571045\n",
      "Epoch 9: |          | 1401/? [20:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1401, loss 3.749943494796753\n",
      "Epoch 9: |          | 1402/? [20:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1402, loss 3.494427442550659\n",
      "Epoch 9: |          | 1403/? [20:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1403, loss 3.703050136566162\n",
      "Epoch 9: |          | 1404/? [20:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1404, loss 3.625405788421631\n",
      "Epoch 9: |          | 1405/? [20:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1405, loss 3.979583263397217\n",
      "Epoch 9: |          | 1406/? [20:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1406, loss 3.8905715942382812\n",
      "Epoch 9: |          | 1407/? [20:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1407, loss 3.93603515625\n",
      "Epoch 9: |          | 1408/? [20:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1408, loss 3.2244057655334473\n",
      "Epoch 9: |          | 1409/? [20:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1409, loss 3.260636806488037\n",
      "Epoch 9: |          | 1410/? [20:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1410, loss 3.302802324295044\n",
      "Epoch 9: |          | 1411/? [20:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1411, loss 3.6159324645996094\n",
      "Epoch 9: |          | 1412/? [20:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1412, loss 3.363771915435791\n",
      "Epoch 9: |          | 1413/? [20:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1413, loss 3.1825482845306396\n",
      "Epoch 9: |          | 1414/? [20:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1414, loss 3.361469268798828\n",
      "Epoch 9: |          | 1415/? [20:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1415, loss 3.552481174468994\n",
      "Epoch 9: |          | 1416/? [20:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1416, loss 3.9154534339904785\n",
      "Epoch 9: |          | 1417/? [20:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1417, loss 3.637467861175537\n",
      "Epoch 9: |          | 1418/? [20:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1418, loss 3.7039618492126465\n",
      "Epoch 9: |          | 1419/? [20:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1419, loss 3.4830386638641357\n",
      "Epoch 9: |          | 1420/? [20:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1420, loss 3.3407599925994873\n",
      "Epoch 9: |          | 1421/? [20:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1421, loss 3.1596100330352783\n",
      "Epoch 9: |          | 1422/? [20:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1422, loss 3.7498068809509277\n",
      "Epoch 9: |          | 1423/? [20:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1423, loss 3.7774784564971924\n",
      "Epoch 9: |          | 1424/? [20:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1424, loss 3.484433650970459\n",
      "Epoch 9: |          | 1425/? [20:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1425, loss 3.6629111766815186\n",
      "Epoch 9: |          | 1426/? [20:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1426, loss 3.2626380920410156\n",
      "Epoch 9: |          | 1427/? [20:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1427, loss 3.875769853591919\n",
      "Epoch 9: |          | 1428/? [20:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1428, loss 3.800226926803589\n",
      "Epoch 9: |          | 1429/? [20:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1429, loss 3.7133662700653076\n",
      "Epoch 9: |          | 1430/? [20:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1430, loss 3.749648332595825\n",
      "Epoch 9: |          | 1431/? [20:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1431, loss 3.551301956176758\n",
      "Epoch 9: |          | 1432/? [20:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1432, loss 3.612990140914917\n",
      "Epoch 9: |          | 1433/? [20:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1433, loss 3.5414719581604004\n",
      "Epoch 9: |          | 1434/? [20:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1434, loss 3.664519786834717\n",
      "Epoch 9: |          | 1435/? [20:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1435, loss 3.286086320877075\n",
      "Epoch 9: |          | 1436/? [20:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1436, loss 3.639319896697998\n",
      "Epoch 9: |          | 1437/? [20:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1437, loss 2.8843836784362793\n",
      "Epoch 9: |          | 1438/? [20:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1438, loss 4.4806742668151855\n",
      "Epoch 9: |          | 1439/? [20:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1439, loss 3.524630069732666\n",
      "Epoch 9: |          | 1440/? [20:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1440, loss 3.7787868976593018\n",
      "Epoch 9: |          | 1441/? [20:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1441, loss 4.051103591918945\n",
      "Epoch 9: |          | 1442/? [20:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1442, loss 4.06542444229126\n",
      "Epoch 9: |          | 1443/? [20:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1443, loss 3.2769558429718018\n",
      "Epoch 9: |          | 1444/? [20:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1444, loss 3.286656618118286\n",
      "Epoch 9: |          | 1445/? [20:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1445, loss 3.754566192626953\n",
      "Epoch 9: |          | 1446/? [20:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1446, loss 3.3029816150665283\n",
      "Epoch 9: |          | 1447/? [20:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1447, loss 3.4726130962371826\n",
      "Epoch 9: |          | 1448/? [20:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1448, loss 3.3411030769348145\n",
      "Epoch 9: |          | 1449/? [20:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1449, loss 3.547156572341919\n",
      "Epoch 9: |          | 1450/? [20:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1450, loss 3.5715434551239014\n",
      "Epoch 9: |          | 1451/? [20:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1451, loss 3.9222817420959473\n",
      "Epoch 9: |          | 1452/? [20:47<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1452, loss 3.620919704437256\n",
      "Epoch 9: |          | 1453/? [20:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1453, loss 3.043355941772461\n",
      "Epoch 9: |          | 1454/? [20:48<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1454, loss 3.5822901725769043\n",
      "Epoch 9: |          | 1455/? [20:49<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1455, loss 3.678058624267578\n",
      "Epoch 9: |          | 1456/? [20:50<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1456, loss 3.257209300994873\n",
      "Epoch 9: |          | 1457/? [20:51<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1457, loss 3.38489031791687\n",
      "Epoch 9: |          | 1458/? [20:52<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1458, loss 3.639267683029175\n",
      "Epoch 9: |          | 1459/? [20:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1459, loss 3.7150158882141113\n",
      "Epoch 9: |          | 1460/? [20:53<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1460, loss 3.7049026489257812\n",
      "Epoch 9: |          | 1461/? [20:54<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1461, loss 3.581587314605713\n",
      "Epoch 9: |          | 1462/? [20:55<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1462, loss 3.9106719493865967\n",
      "Epoch 9: |          | 1463/? [20:56<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1463, loss 3.815762996673584\n",
      "Epoch 9: |          | 1464/? [20:57<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1464, loss 3.082491397857666\n",
      "Epoch 9: |          | 1465/? [20:58<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1465, loss 3.540482997894287\n",
      "Epoch 9: |          | 1466/? [20:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1466, loss 3.245789051055908\n",
      "Epoch 9: |          | 1467/? [20:59<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1467, loss 3.8428795337677\n",
      "Epoch 9: |          | 1468/? [21:00<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1468, loss 3.3690059185028076\n",
      "Epoch 9: |          | 1469/? [21:01<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1469, loss 3.1996006965637207\n",
      "Epoch 9: |          | 1470/? [21:02<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1470, loss 3.6121621131896973\n",
      "Epoch 9: |          | 1471/? [21:03<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1471, loss 3.9262752532958984\n",
      "Epoch 9: |          | 1472/? [21:04<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1472, loss 3.674330234527588\n",
      "Epoch 9: |          | 1473/? [21:05<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1473, loss 3.490309476852417\n",
      "Epoch 9: |          | 1474/? [21:06<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1474, loss 3.407865524291992\n",
      "Epoch 9: |          | 1475/? [21:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1475, loss 2.987753391265869\n",
      "Epoch 9: |          | 1476/? [21:07<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1476, loss 3.5531928539276123\n",
      "Epoch 9: |          | 1477/? [21:08<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1477, loss 3.6059412956237793\n",
      "Epoch 9: |          | 1478/? [21:09<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1478, loss 3.4799892902374268\n",
      "Epoch 9: |          | 1479/? [21:10<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1479, loss 3.982940673828125\n",
      "Epoch 9: |          | 1480/? [21:11<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1480, loss 3.678321123123169\n",
      "Epoch 9: |          | 1481/? [21:12<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1481, loss 3.463517665863037\n",
      "Epoch 9: |          | 1482/? [21:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1482, loss 3.6371910572052\n",
      "Epoch 9: |          | 1483/? [21:13<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1483, loss 3.2465827465057373\n",
      "Epoch 9: |          | 1484/? [21:14<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1484, loss 3.474214553833008\n",
      "Epoch 9: |          | 1485/? [21:15<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1485, loss 3.586804151535034\n",
      "Epoch 9: |          | 1486/? [21:16<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1486, loss 3.650106430053711\n",
      "Epoch 9: |          | 1487/? [21:17<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1487, loss 3.0649850368499756\n",
      "Epoch 9: |          | 1488/? [21:18<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1488, loss 3.6991257667541504\n",
      "Epoch 9: |          | 1489/? [21:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1489, loss 3.6572539806365967\n",
      "Epoch 9: |          | 1490/? [21:19<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1490, loss 3.4836223125457764\n",
      "Epoch 9: |          | 1491/? [21:20<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1491, loss 2.6389174461364746\n",
      "Epoch 9: |          | 1492/? [21:21<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1492, loss 3.164669990539551\n",
      "Epoch 9: |          | 1493/? [21:22<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1493, loss 2.66550350189209\n",
      "Epoch 9: |          | 1494/? [21:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1494, loss 3.492905855178833\n",
      "Epoch 9: |          | 1495/? [21:23<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1495, loss 3.3689606189727783\n",
      "Epoch 9: |          | 1496/? [21:24<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1496, loss 3.693295955657959\n",
      "Epoch 9: |          | 1497/? [21:25<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1497, loss 2.963860273361206\n",
      "Epoch 9: |          | 1498/? [21:26<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1498, loss 3.285548448562622\n",
      "Epoch 9: |          | 1499/? [21:27<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1499, loss 3.7747483253479004\n",
      "Epoch 9: |          | 1500/? [21:28<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1500, loss 3.672405958175659\n",
      "Epoch 9: |          | 1501/? [21:29<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1501, loss 3.592604875564575\n",
      "Epoch 9: |          | 1502/? [21:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1502, loss 3.6271843910217285\n",
      "Epoch 9: |          | 1503/? [21:30<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1503, loss 3.4320626258850098\n",
      "Epoch 9: |          | 1504/? [21:31<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1504, loss 4.069272994995117\n",
      "Epoch 9: |          | 1505/? [21:32<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1505, loss 3.8228421211242676\n",
      "Epoch 9: |          | 1506/? [21:33<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1506, loss 3.4683525562286377\n",
      "Epoch 9: |          | 1507/? [21:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1507, loss 3.3950774669647217\n",
      "Epoch 9: |          | 1508/? [21:34<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1508, loss 3.538968563079834\n",
      "Epoch 9: |          | 1509/? [21:35<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1509, loss 3.509000301361084\n",
      "Epoch 9: |          | 1510/? [21:36<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1510, loss 3.7724602222442627\n",
      "Epoch 9: |          | 1511/? [21:37<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1511, loss 3.411715030670166\n",
      "Epoch 9: |          | 1512/? [21:38<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1512, loss 3.894252300262451\n",
      "Epoch 9: |          | 1513/? [21:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1513, loss 4.185521602630615\n",
      "Epoch 9: |          | 1514/? [21:39<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1514, loss 3.267854690551758\n",
      "Epoch 9: |          | 1515/? [21:40<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1515, loss 3.9842944145202637\n",
      "Epoch 9: |          | 1516/? [21:41<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1516, loss 4.047310829162598\n",
      "Epoch 9: |          | 1517/? [21:42<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1517, loss 3.484455108642578\n",
      "Epoch 9: |          | 1518/? [21:43<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1518, loss 3.305384874343872\n",
      "Epoch 9: |          | 1519/? [21:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1519, loss 3.771017074584961\n",
      "Epoch 9: |          | 1520/? [21:44<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1520, loss 3.724932909011841\n",
      "Epoch 9: |          | 1521/? [21:45<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1521, loss 3.5677077770233154\n",
      "Epoch 9: |          | 1522/? [21:46<00:00,  1.16it/s, v_num=30]   TRRAINING: Batch 1522, loss 3.362206220626831\n",
      "Epoch 9: |          | 1523/? [21:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1523, loss 3.665085554122925\n",
      "Epoch 9: |          | 1524/? [21:47<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1524, loss 3.5150725841522217\n",
      "Epoch 9: |          | 1525/? [21:48<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1525, loss 3.2017822265625\n",
      "Epoch 9: |          | 1526/? [21:49<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1526, loss 3.7905704975128174\n",
      "Epoch 9: |          | 1527/? [21:50<00:00,  1.17it/s, v_num=30]   TRRAINING: Batch 1527, loss 3.7986385822296143\n",
      "Epoch 9: |          | 1528/? [21:51<00:00,  1.17it/s, v_num=30]ERROR: Input has inproper shape\n",
      "Epoch 9: |          | 1529/? [21:51<00:00,  1.17it/s, v_num=30]   VALIDATION: Batch 0, loss 4.644975662231445\n",
      "   VALIDATION: Batch 1, loss 3.5893139839172363\n",
      "   VALIDATION: Batch 2, loss 4.779931545257568\n",
      "   VALIDATION: Batch 3, loss 4.527444362640381\n",
      "   VALIDATION: Batch 4, loss 4.05880069732666\n",
      "   VALIDATION: Batch 5, loss 3.716212511062622\n",
      "   VALIDATION: Batch 6, loss 4.025196075439453\n",
      "   VALIDATION: Batch 7, loss 4.713391304016113\n",
      "   VALIDATION: Batch 8, loss 4.531261920928955\n",
      "   VALIDATION: Batch 9, loss 4.691737174987793\n",
      "   VALIDATION: Batch 10, loss 4.368330001831055\n",
      "   VALIDATION: Batch 11, loss 3.9644687175750732\n",
      "   VALIDATION: Batch 12, loss 4.200821876525879\n",
      "   VALIDATION: Batch 13, loss 4.754463195800781\n",
      "   VALIDATION: Batch 14, loss 4.087718963623047\n",
      "   VALIDATION: Batch 15, loss 3.9861016273498535\n",
      "   VALIDATION: Batch 16, loss 4.581244468688965\n",
      "   VALIDATION: Batch 17, loss 4.244876384735107\n",
      "   VALIDATION: Batch 18, loss 3.559185743331909\n",
      "   VALIDATION: Batch 19, loss 4.483682632446289\n",
      "   VALIDATION: Batch 20, loss 4.761996269226074\n",
      "   VALIDATION: Batch 21, loss 5.065186977386475\n",
      "   VALIDATION: Batch 22, loss 4.64177942276001\n",
      "   VALIDATION: Batch 23, loss 4.121747016906738\n",
      "   VALIDATION: Batch 24, loss 3.9925646781921387\n",
      "   VALIDATION: Batch 25, loss 4.433542728424072\n",
      "   VALIDATION: Batch 26, loss 4.61091947555542\n",
      "   VALIDATION: Batch 27, loss 4.548033237457275\n",
      "   VALIDATION: Batch 28, loss 4.277922630310059\n",
      "   VALIDATION: Batch 29, loss 4.489371299743652\n",
      "   VALIDATION: Batch 30, loss 4.052761077880859\n",
      "   VALIDATION: Batch 31, loss 4.384893894195557\n",
      "   VALIDATION: Batch 32, loss 4.951019763946533\n",
      "   VALIDATION: Batch 33, loss 3.1329331398010254\n",
      "   VALIDATION: Batch 34, loss 4.377209663391113\n",
      "   VALIDATION: Batch 35, loss 4.619340419769287\n",
      "   VALIDATION: Batch 36, loss 3.866576671600342\n",
      "   VALIDATION: Batch 37, loss 3.878282070159912\n",
      "   VALIDATION: Batch 38, loss 3.9841766357421875\n",
      "   VALIDATION: Batch 39, loss 4.386428356170654\n",
      "   VALIDATION: Batch 40, loss 4.437378883361816\n",
      "   VALIDATION: Batch 41, loss 3.255175828933716\n",
      "   VALIDATION: Batch 42, loss 4.590801239013672\n",
      "   VALIDATION: Batch 43, loss 4.600776672363281\n",
      "   VALIDATION: Batch 44, loss 4.205141067504883\n",
      "   VALIDATION: Batch 45, loss 4.588277339935303\n",
      "   VALIDATION: Batch 46, loss 3.690553665161133\n",
      "   VALIDATION: Batch 47, loss 4.7575764656066895\n",
      "   VALIDATION: Batch 48, loss 4.90300178527832\n",
      "   VALIDATION: Batch 49, loss 4.42413854598999\n",
      "   VALIDATION: Batch 50, loss 4.413269996643066\n",
      "   VALIDATION: Batch 51, loss 4.917423248291016\n",
      "   VALIDATION: Batch 52, loss 4.055544853210449\n",
      "   VALIDATION: Batch 53, loss 3.948462724685669\n",
      "   VALIDATION: Batch 54, loss 4.024243354797363\n",
      "   VALIDATION: Batch 55, loss 4.7821831703186035\n",
      "   VALIDATION: Batch 56, loss 4.151371002197266\n",
      "   VALIDATION: Batch 57, loss 5.830265998840332\n",
      "   VALIDATION: Batch 58, loss 4.318436622619629\n",
      "   VALIDATION: Batch 59, loss 3.9646313190460205\n",
      "   VALIDATION: Batch 60, loss 3.4617340564727783\n",
      "   VALIDATION: Batch 61, loss 4.357329845428467\n",
      "   VALIDATION: Batch 62, loss 4.3428754806518555\n",
      "   VALIDATION: Batch 63, loss 4.874847888946533\n",
      "   VALIDATION: Batch 64, loss 4.637804985046387\n",
      "   VALIDATION: Batch 65, loss 3.7686004638671875\n",
      "   VALIDATION: Batch 66, loss 4.686989784240723\n",
      "   VALIDATION: Batch 67, loss 4.120865345001221\n",
      "   VALIDATION: Batch 68, loss 4.284405708312988\n",
      "   VALIDATION: Batch 69, loss 4.555511474609375\n",
      "   VALIDATION: Batch 70, loss 4.708509922027588\n",
      "   VALIDATION: Batch 71, loss 4.216435432434082\n",
      "   VALIDATION: Batch 72, loss 5.099230766296387\n",
      "   VALIDATION: Batch 73, loss 3.8120765686035156\n",
      "   VALIDATION: Batch 74, loss 4.482475280761719\n",
      "   VALIDATION: Batch 75, loss 4.504502296447754\n",
      "   VALIDATION: Batch 76, loss 4.345704078674316\n",
      "   VALIDATION: Batch 77, loss 4.6098198890686035\n",
      "   VALIDATION: Batch 78, loss 4.442477226257324\n",
      "   VALIDATION: Batch 79, loss 4.430052757263184\n",
      "   VALIDATION: Batch 80, loss 4.5128326416015625\n",
      "   VALIDATION: Batch 81, loss 4.242732048034668\n",
      "   VALIDATION: Batch 82, loss 4.631297588348389\n",
      "   VALIDATION: Batch 83, loss 3.807741165161133\n",
      "   VALIDATION: Batch 84, loss 4.5860724449157715\n",
      "   VALIDATION: Batch 85, loss 4.217387676239014\n",
      "   VALIDATION: Batch 86, loss 4.2656965255737305\n",
      "   VALIDATION: Batch 87, loss 4.148748397827148\n",
      "   VALIDATION: Batch 88, loss 3.7435951232910156\n",
      "   VALIDATION: Batch 89, loss 4.0258612632751465\n",
      "   VALIDATION: Batch 90, loss 4.329775333404541\n",
      "   VALIDATION: Batch 91, loss 4.453110694885254\n",
      "   VALIDATION: Batch 92, loss 4.124344825744629\n",
      "   VALIDATION: Batch 93, loss 4.794839382171631\n",
      "   VALIDATION: Batch 94, loss 4.306120872497559\n",
      "   VALIDATION: Batch 95, loss 3.763792037963867\n",
      "   VALIDATION: Batch 96, loss 4.242517948150635\n",
      "   VALIDATION: Batch 97, loss 3.92618989944458\n",
      "   VALIDATION: Batch 98, loss 4.603549480438232\n",
      "   VALIDATION: Batch 99, loss 4.641902923583984\n",
      "   VALIDATION: Batch 100, loss 5.070039749145508\n",
      "   VALIDATION: Batch 101, loss 3.604001522064209\n",
      "   VALIDATION: Batch 102, loss 5.013803482055664\n",
      "   VALIDATION: Batch 103, loss 4.969695091247559\n",
      "   VALIDATION: Batch 104, loss 3.8976035118103027\n",
      "   VALIDATION: Batch 105, loss 4.401312351226807\n",
      "   VALIDATION: Batch 106, loss 4.211759090423584\n",
      "   VALIDATION: Batch 107, loss 4.339574813842773\n",
      "   VALIDATION: Batch 108, loss 4.071007251739502\n",
      "   VALIDATION: Batch 109, loss 4.677942752838135\n",
      "   VALIDATION: Batch 110, loss 4.375155448913574\n",
      "   VALIDATION: Batch 111, loss 4.681227684020996\n",
      "   VALIDATION: Batch 112, loss 5.616801738739014\n",
      "   VALIDATION: Batch 113, loss 4.874151706695557\n",
      "   VALIDATION: Batch 114, loss 4.63657283782959\n",
      "   VALIDATION: Batch 115, loss 4.06636381149292\n",
      "   VALIDATION: Batch 116, loss 3.915041446685791\n",
      "   VALIDATION: Batch 117, loss 4.597119331359863\n",
      "   VALIDATION: Batch 118, loss 4.804804801940918\n",
      "   VALIDATION: Batch 119, loss 3.9113717079162598\n",
      "   VALIDATION: Batch 120, loss 3.5261192321777344\n",
      "   VALIDATION: Batch 121, loss 3.8785312175750732\n",
      "   VALIDATION: Batch 122, loss 4.300252437591553\n",
      "   VALIDATION: Batch 123, loss 4.292781352996826\n",
      "   VALIDATION: Batch 124, loss 3.664112091064453\n",
      "   VALIDATION: Batch 125, loss 4.255888938903809\n",
      "   VALIDATION: Batch 126, loss 4.497222423553467\n",
      "   VALIDATION: Batch 127, loss 4.2928571701049805\n",
      "   VALIDATION: Batch 128, loss 4.411110877990723\n",
      "   VALIDATION: Batch 129, loss 4.085264682769775\n",
      "   VALIDATION: Batch 130, loss 3.632753849029541\n",
      "   VALIDATION: Batch 131, loss 3.7005438804626465\n",
      "   VALIDATION: Batch 132, loss 4.362910747528076\n",
      "   VALIDATION: Batch 133, loss 4.518187522888184\n",
      "   VALIDATION: Batch 134, loss 4.380307197570801\n",
      "   VALIDATION: Batch 135, loss 4.675959587097168\n",
      "   VALIDATION: Batch 136, loss 4.7596259117126465\n",
      "   VALIDATION: Batch 137, loss 4.540508270263672\n",
      "   VALIDATION: Batch 138, loss 4.262844562530518\n",
      "   VALIDATION: Batch 139, loss 4.691808223724365\n",
      "   VALIDATION: Batch 140, loss 3.805100917816162\n",
      "   VALIDATION: Batch 141, loss 4.7472405433654785\n",
      "   VALIDATION: Batch 142, loss 3.428135633468628\n",
      "   VALIDATION: Batch 143, loss 4.264562606811523\n",
      "   VALIDATION: Batch 144, loss 4.545881271362305\n",
      "   VALIDATION: Batch 145, loss 4.326419830322266\n",
      "   VALIDATION: Batch 146, loss 4.156181335449219\n",
      "   VALIDATION: Batch 147, loss 4.476351737976074\n",
      "   VALIDATION: Batch 148, loss 4.542896270751953\n",
      "   VALIDATION: Batch 149, loss 5.06748628616333\n",
      "   VALIDATION: Batch 150, loss 4.621411323547363\n",
      "   VALIDATION: Batch 151, loss 4.933205604553223\n",
      "   VALIDATION: Batch 152, loss 4.285603046417236\n",
      "   VALIDATION: Batch 153, loss 4.532909393310547\n",
      "   VALIDATION: Batch 154, loss 4.413744926452637\n",
      "   VALIDATION: Batch 155, loss 4.12345027923584\n",
      "   VALIDATION: Batch 156, loss 4.786881446838379\n",
      "   VALIDATION: Batch 157, loss 4.4432501792907715\n",
      "   VALIDATION: Batch 158, loss 3.8428549766540527\n",
      "   VALIDATION: Batch 159, loss 4.3125081062316895\n",
      "   VALIDATION: Batch 160, loss 4.710152626037598\n",
      "   VALIDATION: Batch 161, loss 4.916871070861816\n",
      "   VALIDATION: Batch 162, loss 4.417693138122559\n",
      "   VALIDATION: Batch 163, loss 3.7664432525634766\n",
      "   VALIDATION: Batch 164, loss 4.286499977111816\n",
      "   VALIDATION: Batch 165, loss 4.761395454406738\n",
      "   VALIDATION: Batch 166, loss 4.257128715515137\n",
      "   VALIDATION: Batch 167, loss 4.576333045959473\n",
      "   VALIDATION: Batch 168, loss 3.457314968109131\n",
      "   VALIDATION: Batch 169, loss 4.138449668884277\n",
      "   VALIDATION: Batch 170, loss 4.363430500030518\n",
      "   VALIDATION: Batch 171, loss 4.509797096252441\n",
      "   VALIDATION: Batch 172, loss 4.2705183029174805\n",
      "   VALIDATION: Batch 173, loss 4.138448715209961\n",
      "   VALIDATION: Batch 174, loss 4.632498741149902\n",
      "   VALIDATION: Batch 175, loss 4.431366443634033\n",
      "   VALIDATION: Batch 176, loss 4.2338457107543945\n",
      "   VALIDATION: Batch 177, loss 4.159098148345947\n",
      "   VALIDATION: Batch 178, loss 5.292830944061279\n",
      "   VALIDATION: Batch 179, loss 4.491852760314941\n",
      "   VALIDATION: Batch 180, loss 4.01794958114624\n",
      "   VALIDATION: Batch 181, loss 4.179022789001465\n",
      "   VALIDATION: Batch 182, loss 4.469645977020264\n",
      "   VALIDATION: Batch 183, loss 3.468587875366211\n",
      "   VALIDATION: Batch 184, loss 3.1924145221710205\n",
      "   VALIDATION: Batch 185, loss 4.036401271820068\n",
      "   VALIDATION: Batch 186, loss 3.961638927459717\n",
      "   VALIDATION: Batch 187, loss 4.168396949768066\n",
      "   VALIDATION: Batch 188, loss 4.582531929016113\n",
      "   VALIDATION: Batch 189, loss 3.8966445922851562\n",
      "   VALIDATION: Batch 190, loss 3.9807968139648438\n",
      "   VALIDATION: Batch 191, loss 4.471369743347168\n",
      "   VALIDATION: Batch 192, loss 4.872914791107178\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 9: |          | 1529/? [23:25<00:00,  1.09it/s, v_num=30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: |          | 1529/? [23:25<00:00,  1.09it/s, v_num=30]\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "# Define the checkpoint callback to save the model every 1000 batches\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=model_saving_path,  # Directory to save the checkpoints\n",
    "    filename=saving_filename,  # Filename pattern\n",
    "    save_top_k=-1,  # Save all models\n",
    "    save_weights_only=False,  # Save only the weights (or set to False to save the full model)\n",
    "    every_n_train_steps=save_every_n_baches  # Save the model every 1000 batches\n",
    ")\n",
    "#new tensorboard for displaying logs\n",
    "\n",
    "# Define the logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"moe_plain_model\") ### CHANGE NAME FOR DIFFERENT RUN (different model)\n",
    "\n",
    "# Initialize the trainer with the checkpoint callback\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=epochs, # Set the number of epochs\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "trainer.fit(model = model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      batch_idx   val_loss\n",
      "0             0  10.483538\n",
      "1             1  10.502657\n",
      "2             0   5.404570\n",
      "3             1   4.248115\n",
      "4             2   5.552897\n",
      "...         ...        ...\n",
      "1927        188   4.582532\n",
      "1928        189   3.896645\n",
      "1929        190   3.980797\n",
      "1930        191   4.471370\n",
      "1931        192   4.872915\n",
      "\n",
      "[1932 rows x 2 columns]\n",
      "       batch_idx  train_loss\n",
      "0              0   10.508198\n",
      "1              1   10.082255\n",
      "2              2    9.035495\n",
      "3              3    9.248161\n",
      "4              4    8.656418\n",
      "...          ...         ...\n",
      "15275       1523    3.665086\n",
      "15276       1524    3.515073\n",
      "15277       1525    3.201782\n",
      "15278       1526    3.790570\n",
      "15279       1527    3.798639\n",
      "\n",
      "[15280 rows x 2 columns]\n",
      "DataFrame saved to D:/Projekt_NLP/Saved_stuff/logs/vectorized_moe\\logs_train_Normal_moe_model.csv\n",
      "DataFrame saved to D:/Projekt_NLP/Saved_stuff/logs/vectorized_moe\\logs_val_Normal_moe_model.csv\n"
     ]
    }
   ],
   "source": [
    "### Saving logs to csv\n",
    "val_data=model.val_losses_list\n",
    "train_data=model.train_losses_list\n",
    "# Convert list of dictionaries to DataFrame\n",
    "log_val_df = pd.DataFrame(val_data)\n",
    "log_train_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(log_val_df)\n",
    "print(log_train_df)\n",
    "\n",
    "# Directory to save the CSV file\n",
    "save_dir = log_saving_path #'/path/to/your/directory'  # Replace with your desired directory path\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "# save_dir = 'D:/Projekt_NLP/Saved_stuff/logs/vectorized_moe'#'/path/to/your/directory'  # Replace with your desired directory path\n",
    "# os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "model_name='Normal_moe_model'\n",
    "# Define the filename for your CSV file\n",
    "csv_filename_train = f'logs_train_{model_name}.csv'\n",
    "csv_filename_val = f'logs_val_{model_name}.csv'\n",
    "\n",
    "# Construct the full file path\n",
    "csv_file_path_train = os.path.join(save_dir, csv_filename_train)\n",
    "csv_file_path_val = os.path.join(save_dir, csv_filename_val)\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "log_train_df.to_csv(csv_file_path_train, index=False)\n",
    "log_val_df.to_csv(csv_file_path_val, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {csv_file_path_train}\")\n",
    "print(f\"DataFrame saved to {csv_file_path_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training second MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_small= PretrainedConfig(\n",
    "    num_experts_per_token=4,\n",
    "    hidden_size=256,\n",
    "    num_attention_heads = 4,\n",
    "    num_MH_MOE_heads = 4,\n",
    "    num_experts=8,\n",
    "    batch_size = 20,\n",
    "    seq_len = 512,\n",
    "    capacity_factor = 3,\n",
    "    device = device,\n",
    "    intermediate_size = 512,\n",
    "    forward_layer_class = MH_MoE,\n",
    "    vocab_size = 30522,\n",
    "    n_layers = 12,\n",
    "    no_lori_segments = 32,\n",
    "    py_lightning_loging = False,\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(),\n",
    "    lr = 0.001, #SET TO 0.0002\n",
    "    betas = (0.9, 0.98),\n",
    "    treat_mh_lori_as_regular_lori = True,\n",
    "    load_balancing_coefficient=0.01,\n",
    "    proportions = [0, 0.8, 0.1, 0.1] # null, train, validation, test\n",
    ")\n",
    "\n",
    "config = config_small\n",
    "\n",
    "#training hiperparams\n",
    "save_every_n_baches = 500 #how often do you wish to save the model\n",
    "epochs = 10\n",
    "\n",
    "#path to folders where you want to save model checkpoints and val and train logs\n",
    "model_saving_path = 'D:/Projekt_NLP/Saved_stuff/saved_models'\n",
    "log_saving_path = 'D:/Projekt_NLP/Saved_stuff/logs/mh_moe'\n",
    "\n",
    "model_name='MH_MoE_model' #name of the model in saving logs\n",
    "saving_filename = 'MH_MOE_175M-{epoch}-{step}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Model Size: 101.81 MB, total number of parameters: 106,757,352\n",
      "Estimated Model Size: 42.08 MB, total number of parameters: 44,126,208\n",
      "Estimated Model Size: 2.50 MB, total number of parameters: 2,623,488\n",
      "Total GPU memory: 12.8843776 GB\n",
      "Reserved GPU memory: 0.109051904 GB\n",
      "Allocated GPU memory: 0.10729216 GB\n",
      "Free GPU memory: 0.001759744 GB\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(config_small).to(config_small.device)\n",
    "estimate_model_size(model)\n",
    "estimate_model_size(model.layers)\n",
    "estimate_model_size(model.layers[0].forward_layer)\n",
    "get_gpu_memory()\n",
    "print(isinstance(model, LightningModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Komputer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory D:\\Projekt_NLP\\Saved_stuff\\saved_models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | loss_fn    | CrossEntropyLoss | 0     \n",
      "1 | embedding  | Embedding        | 7.8 M \n",
      "2 | layers     | ModuleList       | 11.0 M\n",
      "3 | final_proj | Linear           | 7.8 M \n",
      "------------------------------------------------\n",
      "26.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.7 M    Total params\n",
      "106.757   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komputer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]   VALIDATION: Batch 0, loss 10.493760108947754\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.67it/s]   VALIDATION: Batch 1, loss 10.453999519348145\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komputer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 0/? [00:00<?, ?it/s]    TRRAINING: Batch 0, loss 10.485569953918457\n",
      "Epoch 0: |          | 1/? [00:01<00:00,  0.85it/s, v_num=31]   TRRAINING: Batch 1, loss 10.003713607788086\n",
      "Epoch 0: |          | 2/? [00:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 2, loss 8.94566822052002\n",
      "Epoch 0: |          | 3/? [00:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 3, loss 9.716654777526855\n",
      "Epoch 0: |          | 4/? [00:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 4, loss 8.552960395812988\n",
      "Epoch 0: |          | 5/? [00:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 5, loss 8.86923599243164\n",
      "Epoch 0: |          | 6/? [00:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 6, loss 8.804082870483398\n",
      "Epoch 0: |          | 7/? [00:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 7, loss 8.479351043701172\n",
      "Epoch 0: |          | 8/? [00:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 8, loss 8.204797744750977\n",
      "Epoch 0: |          | 9/? [00:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 9, loss 8.135655403137207\n",
      "Epoch 0: |          | 10/? [00:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 10, loss 8.164201736450195\n",
      "Epoch 0: |          | 11/? [00:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 11, loss 7.732454776763916\n",
      "Epoch 0: |          | 12/? [00:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 12, loss 8.794309616088867\n",
      "Epoch 0: |          | 13/? [00:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 13, loss 7.97097110748291\n",
      "Epoch 0: |          | 14/? [00:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 14, loss 7.8877410888671875\n",
      "Epoch 0: |          | 15/? [00:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 15, loss 7.8887457847595215\n",
      "Epoch 0: |          | 16/? [00:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 16, loss 7.522191047668457\n",
      "Epoch 0: |          | 17/? [00:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 17, loss 7.8691277503967285\n",
      "Epoch 0: |          | 18/? [00:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 18, loss 7.606694221496582\n",
      "Epoch 0: |          | 19/? [00:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 19, loss 7.341682434082031\n",
      "Epoch 0: |          | 20/? [00:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 20, loss 7.5110182762146\n",
      "Epoch 0: |          | 21/? [00:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 21, loss 7.558839321136475\n",
      "Epoch 0: |          | 22/? [00:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 22, loss 7.388501167297363\n",
      "Epoch 0: |          | 23/? [00:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 23, loss 6.7882585525512695\n",
      "Epoch 0: |          | 24/? [00:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 24, loss 7.360835075378418\n",
      "Epoch 0: |          | 25/? [00:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 25, loss 7.283213138580322\n",
      "Epoch 0: |          | 26/? [00:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 26, loss 7.0818657875061035\n",
      "Epoch 0: |          | 27/? [00:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 27, loss 7.081620693206787\n",
      "Epoch 0: |          | 28/? [00:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 28, loss 7.712242126464844\n",
      "Epoch 0: |          | 29/? [00:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 29, loss 7.319648742675781\n",
      "Epoch 0: |          | 30/? [00:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 30, loss 7.145133972167969\n",
      "Epoch 0: |          | 31/? [00:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 31, loss 7.56842041015625\n",
      "Epoch 0: |          | 32/? [00:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 32, loss 7.186038017272949\n",
      "Epoch 0: |          | 33/? [00:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 33, loss 7.186300754547119\n",
      "Epoch 0: |          | 34/? [00:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 34, loss 7.133093357086182\n",
      "Epoch 0: |          | 35/? [00:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 35, loss 6.20722770690918\n",
      "Epoch 0: |          | 36/? [00:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 36, loss 7.4081315994262695\n",
      "Epoch 0: |          | 37/? [00:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 37, loss 7.27017068862915\n",
      "Epoch 0: |          | 38/? [00:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 38, loss 7.447070121765137\n",
      "Epoch 0: |          | 39/? [00:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 39, loss 7.205719947814941\n",
      "Epoch 0: |          | 40/? [00:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 40, loss 6.78497838973999\n",
      "Epoch 0: |          | 41/? [00:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 41, loss 7.013618469238281\n",
      "Epoch 0: |          | 42/? [00:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 42, loss 7.090340614318848\n",
      "Epoch 0: |          | 43/? [01:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 43, loss 6.742835998535156\n",
      "Epoch 0: |          | 44/? [01:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 44, loss 6.579944610595703\n",
      "Epoch 0: |          | 45/? [01:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 45, loss 6.859170436859131\n",
      "Epoch 0: |          | 46/? [01:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 46, loss 7.603341579437256\n",
      "Epoch 0: |          | 47/? [01:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 47, loss 7.1400556564331055\n",
      "Epoch 0: |          | 48/? [01:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 48, loss 7.22144079208374\n",
      "Epoch 0: |          | 49/? [01:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 49, loss 6.855799674987793\n",
      "Epoch 0: |          | 50/? [01:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 50, loss 6.809562683105469\n",
      "Epoch 0: |          | 51/? [01:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 51, loss 7.133070945739746\n",
      "Epoch 0: |          | 52/? [01:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 52, loss 7.0111799240112305\n",
      "Epoch 0: |          | 53/? [01:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 53, loss 6.560222625732422\n",
      "Epoch 0: |          | 54/? [01:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 54, loss 6.773526668548584\n",
      "Epoch 0: |          | 55/? [01:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 55, loss 6.8706512451171875\n",
      "Epoch 0: |          | 56/? [01:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 56, loss 6.805787086486816\n",
      "Epoch 0: |          | 57/? [01:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 57, loss 6.868968963623047\n",
      "Epoch 0: |          | 58/? [01:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 58, loss 8.110766410827637\n",
      "Epoch 0: |          | 59/? [01:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 59, loss 7.271951198577881\n",
      "Epoch 0: |          | 60/? [01:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 60, loss 7.166352272033691\n",
      "Epoch 0: |          | 61/? [01:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 61, loss 7.084964752197266\n",
      "Epoch 0: |          | 62/? [01:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 62, loss 6.5691022872924805\n",
      "Epoch 0: |          | 63/? [01:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 63, loss 6.872251987457275\n",
      "Epoch 0: |          | 64/? [01:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 64, loss 6.843735694885254\n",
      "Epoch 0: |          | 65/? [01:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 65, loss 6.9363579750061035\n",
      "Epoch 0: |          | 66/? [01:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 66, loss 6.785172939300537\n",
      "Epoch 0: |          | 67/? [01:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 67, loss 6.809112548828125\n",
      "Epoch 0: |          | 68/? [01:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 68, loss 7.109160423278809\n",
      "Epoch 0: |          | 69/? [01:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 69, loss 6.809278964996338\n",
      "Epoch 0: |          | 70/? [01:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 70, loss 6.599632263183594\n",
      "Epoch 0: |          | 71/? [01:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 71, loss 6.203195571899414\n",
      "Epoch 0: |          | 72/? [01:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 72, loss 6.871118068695068\n",
      "Epoch 0: |          | 73/? [01:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 73, loss 7.090008735656738\n",
      "Epoch 0: |          | 74/? [01:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 74, loss 6.7427077293396\n",
      "Epoch 0: |          | 75/? [01:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 75, loss 6.751761436462402\n",
      "Epoch 0: |          | 76/? [01:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 76, loss 6.834759712219238\n",
      "Epoch 0: |          | 77/? [01:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 77, loss 6.8270440101623535\n",
      "Epoch 0: |          | 78/? [01:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 78, loss 6.548250675201416\n",
      "Epoch 0: |          | 79/? [01:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 79, loss 6.449062347412109\n",
      "Epoch 0: |          | 80/? [01:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 80, loss 6.3978424072265625\n",
      "Epoch 0: |          | 81/? [01:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 81, loss 6.325739860534668\n",
      "Epoch 0: |          | 82/? [01:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 82, loss 7.0765275955200195\n",
      "Epoch 0: |          | 83/? [01:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 83, loss 6.229111671447754\n",
      "Epoch 0: |          | 84/? [01:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 84, loss 6.192313194274902\n",
      "Epoch 0: |          | 85/? [02:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 85, loss 6.317624568939209\n",
      "Epoch 0: |          | 86/? [02:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 86, loss 6.047971725463867\n",
      "Epoch 0: |          | 87/? [02:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 87, loss 6.302932262420654\n",
      "Epoch 0: |          | 88/? [02:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 88, loss 7.152165412902832\n",
      "Epoch 0: |          | 89/? [02:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 89, loss 6.816239833831787\n",
      "Epoch 0: |          | 90/? [02:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 90, loss 6.708935737609863\n",
      "Epoch 0: |          | 91/? [02:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 91, loss 7.0182952880859375\n",
      "Epoch 0: |          | 92/? [02:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 92, loss 7.269140720367432\n",
      "Epoch 0: |          | 93/? [02:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 93, loss 6.926629066467285\n",
      "Epoch 0: |          | 94/? [02:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 94, loss 6.981803894042969\n",
      "Epoch 0: |          | 95/? [02:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 95, loss 7.890460014343262\n",
      "Epoch 0: |          | 96/? [02:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 96, loss 6.406251430511475\n",
      "Epoch 0: |          | 97/? [02:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 97, loss 6.495942115783691\n",
      "Epoch 0: |          | 98/? [02:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 98, loss 6.718384742736816\n",
      "Epoch 0: |          | 99/? [02:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 99, loss 6.694965362548828\n",
      "Epoch 0: |          | 100/? [02:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 100, loss 6.853555202484131\n",
      "Epoch 0: |          | 101/? [02:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 101, loss 6.571799278259277\n",
      "Epoch 0: |          | 102/? [02:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 102, loss 6.6197052001953125\n",
      "Epoch 0: |          | 103/? [02:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 103, loss 6.256826877593994\n",
      "Epoch 0: |          | 104/? [02:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 104, loss 6.647035121917725\n",
      "Epoch 0: |          | 105/? [02:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 105, loss 6.635775089263916\n",
      "Epoch 0: |          | 106/? [02:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 106, loss 6.444272041320801\n",
      "Epoch 0: |          | 107/? [02:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 107, loss 6.772769927978516\n",
      "Epoch 0: |          | 108/? [02:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 108, loss 6.778123378753662\n",
      "Epoch 0: |          | 109/? [02:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 109, loss 5.881138324737549\n",
      "Epoch 0: |          | 110/? [02:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 110, loss 6.60760498046875\n",
      "Epoch 0: |          | 111/? [02:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 111, loss 6.937827110290527\n",
      "Epoch 0: |          | 112/? [02:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 112, loss 6.243289947509766\n",
      "Epoch 0: |          | 113/? [02:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 113, loss 5.584532737731934\n",
      "Epoch 0: |          | 114/? [02:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 114, loss 6.602517604827881\n",
      "Epoch 0: |          | 115/? [02:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 115, loss 6.647499084472656\n",
      "Epoch 0: |          | 116/? [02:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 116, loss 6.487795829772949\n",
      "Epoch 0: |          | 117/? [02:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 117, loss 6.591560363769531\n",
      "Epoch 0: |          | 118/? [02:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 118, loss 6.6886444091796875\n",
      "Epoch 0: |          | 119/? [02:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 119, loss 6.856967926025391\n",
      "Epoch 0: |          | 120/? [02:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 120, loss 6.506176948547363\n",
      "Epoch 0: |          | 121/? [02:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 121, loss 6.383914947509766\n",
      "Epoch 0: |          | 122/? [02:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 122, loss 6.140617370605469\n",
      "Epoch 0: |          | 123/? [02:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 123, loss 6.2693305015563965\n",
      "Epoch 0: |          | 124/? [02:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 124, loss 6.565837860107422\n",
      "Epoch 0: |          | 125/? [02:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 125, loss 6.4643874168396\n",
      "Epoch 0: |          | 126/? [02:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 126, loss 6.738142967224121\n",
      "Epoch 0: |          | 127/? [03:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 127, loss 6.572035789489746\n",
      "Epoch 0: |          | 128/? [03:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 128, loss 5.749720573425293\n",
      "Epoch 0: |          | 129/? [03:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 129, loss 6.491552829742432\n",
      "Epoch 0: |          | 130/? [03:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 130, loss 5.648016929626465\n",
      "Epoch 0: |          | 131/? [03:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 131, loss 6.294638633728027\n",
      "Epoch 0: |          | 132/? [03:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 132, loss 6.45914363861084\n",
      "Epoch 0: |          | 133/? [03:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 133, loss 6.344629764556885\n",
      "Epoch 0: |          | 134/? [03:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 134, loss 6.146066188812256\n",
      "Epoch 0: |          | 135/? [03:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 135, loss 6.564420223236084\n",
      "Epoch 0: |          | 136/? [03:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 136, loss 6.524907112121582\n",
      "Epoch 0: |          | 137/? [03:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 137, loss 5.393324375152588\n",
      "Epoch 0: |          | 138/? [03:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 138, loss 6.130473613739014\n",
      "Epoch 0: |          | 139/? [03:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 139, loss 6.7545061111450195\n",
      "Epoch 0: |          | 140/? [03:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 140, loss 6.248300552368164\n",
      "Epoch 0: |          | 141/? [03:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 141, loss 5.924659729003906\n",
      "Epoch 0: |          | 142/? [03:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 142, loss 7.549923896789551\n",
      "Epoch 0: |          | 143/? [03:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 143, loss 7.222690582275391\n",
      "Epoch 0: |          | 144/? [03:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 144, loss 6.119210243225098\n",
      "Epoch 0: |          | 145/? [03:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 145, loss 6.101010799407959\n",
      "Epoch 0: |          | 146/? [03:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 146, loss 6.374693393707275\n",
      "Epoch 0: |          | 147/? [03:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 147, loss 6.435662269592285\n",
      "Epoch 0: |          | 148/? [03:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 148, loss 6.267743110656738\n",
      "Epoch 0: |          | 149/? [03:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 149, loss 5.434260368347168\n",
      "Epoch 0: |          | 150/? [03:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 150, loss 6.427293300628662\n",
      "Epoch 0: |          | 151/? [03:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 151, loss 6.4263200759887695\n",
      "Epoch 0: |          | 152/? [03:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 152, loss 6.556879997253418\n",
      "Epoch 0: |          | 153/? [03:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 153, loss 5.669033050537109\n",
      "Epoch 0: |          | 154/? [03:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 154, loss 6.63352108001709\n",
      "Epoch 0: |          | 155/? [03:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 155, loss 6.574221134185791\n",
      "Epoch 0: |          | 156/? [03:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 156, loss 5.653564929962158\n",
      "Epoch 0: |          | 157/? [03:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 157, loss 6.406740665435791\n",
      "Epoch 0: |          | 158/? [03:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 158, loss 6.326972007751465\n",
      "Epoch 0: |          | 159/? [03:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 159, loss 6.206813812255859\n",
      "Epoch 0: |          | 160/? [03:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 160, loss 5.845000267028809\n",
      "Epoch 0: |          | 161/? [03:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 161, loss 6.4290876388549805\n",
      "Epoch 0: |          | 162/? [03:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 162, loss 6.641105651855469\n",
      "Epoch 0: |          | 163/? [03:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 163, loss 5.674227714538574\n",
      "Epoch 0: |          | 164/? [03:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 164, loss 6.061188220977783\n",
      "Epoch 0: |          | 165/? [03:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 165, loss 6.684043884277344\n",
      "Epoch 0: |          | 166/? [03:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 166, loss 6.4443230628967285\n",
      "Epoch 0: |          | 167/? [03:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 167, loss 6.530756950378418\n",
      "Epoch 0: |          | 168/? [03:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 168, loss 6.1568098068237305\n",
      "Epoch 0: |          | 169/? [03:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 169, loss 5.664951324462891\n",
      "Epoch 0: |          | 170/? [04:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 170, loss 6.067574977874756\n",
      "Epoch 0: |          | 171/? [04:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 171, loss 6.304793357849121\n",
      "Epoch 0: |          | 172/? [04:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 172, loss 6.025221347808838\n",
      "Epoch 0: |          | 173/? [04:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 173, loss 6.940053462982178\n",
      "Epoch 0: |          | 174/? [04:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 174, loss 6.962458610534668\n",
      "Epoch 0: |          | 175/? [04:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 175, loss 6.793420314788818\n",
      "Epoch 0: |          | 176/? [04:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 176, loss 5.898282527923584\n",
      "Epoch 0: |          | 177/? [04:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 177, loss 6.204064846038818\n",
      "Epoch 0: |          | 178/? [04:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 178, loss 5.986359119415283\n",
      "Epoch 0: |          | 179/? [04:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 179, loss 6.583934783935547\n",
      "Epoch 0: |          | 180/? [04:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 180, loss 6.086491584777832\n",
      "Epoch 0: |          | 181/? [04:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 181, loss 5.81574821472168\n",
      "Epoch 0: |          | 182/? [04:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 182, loss 6.317370414733887\n",
      "Epoch 0: |          | 183/? [04:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 183, loss 5.772810459136963\n",
      "Epoch 0: |          | 184/? [04:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 184, loss 5.855997085571289\n",
      "Epoch 0: |          | 185/? [04:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 185, loss 6.7432122230529785\n",
      "Epoch 0: |          | 186/? [04:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 186, loss 6.044698238372803\n",
      "Epoch 0: |          | 187/? [04:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 187, loss 6.559739589691162\n",
      "Epoch 0: |          | 188/? [04:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 188, loss 5.998499870300293\n",
      "Epoch 0: |          | 189/? [04:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 189, loss 6.5985846519470215\n",
      "Epoch 0: |          | 190/? [04:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 190, loss 6.028313159942627\n",
      "Epoch 0: |          | 191/? [04:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 191, loss 6.890263557434082\n",
      "Epoch 0: |          | 192/? [04:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 192, loss 6.801258087158203\n",
      "Epoch 0: |          | 193/? [04:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 193, loss 5.869494438171387\n",
      "Epoch 0: |          | 194/? [04:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 194, loss 5.806532859802246\n",
      "Epoch 0: |          | 195/? [04:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 195, loss 6.46646785736084\n",
      "Epoch 0: |          | 196/? [04:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 196, loss 6.462405204772949\n",
      "Epoch 0: |          | 197/? [04:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 197, loss 6.307797908782959\n",
      "Epoch 0: |          | 198/? [04:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 198, loss 5.836402893066406\n",
      "Epoch 0: |          | 199/? [04:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 199, loss 6.256760597229004\n",
      "Epoch 0: |          | 200/? [04:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 200, loss 6.47366189956665\n",
      "Epoch 0: |          | 201/? [04:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 201, loss 6.52260684967041\n",
      "Epoch 0: |          | 202/? [04:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 202, loss 6.279956817626953\n",
      "Epoch 0: |          | 203/? [04:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 203, loss 6.464062690734863\n",
      "Epoch 0: |          | 204/? [04:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 204, loss 6.426187038421631\n",
      "Epoch 0: |          | 205/? [04:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 205, loss 5.754611015319824\n",
      "Epoch 0: |          | 206/? [04:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 206, loss 5.748831748962402\n",
      "Epoch 0: |          | 207/? [04:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 207, loss 6.2520036697387695\n",
      "Epoch 0: |          | 208/? [04:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 208, loss 6.071718692779541\n",
      "Epoch 0: |          | 209/? [04:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 209, loss 6.077757358551025\n",
      "Epoch 0: |          | 210/? [04:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 210, loss 6.620291233062744\n",
      "Epoch 0: |          | 211/? [04:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 211, loss 5.936163902282715\n",
      "Epoch 0: |          | 212/? [04:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 212, loss 6.1811137199401855\n",
      "Epoch 0: |          | 213/? [05:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 213, loss 5.906530857086182\n",
      "Epoch 0: |          | 214/? [05:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 214, loss 6.104460716247559\n",
      "Epoch 0: |          | 215/? [05:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 215, loss 5.644391059875488\n",
      "Epoch 0: |          | 216/? [05:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 216, loss 6.34689474105835\n",
      "Epoch 0: |          | 217/? [05:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 217, loss 6.0251145362854\n",
      "Epoch 0: |          | 218/? [05:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 218, loss 6.373746871948242\n",
      "Epoch 0: |          | 219/? [05:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 219, loss 6.208953857421875\n",
      "Epoch 0: |          | 220/? [05:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 220, loss 6.206137180328369\n",
      "Epoch 0: |          | 221/? [05:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 221, loss 5.88971471786499\n",
      "Epoch 0: |          | 222/? [05:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 222, loss 5.235158920288086\n",
      "Epoch 0: |          | 223/? [05:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 223, loss 6.479389190673828\n",
      "Epoch 0: |          | 224/? [05:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 224, loss 6.419053077697754\n",
      "Epoch 0: |          | 225/? [05:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 225, loss 6.191205024719238\n",
      "Epoch 0: |          | 226/? [05:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 226, loss 6.026393413543701\n",
      "Epoch 0: |          | 227/? [05:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 227, loss 6.338784217834473\n",
      "Epoch 0: |          | 228/? [05:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 228, loss 6.100768566131592\n",
      "Epoch 0: |          | 229/? [05:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 229, loss 6.334591865539551\n",
      "Epoch 0: |          | 230/? [05:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 230, loss 5.95082426071167\n",
      "Epoch 0: |          | 231/? [05:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 231, loss 5.9604949951171875\n",
      "Epoch 0: |          | 232/? [05:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 232, loss 5.931264877319336\n",
      "Epoch 0: |          | 233/? [05:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 233, loss 6.631626129150391\n",
      "Epoch 0: |          | 234/? [05:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 234, loss 6.518662929534912\n",
      "Epoch 0: |          | 235/? [05:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 235, loss 6.454581260681152\n",
      "Epoch 0: |          | 236/? [05:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 236, loss 6.144482612609863\n",
      "Epoch 0: |          | 237/? [05:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 237, loss 6.138492107391357\n",
      "Epoch 0: |          | 238/? [05:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 238, loss 6.265195846557617\n",
      "Epoch 0: |          | 239/? [05:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 239, loss 6.068458080291748\n",
      "Epoch 0: |          | 240/? [05:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 240, loss 5.546878337860107\n",
      "Epoch 0: |          | 241/? [05:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 241, loss 6.199695110321045\n",
      "Epoch 0: |          | 242/? [05:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 242, loss 6.445568084716797\n",
      "Epoch 0: |          | 243/? [05:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 243, loss 5.185797214508057\n",
      "Epoch 0: |          | 244/? [05:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 244, loss 5.897160530090332\n",
      "Epoch 0: |          | 245/? [05:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 245, loss 6.032569885253906\n",
      "Epoch 0: |          | 246/? [05:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 246, loss 6.2399821281433105\n",
      "Epoch 0: |          | 247/? [05:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 247, loss 6.389317512512207\n",
      "Epoch 0: |          | 248/? [05:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 248, loss 5.792670249938965\n",
      "Epoch 0: |          | 249/? [05:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 249, loss 5.39075231552124\n",
      "Epoch 0: |          | 250/? [05:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 250, loss 6.028571128845215\n",
      "Epoch 0: |          | 251/? [05:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 251, loss 6.228760719299316\n",
      "Epoch 0: |          | 252/? [05:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 252, loss 5.97858190536499\n",
      "Epoch 0: |          | 253/? [05:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 253, loss 6.468350410461426\n",
      "Epoch 0: |          | 254/? [05:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 254, loss 6.459353446960449\n",
      "Epoch 0: |          | 255/? [06:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 255, loss 6.022374629974365\n",
      "Epoch 0: |          | 256/? [06:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 256, loss 8.551477432250977\n",
      "Epoch 0: |          | 257/? [06:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 257, loss 5.803088665008545\n",
      "Epoch 0: |          | 258/? [06:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 258, loss 6.008431911468506\n",
      "Epoch 0: |          | 259/? [06:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 259, loss 5.963635444641113\n",
      "Epoch 0: |          | 260/? [06:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 260, loss 5.817358016967773\n",
      "Epoch 0: |          | 261/? [06:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 261, loss 6.05570125579834\n",
      "Epoch 0: |          | 262/? [06:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 262, loss 6.290139198303223\n",
      "Epoch 0: |          | 263/? [06:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 263, loss 5.979288578033447\n",
      "Epoch 0: |          | 264/? [06:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 264, loss 6.183021545410156\n",
      "Epoch 0: |          | 265/? [06:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 265, loss 5.340028762817383\n",
      "Epoch 0: |          | 266/? [06:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 266, loss 5.9732255935668945\n",
      "Epoch 0: |          | 267/? [06:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 267, loss 5.467873573303223\n",
      "Epoch 0: |          | 268/? [06:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 268, loss 5.824502468109131\n",
      "Epoch 0: |          | 269/? [06:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 269, loss 6.192301273345947\n",
      "Epoch 0: |          | 270/? [06:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 270, loss 5.884887218475342\n",
      "Epoch 0: |          | 271/? [06:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 271, loss 6.620596408843994\n",
      "Epoch 0: |          | 272/? [06:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 272, loss 6.541008949279785\n",
      "Epoch 0: |          | 273/? [06:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 273, loss 5.488286018371582\n",
      "Epoch 0: |          | 274/? [06:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 274, loss 6.734797477722168\n",
      "Epoch 0: |          | 275/? [06:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 275, loss 6.109565734863281\n",
      "Epoch 0: |          | 276/? [06:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 276, loss 5.235511779785156\n",
      "Epoch 0: |          | 277/? [06:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 277, loss 5.962279319763184\n",
      "Epoch 0: |          | 278/? [06:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 278, loss 4.929325580596924\n",
      "Epoch 0: |          | 279/? [06:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 279, loss 5.737544059753418\n",
      "Epoch 0: |          | 280/? [06:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 280, loss 5.353571891784668\n",
      "Epoch 0: |          | 281/? [06:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 281, loss 6.183348655700684\n",
      "Epoch 0: |          | 282/? [06:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 282, loss 5.778771877288818\n",
      "Epoch 0: |          | 283/? [06:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 283, loss 5.9994587898254395\n",
      "Epoch 0: |          | 284/? [06:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 284, loss 5.831951141357422\n",
      "Epoch 0: |          | 285/? [06:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 285, loss 5.087281227111816\n",
      "Epoch 0: |          | 286/? [06:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 286, loss 5.844377517700195\n",
      "Epoch 0: |          | 287/? [06:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 287, loss 5.551579475402832\n",
      "Epoch 0: |          | 288/? [06:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 288, loss 5.463521957397461\n",
      "Epoch 0: |          | 289/? [06:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 289, loss 5.806168556213379\n",
      "Epoch 0: |          | 290/? [06:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 290, loss 5.636961460113525\n",
      "Epoch 0: |          | 291/? [06:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 291, loss 5.9550275802612305\n",
      "Epoch 0: |          | 292/? [06:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 292, loss 5.5215582847595215\n",
      "Epoch 0: |          | 293/? [06:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 293, loss 6.121211051940918\n",
      "Epoch 0: |          | 294/? [06:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 294, loss 5.7132248878479\n",
      "Epoch 0: |          | 295/? [06:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 295, loss 6.105135440826416\n",
      "Epoch 0: |          | 296/? [06:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 296, loss 5.667910575866699\n",
      "Epoch 0: |          | 297/? [07:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 297, loss 6.289526462554932\n",
      "Epoch 0: |          | 298/? [07:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 298, loss 6.12714958190918\n",
      "Epoch 0: |          | 299/? [07:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 299, loss 6.9335737228393555\n",
      "Epoch 0: |          | 300/? [07:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 300, loss 6.150956153869629\n",
      "Epoch 0: |          | 301/? [07:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 301, loss 5.718369483947754\n",
      "Epoch 0: |          | 302/? [07:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 302, loss 6.129103183746338\n",
      "Epoch 0: |          | 303/? [07:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 303, loss 5.907454490661621\n",
      "Epoch 0: |          | 304/? [07:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 304, loss 6.167243003845215\n",
      "Epoch 0: |          | 305/? [07:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 305, loss 6.289261817932129\n",
      "Epoch 0: |          | 306/? [07:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 306, loss 5.88394832611084\n",
      "Epoch 0: |          | 307/? [07:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 307, loss 6.072141647338867\n",
      "Epoch 0: |          | 308/? [07:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 308, loss 6.275608062744141\n",
      "Epoch 0: |          | 309/? [07:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 309, loss 6.1732048988342285\n",
      "Epoch 0: |          | 310/? [07:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 310, loss 6.4612321853637695\n",
      "Epoch 0: |          | 311/? [07:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 311, loss 5.895068645477295\n",
      "Epoch 0: |          | 312/? [07:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 312, loss 6.249645233154297\n",
      "Epoch 0: |          | 313/? [07:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 313, loss 5.5405073165893555\n",
      "Epoch 0: |          | 314/? [07:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 314, loss 6.039835453033447\n",
      "Epoch 0: |          | 315/? [07:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 315, loss 5.5492634773254395\n",
      "Epoch 0: |          | 316/? [07:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 316, loss 6.348525047302246\n",
      "Epoch 0: |          | 317/? [07:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 317, loss 6.049502372741699\n",
      "Epoch 0: |          | 318/? [07:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 318, loss 6.293952465057373\n",
      "Epoch 0: |          | 319/? [07:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 319, loss 5.3372802734375\n",
      "Epoch 0: |          | 320/? [07:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 320, loss 5.81896448135376\n",
      "Epoch 0: |          | 321/? [07:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 321, loss 5.59044885635376\n",
      "Epoch 0: |          | 322/? [07:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 322, loss 6.196799278259277\n",
      "Epoch 0: |          | 323/? [07:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 323, loss 6.470207214355469\n",
      "Epoch 0: |          | 324/? [07:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 324, loss 5.857061386108398\n",
      "Epoch 0: |          | 325/? [07:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 325, loss 6.626745700836182\n",
      "Epoch 0: |          | 326/? [07:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 326, loss 5.904204368591309\n",
      "Epoch 0: |          | 327/? [07:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 327, loss 5.8113837242126465\n",
      "Epoch 0: |          | 328/? [07:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 328, loss 5.354704856872559\n",
      "Epoch 0: |          | 329/? [07:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 329, loss 6.045681953430176\n",
      "Epoch 0: |          | 330/? [07:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 330, loss 6.419681549072266\n",
      "Epoch 0: |          | 331/? [07:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 331, loss 4.568605899810791\n",
      "Epoch 0: |          | 332/? [07:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 332, loss 5.81035041809082\n",
      "Epoch 0: |          | 333/? [07:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 333, loss 5.576191425323486\n",
      "Epoch 0: |          | 334/? [07:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 334, loss 6.792575836181641\n",
      "Epoch 0: |          | 335/? [07:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 335, loss 6.880809783935547\n",
      "Epoch 0: |          | 336/? [07:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 336, loss 6.350593090057373\n",
      "Epoch 0: |          | 337/? [07:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 337, loss 7.045385837554932\n",
      "Epoch 0: |          | 338/? [07:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 338, loss 6.621279716491699\n",
      "Epoch 0: |          | 339/? [08:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 339, loss 5.623198986053467\n",
      "Epoch 0: |          | 340/? [08:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 340, loss 6.172506332397461\n",
      "Epoch 0: |          | 341/? [08:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 341, loss 5.524716377258301\n",
      "Epoch 0: |          | 342/? [08:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 342, loss 6.022454261779785\n",
      "Epoch 0: |          | 343/? [08:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 343, loss 5.861205101013184\n",
      "Epoch 0: |          | 344/? [08:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 344, loss 6.4223456382751465\n",
      "Epoch 0: |          | 345/? [08:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 345, loss 6.005291938781738\n",
      "Epoch 0: |          | 346/? [08:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 346, loss 6.04204797744751\n",
      "Epoch 0: |          | 347/? [08:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 347, loss 5.6223835945129395\n",
      "Epoch 0: |          | 348/? [08:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 348, loss 5.029141902923584\n",
      "Epoch 0: |          | 349/? [08:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 349, loss 4.741130352020264\n",
      "Epoch 0: |          | 350/? [08:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 350, loss 6.300573825836182\n",
      "Epoch 0: |          | 351/? [08:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 351, loss 6.248534202575684\n",
      "Epoch 0: |          | 352/? [08:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 352, loss 5.51344108581543\n",
      "Epoch 0: |          | 353/? [08:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 353, loss 5.263119697570801\n",
      "Epoch 0: |          | 354/? [08:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 354, loss 5.554955959320068\n",
      "Epoch 0: |          | 355/? [08:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 355, loss 5.96126651763916\n",
      "Epoch 0: |          | 356/? [08:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 356, loss 5.937874794006348\n",
      "Epoch 0: |          | 357/? [08:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 357, loss 5.617193222045898\n",
      "Epoch 0: |          | 358/? [08:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 358, loss 5.495490074157715\n",
      "Epoch 0: |          | 359/? [08:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 359, loss 6.220252990722656\n",
      "Epoch 0: |          | 360/? [08:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 360, loss 5.646450996398926\n",
      "Epoch 0: |          | 361/? [08:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 361, loss 5.672806739807129\n",
      "Epoch 0: |          | 362/? [08:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 362, loss 5.57874059677124\n",
      "Epoch 0: |          | 363/? [08:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 363, loss 5.579824447631836\n",
      "Epoch 0: |          | 364/? [08:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 364, loss 6.1246562004089355\n",
      "Epoch 0: |          | 365/? [08:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 365, loss 6.180010795593262\n",
      "Epoch 0: |          | 366/? [08:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 366, loss 5.764639854431152\n",
      "Epoch 0: |          | 367/? [08:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 367, loss 5.898736476898193\n",
      "Epoch 0: |          | 368/? [08:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 368, loss 5.422008991241455\n",
      "Epoch 0: |          | 369/? [08:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 369, loss 5.746823310852051\n",
      "Epoch 0: |          | 370/? [08:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 370, loss 5.27843713760376\n",
      "Epoch 0: |          | 371/? [08:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 371, loss 6.394299507141113\n",
      "Epoch 0: |          | 372/? [08:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 372, loss 6.0283050537109375\n",
      "Epoch 0: |          | 373/? [08:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 373, loss 5.84648323059082\n",
      "Epoch 0: |          | 374/? [08:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 374, loss 5.723883152008057\n",
      "Epoch 0: |          | 375/? [08:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 375, loss 6.2763991355896\n",
      "Epoch 0: |          | 376/? [08:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 376, loss 5.686467170715332\n",
      "Epoch 0: |          | 377/? [08:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 377, loss 5.915862083435059\n",
      "Epoch 0: |          | 378/? [08:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 378, loss 6.116684436798096\n",
      "Epoch 0: |          | 379/? [08:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 379, loss 5.986199378967285\n",
      "Epoch 0: |          | 380/? [08:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 380, loss 6.095017433166504\n",
      "Epoch 0: |          | 381/? [09:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 381, loss 5.9312825202941895\n",
      "Epoch 0: |          | 382/? [09:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 382, loss 5.6748480796813965\n",
      "Epoch 0: |          | 383/? [09:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 383, loss 5.667107105255127\n",
      "Epoch 0: |          | 384/? [09:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 384, loss 6.229470729827881\n",
      "Epoch 0: |          | 385/? [09:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 385, loss 5.921630859375\n",
      "Epoch 0: |          | 386/? [09:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 386, loss 4.441221237182617\n",
      "Epoch 0: |          | 387/? [09:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 387, loss 5.437763214111328\n",
      "Epoch 0: |          | 388/? [09:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 388, loss 6.318238258361816\n",
      "Epoch 0: |          | 389/? [09:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 389, loss 6.270391464233398\n",
      "Epoch 0: |          | 390/? [09:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 390, loss 5.693404197692871\n",
      "Epoch 0: |          | 391/? [09:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 391, loss 6.069972038269043\n",
      "Epoch 0: |          | 392/? [09:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 392, loss 6.0598602294921875\n",
      "Epoch 0: |          | 393/? [09:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 393, loss 6.103466510772705\n",
      "Epoch 0: |          | 394/? [09:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 394, loss 5.840271472930908\n",
      "Epoch 0: |          | 395/? [09:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 395, loss 6.14798641204834\n",
      "Epoch 0: |          | 396/? [09:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 396, loss 5.998061656951904\n",
      "Epoch 0: |          | 397/? [09:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 397, loss 5.971059322357178\n",
      "Epoch 0: |          | 398/? [09:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 398, loss 5.689942836761475\n",
      "Epoch 0: |          | 399/? [09:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 399, loss 5.914534568786621\n",
      "Epoch 0: |          | 400/? [09:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 400, loss 5.858323097229004\n",
      "Epoch 0: |          | 401/? [09:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 401, loss 5.663872718811035\n",
      "Epoch 0: |          | 402/? [09:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 402, loss 6.372689247131348\n",
      "Epoch 0: |          | 403/? [09:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 403, loss 5.879668712615967\n",
      "Epoch 0: |          | 404/? [09:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 404, loss 5.383003234863281\n",
      "Epoch 0: |          | 405/? [09:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 405, loss 5.3426642417907715\n",
      "Epoch 0: |          | 406/? [09:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 406, loss 5.776520729064941\n",
      "Epoch 0: |          | 407/? [09:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 407, loss 5.642077445983887\n",
      "Epoch 0: |          | 408/? [09:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 408, loss 5.966984748840332\n",
      "Epoch 0: |          | 409/? [09:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 409, loss 6.023594856262207\n",
      "Epoch 0: |          | 410/? [09:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 410, loss 5.867671489715576\n",
      "Epoch 0: |          | 411/? [09:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 411, loss 5.457238674163818\n",
      "Epoch 0: |          | 412/? [09:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 412, loss 5.29136323928833\n",
      "Epoch 0: |          | 413/? [09:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 413, loss 5.876992225646973\n",
      "Epoch 0: |          | 414/? [09:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 414, loss 5.356706619262695\n",
      "Epoch 0: |          | 415/? [09:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 415, loss 5.81978178024292\n",
      "Epoch 0: |          | 416/? [09:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 416, loss 6.161572456359863\n",
      "Epoch 0: |          | 417/? [09:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 417, loss 6.531846523284912\n",
      "Epoch 0: |          | 418/? [09:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 418, loss 6.066279888153076\n",
      "Epoch 0: |          | 419/? [09:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 419, loss 6.238450050354004\n",
      "Epoch 0: |          | 420/? [09:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 420, loss 5.79455041885376\n",
      "Epoch 0: |          | 421/? [09:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 421, loss 6.290358543395996\n",
      "Epoch 0: |          | 422/? [09:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 422, loss 5.873292922973633\n",
      "Epoch 0: |          | 423/? [09:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 423, loss 5.318323135375977\n",
      "Epoch 0: |          | 424/? [10:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 424, loss 6.437165260314941\n",
      "Epoch 0: |          | 425/? [10:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 425, loss 6.402734279632568\n",
      "Epoch 0: |          | 426/? [10:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 426, loss 5.559386253356934\n",
      "Epoch 0: |          | 427/? [10:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 427, loss 5.692396640777588\n",
      "Epoch 0: |          | 428/? [10:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 428, loss 6.48736572265625\n",
      "Epoch 0: |          | 429/? [10:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 429, loss 5.2648515701293945\n",
      "Epoch 0: |          | 430/? [10:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 430, loss 6.063860893249512\n",
      "Epoch 0: |          | 431/? [10:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 431, loss 5.773289203643799\n",
      "Epoch 0: |          | 432/? [10:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 432, loss 5.874881744384766\n",
      "Epoch 0: |          | 433/? [10:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 433, loss 5.967720985412598\n",
      "Epoch 0: |          | 434/? [10:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 434, loss 5.844394683837891\n",
      "Epoch 0: |          | 435/? [10:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 435, loss 5.625323295593262\n",
      "Epoch 0: |          | 436/? [10:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 436, loss 6.01214599609375\n",
      "Epoch 0: |          | 437/? [10:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 437, loss 5.972963809967041\n",
      "Epoch 0: |          | 438/? [10:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 438, loss 5.5911478996276855\n",
      "Epoch 0: |          | 439/? [10:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 439, loss 6.093299865722656\n",
      "Epoch 0: |          | 440/? [10:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 440, loss 6.164340972900391\n",
      "Epoch 0: |          | 441/? [10:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 441, loss 6.159521579742432\n",
      "Epoch 0: |          | 442/? [10:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 442, loss 5.688503742218018\n",
      "Epoch 0: |          | 443/? [10:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 443, loss 6.117788791656494\n",
      "Epoch 0: |          | 444/? [10:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 444, loss 5.72589111328125\n",
      "Epoch 0: |          | 445/? [10:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 445, loss 6.426368713378906\n",
      "Epoch 0: |          | 446/? [10:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 446, loss 5.837409019470215\n",
      "Epoch 0: |          | 447/? [10:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 447, loss 6.237356185913086\n",
      "Epoch 0: |          | 448/? [10:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 448, loss 5.779514789581299\n",
      "Epoch 0: |          | 449/? [10:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 449, loss 5.644216537475586\n",
      "Epoch 0: |          | 450/? [10:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 450, loss 6.066007137298584\n",
      "Epoch 0: |          | 451/? [10:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 451, loss 5.63895320892334\n",
      "Epoch 0: |          | 452/? [10:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 452, loss 5.597195625305176\n",
      "Epoch 0: |          | 453/? [10:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 453, loss 6.127337455749512\n",
      "Epoch 0: |          | 454/? [10:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 454, loss 5.507754325866699\n",
      "Epoch 0: |          | 455/? [10:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 455, loss 5.951390743255615\n",
      "Epoch 0: |          | 456/? [10:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 456, loss 5.329758644104004\n",
      "Epoch 0: |          | 457/? [10:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 457, loss 5.7407636642456055\n",
      "Epoch 0: |          | 458/? [10:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 458, loss 6.110302925109863\n",
      "Epoch 0: |          | 459/? [10:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 459, loss 6.1070075035095215\n",
      "Epoch 0: |          | 460/? [10:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 460, loss 5.876186370849609\n",
      "Epoch 0: |          | 461/? [10:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 461, loss 5.828367710113525\n",
      "Epoch 0: |          | 462/? [10:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 462, loss 5.783235549926758\n",
      "Epoch 0: |          | 463/? [10:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 463, loss 5.647515296936035\n",
      "Epoch 0: |          | 464/? [10:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 464, loss 5.249001502990723\n",
      "Epoch 0: |          | 465/? [10:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 465, loss 5.648494243621826\n",
      "Epoch 0: |          | 466/? [11:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 466, loss 5.951196193695068\n",
      "Epoch 0: |          | 467/? [11:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 467, loss 5.907989978790283\n",
      "Epoch 0: |          | 468/? [11:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 468, loss 5.840455055236816\n",
      "Epoch 0: |          | 469/? [11:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 469, loss 5.856301307678223\n",
      "Epoch 0: |          | 470/? [11:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 470, loss 5.1357035636901855\n",
      "Epoch 0: |          | 471/? [11:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 471, loss 5.673105239868164\n",
      "Epoch 0: |          | 472/? [11:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 472, loss 5.468621730804443\n",
      "Epoch 0: |          | 473/? [11:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 473, loss 5.3623175621032715\n",
      "Epoch 0: |          | 474/? [11:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 474, loss 5.287326812744141\n",
      "Epoch 0: |          | 475/? [11:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 475, loss 6.420529842376709\n",
      "Epoch 0: |          | 476/? [11:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 476, loss 5.367245674133301\n",
      "Epoch 0: |          | 477/? [11:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 477, loss 4.976093292236328\n",
      "Epoch 0: |          | 478/? [11:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 478, loss 4.901518821716309\n",
      "Epoch 0: |          | 479/? [11:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 479, loss 5.868401050567627\n",
      "Epoch 0: |          | 480/? [11:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 480, loss 6.049868583679199\n",
      "Epoch 0: |          | 481/? [11:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 481, loss 6.025871753692627\n",
      "Epoch 0: |          | 482/? [11:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 482, loss 5.886209487915039\n",
      "Epoch 0: |          | 483/? [11:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 483, loss 5.506021976470947\n",
      "Epoch 0: |          | 484/? [11:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 484, loss 6.104290962219238\n",
      "Epoch 0: |          | 485/? [11:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 485, loss 6.035517692565918\n",
      "Epoch 0: |          | 486/? [11:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 486, loss 5.925925254821777\n",
      "Epoch 0: |          | 487/? [11:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 487, loss 5.849184989929199\n",
      "Epoch 0: |          | 488/? [11:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 488, loss 5.436967849731445\n",
      "Epoch 0: |          | 489/? [11:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 489, loss 4.884340286254883\n",
      "Epoch 0: |          | 490/? [11:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 490, loss 5.7995195388793945\n",
      "Epoch 0: |          | 491/? [11:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 491, loss 5.563765525817871\n",
      "Epoch 0: |          | 492/? [11:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 492, loss 5.122076034545898\n",
      "Epoch 0: |          | 493/? [11:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 493, loss 6.015083312988281\n",
      "Epoch 0: |          | 494/? [11:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 494, loss 5.938939571380615\n",
      "Epoch 0: |          | 495/? [11:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 495, loss 6.081772327423096\n",
      "Epoch 0: |          | 496/? [11:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 496, loss 5.495444297790527\n",
      "Epoch 0: |          | 497/? [11:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 497, loss 5.938040733337402\n",
      "Epoch 0: |          | 498/? [11:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 498, loss 5.775893211364746\n",
      "Epoch 0: |          | 499/? [11:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 499, loss 5.68305778503418\n",
      "Epoch 0: |          | 500/? [11:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 500, loss 5.667356491088867\n",
      "Epoch 0: |          | 501/? [11:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 501, loss 5.177042007446289\n",
      "Epoch 0: |          | 502/? [11:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 502, loss 5.747280597686768\n",
      "Epoch 0: |          | 503/? [11:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 503, loss 6.049289703369141\n",
      "Epoch 0: |          | 504/? [11:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 504, loss 5.636340618133545\n",
      "Epoch 0: |          | 505/? [11:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 505, loss 4.724465370178223\n",
      "Epoch 0: |          | 506/? [11:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 506, loss 5.4600725173950195\n",
      "Epoch 0: |          | 507/? [12:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 507, loss 5.602099895477295\n",
      "Epoch 0: |          | 508/? [12:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 508, loss 5.85988712310791\n",
      "Epoch 0: |          | 509/? [12:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 509, loss 5.3034348487854\n",
      "Epoch 0: |          | 510/? [12:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 510, loss 6.371617794036865\n",
      "Epoch 0: |          | 511/? [12:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 511, loss 5.813384056091309\n",
      "Epoch 0: |          | 512/? [12:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 512, loss 5.099961280822754\n",
      "Epoch 0: |          | 513/? [12:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 513, loss 5.462329387664795\n",
      "Epoch 0: |          | 514/? [12:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 514, loss 5.467150688171387\n",
      "Epoch 0: |          | 515/? [12:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 515, loss 5.256862640380859\n",
      "Epoch 0: |          | 516/? [12:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 516, loss 5.599732398986816\n",
      "Epoch 0: |          | 517/? [12:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 517, loss 5.933489799499512\n",
      "Epoch 0: |          | 518/? [12:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 518, loss 5.5521240234375\n",
      "Epoch 0: |          | 519/? [12:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 519, loss 6.0138349533081055\n",
      "Epoch 0: |          | 520/? [12:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 520, loss 5.62709379196167\n",
      "Epoch 0: |          | 521/? [12:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 521, loss 5.546532154083252\n",
      "Epoch 0: |          | 522/? [12:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 522, loss 6.13775110244751\n",
      "Epoch 0: |          | 523/? [12:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 523, loss 6.113276481628418\n",
      "Epoch 0: |          | 524/? [12:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 524, loss 6.0638298988342285\n",
      "Epoch 0: |          | 525/? [12:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 525, loss 5.580761909484863\n",
      "Epoch 0: |          | 526/? [12:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 526, loss 5.472380638122559\n",
      "Epoch 0: |          | 527/? [12:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 527, loss 5.724112510681152\n",
      "Epoch 0: |          | 528/? [12:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 528, loss 5.908045291900635\n",
      "Epoch 0: |          | 529/? [12:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 529, loss 5.455824375152588\n",
      "Epoch 0: |          | 530/? [12:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 530, loss 6.015916347503662\n",
      "Epoch 0: |          | 531/? [12:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 531, loss 5.452821731567383\n",
      "Epoch 0: |          | 532/? [12:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 532, loss 5.765895366668701\n",
      "Epoch 0: |          | 533/? [12:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 533, loss 5.603269577026367\n",
      "Epoch 0: |          | 534/? [12:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 534, loss 5.3787150382995605\n",
      "Epoch 0: |          | 535/? [12:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 535, loss 6.326042652130127\n",
      "Epoch 0: |          | 536/? [12:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 536, loss 6.052725315093994\n",
      "Epoch 0: |          | 537/? [12:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 537, loss 5.7623395919799805\n",
      "Epoch 0: |          | 538/? [12:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 538, loss 5.558547496795654\n",
      "Epoch 0: |          | 539/? [12:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 539, loss 5.433913230895996\n",
      "Epoch 0: |          | 540/? [12:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 540, loss 6.064892768859863\n",
      "Epoch 0: |          | 541/? [12:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 541, loss 5.709598541259766\n",
      "Epoch 0: |          | 542/? [12:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 542, loss 5.540789604187012\n",
      "Epoch 0: |          | 543/? [12:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 543, loss 5.911336421966553\n",
      "Epoch 0: |          | 544/? [12:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 544, loss 5.642913818359375\n",
      "Epoch 0: |          | 545/? [12:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 545, loss 4.811919689178467\n",
      "Epoch 0: |          | 546/? [12:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 546, loss 5.688231468200684\n",
      "Epoch 0: |          | 547/? [12:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 547, loss 6.019347190856934\n",
      "Epoch 0: |          | 548/? [12:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 548, loss 5.884890556335449\n",
      "Epoch 0: |          | 549/? [13:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 549, loss 5.6401848793029785\n",
      "Epoch 0: |          | 550/? [13:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 550, loss 6.11899471282959\n",
      "Epoch 0: |          | 551/? [13:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 551, loss 5.676390647888184\n",
      "Epoch 0: |          | 552/? [13:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 552, loss 5.891053199768066\n",
      "Epoch 0: |          | 553/? [13:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 553, loss 5.011153221130371\n",
      "Epoch 0: |          | 554/? [13:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 554, loss 5.895049095153809\n",
      "Epoch 0: |          | 555/? [13:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 555, loss 6.194083213806152\n",
      "Epoch 0: |          | 556/? [13:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 556, loss 5.530374050140381\n",
      "Epoch 0: |          | 557/? [13:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 557, loss 5.34153938293457\n",
      "Epoch 0: |          | 558/? [13:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 558, loss 5.3596510887146\n",
      "Epoch 0: |          | 559/? [13:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 559, loss 5.532200813293457\n",
      "Epoch 0: |          | 560/? [13:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 560, loss 4.833571434020996\n",
      "Epoch 0: |          | 561/? [13:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 561, loss 5.086949348449707\n",
      "Epoch 0: |          | 562/? [13:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 562, loss 5.745545864105225\n",
      "Epoch 0: |          | 563/? [13:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 563, loss 4.878124713897705\n",
      "Epoch 0: |          | 564/? [13:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 564, loss 5.690887928009033\n",
      "Epoch 0: |          | 565/? [13:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 565, loss 6.191710472106934\n",
      "Epoch 0: |          | 566/? [13:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 566, loss 5.917484760284424\n",
      "Epoch 0: |          | 567/? [13:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 567, loss 5.9396467208862305\n",
      "Epoch 0: |          | 568/? [13:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 568, loss 5.190303325653076\n",
      "Epoch 0: |          | 569/? [13:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 569, loss 5.988916873931885\n",
      "Epoch 0: |          | 570/? [13:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 570, loss 5.741562366485596\n",
      "Epoch 0: |          | 571/? [13:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 571, loss 5.348341941833496\n",
      "Epoch 0: |          | 572/? [13:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 572, loss 6.199175834655762\n",
      "Epoch 0: |          | 573/? [13:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 573, loss 4.585112571716309\n",
      "Epoch 0: |          | 574/? [13:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 574, loss 5.8786301612854\n",
      "Epoch 0: |          | 575/? [13:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 575, loss 5.219422817230225\n",
      "Epoch 0: |          | 576/? [13:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 576, loss 5.55220890045166\n",
      "Epoch 0: |          | 577/? [13:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 577, loss 5.701169013977051\n",
      "Epoch 0: |          | 578/? [13:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 578, loss 5.936248302459717\n",
      "Epoch 0: |          | 579/? [13:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 579, loss 5.015317916870117\n",
      "Epoch 0: |          | 580/? [13:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 580, loss 5.760514736175537\n",
      "Epoch 0: |          | 581/? [13:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 581, loss 5.859560966491699\n",
      "Epoch 0: |          | 582/? [13:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 582, loss 5.79556131362915\n",
      "Epoch 0: |          | 583/? [13:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 583, loss 5.520693302154541\n",
      "Epoch 0: |          | 584/? [13:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 584, loss 5.618353366851807\n",
      "Epoch 0: |          | 585/? [13:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 585, loss 5.896456241607666\n",
      "Epoch 0: |          | 586/? [13:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 586, loss 5.879684925079346\n",
      "Epoch 0: |          | 587/? [13:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 587, loss 5.684811115264893\n",
      "Epoch 0: |          | 588/? [13:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 588, loss 5.8110575675964355\n",
      "Epoch 0: |          | 589/? [13:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 589, loss 5.256133079528809\n",
      "Epoch 0: |          | 590/? [13:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 590, loss 6.015349388122559\n",
      "Epoch 0: |          | 591/? [13:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 591, loss 5.798608779907227\n",
      "Epoch 0: |          | 592/? [14:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 592, loss 5.554522514343262\n",
      "Epoch 0: |          | 593/? [14:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 593, loss 5.529131889343262\n",
      "Epoch 0: |          | 594/? [14:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 594, loss 6.322561740875244\n",
      "Epoch 0: |          | 595/? [14:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 595, loss 5.049213886260986\n",
      "Epoch 0: |          | 596/? [14:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 596, loss 5.225419044494629\n",
      "Epoch 0: |          | 597/? [14:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 597, loss 5.518601894378662\n",
      "Epoch 0: |          | 598/? [14:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 598, loss 5.9426703453063965\n",
      "Epoch 0: |          | 599/? [14:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 599, loss 5.752998352050781\n",
      "Epoch 0: |          | 600/? [14:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 600, loss 5.1407670974731445\n",
      "Epoch 0: |          | 601/? [14:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 601, loss 5.603034973144531\n",
      "Epoch 0: |          | 602/? [14:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 602, loss 5.031440734863281\n",
      "Epoch 0: |          | 603/? [14:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 603, loss 5.6009016036987305\n",
      "Epoch 0: |          | 604/? [14:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 604, loss 8.52194595336914\n",
      "Epoch 0: |          | 605/? [14:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 605, loss 5.026061058044434\n",
      "Epoch 0: |          | 606/? [14:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 606, loss 5.429375171661377\n",
      "Epoch 0: |          | 607/? [14:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 607, loss 5.9610395431518555\n",
      "Epoch 0: |          | 608/? [14:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 608, loss 5.381010055541992\n",
      "Epoch 0: |          | 609/? [14:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 609, loss 5.420414924621582\n",
      "Epoch 0: |          | 610/? [14:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 610, loss 5.456827640533447\n",
      "Epoch 0: |          | 611/? [14:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 611, loss 5.661393165588379\n",
      "Epoch 0: |          | 612/? [14:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 612, loss 5.380221366882324\n",
      "Epoch 0: |          | 613/? [14:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 613, loss 5.738007068634033\n",
      "Epoch 0: |          | 614/? [14:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 614, loss 5.266142845153809\n",
      "Epoch 0: |          | 615/? [14:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 615, loss 5.92567777633667\n",
      "Epoch 0: |          | 616/? [14:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 616, loss 6.244540691375732\n",
      "Epoch 0: |          | 617/? [14:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 617, loss 5.376124382019043\n",
      "Epoch 0: |          | 618/? [14:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 618, loss 5.719897270202637\n",
      "Epoch 0: |          | 619/? [14:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 619, loss 5.666542053222656\n",
      "Epoch 0: |          | 620/? [14:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 620, loss 5.941641807556152\n",
      "Epoch 0: |          | 621/? [14:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 621, loss 5.154543876647949\n",
      "Epoch 0: |          | 622/? [14:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 622, loss 4.997809410095215\n",
      "Epoch 0: |          | 623/? [14:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 623, loss 4.711705684661865\n",
      "Epoch 0: |          | 624/? [14:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 624, loss 4.211935997009277\n",
      "Epoch 0: |          | 625/? [14:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 625, loss 6.108036041259766\n",
      "Epoch 0: |          | 626/? [14:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 626, loss 5.5448384284973145\n",
      "Epoch 0: |          | 627/? [14:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 627, loss 5.486710548400879\n",
      "Epoch 0: |          | 628/? [14:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 628, loss 5.3105878829956055\n",
      "Epoch 0: |          | 629/? [14:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 629, loss 5.82181978225708\n",
      "Epoch 0: |          | 630/? [14:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 630, loss 5.493209362030029\n",
      "Epoch 0: |          | 631/? [14:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 631, loss 5.795018196105957\n",
      "Epoch 0: |          | 632/? [14:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 632, loss 4.669790267944336\n",
      "Epoch 0: |          | 633/? [14:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 633, loss 5.804640293121338\n",
      "Epoch 0: |          | 634/? [14:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 634, loss 5.4011640548706055\n",
      "Epoch 0: |          | 635/? [15:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 635, loss 5.057829856872559\n",
      "Epoch 0: |          | 636/? [15:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 636, loss 5.5376811027526855\n",
      "Epoch 0: |          | 637/? [15:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 637, loss 5.293377876281738\n",
      "Epoch 0: |          | 638/? [15:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 638, loss 5.553562164306641\n",
      "Epoch 0: |          | 639/? [15:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 639, loss 5.276875972747803\n",
      "Epoch 0: |          | 640/? [15:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 640, loss 6.053361415863037\n",
      "Epoch 0: |          | 641/? [15:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 641, loss 5.280179023742676\n",
      "Epoch 0: |          | 642/? [15:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 642, loss 5.709359645843506\n",
      "Epoch 0: |          | 643/? [15:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 643, loss 5.937003135681152\n",
      "Epoch 0: |          | 644/? [15:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 644, loss 5.379230499267578\n",
      "Epoch 0: |          | 645/? [15:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 645, loss 5.296397686004639\n",
      "Epoch 0: |          | 646/? [15:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 646, loss 5.348888874053955\n",
      "Epoch 0: |          | 647/? [15:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 647, loss 6.168325901031494\n",
      "Epoch 0: |          | 648/? [15:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 648, loss 5.8433966636657715\n",
      "Epoch 0: |          | 649/? [15:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 649, loss 6.5040283203125\n",
      "Epoch 0: |          | 650/? [15:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 650, loss 5.9490485191345215\n",
      "Epoch 0: |          | 651/? [15:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 651, loss 6.101724147796631\n",
      "Epoch 0: |          | 652/? [15:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 652, loss 5.28541374206543\n",
      "Epoch 0: |          | 653/? [15:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 653, loss 5.381022930145264\n",
      "Epoch 0: |          | 654/? [15:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 654, loss 5.826489448547363\n",
      "Epoch 0: |          | 655/? [15:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 655, loss 5.572678089141846\n",
      "Epoch 0: |          | 656/? [15:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 656, loss 5.0756120681762695\n",
      "Epoch 0: |          | 657/? [15:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 657, loss 8.025197982788086\n",
      "Epoch 0: |          | 658/? [15:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 658, loss 5.609586715698242\n",
      "Epoch 0: |          | 659/? [15:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 659, loss 5.500392913818359\n",
      "Epoch 0: |          | 660/? [15:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 660, loss 5.977290630340576\n",
      "Epoch 0: |          | 661/? [15:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 661, loss 5.725682735443115\n",
      "Epoch 0: |          | 662/? [15:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 662, loss 5.554014682769775\n",
      "Epoch 0: |          | 663/? [15:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 663, loss 5.395143985748291\n",
      "Epoch 0: |          | 664/? [15:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 664, loss 5.258946895599365\n",
      "Epoch 0: |          | 665/? [15:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 665, loss 5.589674949645996\n",
      "Epoch 0: |          | 666/? [15:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 666, loss 5.5118207931518555\n",
      "Epoch 0: |          | 667/? [15:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 667, loss 6.238816738128662\n",
      "Epoch 0: |          | 668/? [15:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 668, loss 5.106965065002441\n",
      "Epoch 0: |          | 669/? [15:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 669, loss 5.329338073730469\n",
      "Epoch 0: |          | 670/? [15:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 670, loss 5.970778942108154\n",
      "Epoch 0: |          | 671/? [15:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 671, loss 5.655761241912842\n",
      "Epoch 0: |          | 672/? [15:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 672, loss 5.679409503936768\n",
      "Epoch 0: |          | 673/? [15:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 673, loss 5.445719242095947\n",
      "Epoch 0: |          | 674/? [15:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 674, loss 4.396424770355225\n",
      "Epoch 0: |          | 675/? [15:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 675, loss 2.935366153717041\n",
      "Epoch 0: |          | 676/? [15:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 676, loss 2.639025926589966\n",
      "Epoch 0: |          | 677/? [15:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 677, loss 2.0464742183685303\n",
      "Epoch 0: |          | 678/? [16:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 678, loss 3.182858467102051\n",
      "Epoch 0: |          | 679/? [16:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 679, loss 5.00211238861084\n",
      "Epoch 0: |          | 680/? [16:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 680, loss 5.583468437194824\n",
      "Epoch 0: |          | 681/? [16:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 681, loss 4.636193752288818\n",
      "Epoch 0: |          | 682/? [16:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 682, loss 5.387209892272949\n",
      "Epoch 0: |          | 683/? [16:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 683, loss 5.11098051071167\n",
      "Epoch 0: |          | 684/? [16:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 684, loss 6.0767822265625\n",
      "Epoch 0: |          | 685/? [16:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 685, loss 5.796032428741455\n",
      "Epoch 0: |          | 686/? [16:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 686, loss 5.142945289611816\n",
      "Epoch 0: |          | 687/? [16:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 687, loss 6.05127477645874\n",
      "Epoch 0: |          | 688/? [16:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 688, loss 5.956040382385254\n",
      "Epoch 0: |          | 689/? [16:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 689, loss 5.755082130432129\n",
      "Epoch 0: |          | 690/? [16:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 690, loss 5.957455158233643\n",
      "Epoch 0: |          | 691/? [16:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 691, loss 5.78975772857666\n",
      "Epoch 0: |          | 692/? [16:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 692, loss 5.451008319854736\n",
      "Epoch 0: |          | 693/? [16:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 693, loss 5.963199138641357\n",
      "Epoch 0: |          | 694/? [16:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 694, loss 5.465202331542969\n",
      "Epoch 0: |          | 695/? [16:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 695, loss 6.062121391296387\n",
      "Epoch 0: |          | 696/? [16:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 696, loss 4.864903450012207\n",
      "Epoch 0: |          | 697/? [16:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 697, loss 5.580813407897949\n",
      "Epoch 0: |          | 698/? [16:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 698, loss 4.638261318206787\n",
      "Epoch 0: |          | 699/? [16:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 699, loss 5.573010444641113\n",
      "Epoch 0: |          | 700/? [16:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 700, loss 5.8198676109313965\n",
      "Epoch 0: |          | 701/? [16:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 701, loss 5.23660135269165\n",
      "Epoch 0: |          | 702/? [16:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 702, loss 5.378328323364258\n",
      "Epoch 0: |          | 703/? [16:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 703, loss 5.687239170074463\n",
      "Epoch 0: |          | 704/? [16:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 704, loss 5.637772083282471\n",
      "Epoch 0: |          | 705/? [16:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 705, loss 5.148955345153809\n",
      "Epoch 0: |          | 706/? [16:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 706, loss 5.301016807556152\n",
      "Epoch 0: |          | 707/? [16:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 707, loss 5.672825813293457\n",
      "Epoch 0: |          | 708/? [16:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 708, loss 5.516335487365723\n",
      "Epoch 0: |          | 709/? [16:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 709, loss 5.300350189208984\n",
      "Epoch 0: |          | 710/? [16:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 710, loss 5.921025276184082\n",
      "Epoch 0: |          | 711/? [16:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 711, loss 6.294371604919434\n",
      "Epoch 0: |          | 712/? [16:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 712, loss 5.7739949226379395\n",
      "Epoch 0: |          | 713/? [16:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 713, loss 5.711015224456787\n",
      "Epoch 0: |          | 714/? [16:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 714, loss 6.006805896759033\n",
      "Epoch 0: |          | 715/? [16:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 715, loss 4.660012245178223\n",
      "Epoch 0: |          | 716/? [16:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 716, loss 5.59672737121582\n",
      "Epoch 0: |          | 717/? [16:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 717, loss 5.26802396774292\n",
      "Epoch 0: |          | 718/? [16:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 718, loss 4.755924224853516\n",
      "Epoch 0: |          | 719/? [16:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 719, loss 5.361761093139648\n",
      "Epoch 0: |          | 720/? [17:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 720, loss 5.207032203674316\n",
      "Epoch 0: |          | 721/? [17:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 721, loss 5.927061557769775\n",
      "Epoch 0: |          | 722/? [17:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 722, loss 4.886891841888428\n",
      "Epoch 0: |          | 723/? [17:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 723, loss 5.589620113372803\n",
      "Epoch 0: |          | 724/? [17:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 724, loss 5.653533935546875\n",
      "Epoch 0: |          | 725/? [17:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 725, loss 5.214996337890625\n",
      "Epoch 0: |          | 726/? [17:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 726, loss 5.368571758270264\n",
      "Epoch 0: |          | 727/? [17:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 727, loss 4.923455715179443\n",
      "Epoch 0: |          | 728/? [17:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 728, loss 4.822305202484131\n",
      "Epoch 0: |          | 729/? [17:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 729, loss 5.204887866973877\n",
      "Epoch 0: |          | 730/? [17:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 730, loss 5.2737603187561035\n",
      "Epoch 0: |          | 731/? [17:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 731, loss 5.448729515075684\n",
      "Epoch 0: |          | 732/? [17:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 732, loss 5.824405193328857\n",
      "Epoch 0: |          | 733/? [17:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 733, loss 5.312465190887451\n",
      "Epoch 0: |          | 734/? [17:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 734, loss 5.762155055999756\n",
      "Epoch 0: |          | 735/? [17:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 735, loss 5.601123332977295\n",
      "Epoch 0: |          | 736/? [17:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 736, loss 4.944838047027588\n",
      "Epoch 0: |          | 737/? [17:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 737, loss 5.781696796417236\n",
      "Epoch 0: |          | 738/? [17:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 738, loss 4.944024085998535\n",
      "Epoch 0: |          | 739/? [17:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 739, loss 5.650238037109375\n",
      "Epoch 0: |          | 740/? [17:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 740, loss 5.041440963745117\n",
      "Epoch 0: |          | 741/? [17:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 741, loss 5.394012928009033\n",
      "Epoch 0: |          | 742/? [17:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 742, loss 5.829102516174316\n",
      "Epoch 0: |          | 743/? [17:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 743, loss 5.64162540435791\n",
      "Epoch 0: |          | 744/? [17:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 744, loss 5.517445087432861\n",
      "Epoch 0: |          | 745/? [17:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 745, loss 5.172004222869873\n",
      "Epoch 0: |          | 746/? [17:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 746, loss 5.402795314788818\n",
      "Epoch 0: |          | 747/? [17:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 747, loss 5.364489555358887\n",
      "Epoch 0: |          | 748/? [17:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 748, loss 5.275214672088623\n",
      "Epoch 0: |          | 749/? [17:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 749, loss 5.586808204650879\n",
      "Epoch 0: |          | 750/? [17:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 750, loss 5.8599853515625\n",
      "Epoch 0: |          | 751/? [17:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 751, loss 4.718613624572754\n",
      "Epoch 0: |          | 752/? [17:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 752, loss 5.7932844161987305\n",
      "Epoch 0: |          | 753/? [17:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 753, loss 5.1629438400268555\n",
      "Epoch 0: |          | 754/? [17:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 754, loss 5.541231632232666\n",
      "Epoch 0: |          | 755/? [17:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 755, loss 4.992053031921387\n",
      "Epoch 0: |          | 756/? [17:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 756, loss 5.499016761779785\n",
      "Epoch 0: |          | 757/? [17:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 757, loss 5.441924571990967\n",
      "Epoch 0: |          | 758/? [17:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 758, loss 5.089402198791504\n",
      "Epoch 0: |          | 759/? [17:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 759, loss 5.46399450302124\n",
      "Epoch 0: |          | 760/? [17:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 760, loss 5.612536430358887\n",
      "Epoch 0: |          | 761/? [17:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 761, loss 5.729745388031006\n",
      "Epoch 0: |          | 762/? [18:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 762, loss 5.390289306640625\n",
      "Epoch 0: |          | 763/? [18:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 763, loss 5.94503116607666\n",
      "Epoch 0: |          | 764/? [18:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 764, loss 5.88444185256958\n",
      "Epoch 0: |          | 765/? [18:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 765, loss 5.5010199546813965\n",
      "Epoch 0: |          | 766/? [18:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 766, loss 5.978182315826416\n",
      "Epoch 0: |          | 767/? [18:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 767, loss 6.128994941711426\n",
      "Epoch 0: |          | 768/? [18:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 768, loss 5.539724826812744\n",
      "Epoch 0: |          | 769/? [18:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 769, loss 4.59660530090332\n",
      "Epoch 0: |          | 770/? [18:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 770, loss 5.294999599456787\n",
      "Epoch 0: |          | 771/? [18:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 771, loss 6.013950347900391\n",
      "Epoch 0: |          | 772/? [18:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 772, loss 5.7359771728515625\n",
      "Epoch 0: |          | 773/? [18:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 773, loss 5.369533538818359\n",
      "Epoch 0: |          | 774/? [18:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 774, loss 5.404084205627441\n",
      "Epoch 0: |          | 775/? [18:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 775, loss 5.994208335876465\n",
      "Epoch 0: |          | 776/? [18:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 776, loss 5.406763553619385\n",
      "Epoch 0: |          | 777/? [18:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 777, loss 5.368169784545898\n",
      "Epoch 0: |          | 778/? [18:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 778, loss 5.793320655822754\n",
      "Epoch 0: |          | 779/? [18:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 779, loss 6.149622440338135\n",
      "Epoch 0: |          | 780/? [18:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 780, loss 4.933507919311523\n",
      "Epoch 0: |          | 781/? [18:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 781, loss 5.194413185119629\n",
      "Epoch 0: |          | 782/? [18:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 782, loss 5.735820770263672\n",
      "Epoch 0: |          | 783/? [18:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 783, loss 5.672904968261719\n",
      "Epoch 0: |          | 784/? [18:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 784, loss 5.1746721267700195\n",
      "Epoch 0: |          | 785/? [18:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 785, loss 5.1360392570495605\n",
      "Epoch 0: |          | 786/? [18:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 786, loss 5.904527187347412\n",
      "Epoch 0: |          | 787/? [18:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 787, loss 6.026043891906738\n",
      "Epoch 0: |          | 788/? [18:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 788, loss 4.2110490798950195\n",
      "Epoch 0: |          | 789/? [18:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 789, loss 5.412014484405518\n",
      "Epoch 0: |          | 790/? [18:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 790, loss 6.196403980255127\n",
      "Epoch 0: |          | 791/? [18:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 791, loss 6.033069133758545\n",
      "Epoch 0: |          | 792/? [18:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 792, loss 4.925745487213135\n",
      "Epoch 0: |          | 793/? [18:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 793, loss 5.528473377227783\n",
      "Epoch 0: |          | 794/? [18:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 794, loss 5.874523639678955\n",
      "Epoch 0: |          | 795/? [18:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 795, loss 5.502074718475342\n",
      "Epoch 0: |          | 796/? [18:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 796, loss 5.766749382019043\n",
      "Epoch 0: |          | 797/? [18:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 797, loss 4.642086982727051\n",
      "Epoch 0: |          | 798/? [18:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 798, loss 4.768435955047607\n",
      "Epoch 0: |          | 799/? [18:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 799, loss 5.84331750869751\n",
      "Epoch 0: |          | 800/? [18:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 800, loss 5.6163482666015625\n",
      "Epoch 0: |          | 801/? [18:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 801, loss 5.062601089477539\n",
      "Epoch 0: |          | 802/? [18:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 802, loss 5.542517185211182\n",
      "Epoch 0: |          | 803/? [18:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 803, loss 5.46570348739624\n",
      "Epoch 0: |          | 804/? [19:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 804, loss 5.574671745300293\n",
      "Epoch 0: |          | 805/? [19:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 805, loss 6.018155574798584\n",
      "Epoch 0: |          | 806/? [19:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 806, loss 5.988887310028076\n",
      "Epoch 0: |          | 807/? [19:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 807, loss 5.562408447265625\n",
      "Epoch 0: |          | 808/? [19:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 808, loss 5.0219879150390625\n",
      "Epoch 0: |          | 809/? [19:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 809, loss 5.579878807067871\n",
      "Epoch 0: |          | 810/? [19:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 810, loss 5.569265365600586\n",
      "Epoch 0: |          | 811/? [19:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 811, loss 5.675093650817871\n",
      "Epoch 0: |          | 812/? [19:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 812, loss 6.5985541343688965\n",
      "Epoch 0: |          | 813/? [19:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 813, loss 5.960999965667725\n",
      "Epoch 0: |          | 814/? [19:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 814, loss 5.069235324859619\n",
      "Epoch 0: |          | 815/? [19:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 815, loss 5.810275077819824\n",
      "Epoch 0: |          | 816/? [19:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 816, loss 5.698784828186035\n",
      "Epoch 0: |          | 817/? [19:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 817, loss 4.905600070953369\n",
      "Epoch 0: |          | 818/? [19:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 818, loss 5.959794044494629\n",
      "Epoch 0: |          | 819/? [19:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 819, loss 5.6977643966674805\n",
      "Epoch 0: |          | 820/? [19:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 820, loss 5.524954319000244\n",
      "Epoch 0: |          | 821/? [19:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 821, loss 5.650421142578125\n",
      "Epoch 0: |          | 822/? [19:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 822, loss 5.009486198425293\n",
      "Epoch 0: |          | 823/? [19:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 823, loss 4.931090354919434\n",
      "Epoch 0: |          | 824/? [19:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 824, loss 5.585291862487793\n",
      "Epoch 0: |          | 825/? [19:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 825, loss 5.132505893707275\n",
      "Epoch 0: |          | 826/? [19:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 826, loss 5.46197509765625\n",
      "Epoch 0: |          | 827/? [19:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 827, loss 5.173388957977295\n",
      "Epoch 0: |          | 828/? [19:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 828, loss 5.773016929626465\n",
      "Epoch 0: |          | 829/? [19:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 829, loss 5.385086536407471\n",
      "Epoch 0: |          | 830/? [19:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 830, loss 5.968642234802246\n",
      "Epoch 0: |          | 831/? [19:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 831, loss 3.662707805633545\n",
      "Epoch 0: |          | 832/? [19:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 832, loss 5.43662166595459\n",
      "Epoch 0: |          | 833/? [19:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 833, loss 5.202700138092041\n",
      "Epoch 0: |          | 834/? [19:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 834, loss 5.9721550941467285\n",
      "Epoch 0: |          | 835/? [19:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 835, loss 5.589062213897705\n",
      "Epoch 0: |          | 836/? [19:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 836, loss 6.058570861816406\n",
      "Epoch 0: |          | 837/? [19:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 837, loss 5.669110298156738\n",
      "Epoch 0: |          | 838/? [19:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 838, loss 4.739065647125244\n",
      "Epoch 0: |          | 839/? [19:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 839, loss 5.076323509216309\n",
      "Epoch 0: |          | 840/? [19:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 840, loss 5.76559591293335\n",
      "Epoch 0: |          | 841/? [19:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 841, loss 5.763228416442871\n",
      "Epoch 0: |          | 842/? [19:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 842, loss 5.407927513122559\n",
      "Epoch 0: |          | 843/? [19:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 843, loss 5.739546775817871\n",
      "Epoch 0: |          | 844/? [19:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 844, loss 5.048655986785889\n",
      "Epoch 0: |          | 845/? [19:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 845, loss 5.602547645568848\n",
      "Epoch 0: |          | 846/? [20:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 846, loss 6.208441734313965\n",
      "Epoch 0: |          | 847/? [20:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 847, loss 5.553369998931885\n",
      "Epoch 0: |          | 848/? [20:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 848, loss 5.041119575500488\n",
      "Epoch 0: |          | 849/? [20:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 849, loss 5.303622245788574\n",
      "Epoch 0: |          | 850/? [20:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 850, loss 5.218845367431641\n",
      "Epoch 0: |          | 851/? [20:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 851, loss 5.693665027618408\n",
      "Epoch 0: |          | 852/? [20:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 852, loss 5.526003360748291\n",
      "Epoch 0: |          | 853/? [20:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 853, loss 5.7012128829956055\n",
      "Epoch 0: |          | 854/? [20:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 854, loss 4.673211097717285\n",
      "Epoch 0: |          | 855/? [20:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 855, loss 5.200906276702881\n",
      "Epoch 0: |          | 856/? [20:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 856, loss 5.046401500701904\n",
      "Epoch 0: |          | 857/? [20:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 857, loss 5.712337493896484\n",
      "Epoch 0: |          | 858/? [20:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 858, loss 5.494665145874023\n",
      "Epoch 0: |          | 859/? [20:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 859, loss 5.5429205894470215\n",
      "Epoch 0: |          | 860/? [20:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 860, loss 6.020266532897949\n",
      "Epoch 0: |          | 861/? [20:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 861, loss 5.039316177368164\n",
      "Epoch 0: |          | 862/? [20:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 862, loss 5.652550220489502\n",
      "Epoch 0: |          | 863/? [20:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 863, loss 4.666520118713379\n",
      "Epoch 0: |          | 864/? [20:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 864, loss 5.677926540374756\n",
      "Epoch 0: |          | 865/? [20:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 865, loss 5.488485813140869\n",
      "Epoch 0: |          | 866/? [20:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 866, loss 5.039794445037842\n",
      "Epoch 0: |          | 867/? [20:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 867, loss 4.871705532073975\n",
      "Epoch 0: |          | 868/? [20:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 868, loss 5.618298053741455\n",
      "Epoch 0: |          | 869/? [20:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 869, loss 5.511225700378418\n",
      "Epoch 0: |          | 870/? [20:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 870, loss 5.114622116088867\n",
      "Epoch 0: |          | 871/? [20:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 871, loss 5.641032695770264\n",
      "Epoch 0: |          | 872/? [20:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 872, loss 5.478298187255859\n",
      "Epoch 0: |          | 873/? [20:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 873, loss 5.426738739013672\n",
      "Epoch 0: |          | 874/? [20:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 874, loss 4.774353504180908\n",
      "Epoch 0: |          | 875/? [20:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 875, loss 5.595700263977051\n",
      "Epoch 0: |          | 876/? [20:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 876, loss 5.438782691955566\n",
      "Epoch 0: |          | 877/? [20:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 877, loss 5.423490524291992\n",
      "Epoch 0: |          | 878/? [20:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 878, loss 4.905656814575195\n",
      "Epoch 0: |          | 879/? [20:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 879, loss 5.09591007232666\n",
      "Epoch 0: |          | 880/? [20:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 880, loss 5.996882915496826\n",
      "Epoch 0: |          | 881/? [20:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 881, loss 5.4286723136901855\n",
      "Epoch 0: |          | 882/? [20:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 882, loss 5.411352157592773\n",
      "Epoch 0: |          | 883/? [20:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 883, loss 5.36440372467041\n",
      "Epoch 0: |          | 884/? [20:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 884, loss 5.503230571746826\n",
      "Epoch 0: |          | 885/? [20:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 885, loss 5.240763187408447\n",
      "Epoch 0: |          | 886/? [20:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 886, loss 5.7376484870910645\n",
      "Epoch 0: |          | 887/? [20:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 887, loss 5.940093040466309\n",
      "Epoch 0: |          | 888/? [21:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 888, loss 5.623931407928467\n",
      "Epoch 0: |          | 889/? [21:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 889, loss 5.463257789611816\n",
      "Epoch 0: |          | 890/? [21:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 890, loss 5.955626010894775\n",
      "Epoch 0: |          | 891/? [21:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 891, loss 5.010138511657715\n",
      "Epoch 0: |          | 892/? [21:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 892, loss 5.8829731941223145\n",
      "Epoch 0: |          | 893/? [21:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 893, loss 5.174220085144043\n",
      "Epoch 0: |          | 894/? [21:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 894, loss 4.998797416687012\n",
      "Epoch 0: |          | 895/? [21:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 895, loss 5.890873908996582\n",
      "Epoch 0: |          | 896/? [21:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 896, loss 5.578431606292725\n",
      "Epoch 0: |          | 897/? [21:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 897, loss 5.64901876449585\n",
      "Epoch 0: |          | 898/? [21:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 898, loss 5.632157325744629\n",
      "Epoch 0: |          | 899/? [21:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 899, loss 5.312938690185547\n",
      "Epoch 0: |          | 900/? [21:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 900, loss 5.163435935974121\n",
      "Epoch 0: |          | 901/? [21:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 901, loss 5.743896484375\n",
      "Epoch 0: |          | 902/? [21:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 902, loss 5.694628715515137\n",
      "Epoch 0: |          | 903/? [21:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 903, loss 5.053919315338135\n",
      "Epoch 0: |          | 904/? [21:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 904, loss 5.40681266784668\n",
      "Epoch 0: |          | 905/? [21:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 905, loss 5.708586692810059\n",
      "Epoch 0: |          | 906/? [21:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 906, loss 5.423666477203369\n",
      "Epoch 0: |          | 907/? [21:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 907, loss 5.493204116821289\n",
      "Epoch 0: |          | 908/? [21:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 908, loss 5.581759452819824\n",
      "Epoch 0: |          | 909/? [21:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 909, loss 5.5897932052612305\n",
      "Epoch 0: |          | 910/? [21:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 910, loss 5.319382667541504\n",
      "Epoch 0: |          | 911/? [21:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 911, loss 5.252496242523193\n",
      "Epoch 0: |          | 912/? [21:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 912, loss 5.299993515014648\n",
      "Epoch 0: |          | 913/? [21:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 913, loss 5.29900598526001\n",
      "Epoch 0: |          | 914/? [21:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 914, loss 5.749250411987305\n",
      "Epoch 0: |          | 915/? [21:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 915, loss 5.6548051834106445\n",
      "Epoch 0: |          | 916/? [21:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 916, loss 5.338042259216309\n",
      "Epoch 0: |          | 917/? [21:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 917, loss 5.4065728187561035\n",
      "Epoch 0: |          | 918/? [21:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 918, loss 5.3192458152771\n",
      "Epoch 0: |          | 919/? [21:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 919, loss 5.158242225646973\n",
      "Epoch 0: |          | 920/? [21:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 920, loss 5.4302215576171875\n",
      "Epoch 0: |          | 921/? [21:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 921, loss 5.252499580383301\n",
      "Epoch 0: |          | 922/? [21:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 922, loss 5.3902764320373535\n",
      "Epoch 0: |          | 923/? [21:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 923, loss 5.184383392333984\n",
      "Epoch 0: |          | 924/? [21:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 924, loss 5.267329216003418\n",
      "Epoch 0: |          | 925/? [21:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 925, loss 5.445132732391357\n",
      "Epoch 0: |          | 926/? [21:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 926, loss 5.421172142028809\n",
      "Epoch 0: |          | 927/? [21:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 927, loss 5.4895100593566895\n",
      "Epoch 0: |          | 928/? [21:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 928, loss 4.996628761291504\n",
      "Epoch 0: |          | 929/? [21:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 929, loss 5.180232048034668\n",
      "Epoch 0: |          | 930/? [21:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 930, loss 4.909389972686768\n",
      "Epoch 0: |          | 931/? [22:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 931, loss 4.632473945617676\n",
      "Epoch 0: |          | 932/? [22:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 932, loss 5.349808692932129\n",
      "Epoch 0: |          | 933/? [22:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 933, loss 5.171882152557373\n",
      "Epoch 0: |          | 934/? [22:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 934, loss 5.970854759216309\n",
      "Epoch 0: |          | 935/? [22:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 935, loss 6.02811336517334\n",
      "Epoch 0: |          | 936/? [22:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 936, loss 5.463573455810547\n",
      "Epoch 0: |          | 937/? [22:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 937, loss 6.0329508781433105\n",
      "Epoch 0: |          | 938/? [22:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 938, loss 5.166003227233887\n",
      "Epoch 0: |          | 939/? [22:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 939, loss 5.446829795837402\n",
      "Epoch 0: |          | 940/? [22:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 940, loss 5.822907447814941\n",
      "Epoch 0: |          | 941/? [22:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 941, loss 5.080605983734131\n",
      "Epoch 0: |          | 942/? [22:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 942, loss 4.851758003234863\n",
      "Epoch 0: |          | 943/? [22:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 943, loss 5.687726020812988\n",
      "Epoch 0: |          | 944/? [22:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 944, loss 4.622038841247559\n",
      "Epoch 0: |          | 945/? [22:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 945, loss 5.308692932128906\n",
      "Epoch 0: |          | 946/? [22:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 946, loss 5.425381183624268\n",
      "Epoch 0: |          | 947/? [22:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 947, loss 5.381682872772217\n",
      "Epoch 0: |          | 948/? [22:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 948, loss 5.358237266540527\n",
      "Epoch 0: |          | 949/? [22:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 949, loss 5.269684791564941\n",
      "Epoch 0: |          | 950/? [22:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 950, loss 5.120408058166504\n",
      "Epoch 0: |          | 951/? [22:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 951, loss 5.936556339263916\n",
      "Epoch 0: |          | 952/? [22:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 952, loss 5.727596759796143\n",
      "Epoch 0: |          | 953/? [22:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 953, loss 6.282437801361084\n",
      "Epoch 0: |          | 954/? [22:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 954, loss 5.548033714294434\n",
      "Epoch 0: |          | 955/? [22:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 955, loss 6.0537824630737305\n",
      "Epoch 0: |          | 956/? [22:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 956, loss 5.17795467376709\n",
      "Epoch 0: |          | 957/? [22:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 957, loss 5.744254112243652\n",
      "Epoch 0: |          | 958/? [22:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 958, loss 6.112691402435303\n",
      "Epoch 0: |          | 959/? [22:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 959, loss 6.3686299324035645\n",
      "Epoch 0: |          | 960/? [22:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 960, loss 5.833247661590576\n",
      "Epoch 0: |          | 961/? [22:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 961, loss 5.780919551849365\n",
      "Epoch 0: |          | 962/? [22:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 962, loss 5.605859756469727\n",
      "Epoch 0: |          | 963/? [22:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 963, loss 5.061435699462891\n",
      "Epoch 0: |          | 964/? [22:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 964, loss 5.597964763641357\n",
      "Epoch 0: |          | 965/? [22:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 965, loss 5.201987266540527\n",
      "Epoch 0: |          | 966/? [22:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 966, loss 5.035099506378174\n",
      "Epoch 0: |          | 967/? [22:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 967, loss 5.292422771453857\n",
      "Epoch 0: |          | 968/? [22:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 968, loss 5.397927761077881\n",
      "Epoch 0: |          | 969/? [22:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 969, loss 4.965341091156006\n",
      "Epoch 0: |          | 970/? [22:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 970, loss 5.555551528930664\n",
      "Epoch 0: |          | 971/? [22:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 971, loss 6.249884605407715\n",
      "Epoch 0: |          | 972/? [22:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 972, loss 5.344522476196289\n",
      "Epoch 0: |          | 973/? [23:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 973, loss 5.821426868438721\n",
      "Epoch 0: |          | 974/? [23:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 974, loss 5.226905822753906\n",
      "Epoch 0: |          | 975/? [23:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 975, loss 5.393708229064941\n",
      "Epoch 0: |          | 976/? [23:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 976, loss 5.524830341339111\n",
      "Epoch 0: |          | 977/? [23:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 977, loss 6.046776294708252\n",
      "Epoch 0: |          | 978/? [23:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 978, loss 5.839545249938965\n",
      "Epoch 0: |          | 979/? [23:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 979, loss 5.793330192565918\n",
      "Epoch 0: |          | 980/? [23:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 980, loss 5.239973068237305\n",
      "Epoch 0: |          | 981/? [23:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 981, loss 4.628742694854736\n",
      "Epoch 0: |          | 982/? [23:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 982, loss 5.450333595275879\n",
      "Epoch 0: |          | 983/? [23:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 983, loss 5.767436981201172\n",
      "Epoch 0: |          | 984/? [23:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 984, loss 4.671487331390381\n",
      "Epoch 0: |          | 985/? [23:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 985, loss 5.103153705596924\n",
      "Epoch 0: |          | 986/? [23:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 986, loss 5.090017318725586\n",
      "Epoch 0: |          | 987/? [23:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 987, loss 5.180910587310791\n",
      "Epoch 0: |          | 988/? [23:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 988, loss 5.702286720275879\n",
      "Epoch 0: |          | 989/? [23:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 989, loss 5.321826457977295\n",
      "Epoch 0: |          | 990/? [23:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 990, loss 4.443528175354004\n",
      "Epoch 0: |          | 991/? [23:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 991, loss 5.177121162414551\n",
      "Epoch 0: |          | 992/? [23:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 992, loss 6.025478839874268\n",
      "Epoch 0: |          | 993/? [23:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 993, loss 5.085202217102051\n",
      "Epoch 0: |          | 994/? [23:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 994, loss 5.159201622009277\n",
      "Epoch 0: |          | 995/? [23:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 995, loss 5.6402058601379395\n",
      "Epoch 0: |          | 996/? [23:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 996, loss 5.572935104370117\n",
      "Epoch 0: |          | 997/? [23:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 997, loss 5.203614234924316\n",
      "Epoch 0: |          | 998/? [23:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 998, loss 5.438498020172119\n",
      "Epoch 0: |          | 999/? [23:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 999, loss 5.876739501953125\n",
      "Epoch 0: |          | 1000/? [23:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1000, loss 5.413846969604492\n",
      "Epoch 0: |          | 1001/? [23:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1001, loss 5.674074172973633\n",
      "Epoch 0: |          | 1002/? [23:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1002, loss 5.740551471710205\n",
      "Epoch 0: |          | 1003/? [23:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1003, loss 5.683981895446777\n",
      "Epoch 0: |          | 1004/? [23:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1004, loss 4.556916236877441\n",
      "Epoch 0: |          | 1005/? [23:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1005, loss 5.208104610443115\n",
      "Epoch 0: |          | 1006/? [23:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1006, loss 5.709781169891357\n",
      "Epoch 0: |          | 1007/? [23:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1007, loss 5.315325736999512\n",
      "Epoch 0: |          | 1008/? [23:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1008, loss 5.268174171447754\n",
      "Epoch 0: |          | 1009/? [23:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1009, loss 5.758800029754639\n",
      "Epoch 0: |          | 1010/? [23:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1010, loss 4.660653114318848\n",
      "Epoch 0: |          | 1011/? [23:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1011, loss 5.463385581970215\n",
      "Epoch 0: |          | 1012/? [23:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1012, loss 5.239514350891113\n",
      "Epoch 0: |          | 1013/? [24:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1013, loss 5.530068397521973\n",
      "Epoch 0: |          | 1014/? [24:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1014, loss 5.717658042907715\n",
      "Epoch 0: |          | 1015/? [24:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1015, loss 5.4497270584106445\n",
      "Epoch 0: |          | 1016/? [24:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1016, loss 5.4445295333862305\n",
      "Epoch 0: |          | 1017/? [24:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1017, loss 4.83981990814209\n",
      "Epoch 0: |          | 1018/? [24:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1018, loss 5.4726786613464355\n",
      "Epoch 0: |          | 1019/? [24:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1019, loss 5.418798923492432\n",
      "Epoch 0: |          | 1020/? [24:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1020, loss 4.941405296325684\n",
      "Epoch 0: |          | 1021/? [24:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1021, loss 5.108075141906738\n",
      "Epoch 0: |          | 1022/? [24:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1022, loss 4.770445823669434\n",
      "Epoch 0: |          | 1023/? [24:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1023, loss 4.4091386795043945\n",
      "Epoch 0: |          | 1024/? [24:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1024, loss 5.294189929962158\n",
      "Epoch 0: |          | 1025/? [24:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1025, loss 5.270812034606934\n",
      "Epoch 0: |          | 1026/? [24:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1026, loss 3.9147918224334717\n",
      "Epoch 0: |          | 1027/? [24:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1027, loss 6.002518653869629\n",
      "Epoch 0: |          | 1028/? [24:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1028, loss 5.318646430969238\n",
      "Epoch 0: |          | 1029/? [24:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1029, loss 5.191285133361816\n",
      "Epoch 0: |          | 1030/? [24:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1030, loss 4.9123454093933105\n",
      "Epoch 0: |          | 1031/? [24:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1031, loss 4.993626594543457\n",
      "Epoch 0: |          | 1032/? [24:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1032, loss 5.924478530883789\n",
      "Epoch 0: |          | 1033/? [24:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1033, loss 5.728188991546631\n",
      "Epoch 0: |          | 1034/? [24:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1034, loss 5.20693302154541\n",
      "Epoch 0: |          | 1035/? [24:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1035, loss 5.131030559539795\n",
      "Epoch 0: |          | 1036/? [24:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1036, loss 4.782789707183838\n",
      "Epoch 0: |          | 1037/? [24:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1037, loss 5.715653419494629\n",
      "Epoch 0: |          | 1038/? [24:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1038, loss 5.831825256347656\n",
      "Epoch 0: |          | 1039/? [24:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1039, loss 5.963637351989746\n",
      "Epoch 0: |          | 1040/? [24:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1040, loss 5.376800537109375\n",
      "Epoch 0: |          | 1041/? [24:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1041, loss 5.757025718688965\n",
      "Epoch 0: |          | 1042/? [24:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1042, loss 5.375699043273926\n",
      "Epoch 0: |          | 1043/? [24:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1043, loss 5.617506980895996\n",
      "Epoch 0: |          | 1044/? [24:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1044, loss 5.175581455230713\n",
      "Epoch 0: |          | 1045/? [24:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1045, loss 4.724131107330322\n",
      "Epoch 0: |          | 1046/? [24:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1046, loss 4.541268348693848\n",
      "Epoch 0: |          | 1047/? [24:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1047, loss 5.990138053894043\n",
      "Epoch 0: |          | 1048/? [24:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1048, loss 5.035932540893555\n",
      "Epoch 0: |          | 1049/? [24:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1049, loss 5.271204948425293\n",
      "Epoch 0: |          | 1050/? [24:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1050, loss 4.992560386657715\n",
      "Epoch 0: |          | 1051/? [24:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1051, loss 5.107131004333496\n",
      "Epoch 0: |          | 1052/? [24:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1052, loss 5.592367649078369\n",
      "Epoch 0: |          | 1053/? [24:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1053, loss 5.642461776733398\n",
      "Epoch 0: |          | 1054/? [24:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1054, loss 4.960711479187012\n",
      "Epoch 0: |          | 1055/? [24:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1055, loss 4.791275978088379\n",
      "Epoch 0: |          | 1056/? [25:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1056, loss 4.886929512023926\n",
      "Epoch 0: |          | 1057/? [25:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1057, loss 5.540025234222412\n",
      "Epoch 0: |          | 1058/? [25:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1058, loss 5.08355712890625\n",
      "Epoch 0: |          | 1059/? [25:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1059, loss 5.725101947784424\n",
      "Epoch 0: |          | 1060/? [25:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1060, loss 5.6530914306640625\n",
      "Epoch 0: |          | 1061/? [25:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1061, loss 4.130759239196777\n",
      "Epoch 0: |          | 1062/? [25:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1062, loss 5.917661190032959\n",
      "Epoch 0: |          | 1063/? [25:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1063, loss 5.362856864929199\n",
      "Epoch 0: |          | 1064/? [25:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1064, loss 5.58317756652832\n",
      "Epoch 0: |          | 1065/? [25:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1065, loss 4.435925483703613\n",
      "Epoch 0: |          | 1066/? [25:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1066, loss 5.427560329437256\n",
      "Epoch 0: |          | 1067/? [25:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1067, loss 4.6853251457214355\n",
      "Epoch 0: |          | 1068/? [25:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1068, loss 5.037484645843506\n",
      "Epoch 0: |          | 1069/? [25:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1069, loss 5.358792304992676\n",
      "Epoch 0: |          | 1070/? [25:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1070, loss 5.24362850189209\n",
      "Epoch 0: |          | 1071/? [25:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1071, loss 5.439795970916748\n",
      "Epoch 0: |          | 1072/? [25:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1072, loss 5.552741050720215\n",
      "Epoch 0: |          | 1073/? [25:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1073, loss 5.98434591293335\n",
      "Epoch 0: |          | 1074/? [25:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1074, loss 5.275103569030762\n",
      "Epoch 0: |          | 1075/? [25:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1075, loss 4.7786784172058105\n",
      "Epoch 0: |          | 1076/? [25:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1076, loss 5.81974983215332\n",
      "Epoch 0: |          | 1077/? [25:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1077, loss 5.187256813049316\n",
      "Epoch 0: |          | 1078/? [25:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1078, loss 5.276612758636475\n",
      "Epoch 0: |          | 1079/? [25:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1079, loss 6.135928630828857\n",
      "Epoch 0: |          | 1080/? [25:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1080, loss 5.50372314453125\n",
      "Epoch 0: |          | 1081/? [25:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1081, loss 5.63475227355957\n",
      "Epoch 0: |          | 1082/? [25:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1082, loss 4.932340145111084\n",
      "Epoch 0: |          | 1083/? [25:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1083, loss 5.006684303283691\n",
      "Epoch 0: |          | 1084/? [25:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1084, loss 4.619475841522217\n",
      "Epoch 0: |          | 1085/? [25:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1085, loss 5.131753444671631\n",
      "Epoch 0: |          | 1086/? [25:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1086, loss 5.407926559448242\n",
      "Epoch 0: |          | 1087/? [25:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1087, loss 6.052666187286377\n",
      "Epoch 0: |          | 1088/? [25:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1088, loss 5.598728656768799\n",
      "Epoch 0: |          | 1089/? [25:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1089, loss 6.109377861022949\n",
      "Epoch 0: |          | 1090/? [25:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1090, loss 5.704007148742676\n",
      "Epoch 0: |          | 1091/? [25:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1091, loss 5.256726264953613\n",
      "Epoch 0: |          | 1092/? [25:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1092, loss 5.401381492614746\n",
      "Epoch 0: |          | 1093/? [25:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1093, loss 4.944215297698975\n",
      "Epoch 0: |          | 1094/? [25:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1094, loss 5.367333889007568\n",
      "Epoch 0: |          | 1095/? [25:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1095, loss 5.420223236083984\n",
      "Epoch 0: |          | 1096/? [25:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1096, loss 5.6000285148620605\n",
      "Epoch 0: |          | 1097/? [25:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1097, loss 5.2084856033325195\n",
      "Epoch 0: |          | 1098/? [26:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1098, loss 4.266839504241943\n",
      "Epoch 0: |          | 1099/? [26:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1099, loss 5.196196556091309\n",
      "Epoch 0: |          | 1100/? [26:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1100, loss 5.455471992492676\n",
      "Epoch 0: |          | 1101/? [26:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1101, loss 4.883781433105469\n",
      "Epoch 0: |          | 1102/? [26:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1102, loss 5.809022426605225\n",
      "Epoch 0: |          | 1103/? [26:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1103, loss 6.641915321350098\n",
      "Epoch 0: |          | 1104/? [26:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1104, loss 5.656200408935547\n",
      "Epoch 0: |          | 1105/? [26:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1105, loss 5.576538562774658\n",
      "Epoch 0: |          | 1106/? [26:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1106, loss 5.170976638793945\n",
      "Epoch 0: |          | 1107/? [26:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1107, loss 5.195197105407715\n",
      "Epoch 0: |          | 1108/? [26:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1108, loss 5.1399827003479\n",
      "Epoch 0: |          | 1109/? [26:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1109, loss 4.631302833557129\n",
      "Epoch 0: |          | 1110/? [26:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1110, loss 5.933554649353027\n",
      "Epoch 0: |          | 1111/? [26:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1111, loss 5.481523036956787\n",
      "Epoch 0: |          | 1112/? [26:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1112, loss 5.410231113433838\n",
      "Epoch 0: |          | 1113/? [26:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1113, loss 5.0727667808532715\n",
      "Epoch 0: |          | 1114/? [26:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1114, loss 4.560264587402344\n",
      "Epoch 0: |          | 1115/? [26:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1115, loss 4.141212463378906\n",
      "Epoch 0: |          | 1116/? [26:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1116, loss 4.919661045074463\n",
      "Epoch 0: |          | 1117/? [26:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1117, loss 5.092755317687988\n",
      "Epoch 0: |          | 1118/? [26:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1118, loss 5.0673604011535645\n",
      "Epoch 0: |          | 1119/? [26:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1119, loss 5.827145576477051\n",
      "Epoch 0: |          | 1120/? [26:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1120, loss 5.378814697265625\n",
      "Epoch 0: |          | 1121/? [26:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1121, loss 5.611156940460205\n",
      "Epoch 0: |          | 1122/? [26:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1122, loss 5.6672821044921875\n",
      "Epoch 0: |          | 1123/? [26:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1123, loss 5.331799030303955\n",
      "Epoch 0: |          | 1124/? [26:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1124, loss 5.605224132537842\n",
      "Epoch 0: |          | 1125/? [26:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1125, loss 4.762650489807129\n",
      "Epoch 0: |          | 1126/? [26:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1126, loss 4.783193111419678\n",
      "Epoch 0: |          | 1127/? [26:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1127, loss 5.144859790802002\n",
      "Epoch 0: |          | 1128/? [26:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1128, loss 5.078469753265381\n",
      "Epoch 0: |          | 1129/? [26:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1129, loss 5.331454753875732\n",
      "Epoch 0: |          | 1130/? [26:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1130, loss 5.458149433135986\n",
      "Epoch 0: |          | 1131/? [26:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1131, loss 5.538852214813232\n",
      "Epoch 0: |          | 1132/? [26:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1132, loss 4.030457496643066\n",
      "Epoch 0: |          | 1133/? [26:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1133, loss 5.2829909324646\n",
      "Epoch 0: |          | 1134/? [26:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1134, loss 4.93740177154541\n",
      "Epoch 0: |          | 1135/? [26:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1135, loss 5.553366184234619\n",
      "Epoch 0: |          | 1136/? [26:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1136, loss 5.252193927764893\n",
      "Epoch 0: |          | 1137/? [26:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1137, loss 5.254110336303711\n",
      "Epoch 0: |          | 1138/? [26:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1138, loss 5.870196342468262\n",
      "Epoch 0: |          | 1139/? [26:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1139, loss 6.335129737854004\n",
      "Epoch 0: |          | 1140/? [27:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1140, loss 5.766297817230225\n",
      "Epoch 0: |          | 1141/? [27:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1141, loss 5.723686218261719\n",
      "Epoch 0: |          | 1142/? [27:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1142, loss 5.63245964050293\n",
      "Epoch 0: |          | 1143/? [27:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1143, loss 5.800097465515137\n",
      "Epoch 0: |          | 1144/? [27:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1144, loss 5.4254302978515625\n",
      "Epoch 0: |          | 1145/? [27:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1145, loss 5.202585220336914\n",
      "Epoch 0: |          | 1146/? [27:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1146, loss 5.561601638793945\n",
      "Epoch 0: |          | 1147/? [27:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1147, loss 4.548350811004639\n",
      "Epoch 0: |          | 1148/? [27:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1148, loss 4.995509147644043\n",
      "Epoch 0: |          | 1149/? [27:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1149, loss 6.473411560058594\n",
      "Epoch 0: |          | 1150/? [27:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1150, loss 5.46412992477417\n",
      "Epoch 0: |          | 1151/? [27:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1151, loss 5.845358371734619\n",
      "Epoch 0: |          | 1152/? [27:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1152, loss 4.953925132751465\n",
      "Epoch 0: |          | 1153/? [27:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1153, loss 5.321990966796875\n",
      "Epoch 0: |          | 1154/? [27:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1154, loss 4.986161708831787\n",
      "Epoch 0: |          | 1155/? [27:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1155, loss 5.291224002838135\n",
      "Epoch 0: |          | 1156/? [27:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1156, loss 5.568411827087402\n",
      "Epoch 0: |          | 1157/? [27:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1157, loss 5.525467395782471\n",
      "Epoch 0: |          | 1158/? [27:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1158, loss 6.1354660987854\n",
      "Epoch 0: |          | 1159/? [27:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1159, loss 4.329720497131348\n",
      "Epoch 0: |          | 1160/? [27:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1160, loss 5.688317775726318\n",
      "Epoch 0: |          | 1161/? [27:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1161, loss 5.630774021148682\n",
      "Epoch 0: |          | 1162/? [27:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1162, loss 5.4984259605407715\n",
      "Epoch 0: |          | 1163/? [27:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1163, loss 5.878479480743408\n",
      "Epoch 0: |          | 1164/? [27:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1164, loss 5.806939125061035\n",
      "Epoch 0: |          | 1165/? [27:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1165, loss 4.762303352355957\n",
      "Epoch 0: |          | 1166/? [27:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1166, loss 5.464450359344482\n",
      "Epoch 0: |          | 1167/? [27:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1167, loss 5.722965717315674\n",
      "Epoch 0: |          | 1168/? [27:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1168, loss 6.137365341186523\n",
      "Epoch 0: |          | 1169/? [27:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1169, loss 4.874541282653809\n",
      "Epoch 0: |          | 1170/? [27:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1170, loss 5.508460521697998\n",
      "Epoch 0: |          | 1171/? [27:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1171, loss 5.336304664611816\n",
      "Epoch 0: |          | 1172/? [27:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1172, loss 4.600976943969727\n",
      "Epoch 0: |          | 1173/? [27:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1173, loss 5.457284450531006\n",
      "Epoch 0: |          | 1174/? [27:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1174, loss 4.915404319763184\n",
      "Epoch 0: |          | 1175/? [27:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1175, loss 5.738121032714844\n",
      "Epoch 0: |          | 1176/? [27:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1176, loss 5.598636627197266\n",
      "Epoch 0: |          | 1177/? [27:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1177, loss 5.724862575531006\n",
      "Epoch 0: |          | 1178/? [27:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1178, loss 5.096817970275879\n",
      "Epoch 0: |          | 1179/? [27:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1179, loss 5.502387523651123\n",
      "Epoch 0: |          | 1180/? [27:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1180, loss 5.391343116760254\n",
      "Epoch 0: |          | 1181/? [27:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1181, loss 5.525521278381348\n",
      "Epoch 0: |          | 1182/? [28:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1182, loss 5.236320495605469\n",
      "Epoch 0: |          | 1183/? [28:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1183, loss 4.985711097717285\n",
      "Epoch 0: |          | 1184/? [28:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1184, loss 5.034184455871582\n",
      "Epoch 0: |          | 1185/? [28:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1185, loss 5.2753753662109375\n",
      "Epoch 0: |          | 1186/? [28:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1186, loss 5.393948078155518\n",
      "Epoch 0: |          | 1187/? [28:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1187, loss 5.144923210144043\n",
      "Epoch 0: |          | 1188/? [28:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1188, loss 5.590236663818359\n",
      "Epoch 0: |          | 1189/? [28:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1189, loss 5.6419243812561035\n",
      "Epoch 0: |          | 1190/? [28:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1190, loss 5.084654808044434\n",
      "Epoch 0: |          | 1191/? [28:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1191, loss 5.132256507873535\n",
      "Epoch 0: |          | 1192/? [28:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1192, loss 5.548395156860352\n",
      "Epoch 0: |          | 1193/? [28:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1193, loss 5.017096042633057\n",
      "Epoch 0: |          | 1194/? [28:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1194, loss 4.447366714477539\n",
      "Epoch 0: |          | 1195/? [28:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1195, loss 5.316123962402344\n",
      "Epoch 0: |          | 1196/? [28:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1196, loss 5.4275007247924805\n",
      "Epoch 0: |          | 1197/? [28:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1197, loss 5.42731237411499\n",
      "Epoch 0: |          | 1198/? [28:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1198, loss 5.276968955993652\n",
      "Epoch 0: |          | 1199/? [28:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1199, loss 5.473240375518799\n",
      "Epoch 0: |          | 1200/? [28:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1200, loss 4.643326759338379\n",
      "Epoch 0: |          | 1201/? [28:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1201, loss 5.522621154785156\n",
      "Epoch 0: |          | 1202/? [28:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1202, loss 4.993307113647461\n",
      "Epoch 0: |          | 1203/? [28:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1203, loss 5.131386756896973\n",
      "Epoch 0: |          | 1204/? [28:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1204, loss 4.4264750480651855\n",
      "Epoch 0: |          | 1205/? [28:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1205, loss 5.171438694000244\n",
      "Epoch 0: |          | 1206/? [28:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1206, loss 5.175164222717285\n",
      "Epoch 0: |          | 1207/? [28:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1207, loss 5.677243232727051\n",
      "Epoch 0: |          | 1208/? [28:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1208, loss 5.672176837921143\n",
      "Epoch 0: |          | 1209/? [28:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1209, loss 5.287933349609375\n",
      "Epoch 0: |          | 1210/? [28:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1210, loss 5.597864151000977\n",
      "Epoch 0: |          | 1211/? [28:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1211, loss 5.644986152648926\n",
      "Epoch 0: |          | 1212/? [28:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1212, loss 5.402804851531982\n",
      "Epoch 0: |          | 1213/? [28:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1213, loss 5.101772785186768\n",
      "Epoch 0: |          | 1214/? [28:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1214, loss 5.887207984924316\n",
      "Epoch 0: |          | 1215/? [28:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1215, loss 4.9929609298706055\n",
      "Epoch 0: |          | 1216/? [28:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1216, loss 5.474773406982422\n",
      "Epoch 0: |          | 1217/? [28:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1217, loss 5.55723237991333\n",
      "Epoch 0: |          | 1218/? [28:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1218, loss 5.543210506439209\n",
      "Epoch 0: |          | 1219/? [28:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1219, loss 5.0937981605529785\n",
      "Epoch 0: |          | 1220/? [28:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1220, loss 5.897121906280518\n",
      "Epoch 0: |          | 1221/? [28:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1221, loss 5.730323791503906\n",
      "Epoch 0: |          | 1222/? [28:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1222, loss 4.236632823944092\n",
      "Epoch 0: |          | 1223/? [29:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1223, loss 4.414248466491699\n",
      "Epoch 0: |          | 1224/? [29:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1224, loss 4.870471954345703\n",
      "Epoch 0: |          | 1225/? [29:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1225, loss 5.526279449462891\n",
      "Epoch 0: |          | 1226/? [29:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1226, loss 5.670168876647949\n",
      "Epoch 0: |          | 1227/? [29:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1227, loss 5.187812805175781\n",
      "Epoch 0: |          | 1228/? [29:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1228, loss 5.352293491363525\n",
      "Epoch 0: |          | 1229/? [29:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1229, loss 4.700140953063965\n",
      "Epoch 0: |          | 1230/? [29:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1230, loss 5.387954235076904\n",
      "Epoch 0: |          | 1231/? [29:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1231, loss 5.453283786773682\n",
      "Epoch 0: |          | 1232/? [29:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1232, loss 5.633026123046875\n",
      "Epoch 0: |          | 1233/? [29:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1233, loss 5.433798313140869\n",
      "Epoch 0: |          | 1234/? [29:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1234, loss 4.205053806304932\n",
      "Epoch 0: |          | 1235/? [29:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1235, loss 5.388816833496094\n",
      "Epoch 0: |          | 1236/? [29:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1236, loss 4.760684967041016\n",
      "Epoch 0: |          | 1237/? [29:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1237, loss 5.385024070739746\n",
      "Epoch 0: |          | 1238/? [29:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1238, loss 5.074763298034668\n",
      "Epoch 0: |          | 1239/? [29:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1239, loss 5.070413589477539\n",
      "Epoch 0: |          | 1240/? [29:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1240, loss 5.796072959899902\n",
      "Epoch 0: |          | 1241/? [29:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1241, loss 5.232025146484375\n",
      "Epoch 0: |          | 1242/? [29:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1242, loss 5.040247917175293\n",
      "Epoch 0: |          | 1243/? [29:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1243, loss 4.795258522033691\n",
      "Epoch 0: |          | 1244/? [29:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1244, loss 5.055882930755615\n",
      "Epoch 0: |          | 1245/? [29:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1245, loss 4.5935587882995605\n",
      "Epoch 0: |          | 1246/? [29:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1246, loss 5.456225395202637\n",
      "Epoch 0: |          | 1247/? [29:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1247, loss 5.468203544616699\n",
      "Epoch 0: |          | 1248/? [29:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1248, loss 4.904002666473389\n",
      "Epoch 0: |          | 1249/? [29:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1249, loss 4.995357036590576\n",
      "Epoch 0: |          | 1250/? [29:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1250, loss 5.164109230041504\n",
      "Epoch 0: |          | 1251/? [29:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1251, loss 4.9324631690979\n",
      "Epoch 0: |          | 1252/? [29:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1252, loss 5.687777519226074\n",
      "Epoch 0: |          | 1253/? [29:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1253, loss 5.206140518188477\n",
      "Epoch 0: |          | 1254/? [29:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1254, loss 4.309927940368652\n",
      "Epoch 0: |          | 1255/? [29:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1255, loss 5.765974044799805\n",
      "Epoch 0: |          | 1256/? [29:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1256, loss 4.799614429473877\n",
      "Epoch 0: |          | 1257/? [29:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1257, loss 4.90970516204834\n",
      "Epoch 0: |          | 1258/? [29:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1258, loss 5.575664520263672\n",
      "Epoch 0: |          | 1259/? [29:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1259, loss 5.381307601928711\n",
      "Epoch 0: |          | 1260/? [29:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1260, loss 5.70808744430542\n",
      "Epoch 0: |          | 1261/? [29:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1261, loss 5.201973915100098\n",
      "Epoch 0: |          | 1262/? [29:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1262, loss 5.042527675628662\n",
      "Epoch 0: |          | 1263/? [29:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1263, loss 5.586081027984619\n",
      "Epoch 0: |          | 1264/? [30:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1264, loss 5.7574543952941895\n",
      "Epoch 0: |          | 1265/? [30:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1265, loss 5.465758323669434\n",
      "Epoch 0: |          | 1266/? [30:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1266, loss 5.059393882751465\n",
      "Epoch 0: |          | 1267/? [30:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1267, loss 5.207389831542969\n",
      "Epoch 0: |          | 1268/? [30:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1268, loss 5.087251663208008\n",
      "Epoch 0: |          | 1269/? [30:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1269, loss 4.610691547393799\n",
      "Epoch 0: |          | 1270/? [30:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1270, loss 4.976138114929199\n",
      "Epoch 0: |          | 1271/? [30:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1271, loss 5.107492923736572\n",
      "Epoch 0: |          | 1272/? [30:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1272, loss 4.604536533355713\n",
      "Epoch 0: |          | 1273/? [30:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1273, loss 5.568201541900635\n",
      "Epoch 0: |          | 1274/? [30:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1274, loss 4.359596252441406\n",
      "Epoch 0: |          | 1275/? [30:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1275, loss 4.814785957336426\n",
      "Epoch 0: |          | 1276/? [30:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1276, loss 5.32534122467041\n",
      "Epoch 0: |          | 1277/? [30:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1277, loss 4.775022983551025\n",
      "Epoch 0: |          | 1278/? [30:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1278, loss 5.487042427062988\n",
      "Epoch 0: |          | 1279/? [30:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1279, loss 5.453717231750488\n",
      "Epoch 0: |          | 1280/? [30:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1280, loss 4.500908851623535\n",
      "Epoch 0: |          | 1281/? [30:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1281, loss 4.849318981170654\n",
      "Epoch 0: |          | 1282/? [30:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1282, loss 4.621241569519043\n",
      "Epoch 0: |          | 1283/? [30:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1283, loss 5.474094390869141\n",
      "Epoch 0: |          | 1284/? [30:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1284, loss 4.369692802429199\n",
      "Epoch 0: |          | 1285/? [30:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1285, loss 5.751919269561768\n",
      "Epoch 0: |          | 1286/? [30:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1286, loss 4.060922145843506\n",
      "Epoch 0: |          | 1287/? [30:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1287, loss 5.499634265899658\n",
      "Epoch 0: |          | 1288/? [30:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1288, loss 5.335816383361816\n",
      "Epoch 0: |          | 1289/? [30:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1289, loss 4.118590354919434\n",
      "Epoch 0: |          | 1290/? [30:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1290, loss 5.483656883239746\n",
      "Epoch 0: |          | 1291/? [30:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1291, loss 6.258702754974365\n",
      "Epoch 0: |          | 1292/? [30:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1292, loss 5.700567722320557\n",
      "Epoch 0: |          | 1293/? [30:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1293, loss 4.9430766105651855\n",
      "Epoch 0: |          | 1294/? [30:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1294, loss 5.280519485473633\n",
      "Epoch 0: |          | 1295/? [30:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1295, loss 5.177567958831787\n",
      "Epoch 0: |          | 1296/? [30:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1296, loss 4.354989051818848\n",
      "Epoch 0: |          | 1297/? [30:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1297, loss 5.511349201202393\n",
      "Epoch 0: |          | 1298/? [30:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1298, loss 5.253162384033203\n",
      "Epoch 0: |          | 1299/? [30:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1299, loss 4.077542781829834\n",
      "Epoch 0: |          | 1300/? [30:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1300, loss 5.368313789367676\n",
      "Epoch 0: |          | 1301/? [30:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1301, loss 5.020729064941406\n",
      "Epoch 0: |          | 1302/? [30:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1302, loss 5.0851311683654785\n",
      "Epoch 0: |          | 1303/? [30:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1303, loss 4.863658905029297\n",
      "Epoch 0: |          | 1304/? [30:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1304, loss 5.712172508239746\n",
      "Epoch 0: |          | 1305/? [30:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1305, loss 4.568320274353027\n",
      "Epoch 0: |          | 1306/? [31:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1306, loss 5.126426696777344\n",
      "Epoch 0: |          | 1307/? [31:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1307, loss 4.587369441986084\n",
      "Epoch 0: |          | 1308/? [31:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1308, loss 4.537533283233643\n",
      "Epoch 0: |          | 1309/? [31:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1309, loss 4.983718395233154\n",
      "Epoch 0: |          | 1310/? [31:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1310, loss 5.306802272796631\n",
      "Epoch 0: |          | 1311/? [31:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1311, loss 4.659173965454102\n",
      "Epoch 0: |          | 1312/? [31:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1312, loss 4.4078474044799805\n",
      "Epoch 0: |          | 1313/? [31:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1313, loss 5.660887241363525\n",
      "Epoch 0: |          | 1314/? [31:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1314, loss 4.775429725646973\n",
      "Epoch 0: |          | 1315/? [31:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1315, loss 5.616922378540039\n",
      "Epoch 0: |          | 1316/? [31:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1316, loss 5.3212127685546875\n",
      "Epoch 0: |          | 1317/? [31:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1317, loss 4.877494812011719\n",
      "Epoch 0: |          | 1318/? [31:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1318, loss 5.243016242980957\n",
      "Epoch 0: |          | 1319/? [31:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1319, loss 5.311610221862793\n",
      "Epoch 0: |          | 1320/? [31:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1320, loss 4.968388557434082\n",
      "Epoch 0: |          | 1321/? [31:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1321, loss 5.39711856842041\n",
      "Epoch 0: |          | 1322/? [31:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1322, loss 5.530861854553223\n",
      "Epoch 0: |          | 1323/? [31:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1323, loss 4.732504844665527\n",
      "Epoch 0: |          | 1324/? [31:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1324, loss 5.764413833618164\n",
      "Epoch 0: |          | 1325/? [31:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1325, loss 5.811171531677246\n",
      "Epoch 0: |          | 1326/? [31:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1326, loss 5.284186363220215\n",
      "Epoch 0: |          | 1327/? [31:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1327, loss 5.098649978637695\n",
      "Epoch 0: |          | 1328/? [31:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1328, loss 4.78474760055542\n",
      "Epoch 0: |          | 1329/? [31:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1329, loss 5.5667829513549805\n",
      "Epoch 0: |          | 1330/? [31:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1330, loss 5.221337795257568\n",
      "Epoch 0: |          | 1331/? [31:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1331, loss 5.29883337020874\n",
      "Epoch 0: |          | 1332/? [31:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1332, loss 5.138921737670898\n",
      "Epoch 0: |          | 1333/? [31:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1333, loss 4.933533668518066\n",
      "Epoch 0: |          | 1334/? [31:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1334, loss 5.079554557800293\n",
      "Epoch 0: |          | 1335/? [31:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1335, loss 5.142611026763916\n",
      "Epoch 0: |          | 1336/? [31:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1336, loss 4.642889976501465\n",
      "Epoch 0: |          | 1337/? [31:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1337, loss 5.510423183441162\n",
      "Epoch 0: |          | 1338/? [31:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1338, loss 4.348175525665283\n",
      "Epoch 0: |          | 1339/? [31:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1339, loss 5.067008018493652\n",
      "Epoch 0: |          | 1340/? [31:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1340, loss 4.5052690505981445\n",
      "Epoch 0: |          | 1341/? [31:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1341, loss 5.369190692901611\n",
      "Epoch 0: |          | 1342/? [31:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1342, loss 5.609858989715576\n",
      "Epoch 0: |          | 1343/? [31:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1343, loss 4.99419641494751\n",
      "Epoch 0: |          | 1344/? [31:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1344, loss 5.062417984008789\n",
      "Epoch 0: |          | 1345/? [31:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1345, loss 5.186545372009277\n",
      "Epoch 0: |          | 1346/? [31:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1346, loss 6.530617713928223\n",
      "Epoch 0: |          | 1347/? [31:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1347, loss 5.651076316833496\n",
      "Epoch 0: |          | 1348/? [32:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1348, loss 6.037726402282715\n",
      "Epoch 0: |          | 1349/? [32:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1349, loss 5.496854782104492\n",
      "Epoch 0: |          | 1350/? [32:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1350, loss 6.170440196990967\n",
      "Epoch 0: |          | 1351/? [32:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1351, loss 5.519593238830566\n",
      "Epoch 0: |          | 1352/? [32:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1352, loss 4.481320381164551\n",
      "Epoch 0: |          | 1353/? [32:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1353, loss 4.792917728424072\n",
      "Epoch 0: |          | 1354/? [32:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1354, loss 5.772314548492432\n",
      "Epoch 0: |          | 1355/? [32:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1355, loss 5.7287092208862305\n",
      "Epoch 0: |          | 1356/? [32:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1356, loss 5.309411525726318\n",
      "Epoch 0: |          | 1357/? [32:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1357, loss 4.976513862609863\n",
      "Epoch 0: |          | 1358/? [32:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1358, loss 5.206502437591553\n",
      "Epoch 0: |          | 1359/? [32:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1359, loss 4.980283737182617\n",
      "Epoch 0: |          | 1360/? [32:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1360, loss 5.244194984436035\n",
      "Epoch 0: |          | 1361/? [32:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1361, loss 5.199183464050293\n",
      "Epoch 0: |          | 1362/? [32:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1362, loss 5.095329284667969\n",
      "Epoch 0: |          | 1363/? [32:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1363, loss 4.45993709564209\n",
      "Epoch 0: |          | 1364/? [32:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1364, loss 5.053547382354736\n",
      "Epoch 0: |          | 1365/? [32:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1365, loss 4.63238525390625\n",
      "Epoch 0: |          | 1366/? [32:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1366, loss 5.416875839233398\n",
      "Epoch 0: |          | 1367/? [32:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1367, loss 4.684871673583984\n",
      "Epoch 0: |          | 1368/? [32:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1368, loss 4.536036491394043\n",
      "Epoch 0: |          | 1369/? [32:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1369, loss 5.186959266662598\n",
      "Epoch 0: |          | 1370/? [32:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1370, loss 4.566131591796875\n",
      "Epoch 0: |          | 1371/? [32:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1371, loss 5.782101154327393\n",
      "Epoch 0: |          | 1372/? [32:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1372, loss 4.890935897827148\n",
      "Epoch 0: |          | 1373/? [32:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1373, loss 5.585109710693359\n",
      "Epoch 0: |          | 1374/? [32:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1374, loss 4.521291255950928\n",
      "Epoch 0: |          | 1375/? [32:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1375, loss 5.175175189971924\n",
      "Epoch 0: |          | 1376/? [32:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1376, loss 5.263485908508301\n",
      "Epoch 0: |          | 1377/? [32:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1377, loss 5.068160057067871\n",
      "Epoch 0: |          | 1378/? [32:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1378, loss 5.4885759353637695\n",
      "Epoch 0: |          | 1379/? [32:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1379, loss 5.1959452629089355\n",
      "Epoch 0: |          | 1380/? [32:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1380, loss 5.260376930236816\n",
      "Epoch 0: |          | 1381/? [32:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1381, loss 5.42246150970459\n",
      "Epoch 0: |          | 1382/? [32:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1382, loss 4.894505977630615\n",
      "Epoch 0: |          | 1383/? [32:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1383, loss 5.330994606018066\n",
      "Epoch 0: |          | 1384/? [32:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1384, loss 5.2656097412109375\n",
      "Epoch 0: |          | 1385/? [32:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1385, loss 5.114206790924072\n",
      "Epoch 0: |          | 1386/? [32:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1386, loss 5.025406360626221\n",
      "Epoch 0: |          | 1387/? [32:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1387, loss 5.128985404968262\n",
      "Epoch 0: |          | 1388/? [32:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1388, loss 4.290024280548096\n",
      "Epoch 0: |          | 1389/? [32:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1389, loss 5.409974575042725\n",
      "Epoch 0: |          | 1390/? [32:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1390, loss 5.717995643615723\n",
      "Epoch 0: |          | 1391/? [33:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1391, loss 5.361952304840088\n",
      "Epoch 0: |          | 1392/? [33:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1392, loss 4.715340614318848\n",
      "Epoch 0: |          | 1393/? [33:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1393, loss 5.042505741119385\n",
      "Epoch 0: |          | 1394/? [33:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1394, loss 4.906948566436768\n",
      "Epoch 0: |          | 1395/? [33:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1395, loss 5.324095249176025\n",
      "Epoch 0: |          | 1396/? [33:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1396, loss 5.305258750915527\n",
      "Epoch 0: |          | 1397/? [33:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1397, loss 4.332468032836914\n",
      "Epoch 0: |          | 1398/? [33:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1398, loss 5.540627479553223\n",
      "Epoch 0: |          | 1399/? [33:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1399, loss 5.701290130615234\n",
      "Epoch 0: |          | 1400/? [33:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1400, loss 4.588866233825684\n",
      "Epoch 0: |          | 1401/? [33:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1401, loss 5.672198295593262\n",
      "Epoch 0: |          | 1402/? [33:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1402, loss 5.153472423553467\n",
      "Epoch 0: |          | 1403/? [33:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1403, loss 5.332601547241211\n",
      "Epoch 0: |          | 1404/? [33:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1404, loss 5.282828330993652\n",
      "Epoch 0: |          | 1405/? [33:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1405, loss 5.647048473358154\n",
      "Epoch 0: |          | 1406/? [33:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1406, loss 5.47117805480957\n",
      "Epoch 0: |          | 1407/? [33:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1407, loss 5.738332271575928\n",
      "Epoch 0: |          | 1408/? [33:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1408, loss 4.894155025482178\n",
      "Epoch 0: |          | 1409/? [33:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1409, loss 5.054389953613281\n",
      "Epoch 0: |          | 1410/? [33:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1410, loss 5.1222639083862305\n",
      "Epoch 0: |          | 1411/? [33:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1411, loss 5.500499725341797\n",
      "Epoch 0: |          | 1412/? [33:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1412, loss 4.764176368713379\n",
      "Epoch 0: |          | 1413/? [33:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1413, loss 4.704094886779785\n",
      "Epoch 0: |          | 1414/? [33:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1414, loss 4.707337856292725\n",
      "Epoch 0: |          | 1415/? [33:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1415, loss 5.2174153327941895\n",
      "Epoch 0: |          | 1416/? [33:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1416, loss 5.579158782958984\n",
      "Epoch 0: |          | 1417/? [33:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1417, loss 5.010112285614014\n",
      "Epoch 0: |          | 1418/? [33:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1418, loss 5.3768815994262695\n",
      "Epoch 0: |          | 1419/? [33:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1419, loss 5.058577537536621\n",
      "Epoch 0: |          | 1420/? [33:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1420, loss 5.0270185470581055\n",
      "Epoch 0: |          | 1421/? [33:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1421, loss 4.450214385986328\n",
      "Epoch 0: |          | 1422/? [33:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1422, loss 5.495448112487793\n",
      "Epoch 0: |          | 1423/? [33:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1423, loss 5.583038806915283\n",
      "Epoch 0: |          | 1424/? [33:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1424, loss 5.051272392272949\n",
      "Epoch 0: |          | 1425/? [33:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1425, loss 5.345134258270264\n",
      "Epoch 0: |          | 1426/? [33:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1426, loss 4.8544840812683105\n",
      "Epoch 0: |          | 1427/? [33:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1427, loss 5.393960952758789\n",
      "Epoch 0: |          | 1428/? [33:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1428, loss 5.499309062957764\n",
      "Epoch 0: |          | 1429/? [33:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1429, loss 5.422026634216309\n",
      "Epoch 0: |          | 1430/? [33:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1430, loss 5.447482109069824\n",
      "Epoch 0: |          | 1431/? [33:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1431, loss 5.289708137512207\n",
      "Epoch 0: |          | 1432/? [33:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1432, loss 5.258898735046387\n",
      "Epoch 0: |          | 1433/? [34:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1433, loss 5.123265743255615\n",
      "Epoch 0: |          | 1434/? [34:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1434, loss 5.206978797912598\n",
      "Epoch 0: |          | 1435/? [34:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1435, loss 4.768283367156982\n",
      "Epoch 0: |          | 1436/? [34:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1436, loss 5.137115955352783\n",
      "Epoch 0: |          | 1437/? [34:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1437, loss 4.345133304595947\n",
      "Epoch 0: |          | 1438/? [34:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1438, loss 6.2868499755859375\n",
      "Epoch 0: |          | 1439/? [34:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1439, loss 5.442835330963135\n",
      "Epoch 0: |          | 1440/? [34:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1440, loss 5.333828449249268\n",
      "Epoch 0: |          | 1441/? [34:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1441, loss 5.590056419372559\n",
      "Epoch 0: |          | 1442/? [34:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1442, loss 5.7743611335754395\n",
      "Epoch 0: |          | 1443/? [34:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1443, loss 4.771129608154297\n",
      "Epoch 0: |          | 1444/? [34:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1444, loss 4.90560245513916\n",
      "Epoch 0: |          | 1445/? [34:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1445, loss 5.549822807312012\n",
      "Epoch 0: |          | 1446/? [34:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1446, loss 5.119140625\n",
      "Epoch 0: |          | 1447/? [34:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1447, loss 5.187727451324463\n",
      "Epoch 0: |          | 1448/? [34:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1448, loss 4.958066463470459\n",
      "Epoch 0: |          | 1449/? [34:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1449, loss 5.242671012878418\n",
      "Epoch 0: |          | 1450/? [34:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1450, loss 5.446192741394043\n",
      "Epoch 0: |          | 1451/? [34:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1451, loss 5.743088245391846\n",
      "Epoch 0: |          | 1452/? [34:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1452, loss 5.191440582275391\n",
      "Epoch 0: |          | 1453/? [34:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1453, loss 4.378260135650635\n",
      "Epoch 0: |          | 1454/? [34:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1454, loss 5.061446666717529\n",
      "Epoch 0: |          | 1455/? [34:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1455, loss 5.252503395080566\n",
      "Epoch 0: |          | 1456/? [34:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1456, loss 4.830161094665527\n",
      "Epoch 0: |          | 1457/? [34:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1457, loss 4.979517936706543\n",
      "Epoch 0: |          | 1458/? [34:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1458, loss 4.977823257446289\n",
      "Epoch 0: |          | 1459/? [34:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1459, loss 5.338297367095947\n",
      "Epoch 0: |          | 1460/? [34:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1460, loss 5.130276203155518\n",
      "Epoch 0: |          | 1461/? [34:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1461, loss 5.314597129821777\n",
      "Epoch 0: |          | 1462/? [34:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1462, loss 5.626411437988281\n",
      "Epoch 0: |          | 1463/? [34:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1463, loss 5.482771873474121\n",
      "Epoch 0: |          | 1464/? [34:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1464, loss 4.801214218139648\n",
      "Epoch 0: |          | 1465/? [34:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1465, loss 5.046309471130371\n",
      "Epoch 0: |          | 1466/? [34:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1466, loss 4.677956581115723\n",
      "Epoch 0: |          | 1467/? [34:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1467, loss 5.553024768829346\n",
      "Epoch 0: |          | 1468/? [34:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1468, loss 5.209187984466553\n",
      "Epoch 0: |          | 1469/? [34:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1469, loss 4.533807277679443\n",
      "Epoch 0: |          | 1470/? [34:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1470, loss 5.483939170837402\n",
      "Epoch 0: |          | 1471/? [34:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1471, loss 5.379063606262207\n",
      "Epoch 0: |          | 1472/? [34:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1472, loss 5.033324718475342\n",
      "Epoch 0: |          | 1473/? [34:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1473, loss 5.034555435180664\n",
      "Epoch 0: |          | 1474/? [34:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1474, loss 4.803465366363525\n",
      "Epoch 0: |          | 1475/? [35:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1475, loss 4.527550220489502\n",
      "Epoch 0: |          | 1476/? [35:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1476, loss 5.167355537414551\n",
      "Epoch 0: |          | 1477/? [35:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1477, loss 5.219377040863037\n",
      "Epoch 0: |          | 1478/? [35:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1478, loss 5.028618812561035\n",
      "Epoch 0: |          | 1479/? [35:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1479, loss 5.749260425567627\n",
      "Epoch 0: |          | 1480/? [35:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1480, loss 5.372359275817871\n",
      "Epoch 0: |          | 1481/? [35:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1481, loss 5.095012187957764\n",
      "Epoch 0: |          | 1482/? [35:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1482, loss 5.167092323303223\n",
      "Epoch 0: |          | 1483/? [35:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1483, loss 4.656703948974609\n",
      "Epoch 0: |          | 1484/? [35:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1484, loss 4.984201908111572\n",
      "Epoch 0: |          | 1485/? [35:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1485, loss 5.355218887329102\n",
      "Epoch 0: |          | 1486/? [35:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1486, loss 5.064866065979004\n",
      "Epoch 0: |          | 1487/? [35:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1487, loss 4.659735679626465\n",
      "Epoch 0: |          | 1488/? [35:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1488, loss 5.350327014923096\n",
      "Epoch 0: |          | 1489/? [35:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1489, loss 5.229691505432129\n",
      "Epoch 0: |          | 1490/? [35:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1490, loss 5.181733131408691\n",
      "Epoch 0: |          | 1491/? [35:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1491, loss 3.76798939704895\n",
      "Epoch 0: |          | 1492/? [35:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1492, loss 4.5833024978637695\n",
      "Epoch 0: |          | 1493/? [35:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1493, loss 4.856253147125244\n",
      "Epoch 0: |          | 1494/? [35:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1494, loss 5.0544233322143555\n",
      "Epoch 0: |          | 1495/? [35:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1495, loss 4.977380275726318\n",
      "Epoch 0: |          | 1496/? [35:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1496, loss 5.3421759605407715\n",
      "Epoch 0: |          | 1497/? [35:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1497, loss 4.488575458526611\n",
      "Epoch 0: |          | 1498/? [35:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1498, loss 4.80660343170166\n",
      "Epoch 0: |          | 1499/? [35:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1499, loss 5.52459192276001\n",
      "Epoch 0: |          | 1500/? [35:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1500, loss 5.3510026931762695\n",
      "Epoch 0: |          | 1501/? [35:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1501, loss 5.067402362823486\n",
      "Epoch 0: |          | 1502/? [35:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1502, loss 5.3265790939331055\n",
      "Epoch 0: |          | 1503/? [35:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1503, loss 4.900180816650391\n",
      "Epoch 0: |          | 1504/? [35:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1504, loss 5.500729084014893\n",
      "Epoch 0: |          | 1505/? [35:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1505, loss 5.530543327331543\n",
      "Epoch 0: |          | 1506/? [35:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1506, loss 5.1703996658325195\n",
      "Epoch 0: |          | 1507/? [35:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1507, loss 4.853611946105957\n",
      "Epoch 0: |          | 1508/? [35:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1508, loss 5.028848171234131\n",
      "Epoch 0: |          | 1509/? [35:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1509, loss 5.016819000244141\n",
      "Epoch 0: |          | 1510/? [35:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1510, loss 5.317966461181641\n",
      "Epoch 0: |          | 1511/? [35:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1511, loss 4.787156581878662\n",
      "Epoch 0: |          | 1512/? [35:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1512, loss 5.732970714569092\n",
      "Epoch 0: |          | 1513/? [35:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1513, loss 5.563747882843018\n",
      "Epoch 0: |          | 1514/? [35:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1514, loss 4.711181163787842\n",
      "Epoch 0: |          | 1515/? [36:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1515, loss 5.812877655029297\n",
      "Epoch 0: |          | 1516/? [36:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1516, loss 5.329838752746582\n",
      "Epoch 0: |          | 1517/? [36:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1517, loss 4.944758892059326\n",
      "Epoch 0: |          | 1518/? [36:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1518, loss 4.715916633605957\n",
      "Epoch 0: |          | 1519/? [36:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1519, loss 5.285696983337402\n",
      "Epoch 0: |          | 1520/? [36:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1520, loss 5.645289421081543\n",
      "Epoch 0: |          | 1521/? [36:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1521, loss 5.109909534454346\n",
      "Epoch 0: |          | 1522/? [36:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1522, loss 4.7331647872924805\n",
      "Epoch 0: |          | 1523/? [36:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1523, loss 5.122737884521484\n",
      "Epoch 0: |          | 1524/? [36:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1524, loss 5.161492347717285\n",
      "Epoch 0: |          | 1525/? [36:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1525, loss 5.052542686462402\n",
      "Epoch 0: |          | 1526/? [36:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1526, loss 5.356373310089111\n",
      "Epoch 0: |          | 1527/? [36:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1527, loss 5.336777687072754\n",
      "Epoch 0: |          | 1528/? [36:18<00:00,  0.70it/s, v_num=31]ERROR: Input has inproper shape\n",
      "Epoch 0: |          | 1529/? [36:18<00:00,  0.70it/s, v_num=31]   VALIDATION: Batch 0, loss 5.458967685699463\n",
      "   VALIDATION: Batch 1, loss 4.351249694824219\n",
      "   VALIDATION: Batch 2, loss 5.631401062011719\n",
      "   VALIDATION: Batch 3, loss 5.281502723693848\n",
      "   VALIDATION: Batch 4, loss 4.890976428985596\n",
      "   VALIDATION: Batch 5, loss 4.414787769317627\n",
      "   VALIDATION: Batch 6, loss 4.722820281982422\n",
      "   VALIDATION: Batch 7, loss 5.33770227432251\n",
      "   VALIDATION: Batch 8, loss 5.341500282287598\n",
      "   VALIDATION: Batch 9, loss 5.350794792175293\n",
      "   VALIDATION: Batch 10, loss 5.149409294128418\n",
      "   VALIDATION: Batch 11, loss 4.690842628479004\n",
      "   VALIDATION: Batch 12, loss 5.183058738708496\n",
      "   VALIDATION: Batch 13, loss 5.444033622741699\n",
      "   VALIDATION: Batch 14, loss 5.056929588317871\n",
      "   VALIDATION: Batch 15, loss 4.866189956665039\n",
      "   VALIDATION: Batch 16, loss 5.329022407531738\n",
      "   VALIDATION: Batch 17, loss 5.0133562088012695\n",
      "   VALIDATION: Batch 18, loss 4.188546180725098\n",
      "   VALIDATION: Batch 19, loss 5.302206516265869\n",
      "   VALIDATION: Batch 20, loss 5.453643321990967\n",
      "   VALIDATION: Batch 21, loss 5.665043830871582\n",
      "   VALIDATION: Batch 22, loss 5.300169467926025\n",
      "   VALIDATION: Batch 23, loss 4.887953758239746\n",
      "   VALIDATION: Batch 24, loss 4.804270267486572\n",
      "   VALIDATION: Batch 25, loss 5.216854095458984\n",
      "   VALIDATION: Batch 26, loss 5.3439435958862305\n",
      "   VALIDATION: Batch 27, loss 5.291738986968994\n",
      "   VALIDATION: Batch 28, loss 5.00102424621582\n",
      "   VALIDATION: Batch 29, loss 5.274949073791504\n",
      "   VALIDATION: Batch 30, loss 4.892186164855957\n",
      "   VALIDATION: Batch 31, loss 5.087521553039551\n",
      "   VALIDATION: Batch 32, loss 5.768582344055176\n",
      "   VALIDATION: Batch 33, loss 3.812073230743408\n",
      "   VALIDATION: Batch 34, loss 5.0067596435546875\n",
      "   VALIDATION: Batch 35, loss 5.313029766082764\n",
      "   VALIDATION: Batch 36, loss 4.884449481964111\n",
      "   VALIDATION: Batch 37, loss 4.626602649688721\n",
      "   VALIDATION: Batch 38, loss 4.65914249420166\n",
      "   VALIDATION: Batch 39, loss 5.09812068939209\n",
      "   VALIDATION: Batch 40, loss 5.276578426361084\n",
      "   VALIDATION: Batch 41, loss 4.1848835945129395\n",
      "   VALIDATION: Batch 42, loss 5.330365180969238\n",
      "   VALIDATION: Batch 43, loss 5.3941330909729\n",
      "   VALIDATION: Batch 44, loss 4.871964454650879\n",
      "   VALIDATION: Batch 45, loss 5.454440116882324\n",
      "   VALIDATION: Batch 46, loss 4.604061126708984\n",
      "   VALIDATION: Batch 47, loss 5.5592217445373535\n",
      "   VALIDATION: Batch 48, loss 5.504982948303223\n",
      "   VALIDATION: Batch 49, loss 5.321921348571777\n",
      "   VALIDATION: Batch 50, loss 5.217212677001953\n",
      "   VALIDATION: Batch 51, loss 5.663867950439453\n",
      "   VALIDATION: Batch 52, loss 4.685962677001953\n",
      "   VALIDATION: Batch 53, loss 4.616586208343506\n",
      "   VALIDATION: Batch 54, loss 4.719209671020508\n",
      "   VALIDATION: Batch 55, loss 5.618105888366699\n",
      "   VALIDATION: Batch 56, loss 5.069451332092285\n",
      "   VALIDATION: Batch 57, loss 6.594168663024902\n",
      "   VALIDATION: Batch 58, loss 5.0473551750183105\n",
      "   VALIDATION: Batch 59, loss 4.6888837814331055\n",
      "   VALIDATION: Batch 60, loss 4.145136833190918\n",
      "   VALIDATION: Batch 61, loss 5.0565080642700195\n",
      "   VALIDATION: Batch 62, loss 5.083772659301758\n",
      "   VALIDATION: Batch 63, loss 5.476550102233887\n",
      "   VALIDATION: Batch 64, loss 5.342844009399414\n",
      "   VALIDATION: Batch 65, loss 4.514805793762207\n",
      "   VALIDATION: Batch 66, loss 5.399510860443115\n",
      "   VALIDATION: Batch 67, loss 4.802541732788086\n",
      "   VALIDATION: Batch 68, loss 5.022564888000488\n",
      "   VALIDATION: Batch 69, loss 5.360027313232422\n",
      "   VALIDATION: Batch 70, loss 5.538944244384766\n",
      "   VALIDATION: Batch 71, loss 4.9070940017700195\n",
      "   VALIDATION: Batch 72, loss 5.8593854904174805\n",
      "   VALIDATION: Batch 73, loss 4.690748691558838\n",
      "   VALIDATION: Batch 74, loss 5.31056022644043\n",
      "   VALIDATION: Batch 75, loss 5.4768877029418945\n",
      "   VALIDATION: Batch 76, loss 5.252984046936035\n",
      "   VALIDATION: Batch 77, loss 5.360377311706543\n",
      "   VALIDATION: Batch 78, loss 5.240307807922363\n",
      "   VALIDATION: Batch 79, loss 5.106048583984375\n",
      "   VALIDATION: Batch 80, loss 5.350266456604004\n",
      "   VALIDATION: Batch 81, loss 4.960397243499756\n",
      "   VALIDATION: Batch 82, loss 5.375787734985352\n",
      "   VALIDATION: Batch 83, loss 4.692724227905273\n",
      "   VALIDATION: Batch 84, loss 5.327620506286621\n",
      "   VALIDATION: Batch 85, loss 5.12709903717041\n",
      "   VALIDATION: Batch 86, loss 5.056307792663574\n",
      "   VALIDATION: Batch 87, loss 4.909945487976074\n",
      "   VALIDATION: Batch 88, loss 4.522916316986084\n",
      "   VALIDATION: Batch 89, loss 4.797800064086914\n",
      "   VALIDATION: Batch 90, loss 5.031432151794434\n",
      "   VALIDATION: Batch 91, loss 5.421750068664551\n",
      "   VALIDATION: Batch 92, loss 5.138516426086426\n",
      "   VALIDATION: Batch 93, loss 5.523442268371582\n",
      "   VALIDATION: Batch 94, loss 5.0316057205200195\n",
      "   VALIDATION: Batch 95, loss 4.502579689025879\n",
      "   VALIDATION: Batch 96, loss 4.965395927429199\n",
      "   VALIDATION: Batch 97, loss 4.818415641784668\n",
      "   VALIDATION: Batch 98, loss 5.185086250305176\n",
      "   VALIDATION: Batch 99, loss 5.329739570617676\n",
      "   VALIDATION: Batch 100, loss 5.681623935699463\n",
      "   VALIDATION: Batch 101, loss 4.294598579406738\n",
      "   VALIDATION: Batch 102, loss 5.785583972930908\n",
      "   VALIDATION: Batch 103, loss 5.6482062339782715\n",
      "   VALIDATION: Batch 104, loss 4.586243629455566\n",
      "   VALIDATION: Batch 105, loss 5.214630603790283\n",
      "   VALIDATION: Batch 106, loss 4.9557294845581055\n",
      "   VALIDATION: Batch 107, loss 5.069304466247559\n",
      "   VALIDATION: Batch 108, loss 4.78898286819458\n",
      "   VALIDATION: Batch 109, loss 5.38135290145874\n",
      "   VALIDATION: Batch 110, loss 5.2361249923706055\n",
      "   VALIDATION: Batch 111, loss 5.487085342407227\n",
      "   VALIDATION: Batch 112, loss 6.154806613922119\n",
      "   VALIDATION: Batch 113, loss 5.655503273010254\n",
      "   VALIDATION: Batch 114, loss 5.292881965637207\n",
      "   VALIDATION: Batch 115, loss 4.83493185043335\n",
      "   VALIDATION: Batch 116, loss 4.611028671264648\n",
      "   VALIDATION: Batch 117, loss 5.293181896209717\n",
      "   VALIDATION: Batch 118, loss 5.601830005645752\n",
      "   VALIDATION: Batch 119, loss 4.669756889343262\n",
      "   VALIDATION: Batch 120, loss 4.156768798828125\n",
      "   VALIDATION: Batch 121, loss 4.6048407554626465\n",
      "   VALIDATION: Batch 122, loss 5.066592216491699\n",
      "   VALIDATION: Batch 123, loss 5.195242881774902\n",
      "   VALIDATION: Batch 124, loss 4.2805094718933105\n",
      "   VALIDATION: Batch 125, loss 5.03587532043457\n",
      "   VALIDATION: Batch 126, loss 5.216672897338867\n",
      "   VALIDATION: Batch 127, loss 5.142045497894287\n",
      "   VALIDATION: Batch 128, loss 5.185188293457031\n",
      "   VALIDATION: Batch 129, loss 4.938738822937012\n",
      "   VALIDATION: Batch 130, loss 4.481700897216797\n",
      "   VALIDATION: Batch 131, loss 4.340637683868408\n",
      "   VALIDATION: Batch 132, loss 5.046149253845215\n",
      "   VALIDATION: Batch 133, loss 5.270388126373291\n",
      "   VALIDATION: Batch 134, loss 5.256944179534912\n",
      "   VALIDATION: Batch 135, loss 5.456667900085449\n",
      "   VALIDATION: Batch 136, loss 5.4902191162109375\n",
      "   VALIDATION: Batch 137, loss 5.450900554656982\n",
      "   VALIDATION: Batch 138, loss 5.04332971572876\n",
      "   VALIDATION: Batch 139, loss 5.496582984924316\n",
      "   VALIDATION: Batch 140, loss 4.46865701675415\n",
      "   VALIDATION: Batch 141, loss 5.35892391204834\n",
      "   VALIDATION: Batch 142, loss 4.115705966949463\n",
      "   VALIDATION: Batch 143, loss 5.034287452697754\n",
      "   VALIDATION: Batch 144, loss 5.316212177276611\n",
      "   VALIDATION: Batch 145, loss 5.119334697723389\n",
      "   VALIDATION: Batch 146, loss 4.980650901794434\n",
      "   VALIDATION: Batch 147, loss 5.33181095123291\n",
      "   VALIDATION: Batch 148, loss 5.2763166427612305\n",
      "   VALIDATION: Batch 149, loss 5.711400032043457\n",
      "   VALIDATION: Batch 150, loss 5.555731773376465\n",
      "   VALIDATION: Batch 151, loss 5.539618492126465\n",
      "   VALIDATION: Batch 152, loss 5.110382080078125\n",
      "   VALIDATION: Batch 153, loss 5.291123867034912\n",
      "   VALIDATION: Batch 154, loss 5.15321159362793\n",
      "   VALIDATION: Batch 155, loss 4.7690629959106445\n",
      "   VALIDATION: Batch 156, loss 5.5849199295043945\n",
      "   VALIDATION: Batch 157, loss 5.435326099395752\n",
      "   VALIDATION: Batch 158, loss 4.516430854797363\n",
      "   VALIDATION: Batch 159, loss 5.091084957122803\n",
      "   VALIDATION: Batch 160, loss 5.6307244300842285\n",
      "   VALIDATION: Batch 161, loss 5.738315582275391\n",
      "   VALIDATION: Batch 162, loss 5.2670488357543945\n",
      "   VALIDATION: Batch 163, loss 4.548380374908447\n",
      "   VALIDATION: Batch 164, loss 5.028430461883545\n",
      "   VALIDATION: Batch 165, loss 5.482747554779053\n",
      "   VALIDATION: Batch 166, loss 5.186792850494385\n",
      "   VALIDATION: Batch 167, loss 5.354666709899902\n",
      "   VALIDATION: Batch 168, loss 4.18425989151001\n",
      "   VALIDATION: Batch 169, loss 4.91727352142334\n",
      "   VALIDATION: Batch 170, loss 5.096532821655273\n",
      "   VALIDATION: Batch 171, loss 5.15539026260376\n",
      "   VALIDATION: Batch 172, loss 5.294394016265869\n",
      "   VALIDATION: Batch 173, loss 5.367528438568115\n",
      "   VALIDATION: Batch 174, loss 5.577456474304199\n",
      "   VALIDATION: Batch 175, loss 5.143482685089111\n",
      "   VALIDATION: Batch 176, loss 4.893160820007324\n",
      "   VALIDATION: Batch 177, loss 5.216396331787109\n",
      "   VALIDATION: Batch 178, loss 5.889216899871826\n",
      "   VALIDATION: Batch 179, loss 5.283426761627197\n",
      "   VALIDATION: Batch 180, loss 4.812872886657715\n",
      "   VALIDATION: Batch 181, loss 5.1094136238098145\n",
      "   VALIDATION: Batch 182, loss 5.265738487243652\n",
      "   VALIDATION: Batch 183, loss 4.233639717102051\n",
      "   VALIDATION: Batch 184, loss 4.0450005531311035\n",
      "   VALIDATION: Batch 185, loss 4.735353946685791\n",
      "   VALIDATION: Batch 186, loss 4.773531913757324\n",
      "   VALIDATION: Batch 187, loss 5.121581077575684\n",
      "   VALIDATION: Batch 188, loss 5.33005428314209\n",
      "   VALIDATION: Batch 189, loss 4.6512131690979\n",
      "   VALIDATION: Batch 190, loss 4.611033916473389\n",
      "   VALIDATION: Batch 191, loss 5.285457611083984\n",
      "   VALIDATION: Batch 192, loss 5.73583459854126\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=31]              TRRAINING: Batch 0, loss 5.194523811340332\n",
      "Epoch 1: |          | 1/? [00:01<00:00,  0.58it/s, v_num=31]   TRRAINING: Batch 1, loss 4.672539710998535\n",
      "Epoch 1: |          | 2/? [00:03<00:00,  0.63it/s, v_num=31]   TRRAINING: Batch 2, loss 4.8607659339904785\n",
      "Epoch 1: |          | 3/? [00:04<00:00,  0.64it/s, v_num=31]   TRRAINING: Batch 3, loss 4.470240116119385\n",
      "Epoch 1: |          | 4/? [00:06<00:00,  0.66it/s, v_num=31]   TRRAINING: Batch 4, loss 4.899313926696777\n",
      "Epoch 1: |          | 5/? [00:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 5, loss 5.783915996551514\n",
      "Epoch 1: |          | 6/? [00:08<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 6, loss 5.644680976867676\n",
      "Epoch 1: |          | 7/? [00:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 7, loss 4.861500263214111\n",
      "Epoch 1: |          | 8/? [00:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 8, loss 4.853745460510254\n",
      "Epoch 1: |          | 9/? [00:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 9, loss 5.141365051269531\n",
      "Epoch 1: |          | 10/? [00:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 10, loss 5.36446475982666\n",
      "Epoch 1: |          | 11/? [00:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 11, loss 5.266646862030029\n",
      "Epoch 1: |          | 12/? [00:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 12, loss 6.513545036315918\n",
      "Epoch 1: |          | 13/? [00:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 13, loss 5.088921546936035\n",
      "Epoch 1: |          | 14/? [00:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 14, loss 5.349489688873291\n",
      "Epoch 1: |          | 15/? [00:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 15, loss 4.699807643890381\n",
      "Epoch 1: |          | 16/? [00:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 16, loss 4.259360313415527\n",
      "Epoch 1: |          | 17/? [00:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 17, loss 5.65255880355835\n",
      "Epoch 1: |          | 18/? [00:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 18, loss 5.094109535217285\n",
      "Epoch 1: |          | 19/? [00:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 19, loss 4.8991193771362305\n",
      "Epoch 1: |          | 20/? [00:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 20, loss 5.282382011413574\n",
      "Epoch 1: |          | 21/? [00:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 21, loss 5.358288764953613\n",
      "Epoch 1: |          | 22/? [00:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 22, loss 5.137969017028809\n",
      "Epoch 1: |          | 23/? [00:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 23, loss 4.4818644523620605\n",
      "Epoch 1: |          | 24/? [00:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 24, loss 5.1238484382629395\n",
      "Epoch 1: |          | 25/? [00:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 25, loss 5.184632778167725\n",
      "Epoch 1: |          | 26/? [00:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 26, loss 4.944084167480469\n",
      "Epoch 1: |          | 27/? [00:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 27, loss 4.675055027008057\n",
      "Epoch 1: |          | 28/? [00:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 28, loss 5.56990909576416\n",
      "Epoch 1: |          | 29/? [00:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 29, loss 5.198578834533691\n",
      "Epoch 1: |          | 30/? [00:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 30, loss 5.078497886657715\n",
      "Epoch 1: |          | 31/? [00:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 31, loss 5.702834129333496\n",
      "Epoch 1: |          | 32/? [00:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 32, loss 5.2546586990356445\n",
      "Epoch 1: |          | 33/? [00:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 33, loss 5.023035526275635\n",
      "Epoch 1: |          | 34/? [00:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 34, loss 4.92267370223999\n",
      "Epoch 1: |          | 35/? [00:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 35, loss 4.200817584991455\n",
      "Epoch 1: |          | 36/? [00:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 36, loss 5.364096164703369\n",
      "Epoch 1: |          | 37/? [00:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 37, loss 5.343813896179199\n",
      "Epoch 1: |          | 38/? [00:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 38, loss 5.634072303771973\n",
      "Epoch 1: |          | 39/? [00:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 39, loss 5.551946640014648\n",
      "Epoch 1: |          | 40/? [00:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 40, loss 5.046637535095215\n",
      "Epoch 1: |          | 41/? [00:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 41, loss 4.975005626678467\n",
      "Epoch 1: |          | 42/? [01:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 42, loss 4.845167636871338\n",
      "Epoch 1: |          | 43/? [01:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 43, loss 4.950070381164551\n",
      "Epoch 1: |          | 44/? [01:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 44, loss 4.375308036804199\n",
      "Epoch 1: |          | 45/? [01:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 45, loss 4.643759727478027\n",
      "Epoch 1: |          | 46/? [01:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 46, loss 5.789040565490723\n",
      "Epoch 1: |          | 47/? [01:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 47, loss 4.901165008544922\n",
      "Epoch 1: |          | 48/? [01:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 48, loss 4.62370491027832\n",
      "Epoch 1: |          | 49/? [01:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 49, loss 4.964970588684082\n",
      "Epoch 1: |          | 50/? [01:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 50, loss 4.964346408843994\n",
      "Epoch 1: |          | 51/? [01:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 51, loss 5.070189952850342\n",
      "Epoch 1: |          | 52/? [01:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 52, loss 5.437906742095947\n",
      "Epoch 1: |          | 53/? [01:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 53, loss 5.049773216247559\n",
      "Epoch 1: |          | 54/? [01:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 54, loss 4.974089622497559\n",
      "Epoch 1: |          | 55/? [01:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 55, loss 5.122570991516113\n",
      "Epoch 1: |          | 56/? [01:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 56, loss 5.145682334899902\n",
      "Epoch 1: |          | 57/? [01:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 57, loss 5.124135971069336\n",
      "Epoch 1: |          | 58/? [01:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 58, loss 6.361510276794434\n",
      "Epoch 1: |          | 59/? [01:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 59, loss 5.336855888366699\n",
      "Epoch 1: |          | 60/? [01:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 60, loss 5.383633613586426\n",
      "Epoch 1: |          | 61/? [01:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 61, loss 5.429511070251465\n",
      "Epoch 1: |          | 62/? [01:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 62, loss 4.907376289367676\n",
      "Epoch 1: |          | 63/? [01:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 63, loss 5.1801676750183105\n",
      "Epoch 1: |          | 64/? [01:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 64, loss 5.027705192565918\n",
      "Epoch 1: |          | 65/? [01:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 65, loss 5.1225504875183105\n",
      "Epoch 1: |          | 66/? [01:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 66, loss 4.47888708114624\n",
      "Epoch 1: |          | 67/? [01:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 67, loss 5.085444450378418\n",
      "Epoch 1: |          | 68/? [01:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 68, loss 5.455018520355225\n",
      "Epoch 1: |          | 69/? [01:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 69, loss 5.027218341827393\n",
      "Epoch 1: |          | 70/? [01:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 70, loss 4.723739147186279\n",
      "Epoch 1: |          | 71/? [01:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 71, loss 4.633955955505371\n",
      "Epoch 1: |          | 72/? [01:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 72, loss 5.229343891143799\n",
      "Epoch 1: |          | 73/? [01:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 73, loss 5.353208065032959\n",
      "Epoch 1: |          | 74/? [01:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 74, loss 4.731385231018066\n",
      "Epoch 1: |          | 75/? [01:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 75, loss 4.973249912261963\n",
      "Epoch 1: |          | 76/? [01:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 76, loss 4.990302085876465\n",
      "Epoch 1: |          | 77/? [01:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 77, loss 5.130072593688965\n",
      "Epoch 1: |          | 78/? [01:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 78, loss 4.651180744171143\n",
      "Epoch 1: |          | 79/? [01:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 79, loss 4.95954704284668\n",
      "Epoch 1: |          | 80/? [01:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 80, loss 4.7727179527282715\n",
      "Epoch 1: |          | 81/? [01:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 81, loss 4.415276050567627\n",
      "Epoch 1: |          | 82/? [01:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 82, loss 5.382177352905273\n",
      "Epoch 1: |          | 83/? [01:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 83, loss 4.5553483963012695\n",
      "Epoch 1: |          | 84/? [01:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 84, loss 4.404256820678711\n",
      "Epoch 1: |          | 85/? [02:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 85, loss 4.494604587554932\n",
      "Epoch 1: |          | 86/? [02:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 86, loss 4.430182933807373\n",
      "Epoch 1: |          | 87/? [02:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 87, loss 4.58258056640625\n",
      "Epoch 1: |          | 88/? [02:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 88, loss 5.610823631286621\n",
      "Epoch 1: |          | 89/? [02:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 89, loss 5.294283866882324\n",
      "Epoch 1: |          | 90/? [02:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 90, loss 5.165850639343262\n",
      "Epoch 1: |          | 91/? [02:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 91, loss 5.098198413848877\n",
      "Epoch 1: |          | 92/? [02:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 92, loss 5.463360786437988\n",
      "Epoch 1: |          | 93/? [02:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 93, loss 5.465582847595215\n",
      "Epoch 1: |          | 94/? [02:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 94, loss 5.37517786026001\n",
      "Epoch 1: |          | 95/? [02:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 95, loss 6.226917743682861\n",
      "Epoch 1: |          | 96/? [02:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 96, loss 4.734096527099609\n",
      "Epoch 1: |          | 97/? [02:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 97, loss 4.772016525268555\n",
      "Epoch 1: |          | 98/? [02:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 98, loss 5.171944618225098\n",
      "Epoch 1: |          | 99/? [02:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 99, loss 5.311935901641846\n",
      "Epoch 1: |          | 100/? [02:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 100, loss 5.382925987243652\n",
      "Epoch 1: |          | 101/? [02:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 101, loss 4.995522499084473\n",
      "Epoch 1: |          | 102/? [02:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 102, loss 4.954541206359863\n",
      "Epoch 1: |          | 103/? [02:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 103, loss 4.637620449066162\n",
      "Epoch 1: |          | 104/? [02:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 104, loss 5.204064846038818\n",
      "Epoch 1: |          | 105/? [02:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 105, loss 4.972306251525879\n",
      "Epoch 1: |          | 106/? [02:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 106, loss 5.039334297180176\n",
      "Epoch 1: |          | 107/? [02:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 107, loss 5.221068859100342\n",
      "Epoch 1: |          | 108/? [02:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 108, loss 5.181166648864746\n",
      "Epoch 1: |          | 109/? [02:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 109, loss 4.566082954406738\n",
      "Epoch 1: |          | 110/? [02:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 110, loss 5.184756755828857\n",
      "Epoch 1: |          | 111/? [02:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 111, loss 5.706578254699707\n",
      "Epoch 1: |          | 112/? [02:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 112, loss 4.674877643585205\n",
      "Epoch 1: |          | 113/? [02:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 113, loss 4.050494194030762\n",
      "Epoch 1: |          | 114/? [02:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 114, loss 5.306772232055664\n",
      "Epoch 1: |          | 115/? [02:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 115, loss 5.381756782531738\n",
      "Epoch 1: |          | 116/? [02:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 116, loss 5.085465908050537\n",
      "Epoch 1: |          | 117/? [02:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 117, loss 5.052046775817871\n",
      "Epoch 1: |          | 118/? [02:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 118, loss 5.311854362487793\n",
      "Epoch 1: |          | 119/? [02:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 119, loss 5.499042987823486\n",
      "Epoch 1: |          | 120/? [02:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 120, loss 5.23003625869751\n",
      "Epoch 1: |          | 121/? [02:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 121, loss 4.960291385650635\n",
      "Epoch 1: |          | 122/? [02:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 122, loss 4.460387229919434\n",
      "Epoch 1: |          | 123/? [02:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 123, loss 4.8758368492126465\n",
      "Epoch 1: |          | 124/? [02:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 124, loss 5.2160539627075195\n",
      "Epoch 1: |          | 125/? [02:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 125, loss 4.983362197875977\n",
      "Epoch 1: |          | 126/? [02:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 126, loss 5.382279872894287\n",
      "Epoch 1: |          | 127/? [03:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 127, loss 5.296567440032959\n",
      "Epoch 1: |          | 128/? [03:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 128, loss 4.3787689208984375\n",
      "Epoch 1: |          | 129/? [03:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 129, loss 5.131097793579102\n",
      "Epoch 1: |          | 130/? [03:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 130, loss 4.01663064956665\n",
      "Epoch 1: |          | 131/? [03:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 131, loss 4.988068580627441\n",
      "Epoch 1: |          | 132/? [03:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 132, loss 5.008143424987793\n",
      "Epoch 1: |          | 133/? [03:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 133, loss 5.042619705200195\n",
      "Epoch 1: |          | 134/? [03:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 134, loss 5.05251407623291\n",
      "Epoch 1: |          | 135/? [03:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 135, loss 5.294767379760742\n",
      "Epoch 1: |          | 136/? [03:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 136, loss 5.27047061920166\n",
      "Epoch 1: |          | 137/? [03:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 137, loss 4.068674564361572\n",
      "Epoch 1: |          | 138/? [03:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 138, loss 4.837022304534912\n",
      "Epoch 1: |          | 139/? [03:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 139, loss 5.440690040588379\n",
      "Epoch 1: |          | 140/? [03:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 140, loss 4.390147686004639\n",
      "Epoch 1: |          | 141/? [03:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 141, loss 4.564806938171387\n",
      "Epoch 1: |          | 142/? [03:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 142, loss 6.3558855056762695\n",
      "Epoch 1: |          | 143/? [03:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 143, loss 5.918560028076172\n",
      "Epoch 1: |          | 144/? [03:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 144, loss 4.920637130737305\n",
      "Epoch 1: |          | 145/? [03:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 145, loss 4.529640197753906\n",
      "Epoch 1: |          | 146/? [03:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 146, loss 4.81245756149292\n",
      "Epoch 1: |          | 147/? [03:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 147, loss 5.0370025634765625\n",
      "Epoch 1: |          | 148/? [03:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 148, loss 4.796130180358887\n",
      "Epoch 1: |          | 149/? [03:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 149, loss 4.115914344787598\n",
      "Epoch 1: |          | 150/? [03:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 150, loss 5.133827209472656\n",
      "Epoch 1: |          | 151/? [03:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 151, loss 5.195383071899414\n",
      "Epoch 1: |          | 152/? [03:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 152, loss 5.280798435211182\n",
      "Epoch 1: |          | 153/? [03:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 153, loss 4.258614540100098\n",
      "Epoch 1: |          | 154/? [03:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 154, loss 5.441228866577148\n",
      "Epoch 1: |          | 155/? [03:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 155, loss 5.077280521392822\n",
      "Epoch 1: |          | 156/? [03:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 156, loss 4.327750205993652\n",
      "Epoch 1: |          | 157/? [03:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 157, loss 5.1067094802856445\n",
      "Epoch 1: |          | 158/? [03:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 158, loss 5.149913311004639\n",
      "Epoch 1: |          | 159/? [03:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 159, loss 4.90703821182251\n",
      "Epoch 1: |          | 160/? [03:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 160, loss 4.556929588317871\n",
      "Epoch 1: |          | 161/? [03:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 161, loss 5.157739162445068\n",
      "Epoch 1: |          | 162/? [03:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 162, loss 5.208585262298584\n",
      "Epoch 1: |          | 163/? [03:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 163, loss 4.055517673492432\n",
      "Epoch 1: |          | 164/? [03:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 164, loss 4.53165864944458\n",
      "Epoch 1: |          | 165/? [03:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 165, loss 5.4775190353393555\n",
      "Epoch 1: |          | 166/? [03:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 166, loss 5.186701774597168\n",
      "Epoch 1: |          | 167/? [03:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 167, loss 5.302582740783691\n",
      "Epoch 1: |          | 168/? [03:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 168, loss 4.8515729904174805\n",
      "Epoch 1: |          | 169/? [03:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 169, loss 4.460653781890869\n",
      "Epoch 1: |          | 170/? [04:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 170, loss 4.736577033996582\n",
      "Epoch 1: |          | 171/? [04:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 171, loss 5.1820149421691895\n",
      "Epoch 1: |          | 172/? [04:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 172, loss 4.7769975662231445\n",
      "Epoch 1: |          | 173/? [04:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 173, loss 5.776690483093262\n",
      "Epoch 1: |          | 174/? [04:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 174, loss 5.853321552276611\n",
      "Epoch 1: |          | 175/? [04:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 175, loss 5.715076923370361\n",
      "Epoch 1: |          | 176/? [04:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 176, loss 4.747237205505371\n",
      "Epoch 1: |          | 177/? [04:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 177, loss 4.8470587730407715\n",
      "Epoch 1: |          | 178/? [04:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 178, loss 4.657410144805908\n",
      "Epoch 1: |          | 179/? [04:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 179, loss 5.384582996368408\n",
      "Epoch 1: |          | 180/? [04:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 180, loss 4.858652591705322\n",
      "Epoch 1: |          | 181/? [04:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 181, loss 4.613701820373535\n",
      "Epoch 1: |          | 182/? [04:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 182, loss 5.124273300170898\n",
      "Epoch 1: |          | 183/? [04:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 183, loss 4.521297931671143\n",
      "Epoch 1: |          | 184/? [04:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 184, loss 4.660182952880859\n",
      "Epoch 1: |          | 185/? [04:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 185, loss 5.48275899887085\n",
      "Epoch 1: |          | 186/? [04:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 186, loss 4.749129295349121\n",
      "Epoch 1: |          | 187/? [04:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 187, loss 5.353649139404297\n",
      "Epoch 1: |          | 188/? [04:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 188, loss 4.766737937927246\n",
      "Epoch 1: |          | 189/? [04:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 189, loss 5.458102226257324\n",
      "Epoch 1: |          | 190/? [04:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 190, loss 4.84468936920166\n",
      "Epoch 1: |          | 191/? [04:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 191, loss 5.7807793617248535\n",
      "Epoch 1: |          | 192/? [04:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 192, loss 5.620228290557861\n",
      "Epoch 1: |          | 193/? [04:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 193, loss 4.680181503295898\n",
      "Epoch 1: |          | 194/? [04:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 194, loss 4.6721577644348145\n",
      "Epoch 1: |          | 195/? [04:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 195, loss 5.341333866119385\n",
      "Epoch 1: |          | 196/? [04:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 196, loss 5.328855037689209\n",
      "Epoch 1: |          | 197/? [04:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 197, loss 5.098450660705566\n",
      "Epoch 1: |          | 198/? [04:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 198, loss 4.509230136871338\n",
      "Epoch 1: |          | 199/? [04:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 199, loss 5.224078178405762\n",
      "Epoch 1: |          | 200/? [04:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 200, loss 4.911725044250488\n",
      "Epoch 1: |          | 201/? [04:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 201, loss 5.342979907989502\n",
      "Epoch 1: |          | 202/? [04:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 202, loss 5.1899003982543945\n",
      "Epoch 1: |          | 203/? [04:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 203, loss 5.099592208862305\n",
      "Epoch 1: |          | 204/? [04:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 204, loss 5.101099491119385\n",
      "Epoch 1: |          | 205/? [04:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 205, loss 4.699652671813965\n",
      "Epoch 1: |          | 206/? [04:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 206, loss 4.631391525268555\n",
      "Epoch 1: |          | 207/? [04:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 207, loss 5.139885425567627\n",
      "Epoch 1: |          | 208/? [04:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 208, loss 5.096352577209473\n",
      "Epoch 1: |          | 209/? [04:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 209, loss 4.908204078674316\n",
      "Epoch 1: |          | 210/? [04:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 210, loss 5.586207389831543\n",
      "Epoch 1: |          | 211/? [04:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 211, loss 4.878657341003418\n",
      "Epoch 1: |          | 212/? [05:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 212, loss 5.048299312591553\n",
      "Epoch 1: |          | 213/? [05:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 213, loss 4.847171306610107\n",
      "Epoch 1: |          | 214/? [05:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 214, loss 4.864678382873535\n",
      "Epoch 1: |          | 215/? [05:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 215, loss 4.458271026611328\n",
      "Epoch 1: |          | 216/? [05:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 216, loss 5.221247673034668\n",
      "Epoch 1: |          | 217/? [05:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 217, loss 5.003215789794922\n",
      "Epoch 1: |          | 218/? [05:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 218, loss 5.1883931159973145\n",
      "Epoch 1: |          | 219/? [05:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 219, loss 4.988186836242676\n",
      "Epoch 1: |          | 220/? [05:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 220, loss 5.114690780639648\n",
      "Epoch 1: |          | 221/? [05:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 221, loss 4.858354091644287\n",
      "Epoch 1: |          | 222/? [05:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 222, loss 4.08780574798584\n",
      "Epoch 1: |          | 223/? [05:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 223, loss 5.509791851043701\n",
      "Epoch 1: |          | 224/? [05:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 224, loss 5.3800249099731445\n",
      "Epoch 1: |          | 225/? [05:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 225, loss 5.047650337219238\n",
      "Epoch 1: |          | 226/? [05:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 226, loss 4.845912933349609\n",
      "Epoch 1: |          | 227/? [05:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 227, loss 5.288863182067871\n",
      "Epoch 1: |          | 228/? [05:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 228, loss 4.910006999969482\n",
      "Epoch 1: |          | 229/? [05:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 229, loss 5.150881767272949\n",
      "Epoch 1: |          | 230/? [05:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 230, loss 4.905491828918457\n",
      "Epoch 1: |          | 231/? [05:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 231, loss 4.847820281982422\n",
      "Epoch 1: |          | 232/? [05:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 232, loss 4.737668037414551\n",
      "Epoch 1: |          | 233/? [05:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 233, loss 5.470592975616455\n",
      "Epoch 1: |          | 234/? [05:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 234, loss 5.462779521942139\n",
      "Epoch 1: |          | 235/? [05:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 235, loss 5.447383403778076\n",
      "Epoch 1: |          | 236/? [05:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 236, loss 4.965170383453369\n",
      "Epoch 1: |          | 237/? [05:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 237, loss 5.09079647064209\n",
      "Epoch 1: |          | 238/? [05:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 238, loss 5.272449016571045\n",
      "Epoch 1: |          | 239/? [05:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 239, loss 4.91693639755249\n",
      "Epoch 1: |          | 240/? [05:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 240, loss 4.242252349853516\n",
      "Epoch 1: |          | 241/? [05:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 241, loss 5.063928604125977\n",
      "Epoch 1: |          | 242/? [05:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 242, loss 5.433474540710449\n",
      "Epoch 1: |          | 243/? [05:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 243, loss 4.02107048034668\n",
      "Epoch 1: |          | 244/? [05:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 244, loss 4.717206954956055\n",
      "Epoch 1: |          | 245/? [05:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 245, loss 5.008292198181152\n",
      "Epoch 1: |          | 246/? [05:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 246, loss 5.180105209350586\n",
      "Epoch 1: |          | 247/? [05:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 247, loss 5.230722904205322\n",
      "Epoch 1: |          | 248/? [05:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 248, loss 4.655458927154541\n",
      "Epoch 1: |          | 249/? [05:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 249, loss 4.340163230895996\n",
      "Epoch 1: |          | 250/? [05:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 250, loss 5.019904136657715\n",
      "Epoch 1: |          | 251/? [05:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 251, loss 5.0986647605896\n",
      "Epoch 1: |          | 252/? [05:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 252, loss 4.823304176330566\n",
      "Epoch 1: |          | 253/? [05:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 253, loss 5.6126861572265625\n",
      "Epoch 1: |          | 254/? [06:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 254, loss 5.424971580505371\n",
      "Epoch 1: |          | 255/? [06:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 255, loss 4.908236503601074\n",
      "Epoch 1: |          | 256/? [06:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 256, loss 7.153903007507324\n",
      "Epoch 1: |          | 257/? [06:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 257, loss 4.747313022613525\n",
      "Epoch 1: |          | 258/? [06:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 258, loss 4.906780242919922\n",
      "Epoch 1: |          | 259/? [06:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 259, loss 4.709683418273926\n",
      "Epoch 1: |          | 260/? [06:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 260, loss 4.6677446365356445\n",
      "Epoch 1: |          | 261/? [06:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 261, loss 4.793262481689453\n",
      "Epoch 1: |          | 262/? [06:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 262, loss 5.168965816497803\n",
      "Epoch 1: |          | 263/? [06:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 263, loss 4.957117080688477\n",
      "Epoch 1: |          | 264/? [06:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 264, loss 5.029090404510498\n",
      "Epoch 1: |          | 265/? [06:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 265, loss 4.3452277183532715\n",
      "Epoch 1: |          | 266/? [06:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 266, loss 4.861733436584473\n",
      "Epoch 1: |          | 267/? [06:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 267, loss 4.438582897186279\n",
      "Epoch 1: |          | 268/? [06:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 268, loss 4.808995246887207\n",
      "Epoch 1: |          | 269/? [06:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 269, loss 5.292971134185791\n",
      "Epoch 1: |          | 270/? [06:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 270, loss 4.7960944175720215\n",
      "Epoch 1: |          | 271/? [06:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 271, loss 5.539550304412842\n",
      "Epoch 1: |          | 272/? [06:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 272, loss 5.443437576293945\n",
      "Epoch 1: |          | 273/? [06:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 273, loss 4.493344783782959\n",
      "Epoch 1: |          | 274/? [06:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 274, loss 5.666443824768066\n",
      "Epoch 1: |          | 275/? [06:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 275, loss 5.048909664154053\n",
      "Epoch 1: |          | 276/? [06:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 276, loss 4.320047855377197\n",
      "Epoch 1: |          | 277/? [06:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 277, loss 4.955638408660889\n",
      "Epoch 1: |          | 278/? [06:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 278, loss 3.8985390663146973\n",
      "Epoch 1: |          | 279/? [06:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 279, loss 4.781363010406494\n",
      "Epoch 1: |          | 280/? [06:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 280, loss 4.337624549865723\n",
      "Epoch 1: |          | 281/? [06:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 281, loss 5.235335350036621\n",
      "Epoch 1: |          | 282/? [06:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 282, loss 4.794736385345459\n",
      "Epoch 1: |          | 283/? [06:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 283, loss 4.8677592277526855\n",
      "Epoch 1: |          | 284/? [06:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 284, loss 4.763685703277588\n",
      "Epoch 1: |          | 285/? [06:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 285, loss 4.0954766273498535\n",
      "Epoch 1: |          | 286/? [06:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 286, loss 4.781355857849121\n",
      "Epoch 1: |          | 287/? [06:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 287, loss 4.605686664581299\n",
      "Epoch 1: |          | 288/? [06:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 288, loss 4.567133903503418\n",
      "Epoch 1: |          | 289/? [06:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 289, loss 4.638020992279053\n",
      "Epoch 1: |          | 290/? [06:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 290, loss 4.243907451629639\n",
      "Epoch 1: |          | 291/? [06:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 291, loss 4.98580265045166\n",
      "Epoch 1: |          | 292/? [06:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 292, loss 4.580722808837891\n",
      "Epoch 1: |          | 293/? [06:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 293, loss 5.044185161590576\n",
      "Epoch 1: |          | 294/? [06:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 294, loss 4.743435859680176\n",
      "Epoch 1: |          | 295/? [06:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 295, loss 5.151280879974365\n",
      "Epoch 1: |          | 296/? [06:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 296, loss 4.571479797363281\n",
      "Epoch 1: |          | 297/? [07:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 297, loss 5.330647945404053\n",
      "Epoch 1: |          | 298/? [07:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 298, loss 5.074941158294678\n",
      "Epoch 1: |          | 299/? [07:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 299, loss 6.040241718292236\n",
      "Epoch 1: |          | 300/? [07:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 300, loss 5.074814796447754\n",
      "Epoch 1: |          | 301/? [07:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 301, loss 4.719632625579834\n",
      "Epoch 1: |          | 302/? [07:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 302, loss 5.198782920837402\n",
      "Epoch 1: |          | 303/? [07:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 303, loss 4.921712398529053\n",
      "Epoch 1: |          | 304/? [07:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 304, loss 5.187510013580322\n",
      "Epoch 1: |          | 305/? [07:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 305, loss 5.286494255065918\n",
      "Epoch 1: |          | 306/? [07:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 306, loss 4.891117095947266\n",
      "Epoch 1: |          | 307/? [07:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 307, loss 5.123409748077393\n",
      "Epoch 1: |          | 308/? [07:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 308, loss 5.310291767120361\n",
      "Epoch 1: |          | 309/? [07:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 309, loss 5.073747634887695\n",
      "Epoch 1: |          | 310/? [07:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 310, loss 5.514726638793945\n",
      "Epoch 1: |          | 311/? [07:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 311, loss 4.9457783699035645\n",
      "Epoch 1: |          | 312/? [07:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 312, loss 5.098620414733887\n",
      "Epoch 1: |          | 313/? [07:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 313, loss 4.580593109130859\n",
      "Epoch 1: |          | 314/? [07:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 314, loss 5.04746150970459\n",
      "Epoch 1: |          | 315/? [07:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 315, loss 4.520211219787598\n",
      "Epoch 1: |          | 316/? [07:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 316, loss 5.377223014831543\n",
      "Epoch 1: |          | 317/? [07:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 317, loss 5.070734024047852\n",
      "Epoch 1: |          | 318/? [07:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 318, loss 5.276695251464844\n",
      "Epoch 1: |          | 319/? [07:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 319, loss 4.406538486480713\n",
      "Epoch 1: |          | 320/? [07:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 320, loss 4.839022159576416\n",
      "Epoch 1: |          | 321/? [07:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 321, loss 4.583049297332764\n",
      "Epoch 1: |          | 322/? [07:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 322, loss 5.341404438018799\n",
      "Epoch 1: |          | 323/? [07:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 323, loss 5.458670616149902\n",
      "Epoch 1: |          | 324/? [07:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 324, loss 4.931933403015137\n",
      "Epoch 1: |          | 325/? [07:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 325, loss 5.563591003417969\n",
      "Epoch 1: |          | 326/? [07:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 326, loss 5.061199188232422\n",
      "Epoch 1: |          | 327/? [07:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 327, loss 4.890735626220703\n",
      "Epoch 1: |          | 328/? [07:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 328, loss 4.4904656410217285\n",
      "Epoch 1: |          | 329/? [07:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 329, loss 5.150099754333496\n",
      "Epoch 1: |          | 330/? [07:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 330, loss 5.6000237464904785\n",
      "Epoch 1: |          | 331/? [07:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 331, loss 3.7017645835876465\n",
      "Epoch 1: |          | 332/? [07:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 332, loss 4.943550109863281\n",
      "Epoch 1: |          | 333/? [07:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 333, loss 4.74300479888916\n",
      "Epoch 1: |          | 334/? [07:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 334, loss 5.9575419425964355\n",
      "Epoch 1: |          | 335/? [07:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 335, loss 5.741947650909424\n",
      "Epoch 1: |          | 336/? [07:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 336, loss 5.428013801574707\n",
      "Epoch 1: |          | 337/? [07:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 337, loss 6.202252388000488\n",
      "Epoch 1: |          | 338/? [08:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 338, loss 5.662110805511475\n",
      "Epoch 1: |          | 339/? [08:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 339, loss 4.673089981079102\n",
      "Epoch 1: |          | 340/? [08:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 340, loss 5.13082218170166\n",
      "Epoch 1: |          | 341/? [08:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 341, loss 4.341760635375977\n",
      "Epoch 1: |          | 342/? [08:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 342, loss 5.116429328918457\n",
      "Epoch 1: |          | 343/? [08:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 343, loss 4.841745853424072\n",
      "Epoch 1: |          | 344/? [08:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 344, loss 5.593857765197754\n",
      "Epoch 1: |          | 345/? [08:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 345, loss 4.906778335571289\n",
      "Epoch 1: |          | 346/? [08:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 346, loss 5.123238563537598\n",
      "Epoch 1: |          | 347/? [08:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 347, loss 4.709550380706787\n",
      "Epoch 1: |          | 348/? [08:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 348, loss 4.017754554748535\n",
      "Epoch 1: |          | 349/? [08:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 349, loss 3.8135457038879395\n",
      "Epoch 1: |          | 350/? [08:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 350, loss 5.450501918792725\n",
      "Epoch 1: |          | 351/? [08:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 351, loss 5.404018402099609\n",
      "Epoch 1: |          | 352/? [08:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 352, loss 4.611541271209717\n",
      "Epoch 1: |          | 353/? [08:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 353, loss 4.298652172088623\n",
      "Epoch 1: |          | 354/? [08:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 354, loss 4.69631814956665\n",
      "Epoch 1: |          | 355/? [08:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 355, loss 5.046551704406738\n",
      "Epoch 1: |          | 356/? [08:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 356, loss 5.042679309844971\n",
      "Epoch 1: |          | 357/? [08:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 357, loss 4.600498676300049\n",
      "Epoch 1: |          | 358/? [08:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 358, loss 4.482409954071045\n",
      "Epoch 1: |          | 359/? [08:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 359, loss 5.240652561187744\n",
      "Epoch 1: |          | 360/? [08:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 360, loss 4.701934337615967\n",
      "Epoch 1: |          | 361/? [08:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 361, loss 4.7928056716918945\n",
      "Epoch 1: |          | 362/? [08:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 362, loss 4.654383182525635\n",
      "Epoch 1: |          | 363/? [08:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 363, loss 4.583954334259033\n",
      "Epoch 1: |          | 364/? [08:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 364, loss 5.198597431182861\n",
      "Epoch 1: |          | 365/? [08:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 365, loss 5.2603325843811035\n",
      "Epoch 1: |          | 366/? [08:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 366, loss 4.91155481338501\n",
      "Epoch 1: |          | 367/? [08:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 367, loss 5.043572902679443\n",
      "Epoch 1: |          | 368/? [08:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 368, loss 4.398082733154297\n",
      "Epoch 1: |          | 369/? [08:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 369, loss 4.875798225402832\n",
      "Epoch 1: |          | 370/? [08:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 370, loss 4.345441818237305\n",
      "Epoch 1: |          | 371/? [08:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 371, loss 5.487188816070557\n",
      "Epoch 1: |          | 372/? [08:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 372, loss 5.015852928161621\n",
      "Epoch 1: |          | 373/? [08:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 373, loss 5.000935077667236\n",
      "Epoch 1: |          | 374/? [08:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 374, loss 4.779155731201172\n",
      "Epoch 1: |          | 375/? [08:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 375, loss 5.490984916687012\n",
      "Epoch 1: |          | 376/? [08:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 376, loss 4.7993974685668945\n",
      "Epoch 1: |          | 377/? [08:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 377, loss 4.995490550994873\n",
      "Epoch 1: |          | 378/? [08:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 378, loss 5.195441246032715\n",
      "Epoch 1: |          | 379/? [08:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 379, loss 5.012940883636475\n",
      "Epoch 1: |          | 380/? [09:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 380, loss 5.167450904846191\n",
      "Epoch 1: |          | 381/? [09:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 381, loss 5.0292181968688965\n",
      "Epoch 1: |          | 382/? [09:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 382, loss 4.767843246459961\n",
      "Epoch 1: |          | 383/? [09:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 383, loss 4.789519309997559\n",
      "Epoch 1: |          | 384/? [09:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 384, loss 5.340670108795166\n",
      "Epoch 1: |          | 385/? [09:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 385, loss 4.9369797706604\n",
      "Epoch 1: |          | 386/? [09:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 386, loss 3.6498615741729736\n",
      "Epoch 1: |          | 387/? [09:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 387, loss 4.674833297729492\n",
      "Epoch 1: |          | 388/? [09:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 388, loss 5.386115074157715\n",
      "Epoch 1: |          | 389/? [09:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 389, loss 5.383643627166748\n",
      "Epoch 1: |          | 390/? [09:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 390, loss 4.710869789123535\n",
      "Epoch 1: |          | 391/? [09:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 391, loss 5.260188102722168\n",
      "Epoch 1: |          | 392/? [09:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 392, loss 5.241118431091309\n",
      "Epoch 1: |          | 393/? [09:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 393, loss 5.3279876708984375\n",
      "Epoch 1: |          | 394/? [09:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 394, loss 4.9319610595703125\n",
      "Epoch 1: |          | 395/? [09:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 395, loss 5.22420072555542\n",
      "Epoch 1: |          | 396/? [09:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 396, loss 5.1892194747924805\n",
      "Epoch 1: |          | 397/? [09:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 397, loss 5.015192985534668\n",
      "Epoch 1: |          | 398/? [09:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 398, loss 4.754902362823486\n",
      "Epoch 1: |          | 399/? [09:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 399, loss 4.904740810394287\n",
      "Epoch 1: |          | 400/? [09:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 400, loss 4.973174095153809\n",
      "Epoch 1: |          | 401/? [09:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 401, loss 4.801187515258789\n",
      "Epoch 1: |          | 402/? [09:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 402, loss 5.437809944152832\n",
      "Epoch 1: |          | 403/? [09:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 403, loss 5.063098907470703\n",
      "Epoch 1: |          | 404/? [09:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 404, loss 4.530350208282471\n",
      "Epoch 1: |          | 405/? [09:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 405, loss 4.5032243728637695\n",
      "Epoch 1: |          | 406/? [09:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 406, loss 4.917481899261475\n",
      "Epoch 1: |          | 407/? [09:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 407, loss 4.868953704833984\n",
      "Epoch 1: |          | 408/? [09:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 408, loss 5.2174177169799805\n",
      "Epoch 1: |          | 409/? [09:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 409, loss 5.229308605194092\n",
      "Epoch 1: |          | 410/? [09:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 410, loss 4.966769218444824\n",
      "Epoch 1: |          | 411/? [09:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 411, loss 4.709770202636719\n",
      "Epoch 1: |          | 412/? [09:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 412, loss 4.123827934265137\n",
      "Epoch 1: |          | 413/? [09:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 413, loss 5.041569709777832\n",
      "Epoch 1: |          | 414/? [09:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 414, loss 4.486051082611084\n",
      "Epoch 1: |          | 415/? [09:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 415, loss 4.990853786468506\n",
      "Epoch 1: |          | 416/? [09:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 416, loss 5.404086589813232\n",
      "Epoch 1: |          | 417/? [09:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 417, loss 5.708182334899902\n",
      "Epoch 1: |          | 418/? [09:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 418, loss 5.191435813903809\n",
      "Epoch 1: |          | 419/? [09:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 419, loss 5.239772796630859\n",
      "Epoch 1: |          | 420/? [09:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 420, loss 4.991169452667236\n",
      "Epoch 1: |          | 421/? [09:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 421, loss 5.522336006164551\n",
      "Epoch 1: |          | 422/? [09:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 422, loss 5.071206092834473\n",
      "Epoch 1: |          | 423/? [10:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 423, loss 4.532680511474609\n",
      "Epoch 1: |          | 424/? [10:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 424, loss 5.509080410003662\n",
      "Epoch 1: |          | 425/? [10:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 425, loss 5.36506462097168\n",
      "Epoch 1: |          | 426/? [10:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 426, loss 4.7706217765808105\n",
      "Epoch 1: |          | 427/? [10:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 427, loss 4.760133266448975\n",
      "Epoch 1: |          | 428/? [10:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 428, loss 5.630875110626221\n",
      "Epoch 1: |          | 429/? [10:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 429, loss 4.379397392272949\n",
      "Epoch 1: |          | 430/? [10:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 430, loss 5.193190097808838\n",
      "Epoch 1: |          | 431/? [10:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 431, loss 4.968606472015381\n",
      "Epoch 1: |          | 432/? [10:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 432, loss 5.045490741729736\n",
      "Epoch 1: |          | 433/? [10:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 433, loss 4.99794864654541\n",
      "Epoch 1: |          | 434/? [10:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 434, loss 4.96409273147583\n",
      "Epoch 1: |          | 435/? [10:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 435, loss 4.67406702041626\n",
      "Epoch 1: |          | 436/? [10:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 436, loss 5.118124961853027\n",
      "Epoch 1: |          | 437/? [10:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 437, loss 5.163727283477783\n",
      "Epoch 1: |          | 438/? [10:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 438, loss 4.737631797790527\n",
      "Epoch 1: |          | 439/? [10:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 439, loss 4.982076644897461\n",
      "Epoch 1: |          | 440/? [10:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 440, loss 4.957015037536621\n",
      "Epoch 1: |          | 441/? [10:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 441, loss 5.230627059936523\n",
      "Epoch 1: |          | 442/? [10:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 442, loss 4.80478572845459\n",
      "Epoch 1: |          | 443/? [10:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 443, loss 5.160335540771484\n",
      "Epoch 1: |          | 444/? [10:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 444, loss 4.916228771209717\n",
      "Epoch 1: |          | 445/? [10:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 445, loss 5.808069705963135\n",
      "Epoch 1: |          | 446/? [10:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 446, loss 4.9374308586120605\n",
      "Epoch 1: |          | 447/? [10:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 447, loss 5.464999198913574\n",
      "Epoch 1: |          | 448/? [10:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 448, loss 4.75526762008667\n",
      "Epoch 1: |          | 449/? [10:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 449, loss 4.841464042663574\n",
      "Epoch 1: |          | 450/? [10:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 450, loss 5.296455383300781\n",
      "Epoch 1: |          | 451/? [10:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 451, loss 4.854753494262695\n",
      "Epoch 1: |          | 452/? [10:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 452, loss 4.689998626708984\n",
      "Epoch 1: |          | 453/? [10:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 453, loss 5.301224708557129\n",
      "Epoch 1: |          | 454/? [10:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 454, loss 4.6839518547058105\n",
      "Epoch 1: |          | 455/? [10:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 455, loss 5.157430171966553\n",
      "Epoch 1: |          | 456/? [10:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 456, loss 4.538891792297363\n",
      "Epoch 1: |          | 457/? [10:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 457, loss 4.864130020141602\n",
      "Epoch 1: |          | 458/? [10:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 458, loss 5.273489952087402\n",
      "Epoch 1: |          | 459/? [10:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 459, loss 5.28608512878418\n",
      "Epoch 1: |          | 460/? [10:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 460, loss 5.060983657836914\n",
      "Epoch 1: |          | 461/? [10:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 461, loss 5.037388801574707\n",
      "Epoch 1: |          | 462/? [10:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 462, loss 5.04555606842041\n",
      "Epoch 1: |          | 463/? [10:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 463, loss 4.900091648101807\n",
      "Epoch 1: |          | 464/? [10:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 464, loss 4.394627094268799\n",
      "Epoch 1: |          | 465/? [11:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 465, loss 4.722865104675293\n",
      "Epoch 1: |          | 466/? [11:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 466, loss 5.124300956726074\n",
      "Epoch 1: |          | 467/? [11:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 467, loss 5.06923770904541\n",
      "Epoch 1: |          | 468/? [11:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 468, loss 4.925620079040527\n",
      "Epoch 1: |          | 469/? [11:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 469, loss 5.035659313201904\n",
      "Epoch 1: |          | 470/? [11:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 470, loss 4.139625072479248\n",
      "Epoch 1: |          | 471/? [11:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 471, loss 4.985677242279053\n",
      "Epoch 1: |          | 472/? [11:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 472, loss 4.640236854553223\n",
      "Epoch 1: |          | 473/? [11:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 473, loss 4.648293972015381\n",
      "Epoch 1: |          | 474/? [11:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 474, loss 4.290188312530518\n",
      "Epoch 1: |          | 475/? [11:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 475, loss 5.647494316101074\n",
      "Epoch 1: |          | 476/? [11:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 476, loss 4.503964424133301\n",
      "Epoch 1: |          | 477/? [11:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 477, loss 4.059427261352539\n",
      "Epoch 1: |          | 478/? [11:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 478, loss 4.178954601287842\n",
      "Epoch 1: |          | 479/? [11:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 479, loss 4.983242988586426\n",
      "Epoch 1: |          | 480/? [11:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 480, loss 5.1176652908325195\n",
      "Epoch 1: |          | 481/? [11:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 481, loss 4.72938871383667\n",
      "Epoch 1: |          | 482/? [11:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 482, loss 4.788090705871582\n",
      "Epoch 1: |          | 483/? [11:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 483, loss 4.493752956390381\n",
      "Epoch 1: |          | 484/? [11:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 484, loss 5.359770774841309\n",
      "Epoch 1: |          | 485/? [11:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 485, loss 5.154152870178223\n",
      "Epoch 1: |          | 486/? [11:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 486, loss 5.009886264801025\n",
      "Epoch 1: |          | 487/? [11:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 487, loss 5.108005523681641\n",
      "Epoch 1: |          | 488/? [11:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 488, loss 4.704590320587158\n",
      "Epoch 1: |          | 489/? [11:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 489, loss 4.189145088195801\n",
      "Epoch 1: |          | 490/? [11:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 490, loss 5.0450944900512695\n",
      "Epoch 1: |          | 491/? [11:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 491, loss 4.707721710205078\n",
      "Epoch 1: |          | 492/? [11:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 492, loss 4.279810428619385\n",
      "Epoch 1: |          | 493/? [11:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 493, loss 5.295723915100098\n",
      "Epoch 1: |          | 494/? [11:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 494, loss 5.144908428192139\n",
      "Epoch 1: |          | 495/? [11:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 495, loss 5.162713050842285\n",
      "Epoch 1: |          | 496/? [11:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 496, loss 4.706900119781494\n",
      "Epoch 1: |          | 497/? [11:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 497, loss 5.199184894561768\n",
      "Epoch 1: |          | 498/? [11:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 498, loss 4.955708980560303\n",
      "Epoch 1: |          | 499/? [11:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 499, loss 4.860955715179443\n",
      "Epoch 1: |          | 500/? [11:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 500, loss 4.908257961273193\n",
      "Epoch 1: |          | 501/? [11:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 501, loss 4.4848222732543945\n",
      "Epoch 1: |          | 502/? [11:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 502, loss 4.982184410095215\n",
      "Epoch 1: |          | 503/? [11:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 503, loss 5.122256755828857\n",
      "Epoch 1: |          | 504/? [11:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 504, loss 4.797019004821777\n",
      "Epoch 1: |          | 505/? [11:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 505, loss 4.000777244567871\n",
      "Epoch 1: |          | 506/? [12:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 506, loss 4.756007671356201\n",
      "Epoch 1: |          | 507/? [12:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 507, loss 4.861653804779053\n",
      "Epoch 1: |          | 508/? [12:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 508, loss 5.151976108551025\n",
      "Epoch 1: |          | 509/? [12:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 509, loss 4.483420372009277\n",
      "Epoch 1: |          | 510/? [12:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 510, loss 5.181729316711426\n",
      "Epoch 1: |          | 511/? [12:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 511, loss 4.974603176116943\n",
      "Epoch 1: |          | 512/? [12:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 512, loss 4.360076427459717\n",
      "Epoch 1: |          | 513/? [12:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 513, loss 4.6590895652771\n",
      "Epoch 1: |          | 514/? [12:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 514, loss 4.6853790283203125\n",
      "Epoch 1: |          | 515/? [12:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 515, loss 4.453965663909912\n",
      "Epoch 1: |          | 516/? [12:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 516, loss 4.734829902648926\n",
      "Epoch 1: |          | 517/? [12:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 517, loss 5.108559608459473\n",
      "Epoch 1: |          | 518/? [12:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 518, loss 4.650469779968262\n",
      "Epoch 1: |          | 519/? [12:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 519, loss 5.130382061004639\n",
      "Epoch 1: |          | 520/? [12:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 520, loss 4.827655792236328\n",
      "Epoch 1: |          | 521/? [12:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 521, loss 4.797786712646484\n",
      "Epoch 1: |          | 522/? [12:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 522, loss 5.38636589050293\n",
      "Epoch 1: |          | 523/? [12:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 523, loss 5.423094749450684\n",
      "Epoch 1: |          | 524/? [12:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 524, loss 5.242582321166992\n",
      "Epoch 1: |          | 525/? [12:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 525, loss 4.816730976104736\n",
      "Epoch 1: |          | 526/? [12:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 526, loss 4.65458345413208\n",
      "Epoch 1: |          | 527/? [12:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 527, loss 5.098474979400635\n",
      "Epoch 1: |          | 528/? [12:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 528, loss 5.095433235168457\n",
      "Epoch 1: |          | 529/? [12:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 529, loss 4.659102916717529\n",
      "Epoch 1: |          | 530/? [12:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 530, loss 5.264182090759277\n",
      "Epoch 1: |          | 531/? [12:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 531, loss 4.642775535583496\n",
      "Epoch 1: |          | 532/? [12:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 532, loss 4.992327690124512\n",
      "Epoch 1: |          | 533/? [12:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 533, loss 4.70185661315918\n",
      "Epoch 1: |          | 534/? [12:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 534, loss 4.563684463500977\n",
      "Epoch 1: |          | 535/? [12:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 535, loss 5.486752033233643\n",
      "Epoch 1: |          | 536/? [12:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 536, loss 5.318439483642578\n",
      "Epoch 1: |          | 537/? [12:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 537, loss 4.965359687805176\n",
      "Epoch 1: |          | 538/? [12:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 538, loss 4.6231913566589355\n",
      "Epoch 1: |          | 539/? [12:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 539, loss 4.6848649978637695\n",
      "Epoch 1: |          | 540/? [12:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 540, loss 5.268989086151123\n",
      "Epoch 1: |          | 541/? [12:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 541, loss 4.894993782043457\n",
      "Epoch 1: |          | 542/? [12:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 542, loss 4.741723537445068\n",
      "Epoch 1: |          | 543/? [12:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 543, loss 5.065247535705566\n",
      "Epoch 1: |          | 544/? [12:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 544, loss 4.8910112380981445\n",
      "Epoch 1: |          | 545/? [12:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 545, loss 4.047451972961426\n",
      "Epoch 1: |          | 546/? [12:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 546, loss 5.019474983215332\n",
      "Epoch 1: |          | 547/? [12:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 547, loss 5.363381862640381\n",
      "Epoch 1: |          | 548/? [13:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 548, loss 5.167789936065674\n",
      "Epoch 1: |          | 549/? [13:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 549, loss 4.929184436798096\n",
      "Epoch 1: |          | 550/? [13:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 550, loss 5.39296817779541\n",
      "Epoch 1: |          | 551/? [13:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 551, loss 4.893711090087891\n",
      "Epoch 1: |          | 552/? [13:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 552, loss 5.1299920082092285\n",
      "Epoch 1: |          | 553/? [13:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 553, loss 4.297966957092285\n",
      "Epoch 1: |          | 554/? [13:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 554, loss 5.046234130859375\n",
      "Epoch 1: |          | 555/? [13:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 555, loss 5.441545486450195\n",
      "Epoch 1: |          | 556/? [13:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 556, loss 4.924132347106934\n",
      "Epoch 1: |          | 557/? [13:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 557, loss 4.580446243286133\n",
      "Epoch 1: |          | 558/? [13:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 558, loss 4.614156723022461\n",
      "Epoch 1: |          | 559/? [13:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 559, loss 4.756359577178955\n",
      "Epoch 1: |          | 560/? [13:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 560, loss 4.077713966369629\n",
      "Epoch 1: |          | 561/? [13:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 561, loss 4.396149635314941\n",
      "Epoch 1: |          | 562/? [13:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 562, loss 5.066800117492676\n",
      "Epoch 1: |          | 563/? [13:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 563, loss 4.096636772155762\n",
      "Epoch 1: |          | 564/? [13:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 564, loss 4.8585405349731445\n",
      "Epoch 1: |          | 565/? [13:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 565, loss 5.313552379608154\n",
      "Epoch 1: |          | 566/? [13:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 566, loss 5.187342643737793\n",
      "Epoch 1: |          | 567/? [13:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 567, loss 5.272519588470459\n",
      "Epoch 1: |          | 568/? [13:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 568, loss 4.395672798156738\n",
      "Epoch 1: |          | 569/? [13:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 569, loss 5.07463264465332\n",
      "Epoch 1: |          | 570/? [13:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 570, loss 5.004897117614746\n",
      "Epoch 1: |          | 571/? [13:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 571, loss 4.5828351974487305\n",
      "Epoch 1: |          | 572/? [13:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 572, loss 5.538613796234131\n",
      "Epoch 1: |          | 573/? [13:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 573, loss 3.834141254425049\n",
      "Epoch 1: |          | 574/? [13:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 574, loss 5.172886848449707\n",
      "Epoch 1: |          | 575/? [13:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 575, loss 4.521145343780518\n",
      "Epoch 1: |          | 576/? [13:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 576, loss 4.735350608825684\n",
      "Epoch 1: |          | 577/? [13:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 577, loss 4.971157073974609\n",
      "Epoch 1: |          | 578/? [13:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 578, loss 5.201373100280762\n",
      "Epoch 1: |          | 579/? [13:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 579, loss 4.102011680603027\n",
      "Epoch 1: |          | 580/? [13:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 580, loss 4.998734951019287\n",
      "Epoch 1: |          | 581/? [13:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 581, loss 5.087088584899902\n",
      "Epoch 1: |          | 582/? [13:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 582, loss 5.063096523284912\n",
      "Epoch 1: |          | 583/? [13:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 583, loss 4.805751323699951\n",
      "Epoch 1: |          | 584/? [13:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 584, loss 4.973484039306641\n",
      "Epoch 1: |          | 585/? [13:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 585, loss 5.136814117431641\n",
      "Epoch 1: |          | 586/? [13:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 586, loss 5.107955455780029\n",
      "Epoch 1: |          | 587/? [13:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 587, loss 4.9336042404174805\n",
      "Epoch 1: |          | 588/? [13:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 588, loss 5.135105609893799\n",
      "Epoch 1: |          | 589/? [13:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 589, loss 4.503247261047363\n",
      "Epoch 1: |          | 590/? [13:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 590, loss 5.214049339294434\n",
      "Epoch 1: |          | 591/? [14:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 591, loss 5.005484580993652\n",
      "Epoch 1: |          | 592/? [14:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 592, loss 4.740485191345215\n",
      "Epoch 1: |          | 593/? [14:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 593, loss 4.831085205078125\n",
      "Epoch 1: |          | 594/? [14:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 594, loss 5.689900875091553\n",
      "Epoch 1: |          | 595/? [14:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 595, loss 4.299026966094971\n",
      "Epoch 1: |          | 596/? [14:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 596, loss 4.464804649353027\n",
      "Epoch 1: |          | 597/? [14:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 597, loss 4.7981767654418945\n",
      "Epoch 1: |          | 598/? [14:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 598, loss 5.22862434387207\n",
      "Epoch 1: |          | 599/? [14:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 599, loss 4.902344226837158\n",
      "Epoch 1: |          | 600/? [14:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 600, loss 4.525688171386719\n",
      "Epoch 1: |          | 601/? [14:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 601, loss 4.909518241882324\n",
      "Epoch 1: |          | 602/? [14:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 602, loss 4.407083988189697\n",
      "Epoch 1: |          | 603/? [14:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 603, loss 4.783491134643555\n",
      "Epoch 1: |          | 604/? [14:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 604, loss 7.773804664611816\n",
      "Epoch 1: |          | 605/? [14:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 605, loss 4.30015754699707\n",
      "Epoch 1: |          | 606/? [14:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 606, loss 4.638554096221924\n",
      "Epoch 1: |          | 607/? [14:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 607, loss 5.102566719055176\n",
      "Epoch 1: |          | 608/? [14:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 608, loss 4.713457107543945\n",
      "Epoch 1: |          | 609/? [14:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 609, loss 4.635238170623779\n",
      "Epoch 1: |          | 610/? [14:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 610, loss 4.731536865234375\n",
      "Epoch 1: |          | 611/? [14:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 611, loss 4.920044422149658\n",
      "Epoch 1: |          | 612/? [14:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 612, loss 4.636889934539795\n",
      "Epoch 1: |          | 613/? [14:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 613, loss 5.006858825683594\n",
      "Epoch 1: |          | 614/? [14:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 614, loss 4.645449638366699\n",
      "Epoch 1: |          | 615/? [14:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 615, loss 5.239224433898926\n",
      "Epoch 1: |          | 616/? [14:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 616, loss 5.544349670410156\n",
      "Epoch 1: |          | 617/? [14:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 617, loss 4.586991786956787\n",
      "Epoch 1: |          | 618/? [14:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 618, loss 5.033836364746094\n",
      "Epoch 1: |          | 619/? [14:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 619, loss 4.775357246398926\n",
      "Epoch 1: |          | 620/? [14:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 620, loss 5.158294200897217\n",
      "Epoch 1: |          | 621/? [14:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 621, loss 4.470667839050293\n",
      "Epoch 1: |          | 622/? [14:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 622, loss 4.265904426574707\n",
      "Epoch 1: |          | 623/? [14:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 623, loss 3.8550643920898438\n",
      "Epoch 1: |          | 624/? [14:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 624, loss 3.4446873664855957\n",
      "Epoch 1: |          | 625/? [14:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 625, loss 5.310328483581543\n",
      "Epoch 1: |          | 626/? [14:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 626, loss 4.834092617034912\n",
      "Epoch 1: |          | 627/? [14:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 627, loss 4.770411491394043\n",
      "Epoch 1: |          | 628/? [14:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 628, loss 4.56283712387085\n",
      "Epoch 1: |          | 629/? [14:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 629, loss 5.102616310119629\n",
      "Epoch 1: |          | 630/? [14:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 630, loss 4.829020023345947\n",
      "Epoch 1: |          | 631/? [14:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 631, loss 5.047816276550293\n",
      "Epoch 1: |          | 632/? [14:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 632, loss 4.012894630432129\n",
      "Epoch 1: |          | 633/? [14:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 633, loss 5.118352890014648\n",
      "Epoch 1: |          | 634/? [15:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 634, loss 4.641571998596191\n",
      "Epoch 1: |          | 635/? [15:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 635, loss 4.359696865081787\n",
      "Epoch 1: |          | 636/? [15:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 636, loss 4.828237056732178\n",
      "Epoch 1: |          | 637/? [15:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 637, loss 4.602474212646484\n",
      "Epoch 1: |          | 638/? [15:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 638, loss 4.871644496917725\n",
      "Epoch 1: |          | 639/? [15:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 639, loss 4.600557327270508\n",
      "Epoch 1: |          | 640/? [15:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 640, loss 5.311483860015869\n",
      "Epoch 1: |          | 641/? [15:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 641, loss 4.42479944229126\n",
      "Epoch 1: |          | 642/? [15:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 642, loss 5.015384674072266\n",
      "Epoch 1: |          | 643/? [15:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 643, loss 5.046212196350098\n",
      "Epoch 1: |          | 644/? [15:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 644, loss 4.784322738647461\n",
      "Epoch 1: |          | 645/? [15:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 645, loss 4.641048908233643\n",
      "Epoch 1: |          | 646/? [15:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 646, loss 4.630328178405762\n",
      "Epoch 1: |          | 647/? [15:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 647, loss 5.370800018310547\n",
      "Epoch 1: |          | 648/? [15:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 648, loss 4.8927717208862305\n",
      "Epoch 1: |          | 649/? [15:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 649, loss 5.25709867477417\n",
      "Epoch 1: |          | 650/? [15:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 650, loss 5.230257987976074\n",
      "Epoch 1: |          | 651/? [15:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 651, loss 5.391169548034668\n",
      "Epoch 1: |          | 652/? [15:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 652, loss 4.638237476348877\n",
      "Epoch 1: |          | 653/? [15:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 653, loss 4.7636823654174805\n",
      "Epoch 1: |          | 654/? [15:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 654, loss 5.1068644523620605\n",
      "Epoch 1: |          | 655/? [15:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 655, loss 4.802107810974121\n",
      "Epoch 1: |          | 656/? [15:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 656, loss 4.334149360656738\n",
      "Epoch 1: |          | 657/? [15:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 657, loss 7.123663425445557\n",
      "Epoch 1: |          | 658/? [15:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 658, loss 4.718756675720215\n",
      "Epoch 1: |          | 659/? [15:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 659, loss 4.815712928771973\n",
      "Epoch 1: |          | 660/? [15:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 660, loss 5.248500347137451\n",
      "Epoch 1: |          | 661/? [15:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 661, loss 5.1235151290893555\n",
      "Epoch 1: |          | 662/? [15:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 662, loss 4.93667459487915\n",
      "Epoch 1: |          | 663/? [15:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 663, loss 4.632406234741211\n",
      "Epoch 1: |          | 664/? [15:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 664, loss 4.585569381713867\n",
      "Epoch 1: |          | 665/? [15:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 665, loss 4.920647621154785\n",
      "Epoch 1: |          | 666/? [15:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 666, loss 4.757912635803223\n",
      "Epoch 1: |          | 667/? [15:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 667, loss 5.629371166229248\n",
      "Epoch 1: |          | 668/? [15:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 668, loss 4.314857482910156\n",
      "Epoch 1: |          | 669/? [15:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 669, loss 4.6011762619018555\n",
      "Epoch 1: |          | 670/? [15:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 670, loss 5.2740983963012695\n",
      "Epoch 1: |          | 671/? [15:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 671, loss 5.0827860832214355\n",
      "Epoch 1: |          | 672/? [15:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 672, loss 4.99934720993042\n",
      "Epoch 1: |          | 673/? [15:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 673, loss 4.8232831954956055\n",
      "Epoch 1: |          | 674/? [15:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 674, loss 3.5864386558532715\n",
      "Epoch 1: |          | 675/? [15:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 675, loss 2.1190199851989746\n",
      "Epoch 1: |          | 676/? [15:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 676, loss 1.7530572414398193\n",
      "Epoch 1: |          | 677/? [16:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 677, loss 1.3934919834136963\n",
      "Epoch 1: |          | 678/? [16:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 678, loss 2.506032943725586\n",
      "Epoch 1: |          | 679/? [16:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 679, loss 4.2791619300842285\n",
      "Epoch 1: |          | 680/? [16:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 680, loss 4.8457112312316895\n",
      "Epoch 1: |          | 681/? [16:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 681, loss 4.031882286071777\n",
      "Epoch 1: |          | 682/? [16:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 682, loss 4.727299213409424\n",
      "Epoch 1: |          | 683/? [16:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 683, loss 4.335875511169434\n",
      "Epoch 1: |          | 684/? [16:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 684, loss 5.4396233558654785\n",
      "Epoch 1: |          | 685/? [16:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 685, loss 5.044571876525879\n",
      "Epoch 1: |          | 686/? [16:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 686, loss 4.485699653625488\n",
      "Epoch 1: |          | 687/? [16:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 687, loss 5.2556304931640625\n",
      "Epoch 1: |          | 688/? [16:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 688, loss 5.072265148162842\n",
      "Epoch 1: |          | 689/? [16:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 689, loss 4.974547386169434\n",
      "Epoch 1: |          | 690/? [16:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 690, loss 5.316054344177246\n",
      "Epoch 1: |          | 691/? [16:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 691, loss 4.975091457366943\n",
      "Epoch 1: |          | 692/? [16:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 692, loss 4.7965593338012695\n",
      "Epoch 1: |          | 693/? [16:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 693, loss 5.308120250701904\n",
      "Epoch 1: |          | 694/? [16:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 694, loss 4.671682834625244\n",
      "Epoch 1: |          | 695/? [16:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 695, loss 5.357064247131348\n",
      "Epoch 1: |          | 696/? [16:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 696, loss 4.340336322784424\n",
      "Epoch 1: |          | 697/? [16:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 697, loss 4.838410377502441\n",
      "Epoch 1: |          | 698/? [16:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 698, loss 3.8834481239318848\n",
      "Epoch 1: |          | 699/? [16:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 699, loss 4.9081525802612305\n",
      "Epoch 1: |          | 700/? [16:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 700, loss 5.146811008453369\n",
      "Epoch 1: |          | 701/? [16:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 701, loss 4.563762187957764\n",
      "Epoch 1: |          | 702/? [16:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 702, loss 4.778247833251953\n",
      "Epoch 1: |          | 703/? [16:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 703, loss 4.9889817237854\n",
      "Epoch 1: |          | 704/? [16:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 704, loss 4.918045520782471\n",
      "Epoch 1: |          | 705/? [16:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 705, loss 4.445693016052246\n",
      "Epoch 1: |          | 706/? [16:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 706, loss 4.562727928161621\n",
      "Epoch 1: |          | 707/? [16:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 707, loss 5.004075050354004\n",
      "Epoch 1: |          | 708/? [16:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 708, loss 4.80501651763916\n",
      "Epoch 1: |          | 709/? [16:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 709, loss 4.684463977813721\n",
      "Epoch 1: |          | 710/? [16:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 710, loss 5.276989936828613\n",
      "Epoch 1: |          | 711/? [16:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 711, loss 5.54075288772583\n",
      "Epoch 1: |          | 712/? [16:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 712, loss 5.125605583190918\n",
      "Epoch 1: |          | 713/? [16:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 713, loss 5.043765068054199\n",
      "Epoch 1: |          | 714/? [16:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 714, loss 5.290188312530518\n",
      "Epoch 1: |          | 715/? [16:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 715, loss 4.012872219085693\n",
      "Epoch 1: |          | 716/? [16:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 716, loss 4.895411491394043\n",
      "Epoch 1: |          | 717/? [16:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 717, loss 4.621485233306885\n",
      "Epoch 1: |          | 718/? [16:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 718, loss 4.170369625091553\n",
      "Epoch 1: |          | 719/? [17:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 719, loss 4.734498023986816\n",
      "Epoch 1: |          | 720/? [17:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 720, loss 4.517636299133301\n",
      "Epoch 1: |          | 721/? [17:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 721, loss 5.231180191040039\n",
      "Epoch 1: |          | 722/? [17:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 722, loss 4.262082576751709\n",
      "Epoch 1: |          | 723/? [17:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 723, loss 4.926081657409668\n",
      "Epoch 1: |          | 724/? [17:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 724, loss 4.894079208374023\n",
      "Epoch 1: |          | 725/? [17:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 725, loss 4.5374979972839355\n",
      "Epoch 1: |          | 726/? [17:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 726, loss 4.675906181335449\n",
      "Epoch 1: |          | 727/? [17:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 727, loss 4.335488319396973\n",
      "Epoch 1: |          | 728/? [17:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 728, loss 4.228915691375732\n",
      "Epoch 1: |          | 729/? [17:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 729, loss 4.593634605407715\n",
      "Epoch 1: |          | 730/? [17:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 730, loss 4.6152567863464355\n",
      "Epoch 1: |          | 731/? [17:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 731, loss 4.8077802658081055\n",
      "Epoch 1: |          | 732/? [17:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 732, loss 5.153557777404785\n",
      "Epoch 1: |          | 733/? [17:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 733, loss 4.720133304595947\n",
      "Epoch 1: |          | 734/? [17:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 734, loss 5.0934648513793945\n",
      "Epoch 1: |          | 735/? [17:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 735, loss 4.968922138214111\n",
      "Epoch 1: |          | 736/? [17:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 736, loss 4.377256870269775\n",
      "Epoch 1: |          | 737/? [17:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 737, loss 5.206204891204834\n",
      "Epoch 1: |          | 738/? [17:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 738, loss 4.326298713684082\n",
      "Epoch 1: |          | 739/? [17:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 739, loss 4.9702067375183105\n",
      "Epoch 1: |          | 740/? [17:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 740, loss 4.443045139312744\n",
      "Epoch 1: |          | 741/? [17:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 741, loss 4.746907711029053\n",
      "Epoch 1: |          | 742/? [17:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 742, loss 5.225554943084717\n",
      "Epoch 1: |          | 743/? [17:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 743, loss 4.993399620056152\n",
      "Epoch 1: |          | 744/? [17:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 744, loss 4.928975582122803\n",
      "Epoch 1: |          | 745/? [17:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 745, loss 4.527003288269043\n",
      "Epoch 1: |          | 746/? [17:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 746, loss 4.839876651763916\n",
      "Epoch 1: |          | 747/? [17:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 747, loss 4.697012901306152\n",
      "Epoch 1: |          | 748/? [17:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 748, loss 4.373305797576904\n",
      "Epoch 1: |          | 749/? [17:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 749, loss 4.902865409851074\n",
      "Epoch 1: |          | 750/? [17:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 750, loss 5.27903413772583\n",
      "Epoch 1: |          | 751/? [17:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 751, loss 3.8493988513946533\n",
      "Epoch 1: |          | 752/? [17:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 752, loss 5.077851295471191\n",
      "Epoch 1: |          | 753/? [17:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 753, loss 4.342111110687256\n",
      "Epoch 1: |          | 754/? [17:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 754, loss 4.805256366729736\n",
      "Epoch 1: |          | 755/? [17:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 755, loss 4.3587493896484375\n",
      "Epoch 1: |          | 756/? [17:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 756, loss 4.851513385772705\n",
      "Epoch 1: |          | 757/? [17:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 757, loss 4.825035095214844\n",
      "Epoch 1: |          | 758/? [17:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 758, loss 4.468171119689941\n",
      "Epoch 1: |          | 759/? [17:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 759, loss 4.666263580322266\n",
      "Epoch 1: |          | 760/? [17:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 760, loss 4.978893756866455\n",
      "Epoch 1: |          | 761/? [18:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 761, loss 5.082024574279785\n",
      "Epoch 1: |          | 762/? [18:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 762, loss 4.737090587615967\n",
      "Epoch 1: |          | 763/? [18:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 763, loss 5.177211761474609\n",
      "Epoch 1: |          | 764/? [18:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 764, loss 5.285153865814209\n",
      "Epoch 1: |          | 765/? [18:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 765, loss 4.922545433044434\n",
      "Epoch 1: |          | 766/? [18:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 766, loss 5.371541500091553\n",
      "Epoch 1: |          | 767/? [18:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 767, loss 5.460015296936035\n",
      "Epoch 1: |          | 768/? [18:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 768, loss 4.890406608581543\n",
      "Epoch 1: |          | 769/? [18:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 769, loss 4.0002264976501465\n",
      "Epoch 1: |          | 770/? [18:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 770, loss 4.647111892700195\n",
      "Epoch 1: |          | 771/? [18:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 771, loss 5.406772613525391\n",
      "Epoch 1: |          | 772/? [18:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 772, loss 5.118734359741211\n",
      "Epoch 1: |          | 773/? [18:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 773, loss 4.717954635620117\n",
      "Epoch 1: |          | 774/? [18:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 774, loss 4.8114914894104\n",
      "Epoch 1: |          | 775/? [18:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 775, loss 5.387363910675049\n",
      "Epoch 1: |          | 776/? [18:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 776, loss 4.756522178649902\n",
      "Epoch 1: |          | 777/? [18:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 777, loss 4.784503936767578\n",
      "Epoch 1: |          | 778/? [18:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 778, loss 5.175400733947754\n",
      "Epoch 1: |          | 779/? [18:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 779, loss 5.540426254272461\n",
      "Epoch 1: |          | 780/? [18:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 780, loss 4.328220367431641\n",
      "Epoch 1: |          | 781/? [18:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 781, loss 4.517434597015381\n",
      "Epoch 1: |          | 782/? [18:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 782, loss 5.027927398681641\n",
      "Epoch 1: |          | 783/? [18:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 783, loss 5.037884712219238\n",
      "Epoch 1: |          | 784/? [18:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 784, loss 4.546627998352051\n",
      "Epoch 1: |          | 785/? [18:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 785, loss 4.468299388885498\n",
      "Epoch 1: |          | 786/? [18:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 786, loss 5.286342620849609\n",
      "Epoch 1: |          | 787/? [18:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 787, loss 5.3610615730285645\n",
      "Epoch 1: |          | 788/? [18:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 788, loss 3.3792011737823486\n",
      "Epoch 1: |          | 789/? [18:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 789, loss 4.761601448059082\n",
      "Epoch 1: |          | 790/? [18:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 790, loss 5.572756290435791\n",
      "Epoch 1: |          | 791/? [18:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 791, loss 5.32973575592041\n",
      "Epoch 1: |          | 792/? [18:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 792, loss 4.320703029632568\n",
      "Epoch 1: |          | 793/? [18:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 793, loss 4.891818046569824\n",
      "Epoch 1: |          | 794/? [18:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 794, loss 5.301563262939453\n",
      "Epoch 1: |          | 795/? [18:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 795, loss 4.779263973236084\n",
      "Epoch 1: |          | 796/? [18:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 796, loss 5.154850006103516\n",
      "Epoch 1: |          | 797/? [18:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 797, loss 4.007646083831787\n",
      "Epoch 1: |          | 798/? [18:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 798, loss 4.167210102081299\n",
      "Epoch 1: |          | 799/? [18:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 799, loss 5.222001075744629\n",
      "Epoch 1: |          | 800/? [18:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 800, loss 4.980048179626465\n",
      "Epoch 1: |          | 801/? [18:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 801, loss 4.401796817779541\n",
      "Epoch 1: |          | 802/? [18:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 802, loss 4.907912254333496\n",
      "Epoch 1: |          | 803/? [19:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 803, loss 4.704861640930176\n",
      "Epoch 1: |          | 804/? [19:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 804, loss 4.927050590515137\n",
      "Epoch 1: |          | 805/? [19:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 805, loss 5.336970329284668\n",
      "Epoch 1: |          | 806/? [19:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 806, loss 5.410490989685059\n",
      "Epoch 1: |          | 807/? [19:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 807, loss 4.878612995147705\n",
      "Epoch 1: |          | 808/? [19:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 808, loss 4.369223594665527\n",
      "Epoch 1: |          | 809/? [19:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 809, loss 4.96015739440918\n",
      "Epoch 1: |          | 810/? [19:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 810, loss 4.864079475402832\n",
      "Epoch 1: |          | 811/? [19:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 811, loss 5.13898229598999\n",
      "Epoch 1: |          | 812/? [19:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 812, loss 5.819584846496582\n",
      "Epoch 1: |          | 813/? [19:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 813, loss 5.403679847717285\n",
      "Epoch 1: |          | 814/? [19:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 814, loss 4.351271152496338\n",
      "Epoch 1: |          | 815/? [19:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 815, loss 5.16914701461792\n",
      "Epoch 1: |          | 816/? [19:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 816, loss 4.990758419036865\n",
      "Epoch 1: |          | 817/? [19:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 817, loss 4.2613372802734375\n",
      "Epoch 1: |          | 818/? [19:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 818, loss 5.3645243644714355\n",
      "Epoch 1: |          | 819/? [19:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 819, loss 5.078736305236816\n",
      "Epoch 1: |          | 820/? [19:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 820, loss 4.884693145751953\n",
      "Epoch 1: |          | 821/? [19:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 821, loss 4.8572587966918945\n",
      "Epoch 1: |          | 822/? [19:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 822, loss 4.366831302642822\n",
      "Epoch 1: |          | 823/? [19:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 823, loss 4.299323081970215\n",
      "Epoch 1: |          | 824/? [19:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 824, loss 4.906558036804199\n",
      "Epoch 1: |          | 825/? [19:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 825, loss 4.44841194152832\n",
      "Epoch 1: |          | 826/? [19:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 826, loss 4.909432888031006\n",
      "Epoch 1: |          | 827/? [19:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 827, loss 4.606075763702393\n",
      "Epoch 1: |          | 828/? [19:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 828, loss 5.148464679718018\n",
      "Epoch 1: |          | 829/? [19:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 829, loss 4.749459743499756\n",
      "Epoch 1: |          | 830/? [19:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 830, loss 5.387112617492676\n",
      "Epoch 1: |          | 831/? [19:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 831, loss 3.0852668285369873\n",
      "Epoch 1: |          | 832/? [19:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 832, loss 4.818041801452637\n",
      "Epoch 1: |          | 833/? [19:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 833, loss 4.627206325531006\n",
      "Epoch 1: |          | 834/? [19:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 834, loss 5.396063804626465\n",
      "Epoch 1: |          | 835/? [19:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 835, loss 4.8839311599731445\n",
      "Epoch 1: |          | 836/? [19:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 836, loss 5.436060905456543\n",
      "Epoch 1: |          | 837/? [19:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 837, loss 4.992489814758301\n",
      "Epoch 1: |          | 838/? [19:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 838, loss 4.162471771240234\n",
      "Epoch 1: |          | 839/? [19:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 839, loss 4.4738664627075195\n",
      "Epoch 1: |          | 840/? [19:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 840, loss 5.103581428527832\n",
      "Epoch 1: |          | 841/? [19:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 841, loss 5.148026466369629\n",
      "Epoch 1: |          | 842/? [19:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 842, loss 4.835949897766113\n",
      "Epoch 1: |          | 843/? [19:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 843, loss 5.170137882232666\n",
      "Epoch 1: |          | 844/? [19:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 844, loss 4.497193813323975\n",
      "Epoch 1: |          | 845/? [20:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 845, loss 4.929899215698242\n",
      "Epoch 1: |          | 846/? [20:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 846, loss 5.510053634643555\n",
      "Epoch 1: |          | 847/? [20:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 847, loss 4.9509148597717285\n",
      "Epoch 1: |          | 848/? [20:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 848, loss 4.432077407836914\n",
      "Epoch 1: |          | 849/? [20:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 849, loss 4.628840446472168\n",
      "Epoch 1: |          | 850/? [20:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 850, loss 4.636754035949707\n",
      "Epoch 1: |          | 851/? [20:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 851, loss 5.09812068939209\n",
      "Epoch 1: |          | 852/? [20:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 852, loss 4.997534275054932\n",
      "Epoch 1: |          | 853/? [20:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 853, loss 5.090390682220459\n",
      "Epoch 1: |          | 854/? [20:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 854, loss 4.063143253326416\n",
      "Epoch 1: |          | 855/? [20:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 855, loss 4.535905838012695\n",
      "Epoch 1: |          | 856/? [20:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 856, loss 4.464648723602295\n",
      "Epoch 1: |          | 857/? [20:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 857, loss 5.090623378753662\n",
      "Epoch 1: |          | 858/? [20:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 858, loss 4.933592796325684\n",
      "Epoch 1: |          | 859/? [20:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 859, loss 4.995993614196777\n",
      "Epoch 1: |          | 860/? [20:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 860, loss 5.3628435134887695\n",
      "Epoch 1: |          | 861/? [20:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 861, loss 4.483959674835205\n",
      "Epoch 1: |          | 862/? [20:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 862, loss 5.076356887817383\n",
      "Epoch 1: |          | 863/? [20:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 863, loss 4.110464572906494\n",
      "Epoch 1: |          | 864/? [20:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 864, loss 5.021799087524414\n",
      "Epoch 1: |          | 865/? [20:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 865, loss 4.849089622497559\n",
      "Epoch 1: |          | 866/? [20:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 866, loss 3.9716060161590576\n",
      "Epoch 1: |          | 867/? [20:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 867, loss 4.159728050231934\n",
      "Epoch 1: |          | 868/? [20:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 868, loss 5.004702091217041\n",
      "Epoch 1: |          | 869/? [20:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 869, loss 4.948964595794678\n",
      "Epoch 1: |          | 870/? [20:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 870, loss 4.449588298797607\n",
      "Epoch 1: |          | 871/? [20:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 871, loss 4.996898651123047\n",
      "Epoch 1: |          | 872/? [20:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 872, loss 4.8478803634643555\n",
      "Epoch 1: |          | 873/? [20:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 873, loss 4.732140064239502\n",
      "Epoch 1: |          | 874/? [20:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 874, loss 4.218425750732422\n",
      "Epoch 1: |          | 875/? [20:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 875, loss 4.987238883972168\n",
      "Epoch 1: |          | 876/? [20:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 876, loss 4.842747211456299\n",
      "Epoch 1: |          | 877/? [20:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 877, loss 4.861955642700195\n",
      "Epoch 1: |          | 878/? [20:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 878, loss 4.325778007507324\n",
      "Epoch 1: |          | 879/? [20:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 879, loss 4.434961318969727\n",
      "Epoch 1: |          | 880/? [20:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 880, loss 5.451714038848877\n",
      "Epoch 1: |          | 881/? [20:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 881, loss 4.90229606628418\n",
      "Epoch 1: |          | 882/? [20:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 882, loss 4.772525787353516\n",
      "Epoch 1: |          | 883/? [20:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 883, loss 4.813323974609375\n",
      "Epoch 1: |          | 884/? [20:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 884, loss 4.922482490539551\n",
      "Epoch 1: |          | 885/? [20:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 885, loss 4.646847248077393\n",
      "Epoch 1: |          | 886/? [20:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 886, loss 5.267169952392578\n",
      "Epoch 1: |          | 887/? [21:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 887, loss 5.387419700622559\n",
      "Epoch 1: |          | 888/? [21:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 888, loss 4.993666648864746\n",
      "Epoch 1: |          | 889/? [21:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 889, loss 4.767461776733398\n",
      "Epoch 1: |          | 890/? [21:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 890, loss 5.119420528411865\n",
      "Epoch 1: |          | 891/? [21:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 891, loss 4.410666465759277\n",
      "Epoch 1: |          | 892/? [21:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 892, loss 5.295981407165527\n",
      "Epoch 1: |          | 893/? [21:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 893, loss 4.639779567718506\n",
      "Epoch 1: |          | 894/? [21:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 894, loss 4.411496639251709\n",
      "Epoch 1: |          | 895/? [21:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 895, loss 5.3650407791137695\n",
      "Epoch 1: |          | 896/? [21:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 896, loss 5.061478614807129\n",
      "Epoch 1: |          | 897/? [21:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 897, loss 5.040261745452881\n",
      "Epoch 1: |          | 898/? [21:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 898, loss 5.013394355773926\n",
      "Epoch 1: |          | 899/? [21:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 899, loss 4.761153221130371\n",
      "Epoch 1: |          | 900/? [21:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 900, loss 4.586005210876465\n",
      "Epoch 1: |          | 901/? [21:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 901, loss 5.162081718444824\n",
      "Epoch 1: |          | 902/? [21:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 902, loss 5.099363803863525\n",
      "Epoch 1: |          | 903/? [21:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 903, loss 4.398467063903809\n",
      "Epoch 1: |          | 904/? [21:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 904, loss 4.873956680297852\n",
      "Epoch 1: |          | 905/? [21:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 905, loss 5.161086082458496\n",
      "Epoch 1: |          | 906/? [21:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 906, loss 4.832967281341553\n",
      "Epoch 1: |          | 907/? [21:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 907, loss 4.909964084625244\n",
      "Epoch 1: |          | 908/? [21:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 908, loss 5.009879112243652\n",
      "Epoch 1: |          | 909/? [21:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 909, loss 5.012460708618164\n",
      "Epoch 1: |          | 910/? [21:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 910, loss 4.721482753753662\n",
      "Epoch 1: |          | 911/? [21:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 911, loss 4.716394424438477\n",
      "Epoch 1: |          | 912/? [21:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 912, loss 4.729567050933838\n",
      "Epoch 1: |          | 913/? [21:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 913, loss 4.701062202453613\n",
      "Epoch 1: |          | 914/? [21:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 914, loss 5.153982639312744\n",
      "Epoch 1: |          | 915/? [21:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 915, loss 5.0201005935668945\n",
      "Epoch 1: |          | 916/? [21:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 916, loss 4.732358932495117\n",
      "Epoch 1: |          | 917/? [21:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 917, loss 4.83158540725708\n",
      "Epoch 1: |          | 918/? [21:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 918, loss 4.650866508483887\n",
      "Epoch 1: |          | 919/? [21:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 919, loss 4.564131736755371\n",
      "Epoch 1: |          | 920/? [21:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 920, loss 4.818073749542236\n",
      "Epoch 1: |          | 921/? [21:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 921, loss 4.686776161193848\n",
      "Epoch 1: |          | 922/? [21:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 922, loss 4.8135552406311035\n",
      "Epoch 1: |          | 923/? [21:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 923, loss 4.669576168060303\n",
      "Epoch 1: |          | 924/? [21:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 924, loss 4.624533653259277\n",
      "Epoch 1: |          | 925/? [21:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 925, loss 4.91337251663208\n",
      "Epoch 1: |          | 926/? [21:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 926, loss 4.827150344848633\n",
      "Epoch 1: |          | 927/? [21:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 927, loss 4.9134016036987305\n",
      "Epoch 1: |          | 928/? [21:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 928, loss 4.3753485679626465\n",
      "Epoch 1: |          | 929/? [22:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 929, loss 4.56356143951416\n",
      "Epoch 1: |          | 930/? [22:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 930, loss 4.357951641082764\n",
      "Epoch 1: |          | 931/? [22:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 931, loss 4.0739264488220215\n",
      "Epoch 1: |          | 932/? [22:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 932, loss 4.820671558380127\n",
      "Epoch 1: |          | 933/? [22:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 933, loss 4.637659549713135\n",
      "Epoch 1: |          | 934/? [22:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 934, loss 5.376065254211426\n",
      "Epoch 1: |          | 935/? [22:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 935, loss 5.483766078948975\n",
      "Epoch 1: |          | 936/? [22:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 936, loss 4.79227352142334\n",
      "Epoch 1: |          | 937/? [22:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 937, loss 5.386858940124512\n",
      "Epoch 1: |          | 938/? [22:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 938, loss 4.617317199707031\n",
      "Epoch 1: |          | 939/? [22:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 939, loss 4.924297332763672\n",
      "Epoch 1: |          | 940/? [22:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 940, loss 5.238774299621582\n",
      "Epoch 1: |          | 941/? [22:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 941, loss 4.541901111602783\n",
      "Epoch 1: |          | 942/? [22:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 942, loss 4.141515254974365\n",
      "Epoch 1: |          | 943/? [22:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 943, loss 5.108842372894287\n",
      "Epoch 1: |          | 944/? [22:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 944, loss 4.017101287841797\n",
      "Epoch 1: |          | 945/? [22:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 945, loss 4.732644081115723\n",
      "Epoch 1: |          | 946/? [22:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 946, loss 4.777112007141113\n",
      "Epoch 1: |          | 947/? [22:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 947, loss 4.711318016052246\n",
      "Epoch 1: |          | 948/? [22:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 948, loss 4.794033050537109\n",
      "Epoch 1: |          | 949/? [22:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 949, loss 4.701530933380127\n",
      "Epoch 1: |          | 950/? [22:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 950, loss 4.541669845581055\n",
      "Epoch 1: |          | 951/? [22:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 951, loss 5.388986110687256\n",
      "Epoch 1: |          | 952/? [22:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 952, loss 5.183581352233887\n",
      "Epoch 1: |          | 953/? [22:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 953, loss 5.681778430938721\n",
      "Epoch 1: |          | 954/? [22:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 954, loss 4.791038990020752\n",
      "Epoch 1: |          | 955/? [22:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 955, loss 5.48059606552124\n",
      "Epoch 1: |          | 956/? [22:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 956, loss 4.648582935333252\n",
      "Epoch 1: |          | 957/? [22:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 957, loss 5.106838703155518\n",
      "Epoch 1: |          | 958/? [22:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 958, loss 5.412562847137451\n",
      "Epoch 1: |          | 959/? [22:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 959, loss 5.449202537536621\n",
      "Epoch 1: |          | 960/? [22:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 960, loss 5.2826151847839355\n",
      "Epoch 1: |          | 961/? [22:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 961, loss 5.288689613342285\n",
      "Epoch 1: |          | 962/? [22:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 962, loss 5.026642799377441\n",
      "Epoch 1: |          | 963/? [22:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 963, loss 4.493441581726074\n",
      "Epoch 1: |          | 964/? [22:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 964, loss 5.054922580718994\n",
      "Epoch 1: |          | 965/? [22:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 965, loss 4.608860969543457\n",
      "Epoch 1: |          | 966/? [22:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 966, loss 4.45903205871582\n",
      "Epoch 1: |          | 967/? [22:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 967, loss 4.709293365478516\n",
      "Epoch 1: |          | 968/? [22:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 968, loss 4.800206661224365\n",
      "Epoch 1: |          | 969/? [22:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 969, loss 4.435779571533203\n",
      "Epoch 1: |          | 970/? [22:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 970, loss 4.991315841674805\n",
      "Epoch 1: |          | 971/? [23:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 971, loss 5.709441184997559\n",
      "Epoch 1: |          | 972/? [23:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 972, loss 4.7153825759887695\n",
      "Epoch 1: |          | 973/? [23:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 973, loss 5.131919860839844\n",
      "Epoch 1: |          | 974/? [23:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 974, loss 4.740212440490723\n",
      "Epoch 1: |          | 975/? [23:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 975, loss 4.791353225708008\n",
      "Epoch 1: |          | 976/? [23:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 976, loss 4.932707786560059\n",
      "Epoch 1: |          | 977/? [23:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 977, loss 5.529254913330078\n",
      "Epoch 1: |          | 978/? [23:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 978, loss 5.224129676818848\n",
      "Epoch 1: |          | 979/? [23:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 979, loss 5.294288635253906\n",
      "Epoch 1: |          | 980/? [23:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 980, loss 4.557627201080322\n",
      "Epoch 1: |          | 981/? [23:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 981, loss 4.0706939697265625\n",
      "Epoch 1: |          | 982/? [23:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 982, loss 4.878814697265625\n",
      "Epoch 1: |          | 983/? [23:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 983, loss 5.245206356048584\n",
      "Epoch 1: |          | 984/? [23:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 984, loss 4.178577423095703\n",
      "Epoch 1: |          | 985/? [23:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 985, loss 4.569223403930664\n",
      "Epoch 1: |          | 986/? [23:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 986, loss 4.525961875915527\n",
      "Epoch 1: |          | 987/? [23:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 987, loss 4.382340908050537\n",
      "Epoch 1: |          | 988/? [23:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 988, loss 5.181271553039551\n",
      "Epoch 1: |          | 989/? [23:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 989, loss 4.7982659339904785\n",
      "Epoch 1: |          | 990/? [23:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 990, loss 3.8702197074890137\n",
      "Epoch 1: |          | 991/? [23:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 991, loss 4.660611152648926\n",
      "Epoch 1: |          | 992/? [23:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 992, loss 5.508345603942871\n",
      "Epoch 1: |          | 993/? [23:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 993, loss 4.542271614074707\n",
      "Epoch 1: |          | 994/? [23:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 994, loss 4.646930694580078\n",
      "Epoch 1: |          | 995/? [23:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 995, loss 5.103607177734375\n",
      "Epoch 1: |          | 996/? [23:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 996, loss 5.082374095916748\n",
      "Epoch 1: |          | 997/? [23:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 997, loss 4.627070426940918\n",
      "Epoch 1: |          | 998/? [23:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 998, loss 4.917664527893066\n",
      "Epoch 1: |          | 999/? [23:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 999, loss 5.159554481506348\n",
      "Epoch 1: |          | 1000/? [23:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1000, loss 4.683155536651611\n",
      "Epoch 1: |          | 1001/? [23:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1001, loss 5.113003730773926\n",
      "Epoch 1: |          | 1002/? [23:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1002, loss 5.166530132293701\n",
      "Epoch 1: |          | 1003/? [23:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1003, loss 5.16994047164917\n",
      "Epoch 1: |          | 1004/? [23:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1004, loss 3.9692294597625732\n",
      "Epoch 1: |          | 1005/? [23:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1005, loss 4.709198951721191\n",
      "Epoch 1: |          | 1006/? [23:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1006, loss 5.15516996383667\n",
      "Epoch 1: |          | 1007/? [23:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1007, loss 4.775373935699463\n",
      "Epoch 1: |          | 1008/? [23:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1008, loss 4.722953796386719\n",
      "Epoch 1: |          | 1009/? [23:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1009, loss 5.229861259460449\n",
      "Epoch 1: |          | 1010/? [23:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1010, loss 4.1470255851745605\n",
      "Epoch 1: |          | 1011/? [23:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1011, loss 4.897477626800537\n",
      "Epoch 1: |          | 1012/? [24:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1012, loss 4.5319085121154785\n",
      "Epoch 1: |          | 1013/? [24:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1013, loss 4.9955244064331055\n",
      "Epoch 1: |          | 1014/? [24:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1014, loss 5.192486763000488\n",
      "Epoch 1: |          | 1015/? [24:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1015, loss 4.870904445648193\n",
      "Epoch 1: |          | 1016/? [24:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1016, loss 4.824759483337402\n",
      "Epoch 1: |          | 1017/? [24:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1017, loss 4.33500337600708\n",
      "Epoch 1: |          | 1018/? [24:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1018, loss 4.859455108642578\n",
      "Epoch 1: |          | 1019/? [24:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1019, loss 4.786451816558838\n",
      "Epoch 1: |          | 1020/? [24:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1020, loss 4.3148698806762695\n",
      "Epoch 1: |          | 1021/? [24:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1021, loss 4.5175862312316895\n",
      "Epoch 1: |          | 1022/? [24:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1022, loss 4.23321533203125\n",
      "Epoch 1: |          | 1023/? [24:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1023, loss 3.9311912059783936\n",
      "Epoch 1: |          | 1024/? [24:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1024, loss 4.656978607177734\n",
      "Epoch 1: |          | 1025/? [24:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1025, loss 4.593611717224121\n",
      "Epoch 1: |          | 1026/? [24:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1026, loss 3.445465087890625\n",
      "Epoch 1: |          | 1027/? [24:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1027, loss 4.990857124328613\n",
      "Epoch 1: |          | 1028/? [24:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1028, loss 4.698009967803955\n",
      "Epoch 1: |          | 1029/? [24:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1029, loss 4.6539106369018555\n",
      "Epoch 1: |          | 1030/? [24:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1030, loss 4.309147834777832\n",
      "Epoch 1: |          | 1031/? [24:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1031, loss 4.404488563537598\n",
      "Epoch 1: |          | 1032/? [24:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1032, loss 5.194887161254883\n",
      "Epoch 1: |          | 1033/? [24:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1033, loss 5.191293716430664\n",
      "Epoch 1: |          | 1034/? [24:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1034, loss 4.572188377380371\n",
      "Epoch 1: |          | 1035/? [24:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1035, loss 4.612764358520508\n",
      "Epoch 1: |          | 1036/? [24:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1036, loss 4.3507399559021\n",
      "Epoch 1: |          | 1037/? [24:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1037, loss 5.17351770401001\n",
      "Epoch 1: |          | 1038/? [24:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1038, loss 5.315225601196289\n",
      "Epoch 1: |          | 1039/? [24:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1039, loss 5.456105709075928\n",
      "Epoch 1: |          | 1040/? [24:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1040, loss 4.862250328063965\n",
      "Epoch 1: |          | 1041/? [24:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1041, loss 5.226173400878906\n",
      "Epoch 1: |          | 1042/? [24:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1042, loss 4.794142246246338\n",
      "Epoch 1: |          | 1043/? [24:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1043, loss 5.0493011474609375\n",
      "Epoch 1: |          | 1044/? [24:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1044, loss 4.645444393157959\n",
      "Epoch 1: |          | 1045/? [24:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1045, loss 4.223366737365723\n",
      "Epoch 1: |          | 1046/? [24:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1046, loss 3.9992432594299316\n",
      "Epoch 1: |          | 1047/? [24:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1047, loss 5.400506019592285\n",
      "Epoch 1: |          | 1048/? [24:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1048, loss 4.532398700714111\n",
      "Epoch 1: |          | 1049/? [24:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1049, loss 4.77335262298584\n",
      "Epoch 1: |          | 1050/? [24:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1050, loss 4.4282708168029785\n",
      "Epoch 1: |          | 1051/? [24:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1051, loss 4.568121433258057\n",
      "Epoch 1: |          | 1052/? [24:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1052, loss 5.083773136138916\n",
      "Epoch 1: |          | 1053/? [24:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1053, loss 5.146357536315918\n",
      "Epoch 1: |          | 1054/? [25:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1054, loss 4.4954352378845215\n",
      "Epoch 1: |          | 1055/? [25:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1055, loss 4.271458148956299\n",
      "Epoch 1: |          | 1056/? [25:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1056, loss 4.342535018920898\n",
      "Epoch 1: |          | 1057/? [25:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1057, loss 4.980891704559326\n",
      "Epoch 1: |          | 1058/? [25:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1058, loss 4.567883491516113\n",
      "Epoch 1: |          | 1059/? [25:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1059, loss 5.196203708648682\n",
      "Epoch 1: |          | 1060/? [25:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1060, loss 5.076308250427246\n",
      "Epoch 1: |          | 1061/? [25:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1061, loss 3.5558180809020996\n",
      "Epoch 1: |          | 1062/? [25:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1062, loss 5.022730827331543\n",
      "Epoch 1: |          | 1063/? [25:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1063, loss 4.76413106918335\n",
      "Epoch 1: |          | 1064/? [25:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1064, loss 5.008334636688232\n",
      "Epoch 1: |          | 1065/? [25:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1065, loss 3.7793071269989014\n",
      "Epoch 1: |          | 1066/? [25:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1066, loss 4.867502212524414\n",
      "Epoch 1: |          | 1067/? [25:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1067, loss 4.221350193023682\n",
      "Epoch 1: |          | 1068/? [25:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1068, loss 4.467801570892334\n",
      "Epoch 1: |          | 1069/? [25:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1069, loss 4.898355484008789\n",
      "Epoch 1: |          | 1070/? [25:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1070, loss 4.671459197998047\n",
      "Epoch 1: |          | 1071/? [25:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1071, loss 4.986477851867676\n",
      "Epoch 1: |          | 1072/? [25:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1072, loss 5.014691352844238\n",
      "Epoch 1: |          | 1073/? [25:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1073, loss 5.428151607513428\n",
      "Epoch 1: |          | 1074/? [25:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1074, loss 4.7176513671875\n",
      "Epoch 1: |          | 1075/? [25:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1075, loss 4.288145065307617\n",
      "Epoch 1: |          | 1076/? [25:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1076, loss 5.25103759765625\n",
      "Epoch 1: |          | 1077/? [25:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1077, loss 4.590834617614746\n",
      "Epoch 1: |          | 1078/? [25:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1078, loss 4.705488681793213\n",
      "Epoch 1: |          | 1079/? [25:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1079, loss 5.5749006271362305\n",
      "Epoch 1: |          | 1080/? [25:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1080, loss 4.868047714233398\n",
      "Epoch 1: |          | 1081/? [25:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1081, loss 5.095848083496094\n",
      "Epoch 1: |          | 1082/? [25:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1082, loss 4.383595943450928\n",
      "Epoch 1: |          | 1083/? [25:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1083, loss 4.350348472595215\n",
      "Epoch 1: |          | 1084/? [25:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1084, loss 4.114447116851807\n",
      "Epoch 1: |          | 1085/? [25:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1085, loss 4.607844352722168\n",
      "Epoch 1: |          | 1086/? [25:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1086, loss 4.8628106117248535\n",
      "Epoch 1: |          | 1087/? [25:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1087, loss 5.503021240234375\n",
      "Epoch 1: |          | 1088/? [25:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1088, loss 5.026137351989746\n",
      "Epoch 1: |          | 1089/? [25:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1089, loss 5.503015041351318\n",
      "Epoch 1: |          | 1090/? [25:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1090, loss 5.010983943939209\n",
      "Epoch 1: |          | 1091/? [25:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1091, loss 4.674258708953857\n",
      "Epoch 1: |          | 1092/? [25:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1092, loss 4.820621490478516\n",
      "Epoch 1: |          | 1093/? [25:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1093, loss 4.3607940673828125\n",
      "Epoch 1: |          | 1094/? [25:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1094, loss 4.815406799316406\n",
      "Epoch 1: |          | 1095/? [25:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1095, loss 4.892134666442871\n",
      "Epoch 1: |          | 1096/? [26:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1096, loss 5.079847812652588\n",
      "Epoch 1: |          | 1097/? [26:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1097, loss 4.663889408111572\n",
      "Epoch 1: |          | 1098/? [26:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1098, loss 3.836535692214966\n",
      "Epoch 1: |          | 1099/? [26:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1099, loss 4.6654815673828125\n",
      "Epoch 1: |          | 1100/? [26:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1100, loss 4.904345512390137\n",
      "Epoch 1: |          | 1101/? [26:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1101, loss 4.373289108276367\n",
      "Epoch 1: |          | 1102/? [26:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1102, loss 5.320444107055664\n",
      "Epoch 1: |          | 1103/? [26:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1103, loss 6.057265281677246\n",
      "Epoch 1: |          | 1104/? [26:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1104, loss 5.085879325866699\n",
      "Epoch 1: |          | 1105/? [26:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1105, loss 5.097412586212158\n",
      "Epoch 1: |          | 1106/? [26:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1106, loss 4.6490631103515625\n",
      "Epoch 1: |          | 1107/? [26:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1107, loss 4.710768699645996\n",
      "Epoch 1: |          | 1108/? [26:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1108, loss 4.657514572143555\n",
      "Epoch 1: |          | 1109/? [26:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1109, loss 4.147680759429932\n",
      "Epoch 1: |          | 1110/? [26:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1110, loss 5.442130088806152\n",
      "Epoch 1: |          | 1111/? [26:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1111, loss 4.987913131713867\n",
      "Epoch 1: |          | 1112/? [26:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1112, loss 4.9055280685424805\n",
      "Epoch 1: |          | 1113/? [26:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1113, loss 4.566206932067871\n",
      "Epoch 1: |          | 1114/? [26:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1114, loss 4.0555524826049805\n",
      "Epoch 1: |          | 1115/? [26:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1115, loss 3.702483654022217\n",
      "Epoch 1: |          | 1116/? [26:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1116, loss 4.346773147583008\n",
      "Epoch 1: |          | 1117/? [26:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1117, loss 4.59666109085083\n",
      "Epoch 1: |          | 1118/? [26:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1118, loss 4.512578964233398\n",
      "Epoch 1: |          | 1119/? [26:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1119, loss 5.3008294105529785\n",
      "Epoch 1: |          | 1120/? [26:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1120, loss 4.8020710945129395\n",
      "Epoch 1: |          | 1121/? [26:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1121, loss 5.080404281616211\n",
      "Epoch 1: |          | 1122/? [26:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1122, loss 4.9323625564575195\n",
      "Epoch 1: |          | 1123/? [26:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1123, loss 4.774727821350098\n",
      "Epoch 1: |          | 1124/? [26:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1124, loss 5.139516353607178\n",
      "Epoch 1: |          | 1125/? [26:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1125, loss 4.322897434234619\n",
      "Epoch 1: |          | 1126/? [26:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1126, loss 4.299745082855225\n",
      "Epoch 1: |          | 1127/? [26:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1127, loss 4.629046440124512\n",
      "Epoch 1: |          | 1128/? [26:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1128, loss 4.624586582183838\n",
      "Epoch 1: |          | 1129/? [26:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1129, loss 4.815890789031982\n",
      "Epoch 1: |          | 1130/? [26:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1130, loss 4.944705486297607\n",
      "Epoch 1: |          | 1131/? [26:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1131, loss 5.040892124176025\n",
      "Epoch 1: |          | 1132/? [26:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1132, loss 3.6154797077178955\n",
      "Epoch 1: |          | 1133/? [26:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1133, loss 4.780924320220947\n",
      "Epoch 1: |          | 1134/? [26:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1134, loss 4.406007289886475\n",
      "Epoch 1: |          | 1135/? [26:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1135, loss 5.08036994934082\n",
      "Epoch 1: |          | 1136/? [26:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1136, loss 4.737501621246338\n",
      "Epoch 1: |          | 1137/? [26:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1137, loss 4.748786449432373\n",
      "Epoch 1: |          | 1138/? [27:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1138, loss 5.333859443664551\n",
      "Epoch 1: |          | 1139/? [27:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1139, loss 6.18353796005249\n",
      "Epoch 1: |          | 1140/? [27:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1140, loss 4.947955131530762\n",
      "Epoch 1: |          | 1141/? [27:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1141, loss 5.103209972381592\n",
      "Epoch 1: |          | 1142/? [27:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1142, loss 5.1571502685546875\n",
      "Epoch 1: |          | 1143/? [27:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1143, loss 5.2603840827941895\n",
      "Epoch 1: |          | 1144/? [27:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1144, loss 4.811262607574463\n",
      "Epoch 1: |          | 1145/? [27:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1145, loss 4.732859134674072\n",
      "Epoch 1: |          | 1146/? [27:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1146, loss 4.838435173034668\n",
      "Epoch 1: |          | 1147/? [27:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1147, loss 4.1398210525512695\n",
      "Epoch 1: |          | 1148/? [27:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1148, loss 4.494013786315918\n",
      "Epoch 1: |          | 1149/? [27:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1149, loss 5.651294708251953\n",
      "Epoch 1: |          | 1150/? [27:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1150, loss 4.913181304931641\n",
      "Epoch 1: |          | 1151/? [27:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1151, loss 5.326300621032715\n",
      "Epoch 1: |          | 1152/? [27:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1152, loss 4.429213523864746\n",
      "Epoch 1: |          | 1153/? [27:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1153, loss 4.781764507293701\n",
      "Epoch 1: |          | 1154/? [27:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1154, loss 4.430621147155762\n",
      "Epoch 1: |          | 1155/? [27:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1155, loss 4.77047061920166\n",
      "Epoch 1: |          | 1156/? [27:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1156, loss 5.013674736022949\n",
      "Epoch 1: |          | 1157/? [27:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1157, loss 4.968145370483398\n",
      "Epoch 1: |          | 1158/? [27:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1158, loss 5.46781587600708\n",
      "Epoch 1: |          | 1159/? [27:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1159, loss 3.8426830768585205\n",
      "Epoch 1: |          | 1160/? [27:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1160, loss 5.170392036437988\n",
      "Epoch 1: |          | 1161/? [27:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1161, loss 5.053808212280273\n",
      "Epoch 1: |          | 1162/? [27:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1162, loss 5.005253791809082\n",
      "Epoch 1: |          | 1163/? [27:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1163, loss 5.432374000549316\n",
      "Epoch 1: |          | 1164/? [27:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1164, loss 5.306910037994385\n",
      "Epoch 1: |          | 1165/? [27:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1165, loss 4.278908729553223\n",
      "Epoch 1: |          | 1166/? [27:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1166, loss 4.9564619064331055\n",
      "Epoch 1: |          | 1167/? [27:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1167, loss 5.221012592315674\n",
      "Epoch 1: |          | 1168/? [27:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1168, loss 5.551669120788574\n",
      "Epoch 1: |          | 1169/? [27:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1169, loss 4.377240180969238\n",
      "Epoch 1: |          | 1170/? [27:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1170, loss 5.035459518432617\n",
      "Epoch 1: |          | 1171/? [27:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1171, loss 4.64395809173584\n",
      "Epoch 1: |          | 1172/? [27:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1172, loss 4.225681304931641\n",
      "Epoch 1: |          | 1173/? [27:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1173, loss 4.951128005981445\n",
      "Epoch 1: |          | 1174/? [27:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1174, loss 4.374621391296387\n",
      "Epoch 1: |          | 1175/? [27:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1175, loss 5.158461570739746\n",
      "Epoch 1: |          | 1176/? [27:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1176, loss 5.139880180358887\n",
      "Epoch 1: |          | 1177/? [27:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1177, loss 5.24144172668457\n",
      "Epoch 1: |          | 1178/? [27:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1178, loss 4.612070560455322\n",
      "Epoch 1: |          | 1179/? [27:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1179, loss 5.106750965118408\n",
      "Epoch 1: |          | 1180/? [28:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1180, loss 4.9031453132629395\n",
      "Epoch 1: |          | 1181/? [28:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1181, loss 4.973702430725098\n",
      "Epoch 1: |          | 1182/? [28:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1182, loss 4.779799461364746\n",
      "Epoch 1: |          | 1183/? [28:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1183, loss 4.483804702758789\n",
      "Epoch 1: |          | 1184/? [28:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1184, loss 4.616881370544434\n",
      "Epoch 1: |          | 1185/? [28:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1185, loss 4.710528373718262\n",
      "Epoch 1: |          | 1186/? [28:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1186, loss 4.868817329406738\n",
      "Epoch 1: |          | 1187/? [28:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1187, loss 4.654849529266357\n",
      "Epoch 1: |          | 1188/? [28:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1188, loss 5.11229944229126\n",
      "Epoch 1: |          | 1189/? [28:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1189, loss 5.134820938110352\n",
      "Epoch 1: |          | 1190/? [28:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1190, loss 4.638640403747559\n",
      "Epoch 1: |          | 1191/? [28:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1191, loss 4.6745991706848145\n",
      "Epoch 1: |          | 1192/? [28:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1192, loss 5.059935569763184\n",
      "Epoch 1: |          | 1193/? [28:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1193, loss 4.4746174812316895\n",
      "Epoch 1: |          | 1194/? [28:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1194, loss 4.0079426765441895\n",
      "Epoch 1: |          | 1195/? [28:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1195, loss 4.839338302612305\n",
      "Epoch 1: |          | 1196/? [28:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1196, loss 4.956497669219971\n",
      "Epoch 1: |          | 1197/? [28:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1197, loss 4.887881278991699\n",
      "Epoch 1: |          | 1198/? [28:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1198, loss 4.848533630371094\n",
      "Epoch 1: |          | 1199/? [28:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1199, loss 5.035723686218262\n",
      "Epoch 1: |          | 1200/? [28:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1200, loss 4.244104385375977\n",
      "Epoch 1: |          | 1201/? [28:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1201, loss 4.978931903839111\n",
      "Epoch 1: |          | 1202/? [28:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1202, loss 4.527121543884277\n",
      "Epoch 1: |          | 1203/? [28:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1203, loss 4.624481201171875\n",
      "Epoch 1: |          | 1204/? [28:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1204, loss 3.9535961151123047\n",
      "Epoch 1: |          | 1205/? [28:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1205, loss 4.718079566955566\n",
      "Epoch 1: |          | 1206/? [28:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1206, loss 4.69985294342041\n",
      "Epoch 1: |          | 1207/? [28:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1207, loss 5.1461710929870605\n",
      "Epoch 1: |          | 1208/? [28:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1208, loss 5.197861671447754\n",
      "Epoch 1: |          | 1209/? [28:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1209, loss 4.7951860427856445\n",
      "Epoch 1: |          | 1210/? [28:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1210, loss 5.119500160217285\n",
      "Epoch 1: |          | 1211/? [28:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1211, loss 5.116276741027832\n",
      "Epoch 1: |          | 1212/? [28:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1212, loss 4.9011149406433105\n",
      "Epoch 1: |          | 1213/? [28:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1213, loss 4.590618133544922\n",
      "Epoch 1: |          | 1214/? [28:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1214, loss 5.378934383392334\n",
      "Epoch 1: |          | 1215/? [28:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1215, loss 4.506250858306885\n",
      "Epoch 1: |          | 1216/? [28:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1216, loss 4.903992652893066\n",
      "Epoch 1: |          | 1217/? [28:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1217, loss 5.015960216522217\n",
      "Epoch 1: |          | 1218/? [28:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1218, loss 5.012424945831299\n",
      "Epoch 1: |          | 1219/? [28:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1219, loss 4.561631202697754\n",
      "Epoch 1: |          | 1220/? [29:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1220, loss 5.35812520980835\n",
      "Epoch 1: |          | 1221/? [29:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1221, loss 5.120820045471191\n",
      "Epoch 1: |          | 1222/? [29:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1222, loss 3.7848377227783203\n",
      "Epoch 1: |          | 1223/? [29:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1223, loss 3.939527988433838\n",
      "Epoch 1: |          | 1224/? [29:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1224, loss 4.408543586730957\n",
      "Epoch 1: |          | 1225/? [29:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1225, loss 5.095322608947754\n",
      "Epoch 1: |          | 1226/? [29:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1226, loss 5.193304538726807\n",
      "Epoch 1: |          | 1227/? [29:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1227, loss 4.709805965423584\n",
      "Epoch 1: |          | 1228/? [29:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1228, loss 4.765416622161865\n",
      "Epoch 1: |          | 1229/? [29:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1229, loss 4.204035758972168\n",
      "Epoch 1: |          | 1230/? [29:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1230, loss 4.9120683670043945\n",
      "Epoch 1: |          | 1231/? [29:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1231, loss 4.97424840927124\n",
      "Epoch 1: |          | 1232/? [29:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1232, loss 5.1477789878845215\n",
      "Epoch 1: |          | 1233/? [29:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1233, loss 4.933475017547607\n",
      "Epoch 1: |          | 1234/? [29:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1234, loss 3.7641921043395996\n",
      "Epoch 1: |          | 1235/? [29:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1235, loss 4.912088394165039\n",
      "Epoch 1: |          | 1236/? [29:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1236, loss 4.322822093963623\n",
      "Epoch 1: |          | 1237/? [29:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1237, loss 4.770114421844482\n",
      "Epoch 1: |          | 1238/? [29:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1238, loss 4.601044654846191\n",
      "Epoch 1: |          | 1239/? [29:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1239, loss 4.5765252113342285\n",
      "Epoch 1: |          | 1240/? [29:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1240, loss 5.299902439117432\n",
      "Epoch 1: |          | 1241/? [29:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1241, loss 4.747394561767578\n",
      "Epoch 1: |          | 1242/? [29:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1242, loss 4.611607551574707\n",
      "Epoch 1: |          | 1243/? [29:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1243, loss 4.329785346984863\n",
      "Epoch 1: |          | 1244/? [29:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1244, loss 4.553489685058594\n",
      "Epoch 1: |          | 1245/? [29:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1245, loss 4.080641269683838\n",
      "Epoch 1: |          | 1246/? [29:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1246, loss 4.975310325622559\n",
      "Epoch 1: |          | 1247/? [29:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1247, loss 4.969965934753418\n",
      "Epoch 1: |          | 1248/? [29:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1248, loss 4.382025241851807\n",
      "Epoch 1: |          | 1249/? [29:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1249, loss 4.543964862823486\n",
      "Epoch 1: |          | 1250/? [29:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1250, loss 4.730738639831543\n",
      "Epoch 1: |          | 1251/? [29:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1251, loss 4.494157314300537\n",
      "Epoch 1: |          | 1252/? [29:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1252, loss 5.286768913269043\n",
      "Epoch 1: |          | 1253/? [29:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1253, loss 4.675603866577148\n",
      "Epoch 1: |          | 1254/? [29:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1254, loss 3.879591703414917\n",
      "Epoch 1: |          | 1255/? [29:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1255, loss 5.366342067718506\n",
      "Epoch 1: |          | 1256/? [29:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1256, loss 4.340452194213867\n",
      "Epoch 1: |          | 1257/? [29:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1257, loss 4.413856506347656\n",
      "Epoch 1: |          | 1258/? [29:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1258, loss 5.088767051696777\n",
      "Epoch 1: |          | 1259/? [29:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1259, loss 4.890592575073242\n",
      "Epoch 1: |          | 1260/? [29:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1260, loss 5.2563018798828125\n",
      "Epoch 1: |          | 1261/? [29:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1261, loss 4.697728633880615\n",
      "Epoch 1: |          | 1262/? [30:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1262, loss 4.498429298400879\n",
      "Epoch 1: |          | 1263/? [30:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1263, loss 5.03216028213501\n",
      "Epoch 1: |          | 1264/? [30:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1264, loss 5.245666027069092\n",
      "Epoch 1: |          | 1265/? [30:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1265, loss 4.989994525909424\n",
      "Epoch 1: |          | 1266/? [30:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1266, loss 4.593501091003418\n",
      "Epoch 1: |          | 1267/? [30:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1267, loss 4.737780570983887\n",
      "Epoch 1: |          | 1268/? [30:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1268, loss 4.605981349945068\n",
      "Epoch 1: |          | 1269/? [30:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1269, loss 4.102657318115234\n",
      "Epoch 1: |          | 1270/? [30:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1270, loss 4.493653297424316\n",
      "Epoch 1: |          | 1271/? [30:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1271, loss 4.697472095489502\n",
      "Epoch 1: |          | 1272/? [30:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1272, loss 4.171875953674316\n",
      "Epoch 1: |          | 1273/? [30:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1273, loss 5.1105637550354\n",
      "Epoch 1: |          | 1274/? [30:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1274, loss 3.866875410079956\n",
      "Epoch 1: |          | 1275/? [30:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1275, loss 4.356119632720947\n",
      "Epoch 1: |          | 1276/? [30:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1276, loss 4.84830379486084\n",
      "Epoch 1: |          | 1277/? [30:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1277, loss 4.351039886474609\n",
      "Epoch 1: |          | 1278/? [30:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1278, loss 4.7420783042907715\n",
      "Epoch 1: |          | 1279/? [30:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1279, loss 4.9413933753967285\n",
      "Epoch 1: |          | 1280/? [30:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1280, loss 4.014627933502197\n",
      "Epoch 1: |          | 1281/? [30:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1281, loss 4.444680690765381\n",
      "Epoch 1: |          | 1282/? [30:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1282, loss 4.19562292098999\n",
      "Epoch 1: |          | 1283/? [30:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1283, loss 5.0594282150268555\n",
      "Epoch 1: |          | 1284/? [30:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1284, loss 3.9164586067199707\n",
      "Epoch 1: |          | 1285/? [30:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1285, loss 5.2429914474487305\n",
      "Epoch 1: |          | 1286/? [30:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1286, loss 3.573882579803467\n",
      "Epoch 1: |          | 1287/? [30:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1287, loss 5.015361785888672\n",
      "Epoch 1: |          | 1288/? [30:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1288, loss 4.836338043212891\n",
      "Epoch 1: |          | 1289/? [30:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1289, loss 3.704145908355713\n",
      "Epoch 1: |          | 1290/? [30:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1290, loss 4.906622886657715\n",
      "Epoch 1: |          | 1291/? [30:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1291, loss 5.734438896179199\n",
      "Epoch 1: |          | 1292/? [30:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1292, loss 5.190807342529297\n",
      "Epoch 1: |          | 1293/? [30:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1293, loss 4.455731391906738\n",
      "Epoch 1: |          | 1294/? [30:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1294, loss 4.705105781555176\n",
      "Epoch 1: |          | 1295/? [30:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1295, loss 4.744622707366943\n",
      "Epoch 1: |          | 1296/? [30:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1296, loss 3.8838119506835938\n",
      "Epoch 1: |          | 1297/? [30:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1297, loss 5.038539886474609\n",
      "Epoch 1: |          | 1298/? [30:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1298, loss 4.743434906005859\n",
      "Epoch 1: |          | 1299/? [30:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1299, loss 3.649826765060425\n",
      "Epoch 1: |          | 1300/? [30:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1300, loss 4.7733378410339355\n",
      "Epoch 1: |          | 1301/? [30:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1301, loss 4.516897201538086\n",
      "Epoch 1: |          | 1302/? [30:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1302, loss 4.639039516448975\n",
      "Epoch 1: |          | 1303/? [30:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1303, loss 4.404511451721191\n",
      "Epoch 1: |          | 1304/? [31:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1304, loss 5.294857025146484\n",
      "Epoch 1: |          | 1305/? [31:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1305, loss 4.036762237548828\n",
      "Epoch 1: |          | 1306/? [31:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1306, loss 4.684436321258545\n",
      "Epoch 1: |          | 1307/? [31:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1307, loss 4.165398597717285\n",
      "Epoch 1: |          | 1308/? [31:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1308, loss 4.118536949157715\n",
      "Epoch 1: |          | 1309/? [31:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1309, loss 4.428135871887207\n",
      "Epoch 1: |          | 1310/? [31:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1310, loss 4.824469566345215\n",
      "Epoch 1: |          | 1311/? [31:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1311, loss 4.224738121032715\n",
      "Epoch 1: |          | 1312/? [31:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1312, loss 3.9558463096618652\n",
      "Epoch 1: |          | 1313/? [31:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1313, loss 5.2148637771606445\n",
      "Epoch 1: |          | 1314/? [31:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1314, loss 4.3391218185424805\n",
      "Epoch 1: |          | 1315/? [31:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1315, loss 5.197910785675049\n",
      "Epoch 1: |          | 1316/? [31:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1316, loss 4.885439872741699\n",
      "Epoch 1: |          | 1317/? [31:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1317, loss 4.4898152351379395\n",
      "Epoch 1: |          | 1318/? [31:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1318, loss 4.746453762054443\n",
      "Epoch 1: |          | 1319/? [31:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1319, loss 4.897098541259766\n",
      "Epoch 1: |          | 1320/? [31:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1320, loss 4.526704788208008\n",
      "Epoch 1: |          | 1321/? [31:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1321, loss 4.897865295410156\n",
      "Epoch 1: |          | 1322/? [31:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1322, loss 5.034079551696777\n",
      "Epoch 1: |          | 1323/? [31:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1323, loss 4.3048810958862305\n",
      "Epoch 1: |          | 1324/? [31:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1324, loss 5.309753894805908\n",
      "Epoch 1: |          | 1325/? [31:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1325, loss 5.384693622589111\n",
      "Epoch 1: |          | 1326/? [31:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1326, loss 4.8311238288879395\n",
      "Epoch 1: |          | 1327/? [31:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1327, loss 4.630958557128906\n",
      "Epoch 1: |          | 1328/? [31:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1328, loss 4.301458358764648\n",
      "Epoch 1: |          | 1329/? [31:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1329, loss 5.116208076477051\n",
      "Epoch 1: |          | 1330/? [31:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1330, loss 4.722209453582764\n",
      "Epoch 1: |          | 1331/? [31:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1331, loss 4.817547798156738\n",
      "Epoch 1: |          | 1332/? [31:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1332, loss 4.655365943908691\n",
      "Epoch 1: |          | 1333/? [31:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1333, loss 4.466769218444824\n",
      "Epoch 1: |          | 1334/? [31:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1334, loss 4.635420799255371\n",
      "Epoch 1: |          | 1335/? [31:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1335, loss 4.62581729888916\n",
      "Epoch 1: |          | 1336/? [31:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1336, loss 4.18371057510376\n",
      "Epoch 1: |          | 1337/? [31:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1337, loss 4.937352657318115\n",
      "Epoch 1: |          | 1338/? [31:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1338, loss 3.9055933952331543\n",
      "Epoch 1: |          | 1339/? [31:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1339, loss 4.619769096374512\n",
      "Epoch 1: |          | 1340/? [31:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1340, loss 4.013883590698242\n",
      "Epoch 1: |          | 1341/? [31:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1341, loss 4.923243522644043\n",
      "Epoch 1: |          | 1342/? [31:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1342, loss 5.1635332107543945\n",
      "Epoch 1: |          | 1343/? [31:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1343, loss 4.533844947814941\n",
      "Epoch 1: |          | 1344/? [31:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1344, loss 4.670742988586426\n",
      "Epoch 1: |          | 1345/? [31:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1345, loss 4.75262451171875\n",
      "Epoch 1: |          | 1346/? [32:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1346, loss 6.0106000900268555\n",
      "Epoch 1: |          | 1347/? [32:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1347, loss 5.153096675872803\n",
      "Epoch 1: |          | 1348/? [32:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1348, loss 5.431096076965332\n",
      "Epoch 1: |          | 1349/? [32:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1349, loss 4.972832679748535\n",
      "Epoch 1: |          | 1350/? [32:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1350, loss 5.410876274108887\n",
      "Epoch 1: |          | 1351/? [32:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1351, loss 5.024676322937012\n",
      "Epoch 1: |          | 1352/? [32:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1352, loss 4.045456886291504\n",
      "Epoch 1: |          | 1353/? [32:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1353, loss 4.361985683441162\n",
      "Epoch 1: |          | 1354/? [32:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1354, loss 5.069958209991455\n",
      "Epoch 1: |          | 1355/? [32:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1355, loss 5.212168216705322\n",
      "Epoch 1: |          | 1356/? [32:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1356, loss 4.827247619628906\n",
      "Epoch 1: |          | 1357/? [32:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1357, loss 4.51503849029541\n",
      "Epoch 1: |          | 1358/? [32:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1358, loss 4.782715797424316\n",
      "Epoch 1: |          | 1359/? [32:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1359, loss 4.575342655181885\n",
      "Epoch 1: |          | 1360/? [32:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1360, loss 4.814309120178223\n",
      "Epoch 1: |          | 1361/? [32:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1361, loss 4.760745048522949\n",
      "Epoch 1: |          | 1362/? [32:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1362, loss 4.633251190185547\n",
      "Epoch 1: |          | 1363/? [32:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1363, loss 4.001768589019775\n",
      "Epoch 1: |          | 1364/? [32:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1364, loss 4.550013065338135\n",
      "Epoch 1: |          | 1365/? [32:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1365, loss 4.180715084075928\n",
      "Epoch 1: |          | 1366/? [32:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1366, loss 4.948470115661621\n",
      "Epoch 1: |          | 1367/? [32:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1367, loss 4.225101470947266\n",
      "Epoch 1: |          | 1368/? [32:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1368, loss 4.035571098327637\n",
      "Epoch 1: |          | 1369/? [32:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1369, loss 4.727084159851074\n",
      "Epoch 1: |          | 1370/? [32:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1370, loss 4.153158664703369\n",
      "Epoch 1: |          | 1371/? [32:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1371, loss 5.246359348297119\n",
      "Epoch 1: |          | 1372/? [32:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1372, loss 4.502624034881592\n",
      "Epoch 1: |          | 1373/? [32:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1373, loss 5.097710609436035\n",
      "Epoch 1: |          | 1374/? [32:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1374, loss 4.0917863845825195\n",
      "Epoch 1: |          | 1375/? [32:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1375, loss 4.739135265350342\n",
      "Epoch 1: |          | 1376/? [32:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1376, loss 4.770826816558838\n",
      "Epoch 1: |          | 1377/? [32:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1377, loss 4.612679481506348\n",
      "Epoch 1: |          | 1378/? [32:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1378, loss 4.947292327880859\n",
      "Epoch 1: |          | 1379/? [32:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1379, loss 4.767293453216553\n",
      "Epoch 1: |          | 1380/? [32:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1380, loss 4.83131217956543\n",
      "Epoch 1: |          | 1381/? [32:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1381, loss 4.9479875564575195\n",
      "Epoch 1: |          | 1382/? [32:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1382, loss 4.434238433837891\n",
      "Epoch 1: |          | 1383/? [32:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1383, loss 4.786995887756348\n",
      "Epoch 1: |          | 1384/? [32:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1384, loss 4.73264217376709\n",
      "Epoch 1: |          | 1385/? [32:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1385, loss 4.671565532684326\n",
      "Epoch 1: |          | 1386/? [32:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1386, loss 4.576138019561768\n",
      "Epoch 1: |          | 1387/? [32:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1387, loss 4.697602272033691\n",
      "Epoch 1: |          | 1388/? [33:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1388, loss 3.929871082305908\n",
      "Epoch 1: |          | 1389/? [33:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1389, loss 4.953724384307861\n",
      "Epoch 1: |          | 1390/? [33:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1390, loss 5.250475883483887\n",
      "Epoch 1: |          | 1391/? [33:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1391, loss 4.858853340148926\n",
      "Epoch 1: |          | 1392/? [33:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1392, loss 4.273303031921387\n",
      "Epoch 1: |          | 1393/? [33:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1393, loss 4.569091796875\n",
      "Epoch 1: |          | 1394/? [33:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1394, loss 4.355100631713867\n",
      "Epoch 1: |          | 1395/? [33:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1395, loss 4.849242210388184\n",
      "Epoch 1: |          | 1396/? [33:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1396, loss 4.796296119689941\n",
      "Epoch 1: |          | 1397/? [33:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1397, loss 3.8577167987823486\n",
      "Epoch 1: |          | 1398/? [33:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1398, loss 5.115434169769287\n",
      "Epoch 1: |          | 1399/? [33:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1399, loss 5.227684497833252\n",
      "Epoch 1: |          | 1400/? [33:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1400, loss 4.169118404388428\n",
      "Epoch 1: |          | 1401/? [33:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1401, loss 5.192009925842285\n",
      "Epoch 1: |          | 1402/? [33:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1402, loss 4.648512840270996\n",
      "Epoch 1: |          | 1403/? [33:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1403, loss 4.873051643371582\n",
      "Epoch 1: |          | 1404/? [33:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1404, loss 4.7930073738098145\n",
      "Epoch 1: |          | 1405/? [33:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1405, loss 5.188967227935791\n",
      "Epoch 1: |          | 1406/? [33:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1406, loss 5.034863471984863\n",
      "Epoch 1: |          | 1407/? [33:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1407, loss 5.282727241516113\n",
      "Epoch 1: |          | 1408/? [33:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1408, loss 4.417441368103027\n",
      "Epoch 1: |          | 1409/? [33:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1409, loss 4.547173500061035\n",
      "Epoch 1: |          | 1410/? [33:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1410, loss 4.610062599182129\n",
      "Epoch 1: |          | 1411/? [33:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1411, loss 5.0462646484375\n",
      "Epoch 1: |          | 1412/? [33:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1412, loss 4.367135047912598\n",
      "Epoch 1: |          | 1413/? [33:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1413, loss 4.221751689910889\n",
      "Epoch 1: |          | 1414/? [33:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1414, loss 4.338932991027832\n",
      "Epoch 1: |          | 1415/? [33:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1415, loss 4.759860992431641\n",
      "Epoch 1: |          | 1416/? [33:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1416, loss 5.136946678161621\n",
      "Epoch 1: |          | 1417/? [33:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1417, loss 4.640676498413086\n",
      "Epoch 1: |          | 1418/? [33:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1418, loss 4.942787170410156\n",
      "Epoch 1: |          | 1419/? [33:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1419, loss 4.594727993011475\n",
      "Epoch 1: |          | 1420/? [33:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1420, loss 4.555295467376709\n",
      "Epoch 1: |          | 1421/? [33:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1421, loss 4.0500407218933105\n",
      "Epoch 1: |          | 1422/? [33:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1422, loss 5.036732196807861\n",
      "Epoch 1: |          | 1423/? [33:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1423, loss 5.075264930725098\n",
      "Epoch 1: |          | 1424/? [33:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1424, loss 4.571164608001709\n",
      "Epoch 1: |          | 1425/? [33:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1425, loss 4.901429653167725\n",
      "Epoch 1: |          | 1426/? [33:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1426, loss 4.3490424156188965\n",
      "Epoch 1: |          | 1427/? [33:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1427, loss 4.993202209472656\n",
      "Epoch 1: |          | 1428/? [33:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1428, loss 5.0009765625\n",
      "Epoch 1: |          | 1429/? [33:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1429, loss 4.960455894470215\n",
      "Epoch 1: |          | 1430/? [34:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1430, loss 5.033103942871094\n",
      "Epoch 1: |          | 1431/? [34:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1431, loss 4.771594047546387\n",
      "Epoch 1: |          | 1432/? [34:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1432, loss 4.805954933166504\n",
      "Epoch 1: |          | 1433/? [34:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1433, loss 4.649974822998047\n",
      "Epoch 1: |          | 1434/? [34:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1434, loss 4.788265228271484\n",
      "Epoch 1: |          | 1435/? [34:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1435, loss 4.331752300262451\n",
      "Epoch 1: |          | 1436/? [34:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1436, loss 4.710696220397949\n",
      "Epoch 1: |          | 1437/? [34:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1437, loss 3.9478092193603516\n",
      "Epoch 1: |          | 1438/? [34:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1438, loss 5.781339645385742\n",
      "Epoch 1: |          | 1439/? [34:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1439, loss 4.931988716125488\n",
      "Epoch 1: |          | 1440/? [34:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1440, loss 4.883929252624512\n",
      "Epoch 1: |          | 1441/? [34:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1441, loss 5.2136101722717285\n",
      "Epoch 1: |          | 1442/? [34:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1442, loss 5.269445896148682\n",
      "Epoch 1: |          | 1443/? [34:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1443, loss 4.287051677703857\n",
      "Epoch 1: |          | 1444/? [34:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1444, loss 4.445507049560547\n",
      "Epoch 1: |          | 1445/? [34:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1445, loss 5.0369791984558105\n",
      "Epoch 1: |          | 1446/? [34:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1446, loss 4.56460428237915\n",
      "Epoch 1: |          | 1447/? [34:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1447, loss 4.745092391967773\n",
      "Epoch 1: |          | 1448/? [34:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1448, loss 4.487239360809326\n",
      "Epoch 1: |          | 1449/? [34:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1449, loss 4.804165840148926\n",
      "Epoch 1: |          | 1450/? [34:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1450, loss 4.940672397613525\n",
      "Epoch 1: |          | 1451/? [34:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1451, loss 5.256608486175537\n",
      "Epoch 1: |          | 1452/? [34:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1452, loss 4.75410795211792\n",
      "Epoch 1: |          | 1453/? [34:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1453, loss 4.004553318023682\n",
      "Epoch 1: |          | 1454/? [34:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1454, loss 4.657036781311035\n",
      "Epoch 1: |          | 1455/? [34:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1455, loss 4.831230163574219\n",
      "Epoch 1: |          | 1456/? [34:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1456, loss 4.330117225646973\n",
      "Epoch 1: |          | 1457/? [34:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1457, loss 4.501141548156738\n",
      "Epoch 1: |          | 1458/? [34:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1458, loss 4.589023590087891\n",
      "Epoch 1: |          | 1459/? [34:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1459, loss 4.9200310707092285\n",
      "Epoch 1: |          | 1460/? [34:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1460, loss 4.728459358215332\n",
      "Epoch 1: |          | 1461/? [34:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1461, loss 4.817526340484619\n",
      "Epoch 1: |          | 1462/? [34:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1462, loss 5.1643195152282715\n",
      "Epoch 1: |          | 1463/? [34:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1463, loss 5.035998344421387\n",
      "Epoch 1: |          | 1464/? [34:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1464, loss 4.344124794006348\n",
      "Epoch 1: |          | 1465/? [34:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1465, loss 4.659130573272705\n",
      "Epoch 1: |          | 1466/? [34:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1466, loss 4.302572727203369\n",
      "Epoch 1: |          | 1467/? [34:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1467, loss 5.106832504272461\n",
      "Epoch 1: |          | 1468/? [34:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1468, loss 4.685737609863281\n",
      "Epoch 1: |          | 1469/? [34:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1469, loss 4.190255165100098\n",
      "Epoch 1: |          | 1470/? [34:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1470, loss 4.962599277496338\n",
      "Epoch 1: |          | 1471/? [34:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1471, loss 5.008077621459961\n",
      "Epoch 1: |          | 1472/? [35:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1472, loss 4.694501876831055\n",
      "Epoch 1: |          | 1473/? [35:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1473, loss 4.568584442138672\n",
      "Epoch 1: |          | 1474/? [35:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1474, loss 4.4354119300842285\n",
      "Epoch 1: |          | 1475/? [35:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1475, loss 4.128969192504883\n",
      "Epoch 1: |          | 1476/? [35:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1476, loss 4.716878414154053\n",
      "Epoch 1: |          | 1477/? [35:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1477, loss 4.762283802032471\n",
      "Epoch 1: |          | 1478/? [35:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1478, loss 4.5978169441223145\n",
      "Epoch 1: |          | 1479/? [35:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1479, loss 5.234909534454346\n",
      "Epoch 1: |          | 1480/? [35:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1480, loss 4.931813716888428\n",
      "Epoch 1: |          | 1481/? [35:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1481, loss 4.667773246765137\n",
      "Epoch 1: |          | 1482/? [35:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1482, loss 4.742274284362793\n",
      "Epoch 1: |          | 1483/? [35:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1483, loss 4.262967109680176\n",
      "Epoch 1: |          | 1484/? [35:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1484, loss 4.527660369873047\n",
      "Epoch 1: |          | 1485/? [35:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1485, loss 4.8468427658081055\n",
      "Epoch 1: |          | 1486/? [35:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1486, loss 4.670191764831543\n",
      "Epoch 1: |          | 1487/? [35:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1487, loss 4.196914196014404\n",
      "Epoch 1: |          | 1488/? [35:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1488, loss 4.906401634216309\n",
      "Epoch 1: |          | 1489/? [35:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1489, loss 4.82570743560791\n",
      "Epoch 1: |          | 1490/? [35:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1490, loss 4.777144432067871\n",
      "Epoch 1: |          | 1491/? [35:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1491, loss 3.445880889892578\n",
      "Epoch 1: |          | 1492/? [35:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1492, loss 4.179312229156494\n",
      "Epoch 1: |          | 1493/? [35:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1493, loss 4.166644096374512\n",
      "Epoch 1: |          | 1494/? [35:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1494, loss 4.6459245681762695\n",
      "Epoch 1: |          | 1495/? [35:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1495, loss 4.531018257141113\n",
      "Epoch 1: |          | 1496/? [35:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1496, loss 4.869404315948486\n",
      "Epoch 1: |          | 1497/? [35:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1497, loss 4.019354343414307\n",
      "Epoch 1: |          | 1498/? [35:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1498, loss 4.39359712600708\n",
      "Epoch 1: |          | 1499/? [35:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1499, loss 5.067818641662598\n",
      "Epoch 1: |          | 1500/? [35:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1500, loss 4.916872024536133\n",
      "Epoch 1: |          | 1501/? [35:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1501, loss 4.667347431182861\n",
      "Epoch 1: |          | 1502/? [35:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1502, loss 4.859084129333496\n",
      "Epoch 1: |          | 1503/? [35:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1503, loss 4.47200870513916\n",
      "Epoch 1: |          | 1504/? [35:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1504, loss 5.1153459548950195\n",
      "Epoch 1: |          | 1505/? [35:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1505, loss 5.068742275238037\n",
      "Epoch 1: |          | 1506/? [35:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1506, loss 4.673102378845215\n",
      "Epoch 1: |          | 1507/? [35:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1507, loss 4.434449672698975\n",
      "Epoch 1: |          | 1508/? [35:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1508, loss 4.631470203399658\n",
      "Epoch 1: |          | 1509/? [35:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1509, loss 4.575949668884277\n",
      "Epoch 1: |          | 1510/? [35:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1510, loss 4.877859115600586\n",
      "Epoch 1: |          | 1511/? [35:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1511, loss 4.379853248596191\n",
      "Epoch 1: |          | 1512/? [36:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1512, loss 5.255681037902832\n",
      "Epoch 1: |          | 1513/? [36:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1513, loss 5.207037448883057\n",
      "Epoch 1: |          | 1514/? [36:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1514, loss 4.28133487701416\n",
      "Epoch 1: |          | 1515/? [36:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1515, loss 5.345858573913574\n",
      "Epoch 1: |          | 1516/? [36:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1516, loss 5.00137996673584\n",
      "Epoch 1: |          | 1517/? [36:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1517, loss 4.510129928588867\n",
      "Epoch 1: |          | 1518/? [36:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1518, loss 4.3239006996154785\n",
      "Epoch 1: |          | 1519/? [36:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1519, loss 4.869473457336426\n",
      "Epoch 1: |          | 1520/? [36:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1520, loss 5.639545440673828\n",
      "Epoch 1: |          | 1521/? [36:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1521, loss 4.634524345397949\n",
      "Epoch 1: |          | 1522/? [36:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1522, loss 4.328043460845947\n",
      "Epoch 1: |          | 1523/? [36:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1523, loss 4.764586448669434\n",
      "Epoch 1: |          | 1524/? [36:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1524, loss 4.699690818786621\n",
      "Epoch 1: |          | 1525/? [36:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1525, loss 4.539559841156006\n",
      "Epoch 1: |          | 1526/? [36:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1526, loss 4.938282489776611\n",
      "Epoch 1: |          | 1527/? [36:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1527, loss 4.958155155181885\n",
      "Epoch 1: |          | 1528/? [36:23<00:00,  0.70it/s, v_num=31]ERROR: Input has inproper shape\n",
      "Epoch 1: |          | 1529/? [36:23<00:00,  0.70it/s, v_num=31]   VALIDATION: Batch 0, loss 5.166874408721924\n",
      "   VALIDATION: Batch 1, loss 4.034140586853027\n",
      "   VALIDATION: Batch 2, loss 5.288653373718262\n",
      "   VALIDATION: Batch 3, loss 4.9667840003967285\n",
      "   VALIDATION: Batch 4, loss 4.55257511138916\n",
      "   VALIDATION: Batch 5, loss 4.136272430419922\n",
      "   VALIDATION: Batch 6, loss 4.414438247680664\n",
      "   VALIDATION: Batch 7, loss 5.096083641052246\n",
      "   VALIDATION: Batch 8, loss 5.0543599128723145\n",
      "   VALIDATION: Batch 9, loss 5.053069114685059\n",
      "   VALIDATION: Batch 10, loss 4.829283237457275\n",
      "   VALIDATION: Batch 11, loss 4.403212547302246\n",
      "   VALIDATION: Batch 12, loss 4.779677391052246\n",
      "   VALIDATION: Batch 13, loss 5.171515464782715\n",
      "   VALIDATION: Batch 14, loss 4.649127006530762\n",
      "   VALIDATION: Batch 15, loss 4.501836776733398\n",
      "   VALIDATION: Batch 16, loss 5.046054363250732\n",
      "   VALIDATION: Batch 17, loss 4.749207973480225\n",
      "   VALIDATION: Batch 18, loss 3.9286105632781982\n",
      "   VALIDATION: Batch 19, loss 4.997502326965332\n",
      "   VALIDATION: Batch 20, loss 5.1681718826293945\n",
      "   VALIDATION: Batch 21, loss 5.374359130859375\n",
      "   VALIDATION: Batch 22, loss 5.035608291625977\n",
      "   VALIDATION: Batch 23, loss 4.575206756591797\n",
      "   VALIDATION: Batch 24, loss 4.500508785247803\n",
      "   VALIDATION: Batch 25, loss 4.910561561584473\n",
      "   VALIDATION: Batch 26, loss 5.036986351013184\n",
      "   VALIDATION: Batch 27, loss 4.958877086639404\n",
      "   VALIDATION: Batch 28, loss 4.635332107543945\n",
      "   VALIDATION: Batch 29, loss 4.992846488952637\n",
      "   VALIDATION: Batch 30, loss 4.515936851501465\n",
      "   VALIDATION: Batch 31, loss 4.810262203216553\n",
      "   VALIDATION: Batch 32, loss 5.419323921203613\n",
      "   VALIDATION: Batch 33, loss 3.5201773643493652\n",
      "   VALIDATION: Batch 34, loss 4.731614589691162\n",
      "   VALIDATION: Batch 35, loss 5.007119178771973\n",
      "   VALIDATION: Batch 36, loss 4.5526018142700195\n",
      "   VALIDATION: Batch 37, loss 4.29992151260376\n",
      "   VALIDATION: Batch 38, loss 4.362969398498535\n",
      "   VALIDATION: Batch 39, loss 4.787770748138428\n",
      "   VALIDATION: Batch 40, loss 4.965456485748291\n",
      "   VALIDATION: Batch 41, loss 3.706514358520508\n",
      "   VALIDATION: Batch 42, loss 4.845559120178223\n",
      "   VALIDATION: Batch 43, loss 5.07199764251709\n",
      "   VALIDATION: Batch 44, loss 4.549872398376465\n",
      "   VALIDATION: Batch 45, loss 5.105096817016602\n",
      "   VALIDATION: Batch 46, loss 4.236856937408447\n",
      "   VALIDATION: Batch 47, loss 5.24272346496582\n",
      "   VALIDATION: Batch 48, loss 5.226040840148926\n",
      "   VALIDATION: Batch 49, loss 4.964073181152344\n",
      "   VALIDATION: Batch 50, loss 4.90714693069458\n",
      "   VALIDATION: Batch 51, loss 5.371903419494629\n",
      "   VALIDATION: Batch 52, loss 4.426520347595215\n",
      "   VALIDATION: Batch 53, loss 4.356153964996338\n",
      "   VALIDATION: Batch 54, loss 4.399809837341309\n",
      "   VALIDATION: Batch 55, loss 5.31594181060791\n",
      "   VALIDATION: Batch 56, loss 4.735479354858398\n",
      "   VALIDATION: Batch 57, loss 6.1617536544799805\n",
      "   VALIDATION: Batch 58, loss 4.73862361907959\n",
      "   VALIDATION: Batch 59, loss 4.3858747482299805\n",
      "   VALIDATION: Batch 60, loss 3.8687705993652344\n",
      "   VALIDATION: Batch 61, loss 4.764557838439941\n",
      "   VALIDATION: Batch 62, loss 4.788609027862549\n",
      "   VALIDATION: Batch 63, loss 5.182131767272949\n",
      "   VALIDATION: Batch 64, loss 5.049630165100098\n",
      "   VALIDATION: Batch 65, loss 4.180166244506836\n",
      "   VALIDATION: Batch 66, loss 5.125274181365967\n",
      "   VALIDATION: Batch 67, loss 4.529085636138916\n",
      "   VALIDATION: Batch 68, loss 4.7099223136901855\n",
      "   VALIDATION: Batch 69, loss 5.044268608093262\n",
      "   VALIDATION: Batch 70, loss 5.225849151611328\n",
      "   VALIDATION: Batch 71, loss 4.614461421966553\n",
      "   VALIDATION: Batch 72, loss 5.531363487243652\n",
      "   VALIDATION: Batch 73, loss 4.322541236877441\n",
      "   VALIDATION: Batch 74, loss 4.989068031311035\n",
      "   VALIDATION: Batch 75, loss 5.082309246063232\n",
      "   VALIDATION: Batch 76, loss 4.8852057456970215\n",
      "   VALIDATION: Batch 77, loss 5.0877580642700195\n",
      "   VALIDATION: Batch 78, loss 4.900028228759766\n",
      "   VALIDATION: Batch 79, loss 4.813176155090332\n",
      "   VALIDATION: Batch 80, loss 4.988150119781494\n",
      "   VALIDATION: Batch 81, loss 4.658776760101318\n",
      "   VALIDATION: Batch 82, loss 5.028006553649902\n",
      "   VALIDATION: Batch 83, loss 4.364718437194824\n",
      "   VALIDATION: Batch 84, loss 5.013346195220947\n",
      "   VALIDATION: Batch 85, loss 4.781772613525391\n",
      "   VALIDATION: Batch 86, loss 4.7458906173706055\n",
      "   VALIDATION: Batch 87, loss 4.6046528816223145\n",
      "   VALIDATION: Batch 88, loss 4.185897350311279\n",
      "   VALIDATION: Batch 89, loss 4.4749064445495605\n",
      "   VALIDATION: Batch 90, loss 4.735983371734619\n",
      "   VALIDATION: Batch 91, loss 5.053125858306885\n",
      "   VALIDATION: Batch 92, loss 4.9167866706848145\n",
      "   VALIDATION: Batch 93, loss 5.224087715148926\n",
      "   VALIDATION: Batch 94, loss 4.739964008331299\n",
      "   VALIDATION: Batch 95, loss 4.179912567138672\n",
      "   VALIDATION: Batch 96, loss 4.678446292877197\n",
      "   VALIDATION: Batch 97, loss 4.497330665588379\n",
      "   VALIDATION: Batch 98, loss 4.9107160568237305\n",
      "   VALIDATION: Batch 99, loss 5.017012119293213\n",
      "   VALIDATION: Batch 100, loss 5.401554107666016\n",
      "   VALIDATION: Batch 101, loss 3.9638969898223877\n",
      "   VALIDATION: Batch 102, loss 5.441030502319336\n",
      "   VALIDATION: Batch 103, loss 5.404357433319092\n",
      "   VALIDATION: Batch 104, loss 4.301179885864258\n",
      "   VALIDATION: Batch 105, loss 4.887955188751221\n",
      "   VALIDATION: Batch 106, loss 4.63638162612915\n",
      "   VALIDATION: Batch 107, loss 4.77458381652832\n",
      "   VALIDATION: Batch 108, loss 4.497790336608887\n",
      "   VALIDATION: Batch 109, loss 5.109184741973877\n",
      "   VALIDATION: Batch 110, loss 4.937368392944336\n",
      "   VALIDATION: Batch 111, loss 5.18227481842041\n",
      "   VALIDATION: Batch 112, loss 5.859997749328613\n",
      "   VALIDATION: Batch 113, loss 5.339166641235352\n",
      "   VALIDATION: Batch 114, loss 5.025268077850342\n",
      "   VALIDATION: Batch 115, loss 4.504151344299316\n",
      "   VALIDATION: Batch 116, loss 4.325006484985352\n",
      "   VALIDATION: Batch 117, loss 5.017905235290527\n",
      "   VALIDATION: Batch 118, loss 5.228341102600098\n",
      "   VALIDATION: Batch 119, loss 4.347579479217529\n",
      "   VALIDATION: Batch 120, loss 3.881960391998291\n",
      "   VALIDATION: Batch 121, loss 4.2866621017456055\n",
      "   VALIDATION: Batch 122, loss 4.686694145202637\n",
      "   VALIDATION: Batch 123, loss 4.834374904632568\n",
      "   VALIDATION: Batch 124, loss 3.972611904144287\n",
      "   VALIDATION: Batch 125, loss 4.745716571807861\n",
      "   VALIDATION: Batch 126, loss 4.92333459854126\n",
      "   VALIDATION: Batch 127, loss 4.811732292175293\n",
      "   VALIDATION: Batch 128, loss 4.8318352699279785\n",
      "   VALIDATION: Batch 129, loss 4.531081199645996\n",
      "   VALIDATION: Batch 130, loss 4.114912986755371\n",
      "   VALIDATION: Batch 131, loss 4.053439617156982\n",
      "   VALIDATION: Batch 132, loss 4.736743927001953\n",
      "   VALIDATION: Batch 133, loss 4.973759174346924\n",
      "   VALIDATION: Batch 134, loss 4.9244065284729\n",
      "   VALIDATION: Batch 135, loss 5.122366905212402\n",
      "   VALIDATION: Batch 136, loss 5.200519561767578\n",
      "   VALIDATION: Batch 137, loss 5.097428798675537\n",
      "   VALIDATION: Batch 138, loss 4.7675862312316895\n",
      "   VALIDATION: Batch 139, loss 5.184202194213867\n",
      "   VALIDATION: Batch 140, loss 4.174400806427002\n",
      "   VALIDATION: Batch 141, loss 5.094583511352539\n",
      "   VALIDATION: Batch 142, loss 3.8475661277770996\n",
      "   VALIDATION: Batch 143, loss 4.729046821594238\n",
      "   VALIDATION: Batch 144, loss 5.004208087921143\n",
      "   VALIDATION: Batch 145, loss 4.801616191864014\n",
      "   VALIDATION: Batch 146, loss 4.626074314117432\n",
      "   VALIDATION: Batch 147, loss 4.964136123657227\n",
      "   VALIDATION: Batch 148, loss 4.99431848526001\n",
      "   VALIDATION: Batch 149, loss 5.458300590515137\n",
      "   VALIDATION: Batch 150, loss 5.1885199546813965\n",
      "   VALIDATION: Batch 151, loss 5.286746025085449\n",
      "   VALIDATION: Batch 152, loss 4.740063667297363\n",
      "   VALIDATION: Batch 153, loss 4.981619834899902\n",
      "   VALIDATION: Batch 154, loss 4.821601867675781\n",
      "   VALIDATION: Batch 155, loss 4.495809078216553\n",
      "   VALIDATION: Batch 156, loss 5.285542964935303\n",
      "   VALIDATION: Batch 157, loss 5.052010536193848\n",
      "   VALIDATION: Batch 158, loss 4.215843677520752\n",
      "   VALIDATION: Batch 159, loss 4.703927040100098\n",
      "   VALIDATION: Batch 160, loss 5.167095184326172\n",
      "   VALIDATION: Batch 161, loss 5.40554666519165\n",
      "   VALIDATION: Batch 162, loss 4.902731418609619\n",
      "   VALIDATION: Batch 163, loss 4.241519927978516\n",
      "   VALIDATION: Batch 164, loss 4.708724498748779\n",
      "   VALIDATION: Batch 165, loss 5.199463844299316\n",
      "   VALIDATION: Batch 166, loss 4.746229648590088\n",
      "   VALIDATION: Batch 167, loss 5.06120491027832\n",
      "   VALIDATION: Batch 168, loss 3.8870480060577393\n",
      "   VALIDATION: Batch 169, loss 4.589376926422119\n",
      "   VALIDATION: Batch 170, loss 4.8438262939453125\n",
      "   VALIDATION: Batch 171, loss 4.8645853996276855\n",
      "   VALIDATION: Batch 172, loss 4.824387550354004\n",
      "   VALIDATION: Batch 173, loss 4.839400291442871\n",
      "   VALIDATION: Batch 174, loss 5.1662211418151855\n",
      "   VALIDATION: Batch 175, loss 4.839951038360596\n",
      "   VALIDATION: Batch 176, loss 4.5988264083862305\n",
      "   VALIDATION: Batch 177, loss 4.878762722015381\n",
      "   VALIDATION: Batch 178, loss 5.610909938812256\n",
      "   VALIDATION: Batch 179, loss 4.954488754272461\n",
      "   VALIDATION: Batch 180, loss 4.493924140930176\n",
      "   VALIDATION: Batch 181, loss 4.711765766143799\n",
      "   VALIDATION: Batch 182, loss 4.897332668304443\n",
      "   VALIDATION: Batch 183, loss 3.908029079437256\n",
      "   VALIDATION: Batch 184, loss 3.710206985473633\n",
      "   VALIDATION: Batch 185, loss 4.436697483062744\n",
      "   VALIDATION: Batch 186, loss 4.443905830383301\n",
      "   VALIDATION: Batch 187, loss 4.7309675216674805\n",
      "   VALIDATION: Batch 188, loss 5.026902198791504\n",
      "   VALIDATION: Batch 189, loss 4.33811092376709\n",
      "   VALIDATION: Batch 190, loss 4.325827121734619\n",
      "   VALIDATION: Batch 191, loss 4.946860313415527\n",
      "   VALIDATION: Batch 192, loss 5.4411516189575195\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=31]              TRRAINING: Batch 0, loss 4.850171089172363\n",
      "Epoch 2: |          | 1/? [00:01<00:00,  0.58it/s, v_num=31]   TRRAINING: Batch 1, loss 4.332440376281738\n",
      "Epoch 2: |          | 2/? [00:03<00:00,  0.63it/s, v_num=31]   TRRAINING: Batch 2, loss 4.497414588928223\n",
      "Epoch 2: |          | 3/? [00:04<00:00,  0.64it/s, v_num=31]   TRRAINING: Batch 3, loss 4.1147894859313965\n",
      "Epoch 2: |          | 4/? [00:06<00:00,  0.66it/s, v_num=31]   TRRAINING: Batch 4, loss 4.530149459838867\n",
      "Epoch 2: |          | 5/? [00:07<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 5, loss 5.398504257202148\n",
      "Epoch 2: |          | 6/? [00:08<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 6, loss 5.195034503936768\n",
      "Epoch 2: |          | 7/? [00:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 7, loss 4.312943458557129\n",
      "Epoch 2: |          | 8/? [00:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 8, loss 4.430094242095947\n",
      "Epoch 2: |          | 9/? [00:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 9, loss 4.734543800354004\n",
      "Epoch 2: |          | 10/? [00:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 10, loss 4.956752300262451\n",
      "Epoch 2: |          | 11/? [00:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 11, loss 4.913936138153076\n",
      "Epoch 2: |          | 12/? [00:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 12, loss 6.0684309005737305\n",
      "Epoch 2: |          | 13/? [00:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 13, loss 4.688863277435303\n",
      "Epoch 2: |          | 14/? [00:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 14, loss 4.97298526763916\n",
      "Epoch 2: |          | 15/? [00:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 15, loss 4.266470909118652\n",
      "Epoch 2: |          | 16/? [00:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 16, loss 3.9100029468536377\n",
      "Epoch 2: |          | 17/? [00:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 17, loss 5.263863563537598\n",
      "Epoch 2: |          | 18/? [00:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 18, loss 4.6688055992126465\n",
      "Epoch 2: |          | 19/? [00:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 19, loss 4.487668514251709\n",
      "Epoch 2: |          | 20/? [00:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 20, loss 4.833910942077637\n",
      "Epoch 2: |          | 21/? [00:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 21, loss 4.959559440612793\n",
      "Epoch 2: |          | 22/? [00:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 22, loss 4.755423545837402\n",
      "Epoch 2: |          | 23/? [00:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 23, loss 4.094433784484863\n",
      "Epoch 2: |          | 24/? [00:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 24, loss 4.739717960357666\n",
      "Epoch 2: |          | 25/? [00:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 25, loss 4.659137725830078\n",
      "Epoch 2: |          | 26/? [00:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 26, loss 4.541337013244629\n",
      "Epoch 2: |          | 27/? [00:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 27, loss 4.29302978515625\n",
      "Epoch 2: |          | 28/? [00:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 28, loss 5.215989112854004\n",
      "Epoch 2: |          | 29/? [00:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 29, loss 4.763007164001465\n",
      "Epoch 2: |          | 30/? [00:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 30, loss 4.6116437911987305\n",
      "Epoch 2: |          | 31/? [00:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 31, loss 5.293812274932861\n",
      "Epoch 2: |          | 32/? [00:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 32, loss 4.777289390563965\n",
      "Epoch 2: |          | 33/? [00:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 33, loss 4.601038932800293\n",
      "Epoch 2: |          | 34/? [00:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 34, loss 4.501639366149902\n",
      "Epoch 2: |          | 35/? [00:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 35, loss 3.8365299701690674\n",
      "Epoch 2: |          | 36/? [00:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 36, loss 4.952765464782715\n",
      "Epoch 2: |          | 37/? [00:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 37, loss 4.99265193939209\n",
      "Epoch 2: |          | 38/? [00:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 38, loss 5.267369747161865\n",
      "Epoch 2: |          | 39/? [00:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 39, loss 5.188706398010254\n",
      "Epoch 2: |          | 40/? [00:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 40, loss 4.612763404846191\n",
      "Epoch 2: |          | 41/? [00:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 41, loss 4.557643890380859\n",
      "Epoch 2: |          | 42/? [01:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 42, loss 4.400166034698486\n",
      "Epoch 2: |          | 43/? [01:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 43, loss 4.540018558502197\n",
      "Epoch 2: |          | 44/? [01:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 44, loss 3.92122220993042\n",
      "Epoch 2: |          | 45/? [01:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 45, loss 4.003461837768555\n",
      "Epoch 2: |          | 46/? [01:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 46, loss 5.3129072189331055\n",
      "Epoch 2: |          | 47/? [01:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 47, loss 4.454697608947754\n",
      "Epoch 2: |          | 48/? [01:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 48, loss 4.176587104797363\n",
      "Epoch 2: |          | 49/? [01:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 49, loss 4.606451988220215\n",
      "Epoch 2: |          | 50/? [01:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 50, loss 4.534916877746582\n",
      "Epoch 2: |          | 51/? [01:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 51, loss 4.632277488708496\n",
      "Epoch 2: |          | 52/? [01:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 52, loss 5.134444236755371\n",
      "Epoch 2: |          | 53/? [01:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 53, loss 4.733270645141602\n",
      "Epoch 2: |          | 54/? [01:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 54, loss 4.590619087219238\n",
      "Epoch 2: |          | 55/? [01:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 55, loss 4.700732707977295\n",
      "Epoch 2: |          | 56/? [01:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 56, loss 4.789819240570068\n",
      "Epoch 2: |          | 57/? [01:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 57, loss 4.729922294616699\n",
      "Epoch 2: |          | 58/? [01:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 58, loss 5.998365879058838\n",
      "Epoch 2: |          | 59/? [01:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 59, loss 4.852889060974121\n",
      "Epoch 2: |          | 60/? [01:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 60, loss 4.973362445831299\n",
      "Epoch 2: |          | 61/? [01:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 61, loss 5.029576301574707\n",
      "Epoch 2: |          | 62/? [01:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 62, loss 4.554040431976318\n",
      "Epoch 2: |          | 63/? [01:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 63, loss 4.797916889190674\n",
      "Epoch 2: |          | 64/? [01:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 64, loss 4.6085076332092285\n",
      "Epoch 2: |          | 65/? [01:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 65, loss 4.688479423522949\n",
      "Epoch 2: |          | 66/? [01:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 66, loss 4.0525617599487305\n",
      "Epoch 2: |          | 67/? [01:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 67, loss 4.7255754470825195\n",
      "Epoch 2: |          | 68/? [01:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 68, loss 4.963010311126709\n",
      "Epoch 2: |          | 69/? [01:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 69, loss 4.6456122398376465\n",
      "Epoch 2: |          | 70/? [01:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 70, loss 4.304303169250488\n",
      "Epoch 2: |          | 71/? [01:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 71, loss 4.259485721588135\n",
      "Epoch 2: |          | 72/? [01:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 72, loss 4.805905342102051\n",
      "Epoch 2: |          | 73/? [01:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 73, loss 4.903648853302002\n",
      "Epoch 2: |          | 74/? [01:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 74, loss 4.360040664672852\n",
      "Epoch 2: |          | 75/? [01:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 75, loss 4.58331298828125\n",
      "Epoch 2: |          | 76/? [01:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 76, loss 4.601550579071045\n",
      "Epoch 2: |          | 77/? [01:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 77, loss 4.639339923858643\n",
      "Epoch 2: |          | 78/? [01:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 78, loss 4.29763126373291\n",
      "Epoch 2: |          | 79/? [01:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 79, loss 4.597712993621826\n",
      "Epoch 2: |          | 80/? [01:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 80, loss 4.443673610687256\n",
      "Epoch 2: |          | 81/? [01:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 81, loss 4.01633358001709\n",
      "Epoch 2: |          | 82/? [01:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 82, loss 4.976920127868652\n",
      "Epoch 2: |          | 83/? [01:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 83, loss 4.146078109741211\n",
      "Epoch 2: |          | 84/? [01:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 84, loss 4.047996520996094\n",
      "Epoch 2: |          | 85/? [02:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 85, loss 4.053828716278076\n",
      "Epoch 2: |          | 86/? [02:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 86, loss 4.048102378845215\n",
      "Epoch 2: |          | 87/? [02:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 87, loss 4.217985153198242\n",
      "Epoch 2: |          | 88/? [02:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 88, loss 5.182651996612549\n",
      "Epoch 2: |          | 89/? [02:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 89, loss 4.907957553863525\n",
      "Epoch 2: |          | 90/? [02:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 90, loss 4.784257888793945\n",
      "Epoch 2: |          | 91/? [02:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 91, loss 4.602292060852051\n",
      "Epoch 2: |          | 92/? [02:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 92, loss 4.943460464477539\n",
      "Epoch 2: |          | 93/? [02:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 93, loss 5.079179763793945\n",
      "Epoch 2: |          | 94/? [02:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 94, loss 4.946669101715088\n",
      "Epoch 2: |          | 95/? [02:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 95, loss 5.426129341125488\n",
      "Epoch 2: |          | 96/? [02:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 96, loss 4.36137580871582\n",
      "Epoch 2: |          | 97/? [02:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 97, loss 4.280588626861572\n",
      "Epoch 2: |          | 98/? [02:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 98, loss 4.751230239868164\n",
      "Epoch 2: |          | 99/? [02:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 99, loss 4.944976329803467\n",
      "Epoch 2: |          | 100/? [02:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 100, loss 5.021930694580078\n",
      "Epoch 2: |          | 101/? [02:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 101, loss 4.59323787689209\n",
      "Epoch 2: |          | 102/? [02:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 102, loss 4.56837797164917\n",
      "Epoch 2: |          | 103/? [02:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 103, loss 4.2956132888793945\n",
      "Epoch 2: |          | 104/? [02:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 104, loss 4.788824081420898\n",
      "Epoch 2: |          | 105/? [02:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 105, loss 4.575782299041748\n",
      "Epoch 2: |          | 106/? [02:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 106, loss 4.657695293426514\n",
      "Epoch 2: |          | 107/? [02:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 107, loss 4.820681571960449\n",
      "Epoch 2: |          | 108/? [02:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 108, loss 4.755537986755371\n",
      "Epoch 2: |          | 109/? [02:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 109, loss 4.277616500854492\n",
      "Epoch 2: |          | 110/? [02:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 110, loss 4.736725807189941\n",
      "Epoch 2: |          | 111/? [02:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 111, loss 5.339158535003662\n",
      "Epoch 2: |          | 112/? [02:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 112, loss 4.156695365905762\n",
      "Epoch 2: |          | 113/? [02:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 113, loss 3.602583408355713\n",
      "Epoch 2: |          | 114/? [02:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 114, loss 4.916950702667236\n",
      "Epoch 2: |          | 115/? [02:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 115, loss 5.044960975646973\n",
      "Epoch 2: |          | 116/? [02:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 116, loss 4.36888313293457\n",
      "Epoch 2: |          | 117/? [02:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 117, loss 4.410149574279785\n",
      "Epoch 2: |          | 118/? [02:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 118, loss 4.937859058380127\n",
      "Epoch 2: |          | 119/? [02:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 119, loss 5.153545379638672\n",
      "Epoch 2: |          | 120/? [02:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 120, loss 4.88505744934082\n",
      "Epoch 2: |          | 121/? [02:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 121, loss 4.615396499633789\n",
      "Epoch 2: |          | 122/? [02:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 122, loss 4.074338912963867\n",
      "Epoch 2: |          | 123/? [02:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 123, loss 4.514819622039795\n",
      "Epoch 2: |          | 124/? [02:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 124, loss 4.823652267456055\n",
      "Epoch 2: |          | 125/? [02:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 125, loss 4.583138942718506\n",
      "Epoch 2: |          | 126/? [02:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 126, loss 4.925770282745361\n",
      "Epoch 2: |          | 127/? [03:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 127, loss 4.9506354331970215\n",
      "Epoch 2: |          | 128/? [03:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 128, loss 3.9899070262908936\n",
      "Epoch 2: |          | 129/? [03:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 129, loss 4.761565208435059\n",
      "Epoch 2: |          | 130/? [03:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 130, loss 3.655644178390503\n",
      "Epoch 2: |          | 131/? [03:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 131, loss 4.622974395751953\n",
      "Epoch 2: |          | 132/? [03:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 132, loss 4.592564105987549\n",
      "Epoch 2: |          | 133/? [03:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 133, loss 4.645676612854004\n",
      "Epoch 2: |          | 134/? [03:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 134, loss 4.695738792419434\n",
      "Epoch 2: |          | 135/? [03:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 135, loss 4.938354969024658\n",
      "Epoch 2: |          | 136/? [03:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 136, loss 4.8631272315979\n",
      "Epoch 2: |          | 137/? [03:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 137, loss 3.718989849090576\n",
      "Epoch 2: |          | 138/? [03:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 138, loss 4.492409706115723\n",
      "Epoch 2: |          | 139/? [03:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 139, loss 5.046019554138184\n",
      "Epoch 2: |          | 140/? [03:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 140, loss 4.0058417320251465\n",
      "Epoch 2: |          | 141/? [03:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 141, loss 4.2094855308532715\n",
      "Epoch 2: |          | 142/? [03:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 142, loss 5.879581928253174\n",
      "Epoch 2: |          | 143/? [03:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 143, loss 5.502221584320068\n",
      "Epoch 2: |          | 144/? [03:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 144, loss 4.569314956665039\n",
      "Epoch 2: |          | 145/? [03:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 145, loss 4.145334243774414\n",
      "Epoch 2: |          | 146/? [03:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 146, loss 4.363299369812012\n",
      "Epoch 2: |          | 147/? [03:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 147, loss 4.664909362792969\n",
      "Epoch 2: |          | 148/? [03:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 148, loss 4.346943378448486\n",
      "Epoch 2: |          | 149/? [03:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 149, loss 3.7852542400360107\n",
      "Epoch 2: |          | 150/? [03:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 150, loss 4.741014003753662\n",
      "Epoch 2: |          | 151/? [03:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 151, loss 4.800334453582764\n",
      "Epoch 2: |          | 152/? [03:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 152, loss 4.897989749908447\n",
      "Epoch 2: |          | 153/? [03:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 153, loss 3.8942770957946777\n",
      "Epoch 2: |          | 154/? [03:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 154, loss 5.136844158172607\n",
      "Epoch 2: |          | 155/? [03:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 155, loss 4.687232494354248\n",
      "Epoch 2: |          | 156/? [03:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 156, loss 3.9552369117736816\n",
      "Epoch 2: |          | 157/? [03:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 157, loss 4.741379737854004\n",
      "Epoch 2: |          | 158/? [03:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 158, loss 4.7738800048828125\n",
      "Epoch 2: |          | 159/? [03:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 159, loss 4.5067949295043945\n",
      "Epoch 2: |          | 160/? [03:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 160, loss 4.232579231262207\n",
      "Epoch 2: |          | 161/? [03:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 161, loss 4.755221366882324\n",
      "Epoch 2: |          | 162/? [03:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 162, loss 4.806728363037109\n",
      "Epoch 2: |          | 163/? [03:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 163, loss 3.6958069801330566\n",
      "Epoch 2: |          | 164/? [03:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 164, loss 4.183548450469971\n",
      "Epoch 2: |          | 165/? [03:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 165, loss 5.101746082305908\n",
      "Epoch 2: |          | 166/? [03:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 166, loss 4.820011615753174\n",
      "Epoch 2: |          | 167/? [03:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 167, loss 4.924566745758057\n",
      "Epoch 2: |          | 168/? [03:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 168, loss 4.46658182144165\n",
      "Epoch 2: |          | 169/? [03:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 169, loss 4.059551239013672\n",
      "Epoch 2: |          | 170/? [04:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 170, loss 4.3598527908325195\n",
      "Epoch 2: |          | 171/? [04:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 171, loss 4.789471626281738\n",
      "Epoch 2: |          | 172/? [04:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 172, loss 4.43170166015625\n",
      "Epoch 2: |          | 173/? [04:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 173, loss 5.327785015106201\n",
      "Epoch 2: |          | 174/? [04:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 174, loss 5.259256839752197\n",
      "Epoch 2: |          | 175/? [04:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 175, loss 5.333354473114014\n",
      "Epoch 2: |          | 176/? [04:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 176, loss 4.416378021240234\n",
      "Epoch 2: |          | 177/? [04:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 177, loss 4.40634822845459\n",
      "Epoch 2: |          | 178/? [04:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 178, loss 4.273675441741943\n",
      "Epoch 2: |          | 179/? [04:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 179, loss 5.017080783843994\n",
      "Epoch 2: |          | 180/? [04:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 180, loss 4.470263957977295\n",
      "Epoch 2: |          | 181/? [04:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 181, loss 4.252298831939697\n",
      "Epoch 2: |          | 182/? [04:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 182, loss 4.746092319488525\n",
      "Epoch 2: |          | 183/? [04:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 183, loss 4.160672187805176\n",
      "Epoch 2: |          | 184/? [04:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 184, loss 4.341727256774902\n",
      "Epoch 2: |          | 185/? [04:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 185, loss 5.063982963562012\n",
      "Epoch 2: |          | 186/? [04:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 186, loss 4.394689559936523\n",
      "Epoch 2: |          | 187/? [04:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 187, loss 4.985136985778809\n",
      "Epoch 2: |          | 188/? [04:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 188, loss 4.422087669372559\n",
      "Epoch 2: |          | 189/? [04:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 189, loss 5.075989723205566\n",
      "Epoch 2: |          | 190/? [04:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 190, loss 4.4943718910217285\n",
      "Epoch 2: |          | 191/? [04:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 191, loss 5.361269950866699\n",
      "Epoch 2: |          | 192/? [04:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 192, loss 5.214076042175293\n",
      "Epoch 2: |          | 193/? [04:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 193, loss 4.354422092437744\n",
      "Epoch 2: |          | 194/? [04:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 194, loss 4.291565895080566\n",
      "Epoch 2: |          | 195/? [04:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 195, loss 4.953232765197754\n",
      "Epoch 2: |          | 196/? [04:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 196, loss 4.926722049713135\n",
      "Epoch 2: |          | 197/? [04:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 197, loss 4.714188575744629\n",
      "Epoch 2: |          | 198/? [04:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 198, loss 4.042169570922852\n",
      "Epoch 2: |          | 199/? [04:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 199, loss 4.903014183044434\n",
      "Epoch 2: |          | 200/? [04:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 200, loss 4.505126953125\n",
      "Epoch 2: |          | 201/? [04:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 201, loss 4.825417518615723\n",
      "Epoch 2: |          | 202/? [04:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 202, loss 4.80325174331665\n",
      "Epoch 2: |          | 203/? [04:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 203, loss 4.668456077575684\n",
      "Epoch 2: |          | 204/? [04:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 204, loss 4.67119836807251\n",
      "Epoch 2: |          | 205/? [04:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 205, loss 4.355480194091797\n",
      "Epoch 2: |          | 206/? [04:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 206, loss 4.241672515869141\n",
      "Epoch 2: |          | 207/? [04:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 207, loss 4.768490314483643\n",
      "Epoch 2: |          | 208/? [04:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 208, loss 4.735443592071533\n",
      "Epoch 2: |          | 209/? [04:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 209, loss 4.480257511138916\n",
      "Epoch 2: |          | 210/? [04:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 210, loss 5.20020055770874\n",
      "Epoch 2: |          | 211/? [04:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 211, loss 4.498308181762695\n",
      "Epoch 2: |          | 212/? [05:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 212, loss 4.701056003570557\n",
      "Epoch 2: |          | 213/? [05:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 213, loss 4.528017997741699\n",
      "Epoch 2: |          | 214/? [05:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 214, loss 4.443667411804199\n",
      "Epoch 2: |          | 215/? [05:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 215, loss 4.093442916870117\n",
      "Epoch 2: |          | 216/? [05:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 216, loss 4.843428611755371\n",
      "Epoch 2: |          | 217/? [05:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 217, loss 4.669685363769531\n",
      "Epoch 2: |          | 218/? [05:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 218, loss 4.778440475463867\n",
      "Epoch 2: |          | 219/? [05:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 219, loss 4.627427101135254\n",
      "Epoch 2: |          | 220/? [05:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 220, loss 4.738500595092773\n",
      "Epoch 2: |          | 221/? [05:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 221, loss 4.497690677642822\n",
      "Epoch 2: |          | 222/? [05:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 222, loss 3.682115077972412\n",
      "Epoch 2: |          | 223/? [05:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 223, loss 5.149029731750488\n",
      "Epoch 2: |          | 224/? [05:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 224, loss 4.971365451812744\n",
      "Epoch 2: |          | 225/? [05:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 225, loss 4.699149131774902\n",
      "Epoch 2: |          | 226/? [05:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 226, loss 4.462267875671387\n",
      "Epoch 2: |          | 227/? [05:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 227, loss 4.913857460021973\n",
      "Epoch 2: |          | 228/? [05:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 228, loss 4.530697345733643\n",
      "Epoch 2: |          | 229/? [05:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 229, loss 4.736046314239502\n",
      "Epoch 2: |          | 230/? [05:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 230, loss 4.55893611907959\n",
      "Epoch 2: |          | 231/? [05:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 231, loss 4.50313663482666\n",
      "Epoch 2: |          | 232/? [05:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 232, loss 4.314329624176025\n",
      "Epoch 2: |          | 233/? [05:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 233, loss 5.052248954772949\n",
      "Epoch 2: |          | 234/? [05:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 234, loss 5.082372188568115\n",
      "Epoch 2: |          | 235/? [05:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 235, loss 5.145742893218994\n",
      "Epoch 2: |          | 236/? [05:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 236, loss 4.563593864440918\n",
      "Epoch 2: |          | 237/? [05:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 237, loss 4.725549697875977\n",
      "Epoch 2: |          | 238/? [05:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 238, loss 4.908142566680908\n",
      "Epoch 2: |          | 239/? [05:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 239, loss 4.506665229797363\n",
      "Epoch 2: |          | 240/? [05:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 240, loss 3.9041905403137207\n",
      "Epoch 2: |          | 241/? [05:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 241, loss 4.728405475616455\n",
      "Epoch 2: |          | 242/? [05:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 242, loss 5.0381574630737305\n",
      "Epoch 2: |          | 243/? [05:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 243, loss 3.714630603790283\n",
      "Epoch 2: |          | 244/? [05:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 244, loss 4.3167266845703125\n",
      "Epoch 2: |          | 245/? [05:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 245, loss 4.622944355010986\n",
      "Epoch 2: |          | 246/? [05:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 246, loss 4.800296783447266\n",
      "Epoch 2: |          | 247/? [05:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 247, loss 4.803110122680664\n",
      "Epoch 2: |          | 248/? [05:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 248, loss 4.288006782531738\n",
      "Epoch 2: |          | 249/? [05:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 249, loss 4.005426406860352\n",
      "Epoch 2: |          | 250/? [05:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 250, loss 4.676590919494629\n",
      "Epoch 2: |          | 251/? [05:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 251, loss 4.733840465545654\n",
      "Epoch 2: |          | 252/? [05:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 252, loss 4.467360973358154\n",
      "Epoch 2: |          | 253/? [05:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 253, loss 5.288650989532471\n",
      "Epoch 2: |          | 254/? [06:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 254, loss 5.054865837097168\n",
      "Epoch 2: |          | 255/? [06:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 255, loss 4.5622239112854\n",
      "Epoch 2: |          | 256/? [06:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 256, loss 6.484855651855469\n",
      "Epoch 2: |          | 257/? [06:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 257, loss 4.440995216369629\n",
      "Epoch 2: |          | 258/? [06:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 258, loss 4.52644681930542\n",
      "Epoch 2: |          | 259/? [06:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 259, loss 4.368345260620117\n",
      "Epoch 2: |          | 260/? [06:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 260, loss 4.306805610656738\n",
      "Epoch 2: |          | 261/? [06:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 261, loss 4.41422176361084\n",
      "Epoch 2: |          | 262/? [06:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 262, loss 4.850832462310791\n",
      "Epoch 2: |          | 263/? [06:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 263, loss 4.593932151794434\n",
      "Epoch 2: |          | 264/? [06:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 264, loss 4.67050838470459\n",
      "Epoch 2: |          | 265/? [06:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 265, loss 4.046043395996094\n",
      "Epoch 2: |          | 266/? [06:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 266, loss 4.522730350494385\n",
      "Epoch 2: |          | 267/? [06:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 267, loss 4.125156879425049\n",
      "Epoch 2: |          | 268/? [06:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 268, loss 4.494013786315918\n",
      "Epoch 2: |          | 269/? [06:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 269, loss 4.941302299499512\n",
      "Epoch 2: |          | 270/? [06:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 270, loss 4.494847774505615\n",
      "Epoch 2: |          | 271/? [06:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 271, loss 5.079316139221191\n",
      "Epoch 2: |          | 272/? [06:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 272, loss 5.081283092498779\n",
      "Epoch 2: |          | 273/? [06:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 273, loss 4.20278263092041\n",
      "Epoch 2: |          | 274/? [06:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 274, loss 5.30076789855957\n",
      "Epoch 2: |          | 275/? [06:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 275, loss 4.707674980163574\n",
      "Epoch 2: |          | 276/? [06:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 276, loss 4.029453754425049\n",
      "Epoch 2: |          | 277/? [06:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 277, loss 4.639708518981934\n",
      "Epoch 2: |          | 278/? [06:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 278, loss 3.5987796783447266\n",
      "Epoch 2: |          | 279/? [06:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 279, loss 4.464238166809082\n",
      "Epoch 2: |          | 280/? [06:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 280, loss 4.023594856262207\n",
      "Epoch 2: |          | 281/? [06:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 281, loss 4.886919021606445\n",
      "Epoch 2: |          | 282/? [06:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 282, loss 4.4480485916137695\n",
      "Epoch 2: |          | 283/? [06:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 283, loss 4.516083240509033\n",
      "Epoch 2: |          | 284/? [06:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 284, loss 4.4009857177734375\n",
      "Epoch 2: |          | 285/? [06:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 285, loss 3.785954236984253\n",
      "Epoch 2: |          | 286/? [06:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 286, loss 4.429731845855713\n",
      "Epoch 2: |          | 287/? [06:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 287, loss 4.255504131317139\n",
      "Epoch 2: |          | 288/? [06:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 288, loss 4.230202674865723\n",
      "Epoch 2: |          | 289/? [06:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 289, loss 4.201554775238037\n",
      "Epoch 2: |          | 290/? [06:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 290, loss 3.6531033515930176\n",
      "Epoch 2: |          | 291/? [06:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 291, loss 4.6684794425964355\n",
      "Epoch 2: |          | 292/? [06:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 292, loss 4.31430196762085\n",
      "Epoch 2: |          | 293/? [06:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 293, loss 4.668211460113525\n",
      "Epoch 2: |          | 294/? [06:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 294, loss 4.443897247314453\n",
      "Epoch 2: |          | 295/? [06:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 295, loss 4.8527069091796875\n",
      "Epoch 2: |          | 296/? [07:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 296, loss 4.2328200340271\n",
      "Epoch 2: |          | 297/? [07:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 297, loss 4.984929084777832\n",
      "Epoch 2: |          | 298/? [07:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 298, loss 4.762532711029053\n",
      "Epoch 2: |          | 299/? [07:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 299, loss 5.528661727905273\n",
      "Epoch 2: |          | 300/? [07:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 300, loss 4.688016891479492\n",
      "Epoch 2: |          | 301/? [07:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 301, loss 4.397761344909668\n",
      "Epoch 2: |          | 302/? [07:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 302, loss 4.874425888061523\n",
      "Epoch 2: |          | 303/? [07:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 303, loss 4.605123043060303\n",
      "Epoch 2: |          | 304/? [07:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 304, loss 4.856592655181885\n",
      "Epoch 2: |          | 305/? [07:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 305, loss 4.903807163238525\n",
      "Epoch 2: |          | 306/? [07:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 306, loss 4.548798084259033\n",
      "Epoch 2: |          | 307/? [07:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 307, loss 4.795401573181152\n",
      "Epoch 2: |          | 308/? [07:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 308, loss 4.922749996185303\n",
      "Epoch 2: |          | 309/? [07:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 309, loss 4.635102272033691\n",
      "Epoch 2: |          | 310/? [07:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 310, loss 5.094244480133057\n",
      "Epoch 2: |          | 311/? [07:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 311, loss 4.589509963989258\n",
      "Epoch 2: |          | 312/? [07:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 312, loss 4.702872276306152\n",
      "Epoch 2: |          | 313/? [07:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 313, loss 4.2476983070373535\n",
      "Epoch 2: |          | 314/? [07:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 314, loss 4.713984489440918\n",
      "Epoch 2: |          | 315/? [07:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 315, loss 4.245423316955566\n",
      "Epoch 2: |          | 316/? [07:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 316, loss 5.024304389953613\n",
      "Epoch 2: |          | 317/? [07:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 317, loss 4.717159271240234\n",
      "Epoch 2: |          | 318/? [07:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 318, loss 4.896208763122559\n",
      "Epoch 2: |          | 319/? [07:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 319, loss 4.064786911010742\n",
      "Epoch 2: |          | 320/? [07:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 320, loss 4.504939079284668\n",
      "Epoch 2: |          | 321/? [07:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 321, loss 4.277825355529785\n",
      "Epoch 2: |          | 322/? [07:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 322, loss 5.027762413024902\n",
      "Epoch 2: |          | 323/? [07:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 323, loss 5.08536958694458\n",
      "Epoch 2: |          | 324/? [07:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 324, loss 4.59272575378418\n",
      "Epoch 2: |          | 325/? [07:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 325, loss 5.17800235748291\n",
      "Epoch 2: |          | 326/? [07:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 326, loss 4.694494724273682\n",
      "Epoch 2: |          | 327/? [07:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 327, loss 4.523976802825928\n",
      "Epoch 2: |          | 328/? [07:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 328, loss 4.1548871994018555\n",
      "Epoch 2: |          | 329/? [07:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 329, loss 4.820437431335449\n",
      "Epoch 2: |          | 330/? [07:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 330, loss 5.2434468269348145\n",
      "Epoch 2: |          | 331/? [07:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 331, loss 3.4111766815185547\n",
      "Epoch 2: |          | 332/? [07:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 332, loss 4.628830909729004\n",
      "Epoch 2: |          | 333/? [07:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 333, loss 4.454672813415527\n",
      "Epoch 2: |          | 334/? [07:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 334, loss 5.511809349060059\n",
      "Epoch 2: |          | 335/? [07:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 335, loss 5.283255577087402\n",
      "Epoch 2: |          | 336/? [07:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 336, loss 5.083235263824463\n",
      "Epoch 2: |          | 337/? [08:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 337, loss 5.74456262588501\n",
      "Epoch 2: |          | 338/? [08:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 338, loss 5.330549716949463\n",
      "Epoch 2: |          | 339/? [08:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 339, loss 4.307459354400635\n",
      "Epoch 2: |          | 340/? [08:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 340, loss 4.674178600311279\n",
      "Epoch 2: |          | 341/? [08:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 341, loss 3.952540159225464\n",
      "Epoch 2: |          | 342/? [08:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 342, loss 4.755492687225342\n",
      "Epoch 2: |          | 343/? [08:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 343, loss 4.452891826629639\n",
      "Epoch 2: |          | 344/? [08:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 344, loss 5.251980304718018\n",
      "Epoch 2: |          | 345/? [08:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 345, loss 4.500624656677246\n",
      "Epoch 2: |          | 346/? [08:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 346, loss 4.739838123321533\n",
      "Epoch 2: |          | 347/? [08:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 347, loss 4.402405738830566\n",
      "Epoch 2: |          | 348/? [08:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 348, loss 3.712583065032959\n",
      "Epoch 2: |          | 349/? [08:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 349, loss 3.5313682556152344\n",
      "Epoch 2: |          | 350/? [08:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 350, loss 5.084822177886963\n",
      "Epoch 2: |          | 351/? [08:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 351, loss 5.056461334228516\n",
      "Epoch 2: |          | 352/? [08:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 352, loss 4.266608715057373\n",
      "Epoch 2: |          | 353/? [08:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 353, loss 3.9596400260925293\n",
      "Epoch 2: |          | 354/? [08:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 354, loss 4.37997579574585\n",
      "Epoch 2: |          | 355/? [08:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 355, loss 4.704733848571777\n",
      "Epoch 2: |          | 356/? [08:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 356, loss 4.720946788787842\n",
      "Epoch 2: |          | 357/? [08:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 357, loss 4.265288352966309\n",
      "Epoch 2: |          | 358/? [08:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 358, loss 4.161665916442871\n",
      "Epoch 2: |          | 359/? [08:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 359, loss 4.873180389404297\n",
      "Epoch 2: |          | 360/? [08:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 360, loss 4.377684116363525\n",
      "Epoch 2: |          | 361/? [08:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 361, loss 4.4735894203186035\n",
      "Epoch 2: |          | 362/? [08:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 362, loss 4.312084197998047\n",
      "Epoch 2: |          | 363/? [08:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 363, loss 4.202608585357666\n",
      "Epoch 2: |          | 364/? [08:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 364, loss 4.845727443695068\n",
      "Epoch 2: |          | 365/? [08:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 365, loss 4.913582801818848\n",
      "Epoch 2: |          | 366/? [08:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 366, loss 4.59732723236084\n",
      "Epoch 2: |          | 367/? [08:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 367, loss 4.674534797668457\n",
      "Epoch 2: |          | 368/? [08:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 368, loss 4.074850559234619\n",
      "Epoch 2: |          | 369/? [08:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 369, loss 4.51369047164917\n",
      "Epoch 2: |          | 370/? [08:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 370, loss 4.029422760009766\n",
      "Epoch 2: |          | 371/? [08:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 371, loss 5.13353967666626\n",
      "Epoch 2: |          | 372/? [08:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 372, loss 4.584503173828125\n",
      "Epoch 2: |          | 373/? [08:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 373, loss 4.675970554351807\n",
      "Epoch 2: |          | 374/? [08:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 374, loss 4.421038627624512\n",
      "Epoch 2: |          | 375/? [08:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 375, loss 5.145580291748047\n",
      "Epoch 2: |          | 376/? [08:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 376, loss 4.464216709136963\n",
      "Epoch 2: |          | 377/? [08:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 377, loss 4.664858818054199\n",
      "Epoch 2: |          | 378/? [08:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 378, loss 4.8415398597717285\n",
      "Epoch 2: |          | 379/? [09:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 379, loss 4.669787406921387\n",
      "Epoch 2: |          | 380/? [09:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 380, loss 4.811942100524902\n",
      "Epoch 2: |          | 381/? [09:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 381, loss 4.705782890319824\n",
      "Epoch 2: |          | 382/? [09:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 382, loss 4.427783966064453\n",
      "Epoch 2: |          | 383/? [09:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 383, loss 4.464251518249512\n",
      "Epoch 2: |          | 384/? [09:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 384, loss 5.019442558288574\n",
      "Epoch 2: |          | 385/? [09:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 385, loss 4.571712493896484\n",
      "Epoch 2: |          | 386/? [09:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 386, loss 3.365386486053467\n",
      "Epoch 2: |          | 387/? [09:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 387, loss 4.365363121032715\n",
      "Epoch 2: |          | 388/? [09:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 388, loss 4.862553596496582\n",
      "Epoch 2: |          | 389/? [09:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 389, loss 5.03673791885376\n",
      "Epoch 2: |          | 390/? [09:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 390, loss 4.3508501052856445\n",
      "Epoch 2: |          | 391/? [09:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 391, loss 4.902673244476318\n",
      "Epoch 2: |          | 392/? [09:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 392, loss 4.888409614562988\n",
      "Epoch 2: |          | 393/? [09:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 393, loss 4.988808631896973\n",
      "Epoch 2: |          | 394/? [09:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 394, loss 4.591091156005859\n",
      "Epoch 2: |          | 395/? [09:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 395, loss 4.844967842102051\n",
      "Epoch 2: |          | 396/? [09:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 396, loss 4.798951148986816\n",
      "Epoch 2: |          | 397/? [09:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 397, loss 4.624615669250488\n",
      "Epoch 2: |          | 398/? [09:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 398, loss 4.414500713348389\n",
      "Epoch 2: |          | 399/? [09:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 399, loss 4.548651218414307\n",
      "Epoch 2: |          | 400/? [09:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 400, loss 4.593419075012207\n",
      "Epoch 2: |          | 401/? [09:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 401, loss 4.455369472503662\n",
      "Epoch 2: |          | 402/? [09:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 402, loss 4.998868465423584\n",
      "Epoch 2: |          | 403/? [09:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 403, loss 4.709877967834473\n",
      "Epoch 2: |          | 404/? [09:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 404, loss 4.206082344055176\n",
      "Epoch 2: |          | 405/? [09:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 405, loss 4.214415073394775\n",
      "Epoch 2: |          | 406/? [09:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 406, loss 4.603790283203125\n",
      "Epoch 2: |          | 407/? [09:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 407, loss 4.539669513702393\n",
      "Epoch 2: |          | 408/? [09:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 408, loss 4.9030375480651855\n",
      "Epoch 2: |          | 409/? [09:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 409, loss 4.9057464599609375\n",
      "Epoch 2: |          | 410/? [09:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 410, loss 4.623596668243408\n",
      "Epoch 2: |          | 411/? [09:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 411, loss 4.4201507568359375\n",
      "Epoch 2: |          | 412/? [09:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 412, loss 3.8142921924591064\n",
      "Epoch 2: |          | 413/? [09:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 413, loss 4.712806701660156\n",
      "Epoch 2: |          | 414/? [09:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 414, loss 4.182324409484863\n",
      "Epoch 2: |          | 415/? [09:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 415, loss 4.66636848449707\n",
      "Epoch 2: |          | 416/? [09:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 416, loss 5.107295989990234\n",
      "Epoch 2: |          | 417/? [09:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 417, loss 5.3481950759887695\n",
      "Epoch 2: |          | 418/? [09:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 418, loss 4.848063945770264\n",
      "Epoch 2: |          | 419/? [09:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 419, loss 4.81263542175293\n",
      "Epoch 2: |          | 420/? [09:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 420, loss 4.700405597686768\n",
      "Epoch 2: |          | 421/? [09:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 421, loss 5.196238994598389\n",
      "Epoch 2: |          | 422/? [10:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 422, loss 4.731664657592773\n",
      "Epoch 2: |          | 423/? [10:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 423, loss 4.2188920974731445\n",
      "Epoch 2: |          | 424/? [10:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 424, loss 5.017251014709473\n",
      "Epoch 2: |          | 425/? [10:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 425, loss 4.825913429260254\n",
      "Epoch 2: |          | 426/? [10:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 426, loss 4.417747974395752\n",
      "Epoch 2: |          | 427/? [10:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 427, loss 4.389716148376465\n",
      "Epoch 2: |          | 428/? [10:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 428, loss 5.260434150695801\n",
      "Epoch 2: |          | 429/? [10:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 429, loss 4.050310134887695\n",
      "Epoch 2: |          | 430/? [10:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 430, loss 4.794706344604492\n",
      "Epoch 2: |          | 431/? [10:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 431, loss 4.612806797027588\n",
      "Epoch 2: |          | 432/? [10:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 432, loss 4.7220892906188965\n",
      "Epoch 2: |          | 433/? [10:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 433, loss 4.601107597351074\n",
      "Epoch 2: |          | 434/? [10:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 434, loss 4.61277437210083\n",
      "Epoch 2: |          | 435/? [10:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 435, loss 4.2771382331848145\n",
      "Epoch 2: |          | 436/? [10:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 436, loss 4.728815078735352\n",
      "Epoch 2: |          | 437/? [10:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 437, loss 4.802433967590332\n",
      "Epoch 2: |          | 438/? [10:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 438, loss 4.41827392578125\n",
      "Epoch 2: |          | 439/? [10:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 439, loss 4.489104270935059\n",
      "Epoch 2: |          | 440/? [10:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 440, loss 4.471554279327393\n",
      "Epoch 2: |          | 441/? [10:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 441, loss 4.787561893463135\n",
      "Epoch 2: |          | 442/? [10:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 442, loss 4.480626583099365\n",
      "Epoch 2: |          | 443/? [10:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 443, loss 4.7642412185668945\n",
      "Epoch 2: |          | 444/? [10:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 444, loss 4.585638046264648\n",
      "Epoch 2: |          | 445/? [10:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 445, loss 5.5388288497924805\n",
      "Epoch 2: |          | 446/? [10:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 446, loss 4.599400043487549\n",
      "Epoch 2: |          | 447/? [10:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 447, loss 5.140181541442871\n",
      "Epoch 2: |          | 448/? [10:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 448, loss 4.35880184173584\n",
      "Epoch 2: |          | 449/? [10:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 449, loss 4.522549629211426\n",
      "Epoch 2: |          | 450/? [10:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 450, loss 4.951099872589111\n",
      "Epoch 2: |          | 451/? [10:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 451, loss 4.523181915283203\n",
      "Epoch 2: |          | 452/? [10:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 452, loss 4.317067623138428\n",
      "Epoch 2: |          | 453/? [10:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 453, loss 5.010748863220215\n",
      "Epoch 2: |          | 454/? [10:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 454, loss 4.373293876647949\n",
      "Epoch 2: |          | 455/? [10:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 455, loss 4.803559303283691\n",
      "Epoch 2: |          | 456/? [10:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 456, loss 4.193155765533447\n",
      "Epoch 2: |          | 457/? [10:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 457, loss 4.5390472412109375\n",
      "Epoch 2: |          | 458/? [10:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 458, loss 4.953246116638184\n",
      "Epoch 2: |          | 459/? [10:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 459, loss 4.978372097015381\n",
      "Epoch 2: |          | 460/? [10:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 460, loss 4.724179267883301\n",
      "Epoch 2: |          | 461/? [11:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 461, loss 4.708838939666748\n",
      "Epoch 2: |          | 462/? [11:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 462, loss 4.745673179626465\n",
      "Epoch 2: |          | 463/? [11:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 463, loss 4.599748611450195\n",
      "Epoch 2: |          | 464/? [11:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 464, loss 4.089076519012451\n",
      "Epoch 2: |          | 465/? [11:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 465, loss 4.353061199188232\n",
      "Epoch 2: |          | 466/? [11:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 466, loss 4.79271125793457\n",
      "Epoch 2: |          | 467/? [11:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 467, loss 4.7000579833984375\n",
      "Epoch 2: |          | 468/? [11:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 468, loss 4.555384159088135\n",
      "Epoch 2: |          | 469/? [11:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 469, loss 4.728677272796631\n",
      "Epoch 2: |          | 470/? [11:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 470, loss 3.8947575092315674\n",
      "Epoch 2: |          | 471/? [11:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 471, loss 4.736872673034668\n",
      "Epoch 2: |          | 472/? [11:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 472, loss 4.352773666381836\n",
      "Epoch 2: |          | 473/? [11:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 473, loss 4.35618257522583\n",
      "Epoch 2: |          | 474/? [11:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 474, loss 3.9310951232910156\n",
      "Epoch 2: |          | 475/? [11:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 475, loss 5.345114707946777\n",
      "Epoch 2: |          | 476/? [11:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 476, loss 4.212380886077881\n",
      "Epoch 2: |          | 477/? [11:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 477, loss 3.73657488822937\n",
      "Epoch 2: |          | 478/? [11:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 478, loss 3.905815839767456\n",
      "Epoch 2: |          | 479/? [11:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 479, loss 4.554199695587158\n",
      "Epoch 2: |          | 480/? [11:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 480, loss 4.590840816497803\n",
      "Epoch 2: |          | 481/? [11:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 481, loss 4.119532585144043\n",
      "Epoch 2: |          | 482/? [11:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 482, loss 4.388975143432617\n",
      "Epoch 2: |          | 483/? [11:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 483, loss 4.065435409545898\n",
      "Epoch 2: |          | 484/? [11:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 484, loss 5.000807762145996\n",
      "Epoch 2: |          | 485/? [11:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 485, loss 4.798424243927002\n",
      "Epoch 2: |          | 486/? [11:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 486, loss 4.643131256103516\n",
      "Epoch 2: |          | 487/? [11:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 487, loss 4.772593975067139\n",
      "Epoch 2: |          | 488/? [11:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 488, loss 4.393397808074951\n",
      "Epoch 2: |          | 489/? [11:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 489, loss 3.9165420532226562\n",
      "Epoch 2: |          | 490/? [11:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 490, loss 4.707757472991943\n",
      "Epoch 2: |          | 491/? [11:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 491, loss 4.405323505401611\n",
      "Epoch 2: |          | 492/? [11:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 492, loss 3.751041889190674\n",
      "Epoch 2: |          | 493/? [11:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 493, loss 4.940532684326172\n",
      "Epoch 2: |          | 494/? [11:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 494, loss 4.8028740882873535\n",
      "Epoch 2: |          | 495/? [11:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 495, loss 4.801667213439941\n",
      "Epoch 2: |          | 496/? [11:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 496, loss 4.3740997314453125\n",
      "Epoch 2: |          | 497/? [11:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 497, loss 4.882438659667969\n",
      "Epoch 2: |          | 498/? [11:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 498, loss 4.603085041046143\n",
      "Epoch 2: |          | 499/? [11:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 499, loss 4.555792808532715\n",
      "Epoch 2: |          | 500/? [11:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 500, loss 4.567343235015869\n",
      "Epoch 2: |          | 501/? [11:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 501, loss 4.180449962615967\n",
      "Epoch 2: |          | 502/? [11:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 502, loss 4.645960807800293\n",
      "Epoch 2: |          | 503/? [11:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 503, loss 4.704395294189453\n",
      "Epoch 2: |          | 504/? [12:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 504, loss 4.444083213806152\n",
      "Epoch 2: |          | 505/? [12:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 505, loss 3.729928493499756\n",
      "Epoch 2: |          | 506/? [12:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 506, loss 4.513511657714844\n",
      "Epoch 2: |          | 507/? [12:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 507, loss 4.564370632171631\n",
      "Epoch 2: |          | 508/? [12:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 508, loss 4.868785858154297\n",
      "Epoch 2: |          | 509/? [12:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 509, loss 4.179696083068848\n",
      "Epoch 2: |          | 510/? [12:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 510, loss 4.728542327880859\n",
      "Epoch 2: |          | 511/? [12:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 511, loss 4.638116359710693\n",
      "Epoch 2: |          | 512/? [12:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 512, loss 4.048571586608887\n",
      "Epoch 2: |          | 513/? [12:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 513, loss 4.327198505401611\n",
      "Epoch 2: |          | 514/? [12:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 514, loss 4.37540864944458\n",
      "Epoch 2: |          | 515/? [12:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 515, loss 4.10045862197876\n",
      "Epoch 2: |          | 516/? [12:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 516, loss 4.358767509460449\n",
      "Epoch 2: |          | 517/? [12:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 517, loss 4.724701881408691\n",
      "Epoch 2: |          | 518/? [12:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 518, loss 4.28472900390625\n",
      "Epoch 2: |          | 519/? [12:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 519, loss 4.736110687255859\n",
      "Epoch 2: |          | 520/? [12:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 520, loss 4.486099720001221\n",
      "Epoch 2: |          | 521/? [12:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 521, loss 4.468376636505127\n",
      "Epoch 2: |          | 522/? [12:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 522, loss 5.054173946380615\n",
      "Epoch 2: |          | 523/? [12:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 523, loss 5.105648994445801\n",
      "Epoch 2: |          | 524/? [12:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 524, loss 4.9126715660095215\n",
      "Epoch 2: |          | 525/? [12:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 525, loss 4.4696760177612305\n",
      "Epoch 2: |          | 526/? [12:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 526, loss 4.3104400634765625\n",
      "Epoch 2: |          | 527/? [12:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 527, loss 4.840788841247559\n",
      "Epoch 2: |          | 528/? [12:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 528, loss 4.719669342041016\n",
      "Epoch 2: |          | 529/? [12:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 529, loss 4.3118133544921875\n",
      "Epoch 2: |          | 530/? [12:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 530, loss 4.896694660186768\n",
      "Epoch 2: |          | 531/? [12:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 531, loss 4.333286762237549\n",
      "Epoch 2: |          | 532/? [12:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 532, loss 4.66184139251709\n",
      "Epoch 2: |          | 533/? [12:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 533, loss 4.308745384216309\n",
      "Epoch 2: |          | 534/? [12:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 534, loss 4.097801208496094\n",
      "Epoch 2: |          | 535/? [12:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 535, loss 4.993591785430908\n",
      "Epoch 2: |          | 536/? [12:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 536, loss 4.987462043762207\n",
      "Epoch 2: |          | 537/? [12:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 537, loss 4.647116661071777\n",
      "Epoch 2: |          | 538/? [12:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 538, loss 4.302410125732422\n",
      "Epoch 2: |          | 539/? [12:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 539, loss 4.400476932525635\n",
      "Epoch 2: |          | 540/? [12:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 540, loss 4.9336042404174805\n",
      "Epoch 2: |          | 541/? [12:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 541, loss 4.590700626373291\n",
      "Epoch 2: |          | 542/? [12:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 542, loss 4.402083396911621\n",
      "Epoch 2: |          | 543/? [12:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 543, loss 4.7667131423950195\n",
      "Epoch 2: |          | 544/? [12:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 544, loss 4.602156639099121\n",
      "Epoch 2: |          | 545/? [12:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 545, loss 3.791541337966919\n",
      "Epoch 2: |          | 546/? [12:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 546, loss 4.698636054992676\n",
      "Epoch 2: |          | 547/? [13:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 547, loss 5.0977373123168945\n",
      "Epoch 2: |          | 548/? [13:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 548, loss 4.841527462005615\n",
      "Epoch 2: |          | 549/? [13:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 549, loss 4.62636137008667\n",
      "Epoch 2: |          | 550/? [13:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 550, loss 5.072513580322266\n",
      "Epoch 2: |          | 551/? [13:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 551, loss 4.62166690826416\n",
      "Epoch 2: |          | 552/? [13:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 552, loss 4.767201900482178\n",
      "Epoch 2: |          | 553/? [13:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 553, loss 4.0205183029174805\n",
      "Epoch 2: |          | 554/? [13:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 554, loss 4.702739238739014\n",
      "Epoch 2: |          | 555/? [13:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 555, loss 5.077926158905029\n",
      "Epoch 2: |          | 556/? [13:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 556, loss 4.6365814208984375\n",
      "Epoch 2: |          | 557/? [13:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 557, loss 4.254997253417969\n",
      "Epoch 2: |          | 558/? [13:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 558, loss 4.338879585266113\n",
      "Epoch 2: |          | 559/? [13:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 559, loss 4.404009819030762\n",
      "Epoch 2: |          | 560/? [13:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 560, loss 3.8078174591064453\n",
      "Epoch 2: |          | 561/? [13:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 561, loss 4.012374401092529\n",
      "Epoch 2: |          | 562/? [13:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 562, loss 4.7834649085998535\n",
      "Epoch 2: |          | 563/? [13:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 563, loss 3.8333537578582764\n",
      "Epoch 2: |          | 564/? [13:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 564, loss 4.4596357345581055\n",
      "Epoch 2: |          | 565/? [13:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 565, loss 4.880321502685547\n",
      "Epoch 2: |          | 566/? [13:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 566, loss 4.848498344421387\n",
      "Epoch 2: |          | 567/? [13:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 567, loss 4.9661455154418945\n",
      "Epoch 2: |          | 568/? [13:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 568, loss 4.079387187957764\n",
      "Epoch 2: |          | 569/? [13:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 569, loss 4.712855339050293\n",
      "Epoch 2: |          | 570/? [13:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 570, loss 4.718721389770508\n",
      "Epoch 2: |          | 571/? [13:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 571, loss 4.266562461853027\n",
      "Epoch 2: |          | 572/? [13:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 572, loss 5.238779067993164\n",
      "Epoch 2: |          | 573/? [13:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 573, loss 3.5455989837646484\n",
      "Epoch 2: |          | 574/? [13:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 574, loss 4.820786952972412\n",
      "Epoch 2: |          | 575/? [13:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 575, loss 4.185808181762695\n",
      "Epoch 2: |          | 576/? [13:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 576, loss 4.385537147521973\n",
      "Epoch 2: |          | 577/? [13:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 577, loss 4.652039051055908\n",
      "Epoch 2: |          | 578/? [13:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 578, loss 4.851519584655762\n",
      "Epoch 2: |          | 579/? [13:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 579, loss 3.819384813308716\n",
      "Epoch 2: |          | 580/? [13:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 580, loss 4.697225093841553\n",
      "Epoch 2: |          | 581/? [13:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 581, loss 4.733473300933838\n",
      "Epoch 2: |          | 582/? [13:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 582, loss 4.753876209259033\n",
      "Epoch 2: |          | 583/? [13:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 583, loss 4.499805450439453\n",
      "Epoch 2: |          | 584/? [13:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 584, loss 4.687166690826416\n",
      "Epoch 2: |          | 585/? [13:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 585, loss 4.725709438323975\n",
      "Epoch 2: |          | 586/? [13:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 586, loss 4.761340618133545\n",
      "Epoch 2: |          | 587/? [13:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 587, loss 4.6563520431518555\n",
      "Epoch 2: |          | 588/? [13:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 588, loss 4.820611000061035\n",
      "Epoch 2: |          | 589/? [14:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 589, loss 4.1797380447387695\n",
      "Epoch 2: |          | 590/? [14:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 590, loss 4.851990222930908\n",
      "Epoch 2: |          | 591/? [14:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 591, loss 4.671701908111572\n",
      "Epoch 2: |          | 592/? [14:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 592, loss 4.38065767288208\n",
      "Epoch 2: |          | 593/? [14:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 593, loss 4.531442165374756\n",
      "Epoch 2: |          | 594/? [14:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 594, loss 5.404321193695068\n",
      "Epoch 2: |          | 595/? [14:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 595, loss 4.012892723083496\n",
      "Epoch 2: |          | 596/? [14:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 596, loss 4.1533379554748535\n",
      "Epoch 2: |          | 597/? [14:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 597, loss 4.470484733581543\n",
      "Epoch 2: |          | 598/? [14:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 598, loss 4.918779373168945\n",
      "Epoch 2: |          | 599/? [14:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 599, loss 4.558332920074463\n",
      "Epoch 2: |          | 600/? [14:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 600, loss 4.249328136444092\n",
      "Epoch 2: |          | 601/? [14:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 601, loss 4.599905967712402\n",
      "Epoch 2: |          | 602/? [14:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 602, loss 4.099511623382568\n",
      "Epoch 2: |          | 603/? [14:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 603, loss 4.320819854736328\n",
      "Epoch 2: |          | 604/? [14:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 604, loss 7.003212928771973\n",
      "Epoch 2: |          | 605/? [14:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 605, loss 3.9959781169891357\n",
      "Epoch 2: |          | 606/? [14:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 606, loss 4.297135829925537\n",
      "Epoch 2: |          | 607/? [14:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 607, loss 4.712646961212158\n",
      "Epoch 2: |          | 608/? [14:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 608, loss 4.417443752288818\n",
      "Epoch 2: |          | 609/? [14:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 609, loss 4.319401741027832\n",
      "Epoch 2: |          | 610/? [14:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 610, loss 4.431286811828613\n",
      "Epoch 2: |          | 611/? [14:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 611, loss 4.596511363983154\n",
      "Epoch 2: |          | 612/? [14:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 612, loss 4.329092502593994\n",
      "Epoch 2: |          | 613/? [14:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 613, loss 4.662506103515625\n",
      "Epoch 2: |          | 614/? [14:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 614, loss 4.37494421005249\n",
      "Epoch 2: |          | 615/? [14:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 615, loss 4.9637885093688965\n",
      "Epoch 2: |          | 616/? [14:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 616, loss 5.220186710357666\n",
      "Epoch 2: |          | 617/? [14:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 617, loss 3.748797655105591\n",
      "Epoch 2: |          | 618/? [14:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 618, loss 4.714562892913818\n",
      "Epoch 2: |          | 619/? [14:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 619, loss 4.419085502624512\n",
      "Epoch 2: |          | 620/? [14:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 620, loss 4.857110023498535\n",
      "Epoch 2: |          | 621/? [14:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 621, loss 4.176095485687256\n",
      "Epoch 2: |          | 622/? [14:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 622, loss 3.970038652420044\n",
      "Epoch 2: |          | 623/? [14:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 623, loss 3.5943470001220703\n",
      "Epoch 2: |          | 624/? [14:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 624, loss 3.236163377761841\n",
      "Epoch 2: |          | 625/? [14:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 625, loss 4.99161434173584\n",
      "Epoch 2: |          | 626/? [14:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 626, loss 4.465989112854004\n",
      "Epoch 2: |          | 627/? [14:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 627, loss 4.414660930633545\n",
      "Epoch 2: |          | 628/? [14:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 628, loss 4.307529926300049\n",
      "Epoch 2: |          | 629/? [14:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 629, loss 4.791653633117676\n",
      "Epoch 2: |          | 630/? [14:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 630, loss 4.520942211151123\n",
      "Epoch 2: |          | 631/? [14:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 631, loss 4.7293701171875\n",
      "Epoch 2: |          | 632/? [15:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 632, loss 3.7786145210266113\n",
      "Epoch 2: |          | 633/? [15:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 633, loss 4.772319793701172\n",
      "Epoch 2: |          | 634/? [15:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 634, loss 4.297973155975342\n",
      "Epoch 2: |          | 635/? [15:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 635, loss 4.071413993835449\n",
      "Epoch 2: |          | 636/? [15:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 636, loss 4.515530586242676\n",
      "Epoch 2: |          | 637/? [15:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 637, loss 4.301765441894531\n",
      "Epoch 2: |          | 638/? [15:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 638, loss 4.580776214599609\n",
      "Epoch 2: |          | 639/? [15:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 639, loss 4.301897048950195\n",
      "Epoch 2: |          | 640/? [15:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 640, loss 4.969176769256592\n",
      "Epoch 2: |          | 641/? [15:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 641, loss 4.087986469268799\n",
      "Epoch 2: |          | 642/? [15:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 642, loss 4.697197437286377\n",
      "Epoch 2: |          | 643/? [15:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 643, loss 4.70647668838501\n",
      "Epoch 2: |          | 644/? [15:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 644, loss 4.4997429847717285\n",
      "Epoch 2: |          | 645/? [15:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 645, loss 4.341797351837158\n",
      "Epoch 2: |          | 646/? [15:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 646, loss 4.302559852600098\n",
      "Epoch 2: |          | 647/? [15:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 647, loss 4.975682258605957\n",
      "Epoch 2: |          | 648/? [15:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 648, loss 4.422795295715332\n",
      "Epoch 2: |          | 649/? [15:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 649, loss 4.469183921813965\n",
      "Epoch 2: |          | 650/? [15:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 650, loss 4.876100540161133\n",
      "Epoch 2: |          | 651/? [15:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 651, loss 5.059168815612793\n",
      "Epoch 2: |          | 652/? [15:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 652, loss 4.3638200759887695\n",
      "Epoch 2: |          | 653/? [15:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 653, loss 4.4971723556518555\n",
      "Epoch 2: |          | 654/? [15:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 654, loss 4.784161567687988\n",
      "Epoch 2: |          | 655/? [15:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 655, loss 4.502678871154785\n",
      "Epoch 2: |          | 656/? [15:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 656, loss 4.054198265075684\n",
      "Epoch 2: |          | 657/? [15:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 657, loss 6.607113838195801\n",
      "Epoch 2: |          | 658/? [15:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 658, loss 4.285701274871826\n",
      "Epoch 2: |          | 659/? [15:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 659, loss 4.502462863922119\n",
      "Epoch 2: |          | 660/? [15:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 660, loss 4.89457893371582\n",
      "Epoch 2: |          | 661/? [15:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 661, loss 4.803491592407227\n",
      "Epoch 2: |          | 662/? [15:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 662, loss 4.667519569396973\n",
      "Epoch 2: |          | 663/? [15:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 663, loss 4.347421169281006\n",
      "Epoch 2: |          | 664/? [15:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 664, loss 4.288439750671387\n",
      "Epoch 2: |          | 665/? [15:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 665, loss 4.642037391662598\n",
      "Epoch 2: |          | 666/? [15:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 666, loss 4.419925212860107\n",
      "Epoch 2: |          | 667/? [15:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 667, loss 5.303914546966553\n",
      "Epoch 2: |          | 668/? [15:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 668, loss 3.986661911010742\n",
      "Epoch 2: |          | 669/? [15:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 669, loss 4.266423225402832\n",
      "Epoch 2: |          | 670/? [15:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 670, loss 4.957217216491699\n",
      "Epoch 2: |          | 671/? [15:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 671, loss 4.716601371765137\n",
      "Epoch 2: |          | 672/? [15:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 672, loss 4.677483558654785\n",
      "Epoch 2: |          | 673/? [15:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 673, loss 4.518645286560059\n",
      "Epoch 2: |          | 674/? [16:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 674, loss 3.2682247161865234\n",
      "Epoch 2: |          | 675/? [16:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 675, loss 1.7735116481781006\n",
      "Epoch 2: |          | 676/? [16:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 676, loss 1.4532757997512817\n",
      "Epoch 2: |          | 677/? [16:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 677, loss 1.065564751625061\n",
      "Epoch 2: |          | 678/? [16:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 678, loss 2.257715940475464\n",
      "Epoch 2: |          | 679/? [16:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 679, loss 3.954967498779297\n",
      "Epoch 2: |          | 680/? [16:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 680, loss 4.496063709259033\n",
      "Epoch 2: |          | 681/? [16:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 681, loss 3.771157741546631\n",
      "Epoch 2: |          | 682/? [16:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 682, loss 4.366729736328125\n",
      "Epoch 2: |          | 683/? [16:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 683, loss 3.993452548980713\n",
      "Epoch 2: |          | 684/? [16:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 684, loss 5.129204273223877\n",
      "Epoch 2: |          | 685/? [16:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 685, loss 4.693241596221924\n",
      "Epoch 2: |          | 686/? [16:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 686, loss 4.177684783935547\n",
      "Epoch 2: |          | 687/? [16:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 687, loss 4.910120010375977\n",
      "Epoch 2: |          | 688/? [16:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 688, loss 4.6143999099731445\n",
      "Epoch 2: |          | 689/? [16:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 689, loss 4.521142959594727\n",
      "Epoch 2: |          | 690/? [16:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 690, loss 5.01636266708374\n",
      "Epoch 2: |          | 691/? [16:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 691, loss 4.592959403991699\n",
      "Epoch 2: |          | 692/? [16:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 692, loss 4.4996185302734375\n",
      "Epoch 2: |          | 693/? [16:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 693, loss 5.034581184387207\n",
      "Epoch 2: |          | 694/? [16:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 694, loss 4.356461048126221\n",
      "Epoch 2: |          | 695/? [16:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 695, loss 5.0069379806518555\n",
      "Epoch 2: |          | 696/? [16:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 696, loss 4.0476484298706055\n",
      "Epoch 2: |          | 697/? [16:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 697, loss 4.4956769943237305\n",
      "Epoch 2: |          | 698/? [16:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 698, loss 3.613417387008667\n",
      "Epoch 2: |          | 699/? [16:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 699, loss 4.6096673011779785\n",
      "Epoch 2: |          | 700/? [16:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 700, loss 4.782406330108643\n",
      "Epoch 2: |          | 701/? [16:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 701, loss 4.278848648071289\n",
      "Epoch 2: |          | 702/? [16:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 702, loss 4.5027313232421875\n",
      "Epoch 2: |          | 703/? [16:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 703, loss 4.704026222229004\n",
      "Epoch 2: |          | 704/? [16:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 704, loss 4.607882022857666\n",
      "Epoch 2: |          | 705/? [16:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 705, loss 4.140427589416504\n",
      "Epoch 2: |          | 706/? [16:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 706, loss 4.248803615570068\n",
      "Epoch 2: |          | 707/? [16:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 707, loss 4.719796657562256\n",
      "Epoch 2: |          | 708/? [16:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 708, loss 4.481630325317383\n",
      "Epoch 2: |          | 709/? [16:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 709, loss 4.404606819152832\n",
      "Epoch 2: |          | 710/? [16:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 710, loss 5.017462730407715\n",
      "Epoch 2: |          | 711/? [16:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 711, loss 5.167361259460449\n",
      "Epoch 2: |          | 712/? [16:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 712, loss 4.797677993774414\n",
      "Epoch 2: |          | 713/? [16:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 713, loss 4.789133071899414\n",
      "Epoch 2: |          | 714/? [16:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 714, loss 4.963954925537109\n",
      "Epoch 2: |          | 715/? [16:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 715, loss 3.713747501373291\n",
      "Epoch 2: |          | 716/? [16:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 716, loss 4.572575569152832\n",
      "Epoch 2: |          | 717/? [17:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 717, loss 4.333578109741211\n",
      "Epoch 2: |          | 718/? [17:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 718, loss 3.9008095264434814\n",
      "Epoch 2: |          | 719/? [17:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 719, loss 4.44541597366333\n",
      "Epoch 2: |          | 720/? [17:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 720, loss 4.185603141784668\n",
      "Epoch 2: |          | 721/? [17:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 721, loss 4.899603843688965\n",
      "Epoch 2: |          | 722/? [17:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 722, loss 4.005050182342529\n",
      "Epoch 2: |          | 723/? [17:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 723, loss 4.65172815322876\n",
      "Epoch 2: |          | 724/? [17:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 724, loss 4.489390850067139\n",
      "Epoch 2: |          | 725/? [17:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 725, loss 4.1823530197143555\n",
      "Epoch 2: |          | 726/? [17:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 726, loss 4.390328407287598\n",
      "Epoch 2: |          | 727/? [17:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 727, loss 4.063287258148193\n",
      "Epoch 2: |          | 728/? [17:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 728, loss 3.9342830181121826\n",
      "Epoch 2: |          | 729/? [17:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 729, loss 4.337159156799316\n",
      "Epoch 2: |          | 730/? [17:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 730, loss 4.336991786956787\n",
      "Epoch 2: |          | 731/? [17:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 731, loss 4.513655662536621\n",
      "Epoch 2: |          | 732/? [17:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 732, loss 4.844156265258789\n",
      "Epoch 2: |          | 733/? [17:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 733, loss 4.443120002746582\n",
      "Epoch 2: |          | 734/? [17:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 734, loss 4.784679889678955\n",
      "Epoch 2: |          | 735/? [17:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 735, loss 4.6584577560424805\n",
      "Epoch 2: |          | 736/? [17:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 736, loss 4.115427494049072\n",
      "Epoch 2: |          | 737/? [17:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 737, loss 4.893191337585449\n",
      "Epoch 2: |          | 738/? [17:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 738, loss 4.018523216247559\n",
      "Epoch 2: |          | 739/? [17:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 739, loss 4.663757801055908\n",
      "Epoch 2: |          | 740/? [17:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 740, loss 4.177706718444824\n",
      "Epoch 2: |          | 741/? [17:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 741, loss 4.468474388122559\n",
      "Epoch 2: |          | 742/? [17:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 742, loss 4.912306785583496\n",
      "Epoch 2: |          | 743/? [17:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 743, loss 4.678820610046387\n",
      "Epoch 2: |          | 744/? [17:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 744, loss 4.615948677062988\n",
      "Epoch 2: |          | 745/? [17:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 745, loss 4.226892471313477\n",
      "Epoch 2: |          | 746/? [17:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 746, loss 4.571440696716309\n",
      "Epoch 2: |          | 747/? [17:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 747, loss 4.37185525894165\n",
      "Epoch 2: |          | 748/? [17:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 748, loss 3.769040584564209\n",
      "Epoch 2: |          | 749/? [17:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 749, loss 4.551730155944824\n",
      "Epoch 2: |          | 750/? [17:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 750, loss 4.952901363372803\n",
      "Epoch 2: |          | 751/? [17:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 751, loss 3.3185181617736816\n",
      "Epoch 2: |          | 752/? [17:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 752, loss 4.734494209289551\n",
      "Epoch 2: |          | 753/? [17:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 753, loss 3.9665279388427734\n",
      "Epoch 2: |          | 754/? [17:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 754, loss 4.387065410614014\n",
      "Epoch 2: |          | 755/? [17:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 755, loss 4.073709011077881\n",
      "Epoch 2: |          | 756/? [17:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 756, loss 4.523227214813232\n",
      "Epoch 2: |          | 757/? [17:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 757, loss 4.531937599182129\n",
      "Epoch 2: |          | 758/? [17:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 758, loss 4.193622589111328\n",
      "Epoch 2: |          | 759/? [18:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 759, loss 4.299925804138184\n",
      "Epoch 2: |          | 760/? [18:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 760, loss 4.695127010345459\n",
      "Epoch 2: |          | 761/? [18:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 761, loss 4.782386302947998\n",
      "Epoch 2: |          | 762/? [18:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 762, loss 4.442563056945801\n",
      "Epoch 2: |          | 763/? [18:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 763, loss 4.785377502441406\n",
      "Epoch 2: |          | 764/? [18:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 764, loss 4.971380710601807\n",
      "Epoch 2: |          | 765/? [18:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 765, loss 4.613296031951904\n",
      "Epoch 2: |          | 766/? [18:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 766, loss 5.034214973449707\n",
      "Epoch 2: |          | 767/? [18:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 767, loss 5.151076793670654\n",
      "Epoch 2: |          | 768/? [18:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 768, loss 4.6134443283081055\n",
      "Epoch 2: |          | 769/? [18:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 769, loss 3.7362442016601562\n",
      "Epoch 2: |          | 770/? [18:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 770, loss 4.316448211669922\n",
      "Epoch 2: |          | 771/? [18:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 771, loss 5.1249918937683105\n",
      "Epoch 2: |          | 772/? [18:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 772, loss 4.827956199645996\n",
      "Epoch 2: |          | 773/? [18:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 773, loss 4.428366184234619\n",
      "Epoch 2: |          | 774/? [18:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 774, loss 4.542223930358887\n",
      "Epoch 2: |          | 775/? [18:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 775, loss 5.075772285461426\n",
      "Epoch 2: |          | 776/? [18:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 776, loss 4.456892490386963\n",
      "Epoch 2: |          | 777/? [18:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 777, loss 4.484026908874512\n",
      "Epoch 2: |          | 778/? [18:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 778, loss 4.838362693786621\n",
      "Epoch 2: |          | 779/? [18:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 779, loss 5.225467681884766\n",
      "Epoch 2: |          | 780/? [18:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 780, loss 4.061484336853027\n",
      "Epoch 2: |          | 781/? [18:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 781, loss 4.2419562339782715\n",
      "Epoch 2: |          | 782/? [18:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 782, loss 4.685492515563965\n",
      "Epoch 2: |          | 783/? [18:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 783, loss 4.760654449462891\n",
      "Epoch 2: |          | 784/? [18:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 784, loss 4.278286933898926\n",
      "Epoch 2: |          | 785/? [18:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 785, loss 4.1484527587890625\n",
      "Epoch 2: |          | 786/? [18:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 786, loss 4.970506191253662\n",
      "Epoch 2: |          | 787/? [18:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 787, loss 5.029690265655518\n",
      "Epoch 2: |          | 788/? [18:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 788, loss 3.143974781036377\n",
      "Epoch 2: |          | 789/? [18:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 789, loss 4.485020637512207\n",
      "Epoch 2: |          | 790/? [18:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 790, loss 5.274676322937012\n",
      "Epoch 2: |          | 791/? [18:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 791, loss 4.998323440551758\n",
      "Epoch 2: |          | 792/? [18:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 792, loss 4.162313938140869\n",
      "Epoch 2: |          | 793/? [18:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 793, loss 4.5745110511779785\n",
      "Epoch 2: |          | 794/? [18:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 794, loss 5.016230583190918\n",
      "Epoch 2: |          | 795/? [18:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 795, loss 4.481439113616943\n",
      "Epoch 2: |          | 796/? [18:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 796, loss 4.872617244720459\n",
      "Epoch 2: |          | 797/? [18:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 797, loss 3.762575149536133\n",
      "Epoch 2: |          | 798/? [18:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 798, loss 3.917527437210083\n",
      "Epoch 2: |          | 799/? [18:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 799, loss 4.918435096740723\n",
      "Epoch 2: |          | 800/? [18:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 800, loss 4.682431221008301\n",
      "Epoch 2: |          | 801/? [19:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 801, loss 4.151112079620361\n",
      "Epoch 2: |          | 802/? [19:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 802, loss 4.637203693389893\n",
      "Epoch 2: |          | 803/? [19:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 803, loss 4.416830062866211\n",
      "Epoch 2: |          | 804/? [19:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 804, loss 4.636028289794922\n",
      "Epoch 2: |          | 805/? [19:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 805, loss 4.980091094970703\n",
      "Epoch 2: |          | 806/? [19:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 806, loss 5.142431259155273\n",
      "Epoch 2: |          | 807/? [19:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 807, loss 4.596129417419434\n",
      "Epoch 2: |          | 808/? [19:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 808, loss 4.107910633087158\n",
      "Epoch 2: |          | 809/? [19:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 809, loss 4.67460298538208\n",
      "Epoch 2: |          | 810/? [19:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 810, loss 4.538943290710449\n",
      "Epoch 2: |          | 811/? [19:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 811, loss 4.862309455871582\n",
      "Epoch 2: |          | 812/? [19:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 812, loss 5.489294052124023\n",
      "Epoch 2: |          | 813/? [19:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 813, loss 5.12167501449585\n",
      "Epoch 2: |          | 814/? [19:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 814, loss 4.0870466232299805\n",
      "Epoch 2: |          | 815/? [19:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 815, loss 4.876614570617676\n",
      "Epoch 2: |          | 816/? [19:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 816, loss 4.6822686195373535\n",
      "Epoch 2: |          | 817/? [19:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 817, loss 3.995422840118408\n",
      "Epoch 2: |          | 818/? [19:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 818, loss 5.076938629150391\n",
      "Epoch 2: |          | 819/? [19:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 819, loss 4.765570640563965\n",
      "Epoch 2: |          | 820/? [19:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 820, loss 4.583573341369629\n",
      "Epoch 2: |          | 821/? [19:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 821, loss 4.558811187744141\n",
      "Epoch 2: |          | 822/? [19:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 822, loss 4.096100330352783\n",
      "Epoch 2: |          | 823/? [19:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 823, loss 4.053939342498779\n",
      "Epoch 2: |          | 824/? [19:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 824, loss 4.643243789672852\n",
      "Epoch 2: |          | 825/? [19:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 825, loss 4.183460235595703\n",
      "Epoch 2: |          | 826/? [19:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 826, loss 4.668342113494873\n",
      "Epoch 2: |          | 827/? [19:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 827, loss 4.309661865234375\n",
      "Epoch 2: |          | 828/? [19:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 828, loss 4.874349117279053\n",
      "Epoch 2: |          | 829/? [19:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 829, loss 4.474747657775879\n",
      "Epoch 2: |          | 830/? [19:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 830, loss 5.111175060272217\n",
      "Epoch 2: |          | 831/? [19:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 831, loss 2.883359909057617\n",
      "Epoch 2: |          | 832/? [19:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 832, loss 4.5129008293151855\n",
      "Epoch 2: |          | 833/? [19:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 833, loss 4.3426079750061035\n",
      "Epoch 2: |          | 834/? [19:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 834, loss 5.122134208679199\n",
      "Epoch 2: |          | 835/? [19:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 835, loss 4.563786506652832\n",
      "Epoch 2: |          | 836/? [19:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 836, loss 5.144224166870117\n",
      "Epoch 2: |          | 837/? [19:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 837, loss 4.609688758850098\n",
      "Epoch 2: |          | 838/? [19:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 838, loss 3.869450330734253\n",
      "Epoch 2: |          | 839/? [19:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 839, loss 4.212741851806641\n",
      "Epoch 2: |          | 840/? [19:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 840, loss 4.796474933624268\n",
      "Epoch 2: |          | 841/? [19:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 841, loss 4.881910800933838\n",
      "Epoch 2: |          | 842/? [20:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 842, loss 4.542621612548828\n",
      "Epoch 2: |          | 843/? [20:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 843, loss 4.873708724975586\n",
      "Epoch 2: |          | 844/? [20:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 844, loss 4.208855628967285\n",
      "Epoch 2: |          | 845/? [20:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 845, loss 4.6315412521362305\n",
      "Epoch 2: |          | 846/? [20:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 846, loss 5.178920269012451\n",
      "Epoch 2: |          | 847/? [20:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 847, loss 4.682718753814697\n",
      "Epoch 2: |          | 848/? [20:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 848, loss 4.151532173156738\n",
      "Epoch 2: |          | 849/? [20:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 849, loss 4.317567825317383\n",
      "Epoch 2: |          | 850/? [20:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 850, loss 4.365599632263184\n",
      "Epoch 2: |          | 851/? [20:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 851, loss 4.783696174621582\n",
      "Epoch 2: |          | 852/? [20:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 852, loss 4.739070415496826\n",
      "Epoch 2: |          | 853/? [20:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 853, loss 4.740207672119141\n",
      "Epoch 2: |          | 854/? [20:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 854, loss 3.8014540672302246\n",
      "Epoch 2: |          | 855/? [20:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 855, loss 4.2401533126831055\n",
      "Epoch 2: |          | 856/? [20:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 856, loss 4.1929426193237305\n",
      "Epoch 2: |          | 857/? [20:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 857, loss 4.779217720031738\n",
      "Epoch 2: |          | 858/? [20:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 858, loss 4.642151832580566\n",
      "Epoch 2: |          | 859/? [20:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 859, loss 4.693139553070068\n",
      "Epoch 2: |          | 860/? [20:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 860, loss 5.05734920501709\n",
      "Epoch 2: |          | 861/? [20:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 861, loss 4.221226692199707\n",
      "Epoch 2: |          | 862/? [20:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 862, loss 4.753747940063477\n",
      "Epoch 2: |          | 863/? [20:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 863, loss 3.881277084350586\n",
      "Epoch 2: |          | 864/? [20:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 864, loss 4.678662300109863\n",
      "Epoch 2: |          | 865/? [20:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 865, loss 4.550136566162109\n",
      "Epoch 2: |          | 866/? [20:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 866, loss 3.549145221710205\n",
      "Epoch 2: |          | 867/? [20:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 867, loss 3.796947479248047\n",
      "Epoch 2: |          | 868/? [20:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 868, loss 4.7060370445251465\n",
      "Epoch 2: |          | 869/? [20:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 869, loss 4.65800666809082\n",
      "Epoch 2: |          | 870/? [20:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 870, loss 4.206484794616699\n",
      "Epoch 2: |          | 871/? [20:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 871, loss 4.681621074676514\n",
      "Epoch 2: |          | 872/? [20:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 872, loss 4.528099060058594\n",
      "Epoch 2: |          | 873/? [20:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 873, loss 4.4485249519348145\n",
      "Epoch 2: |          | 874/? [20:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 874, loss 3.921846389770508\n",
      "Epoch 2: |          | 875/? [20:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 875, loss 4.67819881439209\n",
      "Epoch 2: |          | 876/? [20:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 876, loss 4.411334037780762\n",
      "Epoch 2: |          | 877/? [20:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 877, loss 4.602156639099121\n",
      "Epoch 2: |          | 878/? [20:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 878, loss 4.02628231048584\n",
      "Epoch 2: |          | 879/? [20:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 879, loss 4.120510101318359\n",
      "Epoch 2: |          | 880/? [20:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 880, loss 5.2405829429626465\n",
      "Epoch 2: |          | 881/? [20:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 881, loss 4.6292924880981445\n",
      "Epoch 2: |          | 882/? [20:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 882, loss 4.39883279800415\n",
      "Epoch 2: |          | 883/? [20:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 883, loss 4.528864860534668\n",
      "Epoch 2: |          | 884/? [21:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 884, loss 4.6372528076171875\n",
      "Epoch 2: |          | 885/? [21:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 885, loss 4.360721588134766\n",
      "Epoch 2: |          | 886/? [21:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 886, loss 5.0027546882629395\n",
      "Epoch 2: |          | 887/? [21:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 887, loss 5.097503662109375\n",
      "Epoch 2: |          | 888/? [21:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 888, loss 4.748913288116455\n",
      "Epoch 2: |          | 889/? [21:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 889, loss 4.380722522735596\n",
      "Epoch 2: |          | 890/? [21:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 890, loss 4.703824043273926\n",
      "Epoch 2: |          | 891/? [21:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 891, loss 4.184172630310059\n",
      "Epoch 2: |          | 892/? [21:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 892, loss 4.9853363037109375\n",
      "Epoch 2: |          | 893/? [21:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 893, loss 4.355193138122559\n",
      "Epoch 2: |          | 894/? [21:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 894, loss 4.028059959411621\n",
      "Epoch 2: |          | 895/? [21:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 895, loss 5.095322608947754\n",
      "Epoch 2: |          | 896/? [21:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 896, loss 4.725492000579834\n",
      "Epoch 2: |          | 897/? [21:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 897, loss 4.741888046264648\n",
      "Epoch 2: |          | 898/? [21:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 898, loss 4.687036037445068\n",
      "Epoch 2: |          | 899/? [21:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 899, loss 4.425816535949707\n",
      "Epoch 2: |          | 900/? [21:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 900, loss 4.296148777008057\n",
      "Epoch 2: |          | 901/? [21:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 901, loss 4.83202600479126\n",
      "Epoch 2: |          | 902/? [21:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 902, loss 4.835567951202393\n",
      "Epoch 2: |          | 903/? [21:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 903, loss 4.092199802398682\n",
      "Epoch 2: |          | 904/? [21:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 904, loss 4.62438440322876\n",
      "Epoch 2: |          | 905/? [21:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 905, loss 4.878628730773926\n",
      "Epoch 2: |          | 906/? [21:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 906, loss 4.535307884216309\n",
      "Epoch 2: |          | 907/? [21:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 907, loss 4.6292924880981445\n",
      "Epoch 2: |          | 908/? [21:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 908, loss 4.688526630401611\n",
      "Epoch 2: |          | 909/? [21:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 909, loss 4.700525760650635\n",
      "Epoch 2: |          | 910/? [21:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 910, loss 4.434062480926514\n",
      "Epoch 2: |          | 911/? [21:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 911, loss 4.4689483642578125\n",
      "Epoch 2: |          | 912/? [21:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 912, loss 4.453564643859863\n",
      "Epoch 2: |          | 913/? [21:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 913, loss 4.434338092803955\n",
      "Epoch 2: |          | 914/? [21:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 914, loss 4.8002214431762695\n",
      "Epoch 2: |          | 915/? [21:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 915, loss 4.671771049499512\n",
      "Epoch 2: |          | 916/? [21:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 916, loss 4.455831527709961\n",
      "Epoch 2: |          | 917/? [21:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 917, loss 4.566781997680664\n",
      "Epoch 2: |          | 918/? [21:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 918, loss 4.36942195892334\n",
      "Epoch 2: |          | 919/? [21:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 919, loss 4.302856922149658\n",
      "Epoch 2: |          | 920/? [21:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 920, loss 4.512839317321777\n",
      "Epoch 2: |          | 921/? [21:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 921, loss 4.397262096405029\n",
      "Epoch 2: |          | 922/? [21:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 922, loss 4.526733875274658\n",
      "Epoch 2: |          | 923/? [21:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 923, loss 4.398670196533203\n",
      "Epoch 2: |          | 924/? [21:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 924, loss 4.332339286804199\n",
      "Epoch 2: |          | 925/? [21:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 925, loss 4.638299465179443\n",
      "Epoch 2: |          | 926/? [21:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 926, loss 4.476818084716797\n",
      "Epoch 2: |          | 927/? [22:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 927, loss 4.6592512130737305\n",
      "Epoch 2: |          | 928/? [22:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 928, loss 4.120883941650391\n",
      "Epoch 2: |          | 929/? [22:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 929, loss 4.28518533706665\n",
      "Epoch 2: |          | 930/? [22:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 930, loss 4.139317989349365\n",
      "Epoch 2: |          | 931/? [22:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 931, loss 3.838862895965576\n",
      "Epoch 2: |          | 932/? [22:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 932, loss 4.568792819976807\n",
      "Epoch 2: |          | 933/? [22:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 933, loss 4.346450328826904\n",
      "Epoch 2: |          | 934/? [22:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 934, loss 5.037081241607666\n",
      "Epoch 2: |          | 935/? [22:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 935, loss 5.237326622009277\n",
      "Epoch 2: |          | 936/? [22:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 936, loss 4.4825310707092285\n",
      "Epoch 2: |          | 937/? [22:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 937, loss 4.939406394958496\n",
      "Epoch 2: |          | 938/? [22:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 938, loss 4.339306831359863\n",
      "Epoch 2: |          | 939/? [22:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 939, loss 4.646159648895264\n",
      "Epoch 2: |          | 940/? [22:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 940, loss 4.944771766662598\n",
      "Epoch 2: |          | 941/? [22:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 941, loss 4.2821879386901855\n",
      "Epoch 2: |          | 942/? [22:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 942, loss 3.847299575805664\n",
      "Epoch 2: |          | 943/? [22:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 943, loss 4.813111305236816\n",
      "Epoch 2: |          | 944/? [22:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 944, loss 3.8768088817596436\n",
      "Epoch 2: |          | 945/? [22:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 945, loss 4.452292442321777\n",
      "Epoch 2: |          | 946/? [22:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 946, loss 4.489241600036621\n",
      "Epoch 2: |          | 947/? [22:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 947, loss 4.413455009460449\n",
      "Epoch 2: |          | 948/? [22:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 948, loss 4.5458502769470215\n",
      "Epoch 2: |          | 949/? [22:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 949, loss 4.42329216003418\n",
      "Epoch 2: |          | 950/? [22:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 950, loss 4.246027946472168\n",
      "Epoch 2: |          | 951/? [22:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 951, loss 5.02701473236084\n",
      "Epoch 2: |          | 952/? [22:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 952, loss 4.9165754318237305\n",
      "Epoch 2: |          | 953/? [22:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 953, loss 5.395817756652832\n",
      "Epoch 2: |          | 954/? [22:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 954, loss 4.4463725090026855\n",
      "Epoch 2: |          | 955/? [22:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 955, loss 5.172027587890625\n",
      "Epoch 2: |          | 956/? [22:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 956, loss 4.391233921051025\n",
      "Epoch 2: |          | 957/? [22:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 957, loss 4.773589134216309\n",
      "Epoch 2: |          | 958/? [22:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 958, loss 5.010564804077148\n",
      "Epoch 2: |          | 959/? [22:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 959, loss 4.878793716430664\n",
      "Epoch 2: |          | 960/? [22:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 960, loss 4.952807426452637\n",
      "Epoch 2: |          | 961/? [22:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 961, loss 5.020806312561035\n",
      "Epoch 2: |          | 962/? [22:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 962, loss 4.683747291564941\n",
      "Epoch 2: |          | 963/? [22:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 963, loss 4.237083911895752\n",
      "Epoch 2: |          | 964/? [22:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 964, loss 4.778063774108887\n",
      "Epoch 2: |          | 965/? [22:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 965, loss 4.218562126159668\n",
      "Epoch 2: |          | 966/? [22:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 966, loss 4.161288261413574\n",
      "Epoch 2: |          | 967/? [23:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 967, loss 4.396933555603027\n",
      "Epoch 2: |          | 968/? [23:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 968, loss 4.451432228088379\n",
      "Epoch 2: |          | 969/? [23:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 969, loss 4.144920349121094\n",
      "Epoch 2: |          | 970/? [23:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 970, loss 4.707032203674316\n",
      "Epoch 2: |          | 971/? [23:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 971, loss 5.128922462463379\n",
      "Epoch 2: |          | 972/? [23:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 972, loss 4.365884304046631\n",
      "Epoch 2: |          | 973/? [23:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 973, loss 4.695958614349365\n",
      "Epoch 2: |          | 974/? [23:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 974, loss 4.484035491943359\n",
      "Epoch 2: |          | 975/? [23:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 975, loss 4.512889862060547\n",
      "Epoch 2: |          | 976/? [23:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 976, loss 4.626172065734863\n",
      "Epoch 2: |          | 977/? [23:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 977, loss 5.220215797424316\n",
      "Epoch 2: |          | 978/? [23:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 978, loss 4.78660249710083\n",
      "Epoch 2: |          | 979/? [23:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 979, loss 5.0196332931518555\n",
      "Epoch 2: |          | 980/? [23:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 980, loss 4.18389368057251\n",
      "Epoch 2: |          | 981/? [23:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 981, loss 3.832986354827881\n",
      "Epoch 2: |          | 982/? [23:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 982, loss 4.582492351531982\n",
      "Epoch 2: |          | 983/? [23:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 983, loss 4.965216159820557\n",
      "Epoch 2: |          | 984/? [23:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 984, loss 3.9481539726257324\n",
      "Epoch 2: |          | 985/? [23:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 985, loss 4.30432653427124\n",
      "Epoch 2: |          | 986/? [23:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 986, loss 4.267307758331299\n",
      "Epoch 2: |          | 987/? [23:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 987, loss 3.920422315597534\n",
      "Epoch 2: |          | 988/? [23:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 988, loss 4.8654632568359375\n",
      "Epoch 2: |          | 989/? [23:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 989, loss 4.546899795532227\n",
      "Epoch 2: |          | 990/? [23:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 990, loss 3.635876417160034\n",
      "Epoch 2: |          | 991/? [23:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 991, loss 4.468646049499512\n",
      "Epoch 2: |          | 992/? [23:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 992, loss 5.256592273712158\n",
      "Epoch 2: |          | 993/? [23:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 993, loss 4.272144317626953\n",
      "Epoch 2: |          | 994/? [23:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 994, loss 4.349638938903809\n",
      "Epoch 2: |          | 995/? [23:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 995, loss 4.806273937225342\n",
      "Epoch 2: |          | 996/? [23:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 996, loss 4.806430339813232\n",
      "Epoch 2: |          | 997/? [23:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 997, loss 4.345377445220947\n",
      "Epoch 2: |          | 998/? [23:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 998, loss 4.648314476013184\n",
      "Epoch 2: |          | 999/? [23:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 999, loss 4.798147678375244\n",
      "Epoch 2: |          | 1000/? [23:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1000, loss 4.32028341293335\n",
      "Epoch 2: |          | 1001/? [23:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1001, loss 4.82211971282959\n",
      "Epoch 2: |          | 1002/? [23:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1002, loss 4.88618803024292\n",
      "Epoch 2: |          | 1003/? [23:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1003, loss 4.902929782867432\n",
      "Epoch 2: |          | 1004/? [23:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1004, loss 3.7299835681915283\n",
      "Epoch 2: |          | 1005/? [23:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1005, loss 4.4462175369262695\n",
      "Epoch 2: |          | 1006/? [23:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1006, loss 4.8574018478393555\n",
      "Epoch 2: |          | 1007/? [23:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1007, loss 4.450260162353516\n",
      "Epoch 2: |          | 1008/? [23:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1008, loss 4.470035076141357\n",
      "Epoch 2: |          | 1009/? [24:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1009, loss 4.8987956047058105\n",
      "Epoch 2: |          | 1010/? [24:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1010, loss 3.8819808959960938\n",
      "Epoch 2: |          | 1011/? [24:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1011, loss 4.560913562774658\n",
      "Epoch 2: |          | 1012/? [24:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1012, loss 4.254351615905762\n",
      "Epoch 2: |          | 1013/? [24:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1013, loss 4.589688301086426\n",
      "Epoch 2: |          | 1014/? [24:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1014, loss 4.9007768630981445\n",
      "Epoch 2: |          | 1015/? [24:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1015, loss 4.572747230529785\n",
      "Epoch 2: |          | 1016/? [24:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1016, loss 4.450564384460449\n",
      "Epoch 2: |          | 1017/? [24:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1017, loss 3.9395480155944824\n",
      "Epoch 2: |          | 1018/? [24:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1018, loss 4.486428737640381\n",
      "Epoch 2: |          | 1019/? [24:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1019, loss 4.500794410705566\n",
      "Epoch 2: |          | 1020/? [24:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1020, loss 4.036462306976318\n",
      "Epoch 2: |          | 1021/? [24:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1021, loss 4.258471965789795\n",
      "Epoch 2: |          | 1022/? [24:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1022, loss 3.9801807403564453\n",
      "Epoch 2: |          | 1023/? [24:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1023, loss 3.7077534198760986\n",
      "Epoch 2: |          | 1024/? [24:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1024, loss 4.3452606201171875\n",
      "Epoch 2: |          | 1025/? [24:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1025, loss 4.257078170776367\n",
      "Epoch 2: |          | 1026/? [24:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1026, loss 3.214468479156494\n",
      "Epoch 2: |          | 1027/? [24:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1027, loss 4.5778913497924805\n",
      "Epoch 2: |          | 1028/? [24:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1028, loss 4.398953437805176\n",
      "Epoch 2: |          | 1029/? [24:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1029, loss 4.354181289672852\n",
      "Epoch 2: |          | 1030/? [24:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1030, loss 4.029524803161621\n",
      "Epoch 2: |          | 1031/? [24:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1031, loss 4.134702682495117\n",
      "Epoch 2: |          | 1032/? [24:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1032, loss 4.844283103942871\n",
      "Epoch 2: |          | 1033/? [24:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1033, loss 4.922722339630127\n",
      "Epoch 2: |          | 1034/? [24:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1034, loss 4.261498928070068\n",
      "Epoch 2: |          | 1035/? [24:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1035, loss 4.303561210632324\n",
      "Epoch 2: |          | 1036/? [24:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1036, loss 4.110255241394043\n",
      "Epoch 2: |          | 1037/? [24:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1037, loss 4.871943473815918\n",
      "Epoch 2: |          | 1038/? [24:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1038, loss 5.043558120727539\n",
      "Epoch 2: |          | 1039/? [24:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1039, loss 5.206759452819824\n",
      "Epoch 2: |          | 1040/? [24:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1040, loss 4.544777870178223\n",
      "Epoch 2: |          | 1041/? [24:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1041, loss 4.93677282333374\n",
      "Epoch 2: |          | 1042/? [24:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1042, loss 4.489810466766357\n",
      "Epoch 2: |          | 1043/? [24:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1043, loss 4.789044380187988\n",
      "Epoch 2: |          | 1044/? [24:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1044, loss 4.371573448181152\n",
      "Epoch 2: |          | 1045/? [24:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1045, loss 3.947000026702881\n",
      "Epoch 2: |          | 1046/? [24:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1046, loss 3.731032609939575\n",
      "Epoch 2: |          | 1047/? [24:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1047, loss 5.06997013092041\n",
      "Epoch 2: |          | 1048/? [24:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1048, loss 4.294278621673584\n",
      "Epoch 2: |          | 1049/? [24:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1049, loss 4.533513069152832\n",
      "Epoch 2: |          | 1050/? [24:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1050, loss 4.123734474182129\n",
      "Epoch 2: |          | 1051/? [24:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1051, loss 4.25888729095459\n",
      "Epoch 2: |          | 1052/? [25:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1052, loss 4.808765411376953\n",
      "Epoch 2: |          | 1053/? [25:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1053, loss 4.888719081878662\n",
      "Epoch 2: |          | 1054/? [25:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1054, loss 4.2556962966918945\n",
      "Epoch 2: |          | 1055/? [25:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1055, loss 3.97139310836792\n",
      "Epoch 2: |          | 1056/? [25:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1056, loss 4.052311897277832\n",
      "Epoch 2: |          | 1057/? [25:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1057, loss 4.687543869018555\n",
      "Epoch 2: |          | 1058/? [25:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1058, loss 4.259458065032959\n",
      "Epoch 2: |          | 1059/? [25:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1059, loss 4.892380714416504\n",
      "Epoch 2: |          | 1060/? [25:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1060, loss 4.764410018920898\n",
      "Epoch 2: |          | 1061/? [25:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1061, loss 3.3292782306671143\n",
      "Epoch 2: |          | 1062/? [25:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1062, loss 4.538364410400391\n",
      "Epoch 2: |          | 1063/? [25:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1063, loss 4.479617595672607\n",
      "Epoch 2: |          | 1064/? [25:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1064, loss 4.698124885559082\n",
      "Epoch 2: |          | 1065/? [25:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1065, loss 3.332176923751831\n",
      "Epoch 2: |          | 1066/? [25:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1066, loss 4.559034824371338\n",
      "Epoch 2: |          | 1067/? [25:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1067, loss 3.965461254119873\n",
      "Epoch 2: |          | 1068/? [25:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1068, loss 4.174882411956787\n",
      "Epoch 2: |          | 1069/? [25:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1069, loss 4.621373176574707\n",
      "Epoch 2: |          | 1070/? [25:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1070, loss 4.366353511810303\n",
      "Epoch 2: |          | 1071/? [25:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1071, loss 4.716253757476807\n",
      "Epoch 2: |          | 1072/? [25:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1072, loss 4.762207508087158\n",
      "Epoch 2: |          | 1073/? [25:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1073, loss 5.0914483070373535\n",
      "Epoch 2: |          | 1074/? [25:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1074, loss 4.327683448791504\n",
      "Epoch 2: |          | 1075/? [25:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1075, loss 4.040482997894287\n",
      "Epoch 2: |          | 1076/? [25:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1076, loss 4.858802318572998\n",
      "Epoch 2: |          | 1077/? [25:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1077, loss 4.269467353820801\n",
      "Epoch 2: |          | 1078/? [25:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1078, loss 4.431892395019531\n",
      "Epoch 2: |          | 1079/? [25:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1079, loss 5.163896560668945\n",
      "Epoch 2: |          | 1080/? [25:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1080, loss 4.589037895202637\n",
      "Epoch 2: |          | 1081/? [25:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1081, loss 4.763388633728027\n",
      "Epoch 2: |          | 1082/? [25:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1082, loss 4.172824859619141\n",
      "Epoch 2: |          | 1083/? [25:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1083, loss 3.9443397521972656\n",
      "Epoch 2: |          | 1084/? [25:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1084, loss 3.7315268516540527\n",
      "Epoch 2: |          | 1085/? [25:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1085, loss 4.323875427246094\n",
      "Epoch 2: |          | 1086/? [25:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1086, loss 4.592333793640137\n",
      "Epoch 2: |          | 1087/? [25:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1087, loss 5.151858329772949\n",
      "Epoch 2: |          | 1088/? [25:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1088, loss 4.705509185791016\n",
      "Epoch 2: |          | 1089/? [25:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1089, loss 4.972699165344238\n",
      "Epoch 2: |          | 1090/? [25:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1090, loss 4.619527816772461\n",
      "Epoch 2: |          | 1091/? [25:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1091, loss 4.304295539855957\n",
      "Epoch 2: |          | 1092/? [25:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1092, loss 4.538156509399414\n",
      "Epoch 2: |          | 1093/? [26:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1093, loss 3.9718527793884277\n",
      "Epoch 2: |          | 1094/? [26:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1094, loss 4.541256904602051\n",
      "Epoch 2: |          | 1095/? [26:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1095, loss 4.62673282623291\n",
      "Epoch 2: |          | 1096/? [26:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1096, loss 4.814957618713379\n",
      "Epoch 2: |          | 1097/? [26:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1097, loss 4.391021728515625\n",
      "Epoch 2: |          | 1098/? [26:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1098, loss 3.6054916381835938\n",
      "Epoch 2: |          | 1099/? [26:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1099, loss 4.375789165496826\n",
      "Epoch 2: |          | 1100/? [26:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1100, loss 4.602921962738037\n",
      "Epoch 2: |          | 1101/? [26:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1101, loss 4.135049343109131\n",
      "Epoch 2: |          | 1102/? [26:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1102, loss 4.9790191650390625\n",
      "Epoch 2: |          | 1103/? [26:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1103, loss 5.675505638122559\n",
      "Epoch 2: |          | 1104/? [26:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1104, loss 4.793241024017334\n",
      "Epoch 2: |          | 1105/? [26:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1105, loss 4.851733207702637\n",
      "Epoch 2: |          | 1106/? [26:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1106, loss 4.366827964782715\n",
      "Epoch 2: |          | 1107/? [26:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1107, loss 4.453540325164795\n",
      "Epoch 2: |          | 1108/? [26:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1108, loss 4.407712459564209\n",
      "Epoch 2: |          | 1109/? [26:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1109, loss 3.9301013946533203\n",
      "Epoch 2: |          | 1110/? [26:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1110, loss 5.175126552581787\n",
      "Epoch 2: |          | 1111/? [26:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1111, loss 4.698888778686523\n",
      "Epoch 2: |          | 1112/? [26:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1112, loss 4.59694766998291\n",
      "Epoch 2: |          | 1113/? [26:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1113, loss 4.339230537414551\n",
      "Epoch 2: |          | 1114/? [26:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1114, loss 3.787912368774414\n",
      "Epoch 2: |          | 1115/? [26:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1115, loss 3.467095136642456\n",
      "Epoch 2: |          | 1116/? [26:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1116, loss 4.023097991943359\n",
      "Epoch 2: |          | 1117/? [26:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1117, loss 4.264188289642334\n",
      "Epoch 2: |          | 1118/? [26:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1118, loss 4.248852729797363\n",
      "Epoch 2: |          | 1119/? [26:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1119, loss 4.995606899261475\n",
      "Epoch 2: |          | 1120/? [26:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1120, loss 4.454865455627441\n",
      "Epoch 2: |          | 1121/? [26:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1121, loss 4.807002067565918\n",
      "Epoch 2: |          | 1122/? [26:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1122, loss 4.48040771484375\n",
      "Epoch 2: |          | 1123/? [26:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1123, loss 4.520583629608154\n",
      "Epoch 2: |          | 1124/? [26:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1124, loss 4.875574111938477\n",
      "Epoch 2: |          | 1125/? [26:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1125, loss 4.106666564941406\n",
      "Epoch 2: |          | 1126/? [26:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1126, loss 4.0450944900512695\n",
      "Epoch 2: |          | 1127/? [26:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1127, loss 4.3528852462768555\n",
      "Epoch 2: |          | 1128/? [26:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1128, loss 4.389169692993164\n",
      "Epoch 2: |          | 1129/? [26:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1129, loss 4.538582801818848\n",
      "Epoch 2: |          | 1130/? [26:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1130, loss 4.706535816192627\n",
      "Epoch 2: |          | 1131/? [26:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1131, loss 4.794955253601074\n",
      "Epoch 2: |          | 1132/? [26:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1132, loss 3.4155678749084473\n",
      "Epoch 2: |          | 1133/? [26:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1133, loss 4.493163108825684\n",
      "Epoch 2: |          | 1134/? [26:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1134, loss 4.147881507873535\n",
      "Epoch 2: |          | 1135/? [27:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1135, loss 4.832052230834961\n",
      "Epoch 2: |          | 1136/? [27:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1136, loss 4.447137832641602\n",
      "Epoch 2: |          | 1137/? [27:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1137, loss 4.488127708435059\n",
      "Epoch 2: |          | 1138/? [27:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1138, loss 5.072517395019531\n",
      "Epoch 2: |          | 1139/? [27:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1139, loss 5.462665557861328\n",
      "Epoch 2: |          | 1140/? [27:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1140, loss 4.648149013519287\n",
      "Epoch 2: |          | 1141/? [27:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1141, loss 4.794886589050293\n",
      "Epoch 2: |          | 1142/? [27:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1142, loss 4.905858993530273\n",
      "Epoch 2: |          | 1143/? [27:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1143, loss 4.971798896789551\n",
      "Epoch 2: |          | 1144/? [27:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1144, loss 4.455321311950684\n",
      "Epoch 2: |          | 1145/? [27:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1145, loss 4.471651554107666\n",
      "Epoch 2: |          | 1146/? [27:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1146, loss 4.44265079498291\n",
      "Epoch 2: |          | 1147/? [27:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1147, loss 3.9304420948028564\n",
      "Epoch 2: |          | 1148/? [27:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1148, loss 4.232651710510254\n",
      "Epoch 2: |          | 1149/? [27:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1149, loss 5.735814094543457\n",
      "Epoch 2: |          | 1150/? [27:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1150, loss 4.632880687713623\n",
      "Epoch 2: |          | 1151/? [27:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1151, loss 5.046522617340088\n",
      "Epoch 2: |          | 1152/? [27:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1152, loss 4.147439479827881\n",
      "Epoch 2: |          | 1153/? [27:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1153, loss 4.524243354797363\n",
      "Epoch 2: |          | 1154/? [27:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1154, loss 4.176085472106934\n",
      "Epoch 2: |          | 1155/? [27:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1155, loss 4.44570779800415\n",
      "Epoch 2: |          | 1156/? [27:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1156, loss 4.613823890686035\n",
      "Epoch 2: |          | 1157/? [27:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1157, loss 4.698538780212402\n",
      "Epoch 2: |          | 1158/? [27:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1158, loss 5.082376956939697\n",
      "Epoch 2: |          | 1159/? [27:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1159, loss 3.627049684524536\n",
      "Epoch 2: |          | 1160/? [27:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1160, loss 4.920119285583496\n",
      "Epoch 2: |          | 1161/? [27:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1161, loss 4.780673027038574\n",
      "Epoch 2: |          | 1162/? [27:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1162, loss 4.692743301391602\n",
      "Epoch 2: |          | 1163/? [27:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1163, loss 5.175595283508301\n",
      "Epoch 2: |          | 1164/? [27:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1164, loss 5.028849124908447\n",
      "Epoch 2: |          | 1165/? [27:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1165, loss 4.046273231506348\n",
      "Epoch 2: |          | 1166/? [27:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1166, loss 4.661637306213379\n",
      "Epoch 2: |          | 1167/? [27:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1167, loss 4.881377220153809\n",
      "Epoch 2: |          | 1168/? [27:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1168, loss 5.229182243347168\n",
      "Epoch 2: |          | 1169/? [27:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1169, loss 4.125283241271973\n",
      "Epoch 2: |          | 1170/? [27:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1170, loss 4.750566482543945\n",
      "Epoch 2: |          | 1171/? [27:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1171, loss 4.258648872375488\n",
      "Epoch 2: |          | 1172/? [27:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1172, loss 4.001473426818848\n",
      "Epoch 2: |          | 1173/? [27:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1173, loss 4.670449256896973\n",
      "Epoch 2: |          | 1174/? [27:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1174, loss 4.115975379943848\n",
      "Epoch 2: |          | 1175/? [27:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1175, loss 4.82771635055542\n",
      "Epoch 2: |          | 1176/? [27:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1176, loss 4.864955425262451\n",
      "Epoch 2: |          | 1177/? [28:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1177, loss 4.955912113189697\n",
      "Epoch 2: |          | 1178/? [28:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1178, loss 4.322721004486084\n",
      "Epoch 2: |          | 1179/? [28:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1179, loss 4.869534015655518\n",
      "Epoch 2: |          | 1180/? [28:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1180, loss 4.651785850524902\n",
      "Epoch 2: |          | 1181/? [28:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1181, loss 4.689128875732422\n",
      "Epoch 2: |          | 1182/? [28:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1182, loss 4.508267402648926\n",
      "Epoch 2: |          | 1183/? [28:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1183, loss 4.180603981018066\n",
      "Epoch 2: |          | 1184/? [28:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1184, loss 4.416524887084961\n",
      "Epoch 2: |          | 1185/? [28:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1185, loss 4.398794174194336\n",
      "Epoch 2: |          | 1186/? [28:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1186, loss 4.603729724884033\n",
      "Epoch 2: |          | 1187/? [28:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1187, loss 4.383150100708008\n",
      "Epoch 2: |          | 1188/? [28:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1188, loss 4.834624290466309\n",
      "Epoch 2: |          | 1189/? [28:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1189, loss 4.871422290802002\n",
      "Epoch 2: |          | 1190/? [28:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1190, loss 4.357476711273193\n",
      "Epoch 2: |          | 1191/? [28:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1191, loss 4.4375410079956055\n",
      "Epoch 2: |          | 1192/? [28:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1192, loss 4.78701114654541\n",
      "Epoch 2: |          | 1193/? [28:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1193, loss 4.225586414337158\n",
      "Epoch 2: |          | 1194/? [28:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1194, loss 3.7905399799346924\n",
      "Epoch 2: |          | 1195/? [28:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1195, loss 4.534611225128174\n",
      "Epoch 2: |          | 1196/? [28:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1196, loss 4.705363750457764\n",
      "Epoch 2: |          | 1197/? [28:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1197, loss 4.581665992736816\n",
      "Epoch 2: |          | 1198/? [28:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1198, loss 4.584412574768066\n",
      "Epoch 2: |          | 1199/? [28:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1199, loss 4.796045303344727\n",
      "Epoch 2: |          | 1200/? [28:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1200, loss 4.003638744354248\n",
      "Epoch 2: |          | 1201/? [28:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1201, loss 4.699504375457764\n",
      "Epoch 2: |          | 1202/? [28:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1202, loss 4.301499843597412\n",
      "Epoch 2: |          | 1203/? [28:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1203, loss 4.355717658996582\n",
      "Epoch 2: |          | 1204/? [28:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1204, loss 3.7404632568359375\n",
      "Epoch 2: |          | 1205/? [28:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1205, loss 4.4432477951049805\n",
      "Epoch 2: |          | 1206/? [28:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1206, loss 4.4212141036987305\n",
      "Epoch 2: |          | 1207/? [28:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1207, loss 4.848281383514404\n",
      "Epoch 2: |          | 1208/? [28:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1208, loss 4.939330101013184\n",
      "Epoch 2: |          | 1209/? [28:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1209, loss 4.522346496582031\n",
      "Epoch 2: |          | 1210/? [28:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1210, loss 4.852416515350342\n",
      "Epoch 2: |          | 1211/? [28:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1211, loss 4.829930782318115\n",
      "Epoch 2: |          | 1212/? [28:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1212, loss 4.637604713439941\n",
      "Epoch 2: |          | 1213/? [28:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1213, loss 4.306368350982666\n",
      "Epoch 2: |          | 1214/? [28:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1214, loss 5.051019668579102\n",
      "Epoch 2: |          | 1215/? [28:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1215, loss 4.239514350891113\n",
      "Epoch 2: |          | 1216/? [28:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1216, loss 4.568124294281006\n",
      "Epoch 2: |          | 1217/? [28:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1217, loss 4.706598281860352\n",
      "Epoch 2: |          | 1218/? [29:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1218, loss 4.711719036102295\n",
      "Epoch 2: |          | 1219/? [29:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1219, loss 4.303188800811768\n",
      "Epoch 2: |          | 1220/? [29:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1220, loss 5.0779500007629395\n",
      "Epoch 2: |          | 1221/? [29:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1221, loss 4.815932273864746\n",
      "Epoch 2: |          | 1222/? [29:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1222, loss 3.5755209922790527\n",
      "Epoch 2: |          | 1223/? [29:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1223, loss 3.711954116821289\n",
      "Epoch 2: |          | 1224/? [29:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1224, loss 4.166372299194336\n",
      "Epoch 2: |          | 1225/? [29:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1225, loss 4.847899436950684\n",
      "Epoch 2: |          | 1226/? [29:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1226, loss 4.906782150268555\n",
      "Epoch 2: |          | 1227/? [29:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1227, loss 4.461880683898926\n",
      "Epoch 2: |          | 1228/? [29:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1228, loss 4.423616409301758\n",
      "Epoch 2: |          | 1229/? [29:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1229, loss 3.935558319091797\n",
      "Epoch 2: |          | 1230/? [29:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1230, loss 4.643218040466309\n",
      "Epoch 2: |          | 1231/? [29:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1231, loss 4.6907734870910645\n",
      "Epoch 2: |          | 1232/? [29:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1232, loss 4.881440162658691\n",
      "Epoch 2: |          | 1233/? [29:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1233, loss 4.657568454742432\n",
      "Epoch 2: |          | 1234/? [29:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1234, loss 3.5469908714294434\n",
      "Epoch 2: |          | 1235/? [29:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1235, loss 4.668513298034668\n",
      "Epoch 2: |          | 1236/? [29:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1236, loss 4.0547099113464355\n",
      "Epoch 2: |          | 1237/? [29:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1237, loss 4.4551100730896\n",
      "Epoch 2: |          | 1238/? [29:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1238, loss 4.3593621253967285\n",
      "Epoch 2: |          | 1239/? [29:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1239, loss 4.296848297119141\n",
      "Epoch 2: |          | 1240/? [29:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1240, loss 5.00419807434082\n",
      "Epoch 2: |          | 1241/? [29:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1241, loss 4.47603178024292\n",
      "Epoch 2: |          | 1242/? [29:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1242, loss 4.3325605392456055\n",
      "Epoch 2: |          | 1243/? [29:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1243, loss 4.094192028045654\n",
      "Epoch 2: |          | 1244/? [29:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1244, loss 4.302336692810059\n",
      "Epoch 2: |          | 1245/? [29:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1245, loss 3.8332176208496094\n",
      "Epoch 2: |          | 1246/? [29:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1246, loss 4.643494606018066\n",
      "Epoch 2: |          | 1247/? [29:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1247, loss 4.689958095550537\n",
      "Epoch 2: |          | 1248/? [29:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1248, loss 4.129951000213623\n",
      "Epoch 2: |          | 1249/? [29:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1249, loss 4.324070930480957\n",
      "Epoch 2: |          | 1250/? [29:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1250, loss 4.484081745147705\n",
      "Epoch 2: |          | 1251/? [29:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1251, loss 4.234894752502441\n",
      "Epoch 2: |          | 1252/? [29:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1252, loss 5.028637886047363\n",
      "Epoch 2: |          | 1253/? [29:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1253, loss 4.387313365936279\n",
      "Epoch 2: |          | 1254/? [29:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1254, loss 3.6297175884246826\n",
      "Epoch 2: |          | 1255/? [29:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1255, loss 5.104165554046631\n",
      "Epoch 2: |          | 1256/? [29:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1256, loss 4.0725226402282715\n",
      "Epoch 2: |          | 1257/? [29:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1257, loss 4.1401238441467285\n",
      "Epoch 2: |          | 1258/? [29:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1258, loss 4.831762790679932\n",
      "Epoch 2: |          | 1259/? [30:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1259, loss 4.571317195892334\n",
      "Epoch 2: |          | 1260/? [30:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1260, loss 4.977855682373047\n",
      "Epoch 2: |          | 1261/? [30:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1261, loss 4.369153022766113\n",
      "Epoch 2: |          | 1262/? [30:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1262, loss 4.219008445739746\n",
      "Epoch 2: |          | 1263/? [30:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1263, loss 4.7234344482421875\n",
      "Epoch 2: |          | 1264/? [30:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1264, loss 4.961451530456543\n",
      "Epoch 2: |          | 1265/? [30:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1265, loss 4.727953910827637\n",
      "Epoch 2: |          | 1266/? [30:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1266, loss 4.3522138595581055\n",
      "Epoch 2: |          | 1267/? [30:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1267, loss 4.477327823638916\n",
      "Epoch 2: |          | 1268/? [30:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1268, loss 4.3267998695373535\n",
      "Epoch 2: |          | 1269/? [30:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1269, loss 3.8730220794677734\n",
      "Epoch 2: |          | 1270/? [30:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1270, loss 4.248798847198486\n",
      "Epoch 2: |          | 1271/? [30:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1271, loss 4.440118312835693\n",
      "Epoch 2: |          | 1272/? [30:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1272, loss 3.9552454948425293\n",
      "Epoch 2: |          | 1273/? [30:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1273, loss 4.808807849884033\n",
      "Epoch 2: |          | 1274/? [30:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1274, loss 3.5951740741729736\n",
      "Epoch 2: |          | 1275/? [30:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1275, loss 4.106541633605957\n",
      "Epoch 2: |          | 1276/? [30:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1276, loss 4.522914886474609\n",
      "Epoch 2: |          | 1277/? [30:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1277, loss 4.105268478393555\n",
      "Epoch 2: |          | 1278/? [30:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1278, loss 4.201056480407715\n",
      "Epoch 2: |          | 1279/? [30:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1279, loss 4.627732276916504\n",
      "Epoch 2: |          | 1280/? [30:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1280, loss 3.7468814849853516\n",
      "Epoch 2: |          | 1281/? [30:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1281, loss 4.221409797668457\n",
      "Epoch 2: |          | 1282/? [30:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1282, loss 3.9402263164520264\n",
      "Epoch 2: |          | 1283/? [30:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1283, loss 4.768824100494385\n",
      "Epoch 2: |          | 1284/? [30:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1284, loss 3.722376585006714\n",
      "Epoch 2: |          | 1285/? [30:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1285, loss 4.956995964050293\n",
      "Epoch 2: |          | 1286/? [30:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1286, loss 3.3287386894226074\n",
      "Epoch 2: |          | 1287/? [30:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1287, loss 4.778861999511719\n",
      "Epoch 2: |          | 1288/? [30:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1288, loss 4.583506107330322\n",
      "Epoch 2: |          | 1289/? [30:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1289, loss 3.524874210357666\n",
      "Epoch 2: |          | 1290/? [30:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1290, loss 4.587744235992432\n",
      "Epoch 2: |          | 1291/? [30:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1291, loss 5.455544471740723\n",
      "Epoch 2: |          | 1292/? [30:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1292, loss 4.883859157562256\n",
      "Epoch 2: |          | 1293/? [30:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1293, loss 4.208652973175049\n",
      "Epoch 2: |          | 1294/? [30:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1294, loss 4.439541816711426\n",
      "Epoch 2: |          | 1295/? [30:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1295, loss 4.5088300704956055\n",
      "Epoch 2: |          | 1296/? [30:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1296, loss 3.6821625232696533\n",
      "Epoch 2: |          | 1297/? [30:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1297, loss 4.7567877769470215\n",
      "Epoch 2: |          | 1298/? [30:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1298, loss 4.473546504974365\n",
      "Epoch 2: |          | 1299/? [30:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1299, loss 3.4544453620910645\n",
      "Epoch 2: |          | 1300/? [30:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1300, loss 4.535859107971191\n",
      "Epoch 2: |          | 1301/? [31:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1301, loss 4.2181925773620605\n",
      "Epoch 2: |          | 1302/? [31:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1302, loss 4.376527786254883\n",
      "Epoch 2: |          | 1303/? [31:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1303, loss 4.168820858001709\n",
      "Epoch 2: |          | 1304/? [31:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1304, loss 5.027827739715576\n",
      "Epoch 2: |          | 1305/? [31:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1305, loss 3.7799391746520996\n",
      "Epoch 2: |          | 1306/? [31:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1306, loss 4.418498516082764\n",
      "Epoch 2: |          | 1307/? [31:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1307, loss 3.9465126991271973\n",
      "Epoch 2: |          | 1308/? [31:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1308, loss 3.900913953781128\n",
      "Epoch 2: |          | 1309/? [31:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1309, loss 4.144669532775879\n",
      "Epoch 2: |          | 1310/? [31:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1310, loss 4.569117069244385\n",
      "Epoch 2: |          | 1311/? [31:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1311, loss 3.9907615184783936\n",
      "Epoch 2: |          | 1312/? [31:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1312, loss 3.7253670692443848\n",
      "Epoch 2: |          | 1313/? [31:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1313, loss 4.948929309844971\n",
      "Epoch 2: |          | 1314/? [31:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1314, loss 4.092186450958252\n",
      "Epoch 2: |          | 1315/? [31:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1315, loss 4.928948402404785\n",
      "Epoch 2: |          | 1316/? [31:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1316, loss 4.649251937866211\n",
      "Epoch 2: |          | 1317/? [31:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1317, loss 4.263641357421875\n",
      "Epoch 2: |          | 1318/? [31:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1318, loss 4.483267784118652\n",
      "Epoch 2: |          | 1319/? [31:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1319, loss 4.661965370178223\n",
      "Epoch 2: |          | 1320/? [31:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1320, loss 4.248524188995361\n",
      "Epoch 2: |          | 1321/? [31:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1321, loss 4.653144836425781\n",
      "Epoch 2: |          | 1322/? [31:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1322, loss 4.721776485443115\n",
      "Epoch 2: |          | 1323/? [31:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1323, loss 4.043211936950684\n",
      "Epoch 2: |          | 1324/? [31:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1324, loss 5.04132080078125\n",
      "Epoch 2: |          | 1325/? [31:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1325, loss 5.148623943328857\n",
      "Epoch 2: |          | 1326/? [31:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1326, loss 4.5523881912231445\n",
      "Epoch 2: |          | 1327/? [31:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1327, loss 4.3638129234313965\n",
      "Epoch 2: |          | 1328/? [31:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1328, loss 4.064665794372559\n",
      "Epoch 2: |          | 1329/? [31:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1329, loss 4.815047740936279\n",
      "Epoch 2: |          | 1330/? [31:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1330, loss 4.448572635650635\n",
      "Epoch 2: |          | 1331/? [31:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1331, loss 4.532456398010254\n",
      "Epoch 2: |          | 1332/? [31:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1332, loss 4.387894630432129\n",
      "Epoch 2: |          | 1333/? [31:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1333, loss 4.224851131439209\n",
      "Epoch 2: |          | 1334/? [31:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1334, loss 4.392874717712402\n",
      "Epoch 2: |          | 1335/? [31:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1335, loss 4.331007480621338\n",
      "Epoch 2: |          | 1336/? [31:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1336, loss 3.934755325317383\n",
      "Epoch 2: |          | 1337/? [31:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1337, loss 4.6228790283203125\n",
      "Epoch 2: |          | 1338/? [31:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1338, loss 3.686788558959961\n",
      "Epoch 2: |          | 1339/? [31:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1339, loss 4.39155912399292\n",
      "Epoch 2: |          | 1340/? [31:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1340, loss 3.7554664611816406\n",
      "Epoch 2: |          | 1341/? [31:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1341, loss 4.652185916900635\n",
      "Epoch 2: |          | 1342/? [31:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1342, loss 4.839930057525635\n",
      "Epoch 2: |          | 1343/? [32:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1343, loss 4.295040130615234\n",
      "Epoch 2: |          | 1344/? [32:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1344, loss 4.409969329833984\n",
      "Epoch 2: |          | 1345/? [32:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1345, loss 4.523917198181152\n",
      "Epoch 2: |          | 1346/? [32:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1346, loss 5.7922444343566895\n",
      "Epoch 2: |          | 1347/? [32:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1347, loss 4.839935779571533\n",
      "Epoch 2: |          | 1348/? [32:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1348, loss 5.057406425476074\n",
      "Epoch 2: |          | 1349/? [32:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1349, loss 4.670510292053223\n",
      "Epoch 2: |          | 1350/? [32:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1350, loss 5.011580944061279\n",
      "Epoch 2: |          | 1351/? [32:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1351, loss 4.7338128089904785\n",
      "Epoch 2: |          | 1352/? [32:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1352, loss 3.807189464569092\n",
      "Epoch 2: |          | 1353/? [32:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1353, loss 4.149069309234619\n",
      "Epoch 2: |          | 1354/? [32:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1354, loss 4.742636680603027\n",
      "Epoch 2: |          | 1355/? [32:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1355, loss 4.894352912902832\n",
      "Epoch 2: |          | 1356/? [32:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1356, loss 4.527906894683838\n",
      "Epoch 2: |          | 1357/? [32:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1357, loss 4.256888389587402\n",
      "Epoch 2: |          | 1358/? [32:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1358, loss 4.540511131286621\n",
      "Epoch 2: |          | 1359/? [32:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1359, loss 4.338721752166748\n",
      "Epoch 2: |          | 1360/? [32:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1360, loss 4.555342674255371\n",
      "Epoch 2: |          | 1361/? [32:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1361, loss 4.485589027404785\n",
      "Epoch 2: |          | 1362/? [32:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1362, loss 4.327672481536865\n",
      "Epoch 2: |          | 1363/? [32:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1363, loss 3.6975226402282715\n",
      "Epoch 2: |          | 1364/? [32:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1364, loss 4.290196418762207\n",
      "Epoch 2: |          | 1365/? [32:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1365, loss 3.931248188018799\n",
      "Epoch 2: |          | 1366/? [32:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1366, loss 4.694785118103027\n",
      "Epoch 2: |          | 1367/? [32:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1367, loss 3.949911117553711\n",
      "Epoch 2: |          | 1368/? [32:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1368, loss 3.7436020374298096\n",
      "Epoch 2: |          | 1369/? [32:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1369, loss 4.457253932952881\n",
      "Epoch 2: |          | 1370/? [32:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1370, loss 3.9312119483947754\n",
      "Epoch 2: |          | 1371/? [32:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1371, loss 4.936748504638672\n",
      "Epoch 2: |          | 1372/? [32:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1372, loss 4.298650741577148\n",
      "Epoch 2: |          | 1373/? [32:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1373, loss 4.798282623291016\n",
      "Epoch 2: |          | 1374/? [32:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1374, loss 3.84307599067688\n",
      "Epoch 2: |          | 1375/? [32:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1375, loss 4.494434833526611\n",
      "Epoch 2: |          | 1376/? [32:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1376, loss 4.4464521408081055\n",
      "Epoch 2: |          | 1377/? [32:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1377, loss 4.335845947265625\n",
      "Epoch 2: |          | 1378/? [32:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1378, loss 4.67382287979126\n",
      "Epoch 2: |          | 1379/? [32:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1379, loss 4.486405372619629\n",
      "Epoch 2: |          | 1380/? [32:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1380, loss 4.555161952972412\n",
      "Epoch 2: |          | 1381/? [32:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1381, loss 4.690352439880371\n",
      "Epoch 2: |          | 1382/? [32:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1382, loss 4.184269428253174\n",
      "Epoch 2: |          | 1383/? [32:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1383, loss 4.4743523597717285\n",
      "Epoch 2: |          | 1384/? [32:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1384, loss 4.447108268737793\n",
      "Epoch 2: |          | 1385/? [33:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1385, loss 4.389113903045654\n",
      "Epoch 2: |          | 1386/? [33:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1386, loss 4.364792346954346\n",
      "Epoch 2: |          | 1387/? [33:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1387, loss 4.408267021179199\n",
      "Epoch 2: |          | 1388/? [33:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1388, loss 3.7431111335754395\n",
      "Epoch 2: |          | 1389/? [33:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1389, loss 4.681540012359619\n",
      "Epoch 2: |          | 1390/? [33:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1390, loss 4.958998203277588\n",
      "Epoch 2: |          | 1391/? [33:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1391, loss 4.577185153961182\n",
      "Epoch 2: |          | 1392/? [33:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1392, loss 4.027698040008545\n",
      "Epoch 2: |          | 1393/? [33:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1393, loss 4.276584148406982\n",
      "Epoch 2: |          | 1394/? [33:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1394, loss 3.993698835372925\n",
      "Epoch 2: |          | 1395/? [33:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1395, loss 4.550547122955322\n",
      "Epoch 2: |          | 1396/? [33:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1396, loss 4.504887580871582\n",
      "Epoch 2: |          | 1397/? [33:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1397, loss 3.6407570838928223\n",
      "Epoch 2: |          | 1398/? [33:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1398, loss 4.863877296447754\n",
      "Epoch 2: |          | 1399/? [33:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1399, loss 4.940671920776367\n",
      "Epoch 2: |          | 1400/? [33:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1400, loss 3.9658565521240234\n",
      "Epoch 2: |          | 1401/? [33:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1401, loss 4.889809608459473\n",
      "Epoch 2: |          | 1402/? [33:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1402, loss 4.380870342254639\n",
      "Epoch 2: |          | 1403/? [33:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1403, loss 4.595682621002197\n",
      "Epoch 2: |          | 1404/? [33:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1404, loss 4.561753273010254\n",
      "Epoch 2: |          | 1405/? [33:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1405, loss 4.936985969543457\n",
      "Epoch 2: |          | 1406/? [33:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1406, loss 4.801995277404785\n",
      "Epoch 2: |          | 1407/? [33:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1407, loss 5.021596908569336\n",
      "Epoch 2: |          | 1408/? [33:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1408, loss 4.164637565612793\n",
      "Epoch 2: |          | 1409/? [33:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1409, loss 4.259791374206543\n",
      "Epoch 2: |          | 1410/? [33:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1410, loss 4.288607120513916\n",
      "Epoch 2: |          | 1411/? [33:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1411, loss 4.7135467529296875\n",
      "Epoch 2: |          | 1412/? [33:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1412, loss 4.160863876342773\n",
      "Epoch 2: |          | 1413/? [33:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1413, loss 4.015559673309326\n",
      "Epoch 2: |          | 1414/? [33:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1414, loss 4.117609024047852\n",
      "Epoch 2: |          | 1415/? [33:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1415, loss 4.485153675079346\n",
      "Epoch 2: |          | 1416/? [33:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1416, loss 4.876980781555176\n",
      "Epoch 2: |          | 1417/? [33:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1417, loss 4.418371200561523\n",
      "Epoch 2: |          | 1418/? [33:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1418, loss 4.668734550476074\n",
      "Epoch 2: |          | 1419/? [33:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1419, loss 4.346447944641113\n",
      "Epoch 2: |          | 1420/? [33:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1420, loss 4.273232936859131\n",
      "Epoch 2: |          | 1421/? [33:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1421, loss 3.8647847175598145\n",
      "Epoch 2: |          | 1422/? [33:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1422, loss 4.7605814933776855\n",
      "Epoch 2: |          | 1423/? [33:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1423, loss 4.792185306549072\n",
      "Epoch 2: |          | 1424/? [33:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1424, loss 4.3346028327941895\n",
      "Epoch 2: |          | 1425/? [33:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1425, loss 4.631966590881348\n",
      "Epoch 2: |          | 1426/? [33:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1426, loss 4.079140663146973\n",
      "Epoch 2: |          | 1427/? [34:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1427, loss 4.731727600097656\n",
      "Epoch 2: |          | 1428/? [34:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1428, loss 4.727417469024658\n",
      "Epoch 2: |          | 1429/? [34:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1429, loss 4.65792179107666\n",
      "Epoch 2: |          | 1430/? [34:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1430, loss 4.733736991882324\n",
      "Epoch 2: |          | 1431/? [34:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1431, loss 4.472074031829834\n",
      "Epoch 2: |          | 1432/? [34:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1432, loss 4.508200168609619\n",
      "Epoch 2: |          | 1433/? [34:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1433, loss 4.366733074188232\n",
      "Epoch 2: |          | 1434/? [34:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1434, loss 4.527937889099121\n",
      "Epoch 2: |          | 1435/? [34:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1435, loss 4.090676784515381\n",
      "Epoch 2: |          | 1436/? [34:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1436, loss 4.467008113861084\n",
      "Epoch 2: |          | 1437/? [34:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1437, loss 3.707019329071045\n",
      "Epoch 2: |          | 1438/? [34:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1438, loss 5.486003875732422\n",
      "Epoch 2: |          | 1439/? [34:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1439, loss 4.655149936676025\n",
      "Epoch 2: |          | 1440/? [34:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1440, loss 4.630677700042725\n",
      "Epoch 2: |          | 1441/? [34:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1441, loss 4.96407413482666\n",
      "Epoch 2: |          | 1442/? [34:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1442, loss 5.004447937011719\n",
      "Epoch 2: |          | 1443/? [34:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1443, loss 4.056900978088379\n",
      "Epoch 2: |          | 1444/? [34:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1444, loss 4.172814846038818\n",
      "Epoch 2: |          | 1445/? [34:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1445, loss 4.7477827072143555\n",
      "Epoch 2: |          | 1446/? [34:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1446, loss 4.323126792907715\n",
      "Epoch 2: |          | 1447/? [34:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1447, loss 4.466454982757568\n",
      "Epoch 2: |          | 1448/? [34:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1448, loss 4.212433815002441\n",
      "Epoch 2: |          | 1449/? [34:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1449, loss 4.5081400871276855\n",
      "Epoch 2: |          | 1450/? [34:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1450, loss 4.634795188903809\n",
      "Epoch 2: |          | 1451/? [34:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1451, loss 4.956928730010986\n",
      "Epoch 2: |          | 1452/? [34:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1452, loss 4.507783889770508\n",
      "Epoch 2: |          | 1453/? [34:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1453, loss 3.773393154144287\n",
      "Epoch 2: |          | 1454/? [34:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1454, loss 4.403645992279053\n",
      "Epoch 2: |          | 1455/? [34:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1455, loss 4.551033973693848\n",
      "Epoch 2: |          | 1456/? [34:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1456, loss 4.096312046051025\n",
      "Epoch 2: |          | 1457/? [34:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1457, loss 4.249752044677734\n",
      "Epoch 2: |          | 1458/? [34:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1458, loss 4.376956462860107\n",
      "Epoch 2: |          | 1459/? [34:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1459, loss 4.668098449707031\n",
      "Epoch 2: |          | 1460/? [34:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1460, loss 4.485013961791992\n",
      "Epoch 2: |          | 1461/? [34:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1461, loss 4.543809413909912\n",
      "Epoch 2: |          | 1462/? [34:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1462, loss 4.850802421569824\n",
      "Epoch 2: |          | 1463/? [34:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1463, loss 4.753053188323975\n",
      "Epoch 2: |          | 1464/? [34:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1464, loss 4.06518030166626\n",
      "Epoch 2: |          | 1465/? [34:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1465, loss 4.422555446624756\n",
      "Epoch 2: |          | 1466/? [34:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1466, loss 4.052785873413086\n",
      "Epoch 2: |          | 1467/? [35:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1467, loss 4.807154655456543\n",
      "Epoch 2: |          | 1468/? [35:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1468, loss 4.358665943145752\n",
      "Epoch 2: |          | 1469/? [35:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1469, loss 3.9646575450897217\n",
      "Epoch 2: |          | 1470/? [35:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1470, loss 4.662728786468506\n",
      "Epoch 2: |          | 1471/? [35:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1471, loss 4.767733097076416\n",
      "Epoch 2: |          | 1472/? [35:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1472, loss 4.473916053771973\n",
      "Epoch 2: |          | 1473/? [35:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1473, loss 4.334079742431641\n",
      "Epoch 2: |          | 1474/? [35:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1474, loss 4.208775997161865\n",
      "Epoch 2: |          | 1475/? [35:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1475, loss 3.8525967597961426\n",
      "Epoch 2: |          | 1476/? [35:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1476, loss 4.463266372680664\n",
      "Epoch 2: |          | 1477/? [35:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1477, loss 4.486104965209961\n",
      "Epoch 2: |          | 1478/? [35:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1478, loss 4.333493232727051\n",
      "Epoch 2: |          | 1479/? [35:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1479, loss 4.928471088409424\n",
      "Epoch 2: |          | 1480/? [35:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1480, loss 4.6487884521484375\n",
      "Epoch 2: |          | 1481/? [35:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1481, loss 4.39472770690918\n",
      "Epoch 2: |          | 1482/? [35:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1482, loss 4.499848365783691\n",
      "Epoch 2: |          | 1483/? [35:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1483, loss 4.073190689086914\n",
      "Epoch 2: |          | 1484/? [35:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1484, loss 4.293622016906738\n",
      "Epoch 2: |          | 1485/? [35:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1485, loss 4.532773971557617\n",
      "Epoch 2: |          | 1486/? [35:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1486, loss 4.4321818351745605\n",
      "Epoch 2: |          | 1487/? [35:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1487, loss 3.909522294998169\n",
      "Epoch 2: |          | 1488/? [35:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1488, loss 4.624175071716309\n",
      "Epoch 2: |          | 1489/? [35:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1489, loss 4.550005912780762\n",
      "Epoch 2: |          | 1490/? [35:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1490, loss 4.511346340179443\n",
      "Epoch 2: |          | 1491/? [35:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1491, loss 3.270859956741333\n",
      "Epoch 2: |          | 1492/? [35:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1492, loss 3.944338321685791\n",
      "Epoch 2: |          | 1493/? [35:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1493, loss 3.7628536224365234\n",
      "Epoch 2: |          | 1494/? [35:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1494, loss 4.372864723205566\n",
      "Epoch 2: |          | 1495/? [35:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1495, loss 4.261148452758789\n",
      "Epoch 2: |          | 1496/? [35:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1496, loss 4.620587348937988\n",
      "Epoch 2: |          | 1497/? [35:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1497, loss 3.7803871631622314\n",
      "Epoch 2: |          | 1498/? [35:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1498, loss 4.120993614196777\n",
      "Epoch 2: |          | 1499/? [35:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1499, loss 4.772864818572998\n",
      "Epoch 2: |          | 1500/? [35:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1500, loss 4.645993709564209\n",
      "Epoch 2: |          | 1501/? [35:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1501, loss 4.435944080352783\n",
      "Epoch 2: |          | 1502/? [35:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1502, loss 4.56493616104126\n",
      "Epoch 2: |          | 1503/? [35:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1503, loss 4.2322282791137695\n",
      "Epoch 2: |          | 1504/? [35:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1504, loss 4.877543926239014\n",
      "Epoch 2: |          | 1505/? [35:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1505, loss 4.80295467376709\n",
      "Epoch 2: |          | 1506/? [35:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1506, loss 4.376696586608887\n",
      "Epoch 2: |          | 1507/? [35:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1507, loss 4.194369316101074\n",
      "Epoch 2: |          | 1508/? [35:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1508, loss 4.400673866271973\n",
      "Epoch 2: |          | 1509/? [36:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1509, loss 4.3415937423706055\n",
      "Epoch 2: |          | 1510/? [36:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1510, loss 4.638505935668945\n",
      "Epoch 2: |          | 1511/? [36:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1511, loss 4.146027565002441\n",
      "Epoch 2: |          | 1512/? [36:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1512, loss 4.956783771514893\n",
      "Epoch 2: |          | 1513/? [36:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1513, loss 4.982440948486328\n",
      "Epoch 2: |          | 1514/? [36:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1514, loss 4.057892322540283\n",
      "Epoch 2: |          | 1515/? [36:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1515, loss 5.070919036865234\n",
      "Epoch 2: |          | 1516/? [36:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1516, loss 4.804053783416748\n",
      "Epoch 2: |          | 1517/? [36:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1517, loss 4.270394802093506\n",
      "Epoch 2: |          | 1518/? [36:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1518, loss 4.087187767028809\n",
      "Epoch 2: |          | 1519/? [36:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1519, loss 4.641648292541504\n",
      "Epoch 2: |          | 1520/? [36:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1520, loss 4.931924343109131\n",
      "Epoch 2: |          | 1521/? [36:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1521, loss 4.388110637664795\n",
      "Epoch 2: |          | 1522/? [36:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1522, loss 4.10686731338501\n",
      "Epoch 2: |          | 1523/? [36:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1523, loss 4.505660057067871\n",
      "Epoch 2: |          | 1524/? [36:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1524, loss 4.423104286193848\n",
      "Epoch 2: |          | 1525/? [36:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1525, loss 4.198745250701904\n",
      "Epoch 2: |          | 1526/? [36:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1526, loss 4.680715084075928\n",
      "Epoch 2: |          | 1527/? [36:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1527, loss 4.6881232261657715\n",
      "Epoch 2: |          | 1528/? [36:27<00:00,  0.70it/s, v_num=31]ERROR: Input has inproper shape\n",
      "Epoch 2: |          | 1529/? [36:27<00:00,  0.70it/s, v_num=31]   VALIDATION: Batch 0, loss 4.94382905960083\n",
      "   VALIDATION: Batch 1, loss 3.8537838459014893\n",
      "   VALIDATION: Batch 2, loss 5.090794563293457\n",
      "   VALIDATION: Batch 3, loss 4.736238479614258\n",
      "   VALIDATION: Batch 4, loss 4.347783088684082\n",
      "   VALIDATION: Batch 5, loss 3.9630439281463623\n",
      "   VALIDATION: Batch 6, loss 4.247397422790527\n",
      "   VALIDATION: Batch 7, loss 4.932420253753662\n",
      "   VALIDATION: Batch 8, loss 4.809578895568848\n",
      "   VALIDATION: Batch 9, loss 4.8906755447387695\n",
      "   VALIDATION: Batch 10, loss 4.628127098083496\n",
      "   VALIDATION: Batch 11, loss 4.255427837371826\n",
      "   VALIDATION: Batch 12, loss 4.517427921295166\n",
      "   VALIDATION: Batch 13, loss 4.991307258605957\n",
      "   VALIDATION: Batch 14, loss 4.421842098236084\n",
      "   VALIDATION: Batch 15, loss 4.25430965423584\n",
      "   VALIDATION: Batch 16, loss 4.886956214904785\n",
      "   VALIDATION: Batch 17, loss 4.495444297790527\n",
      "   VALIDATION: Batch 18, loss 3.7818551063537598\n",
      "   VALIDATION: Batch 19, loss 4.774214744567871\n",
      "   VALIDATION: Batch 20, loss 4.983124732971191\n",
      "   VALIDATION: Batch 21, loss 5.1783576011657715\n",
      "   VALIDATION: Batch 22, loss 4.846419334411621\n",
      "   VALIDATION: Batch 23, loss 4.378125190734863\n",
      "   VALIDATION: Batch 24, loss 4.270159721374512\n",
      "   VALIDATION: Batch 25, loss 4.700087070465088\n",
      "   VALIDATION: Batch 26, loss 4.852713108062744\n",
      "   VALIDATION: Batch 27, loss 4.744290828704834\n",
      "   VALIDATION: Batch 28, loss 4.462815761566162\n",
      "   VALIDATION: Batch 29, loss 4.762498378753662\n",
      "   VALIDATION: Batch 30, loss 4.334770202636719\n",
      "   VALIDATION: Batch 31, loss 4.636074066162109\n",
      "   VALIDATION: Batch 32, loss 5.169948101043701\n",
      "   VALIDATION: Batch 33, loss 3.370640993118286\n",
      "   VALIDATION: Batch 34, loss 4.572390079498291\n",
      "   VALIDATION: Batch 35, loss 4.806545734405518\n",
      "   VALIDATION: Batch 36, loss 4.338942050933838\n",
      "   VALIDATION: Batch 37, loss 4.13015079498291\n",
      "   VALIDATION: Batch 38, loss 4.204878330230713\n",
      "   VALIDATION: Batch 39, loss 4.584735870361328\n",
      "   VALIDATION: Batch 40, loss 4.720015525817871\n",
      "   VALIDATION: Batch 41, loss 3.543466091156006\n",
      "   VALIDATION: Batch 42, loss 4.7738471031188965\n",
      "   VALIDATION: Batch 43, loss 4.853312015533447\n",
      "   VALIDATION: Batch 44, loss 4.376507759094238\n",
      "   VALIDATION: Batch 45, loss 4.870345592498779\n",
      "   VALIDATION: Batch 46, loss 4.0123066902160645\n",
      "   VALIDATION: Batch 47, loss 4.995168685913086\n",
      "   VALIDATION: Batch 48, loss 5.051773548126221\n",
      "   VALIDATION: Batch 49, loss 4.760532855987549\n",
      "   VALIDATION: Batch 50, loss 4.655071258544922\n",
      "   VALIDATION: Batch 51, loss 5.163098335266113\n",
      "   VALIDATION: Batch 52, loss 4.274213790893555\n",
      "   VALIDATION: Batch 53, loss 4.178369998931885\n",
      "   VALIDATION: Batch 54, loss 4.228687763214111\n",
      "   VALIDATION: Batch 55, loss 5.089020729064941\n",
      "   VALIDATION: Batch 56, loss 4.466835975646973\n",
      "   VALIDATION: Batch 57, loss 5.823460578918457\n",
      "   VALIDATION: Batch 58, loss 4.5424418449401855\n",
      "   VALIDATION: Batch 59, loss 4.189671516418457\n",
      "   VALIDATION: Batch 60, loss 3.7202486991882324\n",
      "   VALIDATION: Batch 61, loss 4.570128917694092\n",
      "   VALIDATION: Batch 62, loss 4.5818867683410645\n",
      "   VALIDATION: Batch 63, loss 5.023608207702637\n",
      "   VALIDATION: Batch 64, loss 4.856690406799316\n",
      "   VALIDATION: Batch 65, loss 3.990036725997925\n",
      "   VALIDATION: Batch 66, loss 4.943500995635986\n",
      "   VALIDATION: Batch 67, loss 4.352136135101318\n",
      "   VALIDATION: Batch 68, loss 4.52086067199707\n",
      "   VALIDATION: Batch 69, loss 4.810179233551025\n",
      "   VALIDATION: Batch 70, loss 5.001575946807861\n",
      "   VALIDATION: Batch 71, loss 4.4291768074035645\n",
      "   VALIDATION: Batch 72, loss 5.3476972579956055\n",
      "   VALIDATION: Batch 73, loss 4.139575004577637\n",
      "   VALIDATION: Batch 74, loss 4.7727861404418945\n",
      "   VALIDATION: Batch 75, loss 4.8264336585998535\n",
      "   VALIDATION: Batch 76, loss 4.684535026550293\n",
      "   VALIDATION: Batch 77, loss 4.868459701538086\n",
      "   VALIDATION: Batch 78, loss 4.716129302978516\n",
      "   VALIDATION: Batch 79, loss 4.6411590576171875\n",
      "   VALIDATION: Batch 80, loss 4.752954006195068\n",
      "   VALIDATION: Batch 81, loss 4.470462322235107\n",
      "   VALIDATION: Batch 82, loss 4.8240742683410645\n",
      "   VALIDATION: Batch 83, loss 4.151617527008057\n",
      "   VALIDATION: Batch 84, loss 4.829859733581543\n",
      "   VALIDATION: Batch 85, loss 4.597882270812988\n",
      "   VALIDATION: Batch 86, loss 4.537413597106934\n",
      "   VALIDATION: Batch 87, loss 4.380086898803711\n",
      "   VALIDATION: Batch 88, loss 3.986017942428589\n",
      "   VALIDATION: Batch 89, loss 4.300849437713623\n",
      "   VALIDATION: Batch 90, loss 4.537106513977051\n",
      "   VALIDATION: Batch 91, loss 4.829897880554199\n",
      "   VALIDATION: Batch 92, loss 4.646591663360596\n",
      "   VALIDATION: Batch 93, loss 5.027863502502441\n",
      "   VALIDATION: Batch 94, loss 4.551952362060547\n",
      "   VALIDATION: Batch 95, loss 3.9879658222198486\n",
      "   VALIDATION: Batch 96, loss 4.455887794494629\n",
      "   VALIDATION: Batch 97, loss 4.253413200378418\n",
      "   VALIDATION: Batch 98, loss 4.758730888366699\n",
      "   VALIDATION: Batch 99, loss 4.861903190612793\n",
      "   VALIDATION: Batch 100, loss 5.202903747558594\n",
      "   VALIDATION: Batch 101, loss 3.778338670730591\n",
      "   VALIDATION: Batch 102, loss 5.245029926300049\n",
      "   VALIDATION: Batch 103, loss 5.193455696105957\n",
      "   VALIDATION: Batch 104, loss 4.121489524841309\n",
      "   VALIDATION: Batch 105, loss 4.658200263977051\n",
      "   VALIDATION: Batch 106, loss 4.452156066894531\n",
      "   VALIDATION: Batch 107, loss 4.589362144470215\n",
      "   VALIDATION: Batch 108, loss 4.297747611999512\n",
      "   VALIDATION: Batch 109, loss 4.916090488433838\n",
      "   VALIDATION: Batch 110, loss 4.677407264709473\n",
      "   VALIDATION: Batch 111, loss 4.959794998168945\n",
      "   VALIDATION: Batch 112, loss 5.694338321685791\n",
      "   VALIDATION: Batch 113, loss 5.121170997619629\n",
      "   VALIDATION: Batch 114, loss 4.8502655029296875\n",
      "   VALIDATION: Batch 115, loss 4.317864418029785\n",
      "   VALIDATION: Batch 116, loss 4.141219615936279\n",
      "   VALIDATION: Batch 117, loss 4.824019432067871\n",
      "   VALIDATION: Batch 118, loss 5.023473739624023\n",
      "   VALIDATION: Batch 119, loss 4.160000801086426\n",
      "   VALIDATION: Batch 120, loss 3.7418415546417236\n",
      "   VALIDATION: Batch 121, loss 4.093522548675537\n",
      "   VALIDATION: Batch 122, loss 4.465249538421631\n",
      "   VALIDATION: Batch 123, loss 4.594693183898926\n",
      "   VALIDATION: Batch 124, loss 3.812678813934326\n",
      "   VALIDATION: Batch 125, loss 4.542584419250488\n",
      "   VALIDATION: Batch 126, loss 4.732480049133301\n",
      "   VALIDATION: Batch 127, loss 4.561002254486084\n",
      "   VALIDATION: Batch 128, loss 4.634488105773926\n",
      "   VALIDATION: Batch 129, loss 4.307644844055176\n",
      "   VALIDATION: Batch 130, loss 3.923121690750122\n",
      "   VALIDATION: Batch 131, loss 3.8876781463623047\n",
      "   VALIDATION: Batch 132, loss 4.559487342834473\n",
      "   VALIDATION: Batch 133, loss 4.752373695373535\n",
      "   VALIDATION: Batch 134, loss 4.686720371246338\n",
      "   VALIDATION: Batch 135, loss 4.884443759918213\n",
      "   VALIDATION: Batch 136, loss 4.997315406799316\n",
      "   VALIDATION: Batch 137, loss 4.865309238433838\n",
      "   VALIDATION: Batch 138, loss 4.571350574493408\n",
      "   VALIDATION: Batch 139, loss 4.965167045593262\n",
      "   VALIDATION: Batch 140, loss 4.003788948059082\n",
      "   VALIDATION: Batch 141, loss 4.947856903076172\n",
      "   VALIDATION: Batch 142, loss 3.6835086345672607\n",
      "   VALIDATION: Batch 143, loss 4.52217960357666\n",
      "   VALIDATION: Batch 144, loss 4.782320976257324\n",
      "   VALIDATION: Batch 145, loss 4.591403484344482\n",
      "   VALIDATION: Batch 146, loss 4.383662700653076\n",
      "   VALIDATION: Batch 147, loss 4.738717555999756\n",
      "   VALIDATION: Batch 148, loss 4.818240642547607\n",
      "   VALIDATION: Batch 149, loss 5.283423900604248\n",
      "   VALIDATION: Batch 150, loss 4.917792320251465\n",
      "   VALIDATION: Batch 151, loss 5.115862846374512\n",
      "   VALIDATION: Batch 152, loss 4.520350456237793\n",
      "   VALIDATION: Batch 153, loss 4.761970043182373\n",
      "   VALIDATION: Batch 154, loss 4.624144077301025\n",
      "   VALIDATION: Batch 155, loss 4.324154376983643\n",
      "   VALIDATION: Batch 156, loss 5.058338642120361\n",
      "   VALIDATION: Batch 157, loss 4.766077995300293\n",
      "   VALIDATION: Batch 158, loss 4.049548149108887\n",
      "   VALIDATION: Batch 159, loss 4.590703010559082\n",
      "   VALIDATION: Batch 160, loss 4.999996185302734\n",
      "   VALIDATION: Batch 161, loss 5.1544623374938965\n",
      "   VALIDATION: Batch 162, loss 4.629576683044434\n",
      "   VALIDATION: Batch 163, loss 4.043026924133301\n",
      "   VALIDATION: Batch 164, loss 4.532548904418945\n",
      "   VALIDATION: Batch 165, loss 4.98792839050293\n",
      "   VALIDATION: Batch 166, loss 4.473513603210449\n",
      "   VALIDATION: Batch 167, loss 4.897026538848877\n",
      "   VALIDATION: Batch 168, loss 3.690654754638672\n",
      "   VALIDATION: Batch 169, loss 4.380387306213379\n",
      "   VALIDATION: Batch 170, loss 4.633390426635742\n",
      "   VALIDATION: Batch 171, loss 4.7373270988464355\n",
      "   VALIDATION: Batch 172, loss 4.6391191482543945\n",
      "   VALIDATION: Batch 173, loss 4.58767557144165\n",
      "   VALIDATION: Batch 174, loss 4.944815635681152\n",
      "   VALIDATION: Batch 175, loss 4.645321846008301\n",
      "   VALIDATION: Batch 176, loss 4.421341896057129\n",
      "   VALIDATION: Batch 177, loss 4.588587760925293\n",
      "   VALIDATION: Batch 178, loss 5.428648948669434\n",
      "   VALIDATION: Batch 179, loss 4.75896692276001\n",
      "   VALIDATION: Batch 180, loss 4.284285068511963\n",
      "   VALIDATION: Batch 181, loss 4.471458435058594\n",
      "   VALIDATION: Batch 182, loss 4.67531156539917\n",
      "   VALIDATION: Batch 183, loss 3.7225940227508545\n",
      "   VALIDATION: Batch 184, loss 3.4983410835266113\n",
      "   VALIDATION: Batch 185, loss 4.265703201293945\n",
      "   VALIDATION: Batch 186, loss 4.2145562171936035\n",
      "   VALIDATION: Batch 187, loss 4.447552680969238\n",
      "   VALIDATION: Batch 188, loss 4.806105613708496\n",
      "   VALIDATION: Batch 189, loss 4.145071983337402\n",
      "   VALIDATION: Batch 190, loss 4.174765586853027\n",
      "   VALIDATION: Batch 191, loss 4.730868339538574\n",
      "   VALIDATION: Batch 192, loss 5.164947032928467\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=31]              TRRAINING: Batch 0, loss 4.622560024261475\n",
      "Epoch 3: |          | 1/? [00:01<00:00,  0.57it/s, v_num=31]   TRRAINING: Batch 1, loss 4.133930206298828\n",
      "Epoch 3: |          | 2/? [00:03<00:00,  0.63it/s, v_num=31]   TRRAINING: Batch 2, loss 4.263526439666748\n",
      "Epoch 3: |          | 3/? [00:04<00:00,  0.64it/s, v_num=31]   TRRAINING: Batch 3, loss 3.910973072052002\n",
      "Epoch 3: |          | 4/? [00:06<00:00,  0.66it/s, v_num=31]   TRRAINING: Batch 4, loss 4.329554557800293\n",
      "Epoch 3: |          | 5/? [00:07<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 5, loss 5.176027774810791\n",
      "Epoch 3: |          | 6/? [00:08<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 6, loss 4.855977535247803\n",
      "Epoch 3: |          | 7/? [00:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 7, loss 4.054124355316162\n",
      "Epoch 3: |          | 8/? [00:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 8, loss 4.185647964477539\n",
      "Epoch 3: |          | 9/? [00:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 9, loss 4.462954998016357\n",
      "Epoch 3: |          | 10/? [00:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 10, loss 4.692273139953613\n",
      "Epoch 3: |          | 11/? [00:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 11, loss 4.630979061126709\n",
      "Epoch 3: |          | 12/? [00:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 12, loss 5.678446292877197\n",
      "Epoch 3: |          | 13/? [00:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 13, loss 4.480271339416504\n",
      "Epoch 3: |          | 14/? [00:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 14, loss 4.716916561126709\n",
      "Epoch 3: |          | 15/? [00:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 15, loss 3.974126100540161\n",
      "Epoch 3: |          | 16/? [00:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 16, loss 3.697411060333252\n",
      "Epoch 3: |          | 17/? [00:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 17, loss 5.015995979309082\n",
      "Epoch 3: |          | 18/? [00:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 18, loss 4.42974328994751\n",
      "Epoch 3: |          | 19/? [00:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 19, loss 4.241652965545654\n",
      "Epoch 3: |          | 20/? [00:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 20, loss 4.568003177642822\n",
      "Epoch 3: |          | 21/? [00:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 21, loss 4.68886661529541\n",
      "Epoch 3: |          | 22/? [00:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 22, loss 4.531052112579346\n",
      "Epoch 3: |          | 23/? [00:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 23, loss 3.8745875358581543\n",
      "Epoch 3: |          | 24/? [00:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 24, loss 4.540858268737793\n",
      "Epoch 3: |          | 25/? [00:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 25, loss 4.5845770835876465\n",
      "Epoch 3: |          | 26/? [00:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 26, loss 4.272514343261719\n",
      "Epoch 3: |          | 27/? [00:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 27, loss 4.058896541595459\n",
      "Epoch 3: |          | 28/? [00:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 28, loss 5.000933647155762\n",
      "Epoch 3: |          | 29/? [00:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 29, loss 4.501631736755371\n",
      "Epoch 3: |          | 30/? [00:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 30, loss 4.383285999298096\n",
      "Epoch 3: |          | 31/? [00:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 31, loss 5.043295383453369\n",
      "Epoch 3: |          | 32/? [00:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 32, loss 4.565212249755859\n",
      "Epoch 3: |          | 33/? [00:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 33, loss 4.362112045288086\n",
      "Epoch 3: |          | 34/? [00:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 34, loss 4.27071475982666\n",
      "Epoch 3: |          | 35/? [00:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 35, loss 3.6493732929229736\n",
      "Epoch 3: |          | 36/? [00:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 36, loss 4.6403913497924805\n",
      "Epoch 3: |          | 37/? [00:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 37, loss 4.753091335296631\n",
      "Epoch 3: |          | 38/? [00:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 38, loss 5.041992664337158\n",
      "Epoch 3: |          | 39/? [00:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 39, loss 4.931103706359863\n",
      "Epoch 3: |          | 40/? [00:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 40, loss 4.338385581970215\n",
      "Epoch 3: |          | 41/? [00:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 41, loss 4.335635185241699\n",
      "Epoch 3: |          | 42/? [01:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 42, loss 4.160513877868652\n",
      "Epoch 3: |          | 43/? [01:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 43, loss 4.2963762283325195\n",
      "Epoch 3: |          | 44/? [01:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 44, loss 3.6016616821289062\n",
      "Epoch 3: |          | 45/? [01:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 45, loss 3.5277962684631348\n",
      "Epoch 3: |          | 46/? [01:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 46, loss 5.0325026512146\n",
      "Epoch 3: |          | 47/? [01:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 47, loss 4.190633296966553\n",
      "Epoch 3: |          | 48/? [01:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 48, loss 3.9278712272644043\n",
      "Epoch 3: |          | 49/? [01:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 49, loss 4.434319496154785\n",
      "Epoch 3: |          | 50/? [01:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 50, loss 4.301621437072754\n",
      "Epoch 3: |          | 51/? [01:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 51, loss 4.345219612121582\n",
      "Epoch 3: |          | 52/? [01:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 52, loss 4.916304111480713\n",
      "Epoch 3: |          | 53/? [01:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 53, loss 4.548150539398193\n",
      "Epoch 3: |          | 54/? [01:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 54, loss 4.355896949768066\n",
      "Epoch 3: |          | 55/? [01:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 55, loss 4.461639881134033\n",
      "Epoch 3: |          | 56/? [01:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 56, loss 4.550763130187988\n",
      "Epoch 3: |          | 57/? [01:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 57, loss 4.49232292175293\n",
      "Epoch 3: |          | 58/? [01:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 58, loss 5.773391246795654\n",
      "Epoch 3: |          | 59/? [01:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 59, loss 4.634624004364014\n",
      "Epoch 3: |          | 60/? [01:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 60, loss 4.736474990844727\n",
      "Epoch 3: |          | 61/? [01:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 61, loss 4.769364356994629\n",
      "Epoch 3: |          | 62/? [01:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 62, loss 4.332507610321045\n",
      "Epoch 3: |          | 63/? [01:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 63, loss 4.566330432891846\n",
      "Epoch 3: |          | 64/? [01:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 64, loss 4.360824108123779\n",
      "Epoch 3: |          | 65/? [01:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 65, loss 4.413125038146973\n",
      "Epoch 3: |          | 66/? [01:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 66, loss 3.8324100971221924\n",
      "Epoch 3: |          | 67/? [01:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 67, loss 4.508869171142578\n",
      "Epoch 3: |          | 68/? [01:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 68, loss 4.669372081756592\n",
      "Epoch 3: |          | 69/? [01:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 69, loss 4.426736831665039\n",
      "Epoch 3: |          | 70/? [01:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 70, loss 4.066234588623047\n",
      "Epoch 3: |          | 71/? [01:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 71, loss 4.042644023895264\n",
      "Epoch 3: |          | 72/? [01:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 72, loss 4.497690200805664\n",
      "Epoch 3: |          | 73/? [01:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 73, loss 4.652083873748779\n",
      "Epoch 3: |          | 74/? [01:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 74, loss 4.181418418884277\n",
      "Epoch 3: |          | 75/? [01:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 75, loss 4.358912467956543\n",
      "Epoch 3: |          | 76/? [01:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 76, loss 4.35495662689209\n",
      "Epoch 3: |          | 77/? [01:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 77, loss 4.399636745452881\n",
      "Epoch 3: |          | 78/? [01:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 78, loss 4.119252681732178\n",
      "Epoch 3: |          | 79/? [01:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 79, loss 4.3789801597595215\n",
      "Epoch 3: |          | 80/? [01:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 80, loss 4.26854944229126\n",
      "Epoch 3: |          | 81/? [01:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 81, loss 3.7893242835998535\n",
      "Epoch 3: |          | 82/? [01:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 82, loss 4.681190013885498\n",
      "Epoch 3: |          | 83/? [01:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 83, loss 3.9628162384033203\n",
      "Epoch 3: |          | 84/? [02:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 84, loss 3.8203883171081543\n",
      "Epoch 3: |          | 85/? [02:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 85, loss 3.7775375843048096\n",
      "Epoch 3: |          | 86/? [02:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 86, loss 3.8490257263183594\n",
      "Epoch 3: |          | 87/? [02:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 87, loss 4.021369934082031\n",
      "Epoch 3: |          | 88/? [02:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 88, loss 4.888101100921631\n",
      "Epoch 3: |          | 89/? [02:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 89, loss 4.693240642547607\n",
      "Epoch 3: |          | 90/? [02:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 90, loss 4.546296119689941\n",
      "Epoch 3: |          | 91/? [02:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 91, loss 4.287425518035889\n",
      "Epoch 3: |          | 92/? [02:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 92, loss 4.699490070343018\n",
      "Epoch 3: |          | 93/? [02:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 93, loss 4.795296669006348\n",
      "Epoch 3: |          | 94/? [02:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 94, loss 4.681478023529053\n",
      "Epoch 3: |          | 95/? [02:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 95, loss 4.990462779998779\n",
      "Epoch 3: |          | 96/? [02:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 96, loss 4.161685943603516\n",
      "Epoch 3: |          | 97/? [02:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 97, loss 4.068268775939941\n",
      "Epoch 3: |          | 98/? [02:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 98, loss 4.5127973556518555\n",
      "Epoch 3: |          | 99/? [02:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 99, loss 4.71926212310791\n",
      "Epoch 3: |          | 100/? [02:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 100, loss 4.779688358306885\n",
      "Epoch 3: |          | 101/? [02:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 101, loss 4.343920707702637\n",
      "Epoch 3: |          | 102/? [02:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 102, loss 4.338310718536377\n",
      "Epoch 3: |          | 103/? [02:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 103, loss 4.076632499694824\n",
      "Epoch 3: |          | 104/? [02:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 104, loss 4.523444175720215\n",
      "Epoch 3: |          | 105/? [02:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 105, loss 4.367692947387695\n",
      "Epoch 3: |          | 106/? [02:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 106, loss 4.4675188064575195\n",
      "Epoch 3: |          | 107/? [02:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 107, loss 4.569973945617676\n",
      "Epoch 3: |          | 108/? [02:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 108, loss 4.507523059844971\n",
      "Epoch 3: |          | 109/? [02:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 109, loss 4.11096715927124\n",
      "Epoch 3: |          | 110/? [02:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 110, loss 4.470772743225098\n",
      "Epoch 3: |          | 111/? [02:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 111, loss 5.104014873504639\n",
      "Epoch 3: |          | 112/? [02:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 112, loss 3.816920042037964\n",
      "Epoch 3: |          | 113/? [02:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 113, loss 3.343186140060425\n",
      "Epoch 3: |          | 114/? [02:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 114, loss 4.669821739196777\n",
      "Epoch 3: |          | 115/? [02:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 115, loss 4.813919544219971\n",
      "Epoch 3: |          | 116/? [02:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 116, loss 4.043450832366943\n",
      "Epoch 3: |          | 117/? [02:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 117, loss 4.023809909820557\n",
      "Epoch 3: |          | 118/? [02:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 118, loss 4.708496570587158\n",
      "Epoch 3: |          | 119/? [02:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 119, loss 4.931623935699463\n",
      "Epoch 3: |          | 120/? [02:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 120, loss 4.663818836212158\n",
      "Epoch 3: |          | 121/? [02:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 121, loss 4.414775371551514\n",
      "Epoch 3: |          | 122/? [02:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 122, loss 3.847175121307373\n",
      "Epoch 3: |          | 123/? [02:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 123, loss 4.321144104003906\n",
      "Epoch 3: |          | 124/? [02:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 124, loss 4.565535545349121\n",
      "Epoch 3: |          | 125/? [02:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 125, loss 4.283682346343994\n",
      "Epoch 3: |          | 126/? [02:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 126, loss 4.679994583129883\n",
      "Epoch 3: |          | 127/? [03:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 127, loss 4.732279300689697\n",
      "Epoch 3: |          | 128/? [03:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 128, loss 3.784790515899658\n",
      "Epoch 3: |          | 129/? [03:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 129, loss 4.511343955993652\n",
      "Epoch 3: |          | 130/? [03:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 130, loss 3.482098340988159\n",
      "Epoch 3: |          | 131/? [03:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 131, loss 4.386076927185059\n",
      "Epoch 3: |          | 132/? [03:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 132, loss 4.389931678771973\n",
      "Epoch 3: |          | 133/? [03:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 133, loss 4.381627082824707\n",
      "Epoch 3: |          | 134/? [03:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 134, loss 4.4913411140441895\n",
      "Epoch 3: |          | 135/? [03:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 135, loss 4.657610893249512\n",
      "Epoch 3: |          | 136/? [03:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 136, loss 4.613670349121094\n",
      "Epoch 3: |          | 137/? [03:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 137, loss 3.5414443016052246\n",
      "Epoch 3: |          | 138/? [03:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 138, loss 4.246380805969238\n",
      "Epoch 3: |          | 139/? [03:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 139, loss 4.785175800323486\n",
      "Epoch 3: |          | 140/? [03:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 140, loss 3.8242275714874268\n",
      "Epoch 3: |          | 141/? [03:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 141, loss 3.9963557720184326\n",
      "Epoch 3: |          | 142/? [03:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 142, loss 5.599859237670898\n",
      "Epoch 3: |          | 143/? [03:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 143, loss 5.253874778747559\n",
      "Epoch 3: |          | 144/? [03:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 144, loss 4.360447406768799\n",
      "Epoch 3: |          | 145/? [03:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 145, loss 3.8778469562530518\n",
      "Epoch 3: |          | 146/? [03:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 146, loss 4.091896057128906\n",
      "Epoch 3: |          | 147/? [03:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 147, loss 4.41867733001709\n",
      "Epoch 3: |          | 148/? [03:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 148, loss 4.125392913818359\n",
      "Epoch 3: |          | 149/? [03:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 149, loss 3.619464159011841\n",
      "Epoch 3: |          | 150/? [03:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 150, loss 4.528957843780518\n",
      "Epoch 3: |          | 151/? [03:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 151, loss 4.597966194152832\n",
      "Epoch 3: |          | 152/? [03:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 152, loss 4.622351169586182\n",
      "Epoch 3: |          | 153/? [03:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 153, loss 3.6795096397399902\n",
      "Epoch 3: |          | 154/? [03:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 154, loss 4.933913707733154\n",
      "Epoch 3: |          | 155/? [03:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 155, loss 4.4513630867004395\n",
      "Epoch 3: |          | 156/? [03:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 156, loss 3.771589756011963\n",
      "Epoch 3: |          | 157/? [03:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 157, loss 4.507932186126709\n",
      "Epoch 3: |          | 158/? [03:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 158, loss 4.570258140563965\n",
      "Epoch 3: |          | 159/? [03:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 159, loss 4.301538467407227\n",
      "Epoch 3: |          | 160/? [03:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 160, loss 4.0220465660095215\n",
      "Epoch 3: |          | 161/? [03:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 161, loss 4.489378929138184\n",
      "Epoch 3: |          | 162/? [03:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 162, loss 4.541004657745361\n",
      "Epoch 3: |          | 163/? [03:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 163, loss 3.533939838409424\n",
      "Epoch 3: |          | 164/? [03:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 164, loss 4.017936706542969\n",
      "Epoch 3: |          | 165/? [03:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 165, loss 4.879040241241455\n",
      "Epoch 3: |          | 166/? [03:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 166, loss 4.58872652053833\n",
      "Epoch 3: |          | 167/? [03:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 167, loss 4.672390937805176\n",
      "Epoch 3: |          | 168/? [03:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 168, loss 4.2342424392700195\n",
      "Epoch 3: |          | 169/? [04:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 169, loss 3.881948471069336\n",
      "Epoch 3: |          | 170/? [04:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 170, loss 4.132018089294434\n",
      "Epoch 3: |          | 171/? [04:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 171, loss 4.518621921539307\n",
      "Epoch 3: |          | 172/? [04:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 172, loss 4.240398406982422\n",
      "Epoch 3: |          | 173/? [04:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 173, loss 5.001161098480225\n",
      "Epoch 3: |          | 174/? [04:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 174, loss 4.823624134063721\n",
      "Epoch 3: |          | 175/? [04:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 175, loss 5.069593906402588\n",
      "Epoch 3: |          | 176/? [04:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 176, loss 4.181949138641357\n",
      "Epoch 3: |          | 177/? [04:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 177, loss 4.232623100280762\n",
      "Epoch 3: |          | 178/? [04:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 178, loss 4.048710346221924\n",
      "Epoch 3: |          | 179/? [04:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 179, loss 4.7812933921813965\n",
      "Epoch 3: |          | 180/? [04:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 180, loss 4.270452976226807\n",
      "Epoch 3: |          | 181/? [04:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 181, loss 4.066670894622803\n",
      "Epoch 3: |          | 182/? [04:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 182, loss 4.518660545349121\n",
      "Epoch 3: |          | 183/? [04:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 183, loss 3.9570000171661377\n",
      "Epoch 3: |          | 184/? [04:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 184, loss 4.150168418884277\n",
      "Epoch 3: |          | 185/? [04:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 185, loss 4.812433242797852\n",
      "Epoch 3: |          | 186/? [04:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 186, loss 4.196923732757568\n",
      "Epoch 3: |          | 187/? [04:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 187, loss 4.747376441955566\n",
      "Epoch 3: |          | 188/? [04:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 188, loss 4.161141395568848\n",
      "Epoch 3: |          | 189/? [04:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 189, loss 4.813784599304199\n",
      "Epoch 3: |          | 190/? [04:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 190, loss 4.283832550048828\n",
      "Epoch 3: |          | 191/? [04:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 191, loss 5.099910736083984\n",
      "Epoch 3: |          | 192/? [04:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 192, loss 4.927811622619629\n",
      "Epoch 3: |          | 193/? [04:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 193, loss 4.12063455581665\n",
      "Epoch 3: |          | 194/? [04:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 194, loss 4.047297954559326\n",
      "Epoch 3: |          | 195/? [04:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 195, loss 4.7377824783325195\n",
      "Epoch 3: |          | 196/? [04:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 196, loss 4.661811351776123\n",
      "Epoch 3: |          | 197/? [04:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 197, loss 4.500148296356201\n",
      "Epoch 3: |          | 198/? [04:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 198, loss 3.8234457969665527\n",
      "Epoch 3: |          | 199/? [04:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 199, loss 4.675473213195801\n",
      "Epoch 3: |          | 200/? [04:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 200, loss 4.229353904724121\n",
      "Epoch 3: |          | 201/? [04:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 201, loss 4.550339698791504\n",
      "Epoch 3: |          | 202/? [04:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 202, loss 4.58723783493042\n",
      "Epoch 3: |          | 203/? [04:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 203, loss 4.361520290374756\n",
      "Epoch 3: |          | 204/? [04:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 204, loss 4.4127702713012695\n",
      "Epoch 3: |          | 205/? [04:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 205, loss 4.1609086990356445\n",
      "Epoch 3: |          | 206/? [04:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 206, loss 4.000132083892822\n",
      "Epoch 3: |          | 207/? [04:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 207, loss 4.53480339050293\n",
      "Epoch 3: |          | 208/? [04:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 208, loss 4.471556186676025\n",
      "Epoch 3: |          | 209/? [04:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 209, loss 4.261658668518066\n",
      "Epoch 3: |          | 210/? [04:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 210, loss 4.954668998718262\n",
      "Epoch 3: |          | 211/? [05:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 211, loss 4.230832099914551\n",
      "Epoch 3: |          | 212/? [05:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 212, loss 4.488642692565918\n",
      "Epoch 3: |          | 213/? [05:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 213, loss 4.340579986572266\n",
      "Epoch 3: |          | 214/? [05:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 214, loss 4.231579303741455\n",
      "Epoch 3: |          | 215/? [05:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 215, loss 3.8875091075897217\n",
      "Epoch 3: |          | 216/? [05:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 216, loss 4.618851184844971\n",
      "Epoch 3: |          | 217/? [05:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 217, loss 4.455498695373535\n",
      "Epoch 3: |          | 218/? [05:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 218, loss 4.50814151763916\n",
      "Epoch 3: |          | 219/? [05:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 219, loss 4.38670015335083\n",
      "Epoch 3: |          | 220/? [05:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 220, loss 4.497447967529297\n",
      "Epoch 3: |          | 221/? [05:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 221, loss 4.296276092529297\n",
      "Epoch 3: |          | 222/? [05:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 222, loss 3.5110135078430176\n",
      "Epoch 3: |          | 223/? [05:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 223, loss 4.817043781280518\n",
      "Epoch 3: |          | 224/? [05:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 224, loss 4.683689594268799\n",
      "Epoch 3: |          | 225/? [05:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 225, loss 4.443051815032959\n",
      "Epoch 3: |          | 226/? [05:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 226, loss 4.253746032714844\n",
      "Epoch 3: |          | 227/? [05:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 227, loss 4.691281318664551\n",
      "Epoch 3: |          | 228/? [05:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 228, loss 4.3241400718688965\n",
      "Epoch 3: |          | 229/? [05:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 229, loss 4.486733913421631\n",
      "Epoch 3: |          | 230/? [05:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 230, loss 4.350642681121826\n",
      "Epoch 3: |          | 231/? [05:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 231, loss 4.292762279510498\n",
      "Epoch 3: |          | 232/? [05:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 232, loss 4.079754829406738\n",
      "Epoch 3: |          | 233/? [05:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 233, loss 4.793036937713623\n",
      "Epoch 3: |          | 234/? [05:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 234, loss 4.821351051330566\n",
      "Epoch 3: |          | 235/? [05:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 235, loss 4.83806848526001\n",
      "Epoch 3: |          | 236/? [05:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 236, loss 4.292189598083496\n",
      "Epoch 3: |          | 237/? [05:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 237, loss 4.464122772216797\n",
      "Epoch 3: |          | 238/? [05:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 238, loss 4.6845197677612305\n",
      "Epoch 3: |          | 239/? [05:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 239, loss 4.28005313873291\n",
      "Epoch 3: |          | 240/? [05:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 240, loss 3.711798906326294\n",
      "Epoch 3: |          | 241/? [05:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 241, loss 4.484776973724365\n",
      "Epoch 3: |          | 242/? [05:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 242, loss 4.794778347015381\n",
      "Epoch 3: |          | 243/? [05:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 243, loss 3.5326340198516846\n",
      "Epoch 3: |          | 244/? [05:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 244, loss 4.059647560119629\n",
      "Epoch 3: |          | 245/? [05:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 245, loss 4.371215343475342\n",
      "Epoch 3: |          | 246/? [05:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 246, loss 4.550776481628418\n",
      "Epoch 3: |          | 247/? [05:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 247, loss 4.557178974151611\n",
      "Epoch 3: |          | 248/? [05:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 248, loss 4.025623321533203\n",
      "Epoch 3: |          | 249/? [05:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 249, loss 3.823178768157959\n",
      "Epoch 3: |          | 250/? [05:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 250, loss 4.459273338317871\n",
      "Epoch 3: |          | 251/? [05:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 251, loss 4.500414848327637\n",
      "Epoch 3: |          | 252/? [05:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 252, loss 4.252676963806152\n",
      "Epoch 3: |          | 253/? [06:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 253, loss 5.060315132141113\n",
      "Epoch 3: |          | 254/? [06:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 254, loss 4.818238258361816\n",
      "Epoch 3: |          | 255/? [06:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 255, loss 4.341432094573975\n",
      "Epoch 3: |          | 256/? [06:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 256, loss 5.909388542175293\n",
      "Epoch 3: |          | 257/? [06:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 257, loss 4.216996192932129\n",
      "Epoch 3: |          | 258/? [06:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 258, loss 4.288986682891846\n",
      "Epoch 3: |          | 259/? [06:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 259, loss 4.159383296966553\n",
      "Epoch 3: |          | 260/? [06:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 260, loss 4.110042095184326\n",
      "Epoch 3: |          | 261/? [06:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 261, loss 4.08873987197876\n",
      "Epoch 3: |          | 262/? [06:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 262, loss 4.64699649810791\n",
      "Epoch 3: |          | 263/? [06:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 263, loss 4.326272964477539\n",
      "Epoch 3: |          | 264/? [06:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 264, loss 4.450181007385254\n",
      "Epoch 3: |          | 265/? [06:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 265, loss 3.8362879753112793\n",
      "Epoch 3: |          | 266/? [06:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 266, loss 4.338050365447998\n",
      "Epoch 3: |          | 267/? [06:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 267, loss 3.908583402633667\n",
      "Epoch 3: |          | 268/? [06:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 268, loss 4.275879859924316\n",
      "Epoch 3: |          | 269/? [06:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 269, loss 4.704375267028809\n",
      "Epoch 3: |          | 270/? [06:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 270, loss 4.287191390991211\n",
      "Epoch 3: |          | 271/? [06:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 271, loss 4.742011070251465\n",
      "Epoch 3: |          | 272/? [06:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 272, loss 4.840130805969238\n",
      "Epoch 3: |          | 273/? [06:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 273, loss 3.9857094287872314\n",
      "Epoch 3: |          | 274/? [06:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 274, loss 5.005825996398926\n",
      "Epoch 3: |          | 275/? [06:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 275, loss 4.485166549682617\n",
      "Epoch 3: |          | 276/? [06:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 276, loss 3.7801544666290283\n",
      "Epoch 3: |          | 277/? [06:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 277, loss 4.4430131912231445\n",
      "Epoch 3: |          | 278/? [06:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 278, loss 3.4165611267089844\n",
      "Epoch 3: |          | 279/? [06:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 279, loss 4.187906265258789\n",
      "Epoch 3: |          | 280/? [06:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 280, loss 3.8361706733703613\n",
      "Epoch 3: |          | 281/? [06:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 281, loss 4.6232829093933105\n",
      "Epoch 3: |          | 282/? [06:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 282, loss 4.18458366394043\n",
      "Epoch 3: |          | 283/? [06:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 283, loss 4.2645978927612305\n",
      "Epoch 3: |          | 284/? [06:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 284, loss 4.175972938537598\n",
      "Epoch 3: |          | 285/? [06:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 285, loss 3.581172227859497\n",
      "Epoch 3: |          | 286/? [06:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 286, loss 4.212161064147949\n",
      "Epoch 3: |          | 287/? [06:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 287, loss 3.9966652393341064\n",
      "Epoch 3: |          | 288/? [06:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 288, loss 3.980503797531128\n",
      "Epoch 3: |          | 289/? [06:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 289, loss 3.8713066577911377\n",
      "Epoch 3: |          | 290/? [06:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 290, loss 3.344355821609497\n",
      "Epoch 3: |          | 291/? [06:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 291, loss 4.445794582366943\n",
      "Epoch 3: |          | 292/? [06:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 292, loss 4.080195426940918\n",
      "Epoch 3: |          | 293/? [06:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 293, loss 4.398078441619873\n",
      "Epoch 3: |          | 294/? [06:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 294, loss 4.243871212005615\n",
      "Epoch 3: |          | 295/? [07:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 295, loss 4.612268447875977\n",
      "Epoch 3: |          | 296/? [07:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 296, loss 4.014148712158203\n",
      "Epoch 3: |          | 297/? [07:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 297, loss 4.743561267852783\n",
      "Epoch 3: |          | 298/? [07:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 298, loss 4.563833713531494\n",
      "Epoch 3: |          | 299/? [07:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 299, loss 5.172852039337158\n",
      "Epoch 3: |          | 300/? [07:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 300, loss 4.412309169769287\n",
      "Epoch 3: |          | 301/? [07:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 301, loss 4.155704021453857\n",
      "Epoch 3: |          | 302/? [07:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 302, loss 4.625093936920166\n",
      "Epoch 3: |          | 303/? [07:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 303, loss 4.363861083984375\n",
      "Epoch 3: |          | 304/? [07:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 304, loss 4.621828556060791\n",
      "Epoch 3: |          | 305/? [07:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 305, loss 4.661130905151367\n",
      "Epoch 3: |          | 306/? [07:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 306, loss 4.281861782073975\n",
      "Epoch 3: |          | 307/? [07:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 307, loss 4.579627990722656\n",
      "Epoch 3: |          | 308/? [07:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 308, loss 4.681550025939941\n",
      "Epoch 3: |          | 309/? [07:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 309, loss 4.368304252624512\n",
      "Epoch 3: |          | 310/? [07:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 310, loss 4.825474739074707\n",
      "Epoch 3: |          | 311/? [07:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 311, loss 4.347304344177246\n",
      "Epoch 3: |          | 312/? [07:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 312, loss 4.400514602661133\n",
      "Epoch 3: |          | 313/? [07:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 313, loss 4.074064254760742\n",
      "Epoch 3: |          | 314/? [07:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 314, loss 4.467280387878418\n",
      "Epoch 3: |          | 315/? [07:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 315, loss 4.052847385406494\n",
      "Epoch 3: |          | 316/? [07:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 316, loss 4.753546237945557\n",
      "Epoch 3: |          | 317/? [07:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 317, loss 4.506546974182129\n",
      "Epoch 3: |          | 318/? [07:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 318, loss 4.652485370635986\n",
      "Epoch 3: |          | 319/? [07:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 319, loss 3.8474507331848145\n",
      "Epoch 3: |          | 320/? [07:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 320, loss 4.308916091918945\n",
      "Epoch 3: |          | 321/? [07:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 321, loss 4.093438625335693\n",
      "Epoch 3: |          | 322/? [07:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 322, loss 4.786373138427734\n",
      "Epoch 3: |          | 323/? [07:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 323, loss 4.825539588928223\n",
      "Epoch 3: |          | 324/? [07:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 324, loss 4.4047746658325195\n",
      "Epoch 3: |          | 325/? [07:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 325, loss 4.919472694396973\n",
      "Epoch 3: |          | 326/? [07:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 326, loss 4.441617012023926\n",
      "Epoch 3: |          | 327/? [07:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 327, loss 4.2079973220825195\n",
      "Epoch 3: |          | 328/? [07:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 328, loss 3.906763792037964\n",
      "Epoch 3: |          | 329/? [07:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 329, loss 4.581669807434082\n",
      "Epoch 3: |          | 330/? [07:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 330, loss 5.0408406257629395\n",
      "Epoch 3: |          | 331/? [07:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 331, loss 3.21759033203125\n",
      "Epoch 3: |          | 332/? [07:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 332, loss 4.37637996673584\n",
      "Epoch 3: |          | 333/? [07:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 333, loss 4.234442710876465\n",
      "Epoch 3: |          | 334/? [07:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 334, loss 5.12429666519165\n",
      "Epoch 3: |          | 335/? [07:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 335, loss 4.954316139221191\n",
      "Epoch 3: |          | 336/? [08:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 336, loss 4.819411277770996\n",
      "Epoch 3: |          | 337/? [08:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 337, loss 5.500191688537598\n",
      "Epoch 3: |          | 338/? [08:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 338, loss 5.13564395904541\n",
      "Epoch 3: |          | 339/? [08:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 339, loss 4.103959560394287\n",
      "Epoch 3: |          | 340/? [08:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 340, loss 4.24599027633667\n",
      "Epoch 3: |          | 341/? [08:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 341, loss 3.7422356605529785\n",
      "Epoch 3: |          | 342/? [08:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 342, loss 4.497404098510742\n",
      "Epoch 3: |          | 343/? [08:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 343, loss 4.196199893951416\n",
      "Epoch 3: |          | 344/? [08:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 344, loss 5.028240203857422\n",
      "Epoch 3: |          | 345/? [08:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 345, loss 4.24025297164917\n",
      "Epoch 3: |          | 346/? [08:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 346, loss 4.488652229309082\n",
      "Epoch 3: |          | 347/? [08:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 347, loss 4.203505039215088\n",
      "Epoch 3: |          | 348/? [08:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 348, loss 3.5464463233947754\n",
      "Epoch 3: |          | 349/? [08:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 349, loss 3.374009609222412\n",
      "Epoch 3: |          | 350/? [08:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 350, loss 4.814245223999023\n",
      "Epoch 3: |          | 351/? [08:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 351, loss 4.827509880065918\n",
      "Epoch 3: |          | 352/? [08:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 352, loss 4.0261054039001465\n",
      "Epoch 3: |          | 353/? [08:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 353, loss 3.7550578117370605\n",
      "Epoch 3: |          | 354/? [08:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 354, loss 4.179198265075684\n",
      "Epoch 3: |          | 355/? [08:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 355, loss 4.483059406280518\n",
      "Epoch 3: |          | 356/? [08:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 356, loss 4.514791011810303\n",
      "Epoch 3: |          | 357/? [08:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 357, loss 4.033343315124512\n",
      "Epoch 3: |          | 358/? [08:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 358, loss 3.942534923553467\n",
      "Epoch 3: |          | 359/? [08:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 359, loss 4.6175994873046875\n",
      "Epoch 3: |          | 360/? [08:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 360, loss 4.170535564422607\n",
      "Epoch 3: |          | 361/? [08:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 361, loss 4.263903617858887\n",
      "Epoch 3: |          | 362/? [08:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 362, loss 4.0998969078063965\n",
      "Epoch 3: |          | 363/? [08:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 363, loss 3.983724594116211\n",
      "Epoch 3: |          | 364/? [08:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 364, loss 4.6285176277160645\n",
      "Epoch 3: |          | 365/? [08:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 365, loss 4.677180290222168\n",
      "Epoch 3: |          | 366/? [08:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 366, loss 4.3893938064575195\n",
      "Epoch 3: |          | 367/? [08:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 367, loss 4.429767608642578\n",
      "Epoch 3: |          | 368/? [08:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 368, loss 3.8785908222198486\n",
      "Epoch 3: |          | 369/? [08:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 369, loss 4.256134033203125\n",
      "Epoch 3: |          | 370/? [08:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 370, loss 3.830305576324463\n",
      "Epoch 3: |          | 371/? [08:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 371, loss 4.894558906555176\n",
      "Epoch 3: |          | 372/? [08:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 372, loss 4.310428142547607\n",
      "Epoch 3: |          | 373/? [08:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 373, loss 4.474496841430664\n",
      "Epoch 3: |          | 374/? [08:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 374, loss 4.194782257080078\n",
      "Epoch 3: |          | 375/? [08:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 375, loss 4.891216278076172\n",
      "Epoch 3: |          | 376/? [08:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 376, loss 4.236374855041504\n",
      "Epoch 3: |          | 377/? [08:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 377, loss 4.445494651794434\n",
      "Epoch 3: |          | 378/? [09:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 378, loss 4.611961364746094\n",
      "Epoch 3: |          | 379/? [09:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 379, loss 4.433681011199951\n",
      "Epoch 3: |          | 380/? [09:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 380, loss 4.5355730056762695\n",
      "Epoch 3: |          | 381/? [09:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 381, loss 4.485657691955566\n",
      "Epoch 3: |          | 382/? [09:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 382, loss 4.229142189025879\n",
      "Epoch 3: |          | 383/? [09:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 383, loss 4.259631156921387\n",
      "Epoch 3: |          | 384/? [09:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 384, loss 4.790217399597168\n",
      "Epoch 3: |          | 385/? [09:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 385, loss 4.379014015197754\n",
      "Epoch 3: |          | 386/? [09:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 386, loss 3.2330195903778076\n",
      "Epoch 3: |          | 387/? [09:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 387, loss 4.1571197509765625\n",
      "Epoch 3: |          | 388/? [09:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 388, loss 4.414497375488281\n",
      "Epoch 3: |          | 389/? [09:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 389, loss 4.78342342376709\n",
      "Epoch 3: |          | 390/? [09:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 390, loss 4.148965835571289\n",
      "Epoch 3: |          | 391/? [09:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 391, loss 4.633622646331787\n",
      "Epoch 3: |          | 392/? [09:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 392, loss 4.655492782592773\n",
      "Epoch 3: |          | 393/? [09:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 393, loss 4.736525535583496\n",
      "Epoch 3: |          | 394/? [09:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 394, loss 4.356932640075684\n",
      "Epoch 3: |          | 395/? [09:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 395, loss 4.6124467849731445\n",
      "Epoch 3: |          | 396/? [09:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 396, loss 4.527274131774902\n",
      "Epoch 3: |          | 397/? [09:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 397, loss 4.3480000495910645\n",
      "Epoch 3: |          | 398/? [09:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 398, loss 4.206137657165527\n",
      "Epoch 3: |          | 399/? [09:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 399, loss 4.307807445526123\n",
      "Epoch 3: |          | 400/? [09:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 400, loss 4.289379119873047\n",
      "Epoch 3: |          | 401/? [09:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 401, loss 4.190990447998047\n",
      "Epoch 3: |          | 402/? [09:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 402, loss 4.684016227722168\n",
      "Epoch 3: |          | 403/? [09:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 403, loss 4.483994483947754\n",
      "Epoch 3: |          | 404/? [09:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 404, loss 3.996471405029297\n",
      "Epoch 3: |          | 405/? [09:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 405, loss 4.0101318359375\n",
      "Epoch 3: |          | 406/? [09:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 406, loss 4.367697715759277\n",
      "Epoch 3: |          | 407/? [09:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 407, loss 4.319971561431885\n",
      "Epoch 3: |          | 408/? [09:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 408, loss 4.689347267150879\n",
      "Epoch 3: |          | 409/? [09:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 409, loss 4.665182590484619\n",
      "Epoch 3: |          | 410/? [09:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 410, loss 4.350687503814697\n",
      "Epoch 3: |          | 411/? [09:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 411, loss 4.201594352722168\n",
      "Epoch 3: |          | 412/? [09:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 412, loss 3.6581027507781982\n",
      "Epoch 3: |          | 413/? [09:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 413, loss 4.479196548461914\n",
      "Epoch 3: |          | 414/? [09:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 414, loss 4.014034748077393\n",
      "Epoch 3: |          | 415/? [09:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 415, loss 4.463409423828125\n",
      "Epoch 3: |          | 416/? [09:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 416, loss 4.911551475524902\n",
      "Epoch 3: |          | 417/? [09:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 417, loss 5.061583995819092\n",
      "Epoch 3: |          | 418/? [10:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 418, loss 4.598343372344971\n",
      "Epoch 3: |          | 419/? [10:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 419, loss 4.464153289794922\n",
      "Epoch 3: |          | 420/? [10:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 420, loss 4.467795372009277\n",
      "Epoch 3: |          | 421/? [10:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 421, loss 4.976924896240234\n",
      "Epoch 3: |          | 422/? [10:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 422, loss 4.482462406158447\n",
      "Epoch 3: |          | 423/? [10:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 423, loss 4.041285991668701\n",
      "Epoch 3: |          | 424/? [10:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 424, loss 4.73135232925415\n",
      "Epoch 3: |          | 425/? [10:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 425, loss 4.526393890380859\n",
      "Epoch 3: |          | 426/? [10:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 426, loss 4.125441551208496\n",
      "Epoch 3: |          | 427/? [10:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 427, loss 4.161530494689941\n",
      "Epoch 3: |          | 428/? [10:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 428, loss 4.994526386260986\n",
      "Epoch 3: |          | 429/? [10:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 429, loss 3.8373801708221436\n",
      "Epoch 3: |          | 430/? [10:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 430, loss 4.516474723815918\n",
      "Epoch 3: |          | 431/? [10:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 431, loss 4.378922939300537\n",
      "Epoch 3: |          | 432/? [10:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 432, loss 4.500406265258789\n",
      "Epoch 3: |          | 433/? [10:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 433, loss 4.428074836730957\n",
      "Epoch 3: |          | 434/? [10:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 434, loss 4.3589887619018555\n",
      "Epoch 3: |          | 435/? [10:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 435, loss 4.039209842681885\n",
      "Epoch 3: |          | 436/? [10:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 436, loss 4.45327091217041\n",
      "Epoch 3: |          | 437/? [10:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 437, loss 4.568404197692871\n",
      "Epoch 3: |          | 438/? [10:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 438, loss 4.206325531005859\n",
      "Epoch 3: |          | 439/? [10:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 439, loss 4.186268329620361\n",
      "Epoch 3: |          | 440/? [10:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 440, loss 4.169992923736572\n",
      "Epoch 3: |          | 441/? [10:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 441, loss 4.539791107177734\n",
      "Epoch 3: |          | 442/? [10:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 442, loss 4.244328498840332\n",
      "Epoch 3: |          | 443/? [10:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 443, loss 4.487414360046387\n",
      "Epoch 3: |          | 444/? [10:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 444, loss 4.35947322845459\n",
      "Epoch 3: |          | 445/? [10:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 445, loss 5.345032691955566\n",
      "Epoch 3: |          | 446/? [10:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 446, loss 4.352856159210205\n",
      "Epoch 3: |          | 447/? [10:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 447, loss 4.911098480224609\n",
      "Epoch 3: |          | 448/? [10:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 448, loss 4.059083461761475\n",
      "Epoch 3: |          | 449/? [10:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 449, loss 4.330824851989746\n",
      "Epoch 3: |          | 450/? [10:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 450, loss 4.721797466278076\n",
      "Epoch 3: |          | 451/? [10:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 451, loss 4.307648658752441\n",
      "Epoch 3: |          | 452/? [10:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 452, loss 4.076598167419434\n",
      "Epoch 3: |          | 453/? [10:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 453, loss 4.784196376800537\n",
      "Epoch 3: |          | 454/? [10:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 454, loss 4.16031551361084\n",
      "Epoch 3: |          | 455/? [10:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 455, loss 4.570639610290527\n",
      "Epoch 3: |          | 456/? [10:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 456, loss 3.9141018390655518\n",
      "Epoch 3: |          | 457/? [10:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 457, loss 4.311611652374268\n",
      "Epoch 3: |          | 458/? [10:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 458, loss 4.751202583312988\n",
      "Epoch 3: |          | 459/? [10:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 459, loss 4.729423522949219\n",
      "Epoch 3: |          | 460/? [10:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 460, loss 4.49698543548584\n",
      "Epoch 3: |          | 461/? [11:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 461, loss 4.437942981719971\n",
      "Epoch 3: |          | 462/? [11:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 462, loss 4.521027565002441\n",
      "Epoch 3: |          | 463/? [11:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 463, loss 4.361666679382324\n",
      "Epoch 3: |          | 464/? [11:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 464, loss 3.850154161453247\n",
      "Epoch 3: |          | 465/? [11:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 465, loss 4.118610382080078\n",
      "Epoch 3: |          | 466/? [11:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 466, loss 4.59450626373291\n",
      "Epoch 3: |          | 467/? [11:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 467, loss 4.415330410003662\n",
      "Epoch 3: |          | 468/? [11:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 468, loss 4.3265862464904785\n",
      "Epoch 3: |          | 469/? [11:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 469, loss 4.491351127624512\n",
      "Epoch 3: |          | 470/? [11:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 470, loss 3.7516422271728516\n",
      "Epoch 3: |          | 471/? [11:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 471, loss 4.557546138763428\n",
      "Epoch 3: |          | 472/? [11:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 472, loss 4.122097492218018\n",
      "Epoch 3: |          | 473/? [11:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 473, loss 4.137803077697754\n",
      "Epoch 3: |          | 474/? [11:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 474, loss 3.7320969104766846\n",
      "Epoch 3: |          | 475/? [11:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 475, loss 5.114573955535889\n",
      "Epoch 3: |          | 476/? [11:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 476, loss 4.019570827484131\n",
      "Epoch 3: |          | 477/? [11:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 477, loss 3.5456204414367676\n",
      "Epoch 3: |          | 478/? [11:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 478, loss 3.7341856956481934\n",
      "Epoch 3: |          | 479/? [11:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 479, loss 4.338534355163574\n",
      "Epoch 3: |          | 480/? [11:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 480, loss 4.299460411071777\n",
      "Epoch 3: |          | 481/? [11:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 481, loss 3.8543694019317627\n",
      "Epoch 3: |          | 482/? [11:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 482, loss 4.133235931396484\n",
      "Epoch 3: |          | 483/? [11:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 483, loss 3.8356151580810547\n",
      "Epoch 3: |          | 484/? [11:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 484, loss 4.774794578552246\n",
      "Epoch 3: |          | 485/? [11:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 485, loss 4.5978522300720215\n",
      "Epoch 3: |          | 486/? [11:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 486, loss 4.321615695953369\n",
      "Epoch 3: |          | 487/? [11:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 487, loss 4.544806003570557\n",
      "Epoch 3: |          | 488/? [11:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 488, loss 4.1907548904418945\n",
      "Epoch 3: |          | 489/? [11:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 489, loss 3.7320849895477295\n",
      "Epoch 3: |          | 490/? [11:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 490, loss 4.439929962158203\n",
      "Epoch 3: |          | 491/? [11:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 491, loss 4.190461158752441\n",
      "Epoch 3: |          | 492/? [11:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 492, loss 3.5167179107666016\n",
      "Epoch 3: |          | 493/? [11:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 493, loss 4.645976543426514\n",
      "Epoch 3: |          | 494/? [11:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 494, loss 4.501230239868164\n",
      "Epoch 3: |          | 495/? [11:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 495, loss 4.54427433013916\n",
      "Epoch 3: |          | 496/? [11:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 496, loss 4.112958908081055\n",
      "Epoch 3: |          | 497/? [11:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 497, loss 4.650361061096191\n",
      "Epoch 3: |          | 498/? [11:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 498, loss 4.358238697052002\n",
      "Epoch 3: |          | 499/? [11:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 499, loss 4.364535331726074\n",
      "Epoch 3: |          | 500/? [11:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 500, loss 4.253073692321777\n",
      "Epoch 3: |          | 501/? [11:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 501, loss 3.9422805309295654\n",
      "Epoch 3: |          | 502/? [11:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 502, loss 4.398622989654541\n",
      "Epoch 3: |          | 503/? [11:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 503, loss 4.4442243576049805\n",
      "Epoch 3: |          | 504/? [12:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 504, loss 4.201614856719971\n",
      "Epoch 3: |          | 505/? [12:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 505, loss 3.5707848072052\n",
      "Epoch 3: |          | 506/? [12:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 506, loss 4.28749942779541\n",
      "Epoch 3: |          | 507/? [12:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 507, loss 4.317065238952637\n",
      "Epoch 3: |          | 508/? [12:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 508, loss 4.660380840301514\n",
      "Epoch 3: |          | 509/? [12:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 509, loss 3.9901680946350098\n",
      "Epoch 3: |          | 510/? [12:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 510, loss 4.529402732849121\n",
      "Epoch 3: |          | 511/? [12:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 511, loss 4.370738983154297\n",
      "Epoch 3: |          | 512/? [12:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 512, loss 3.7845120429992676\n",
      "Epoch 3: |          | 513/? [12:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 513, loss 4.051345348358154\n",
      "Epoch 3: |          | 514/? [12:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 514, loss 4.1499128341674805\n",
      "Epoch 3: |          | 515/? [12:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 515, loss 3.8591513633728027\n",
      "Epoch 3: |          | 516/? [12:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 516, loss 4.1346354484558105\n",
      "Epoch 3: |          | 517/? [12:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 517, loss 4.4533185958862305\n",
      "Epoch 3: |          | 518/? [12:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 518, loss 4.024418830871582\n",
      "Epoch 3: |          | 519/? [12:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 519, loss 4.444523334503174\n",
      "Epoch 3: |          | 520/? [12:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 520, loss 4.263098239898682\n",
      "Epoch 3: |          | 521/? [12:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 521, loss 4.239546298980713\n",
      "Epoch 3: |          | 522/? [12:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 522, loss 4.808689594268799\n",
      "Epoch 3: |          | 523/? [12:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 523, loss 4.871997356414795\n",
      "Epoch 3: |          | 524/? [12:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 524, loss 4.6736741065979\n",
      "Epoch 3: |          | 525/? [12:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 525, loss 4.245866298675537\n",
      "Epoch 3: |          | 526/? [12:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 526, loss 4.053707122802734\n",
      "Epoch 3: |          | 527/? [12:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 527, loss 4.673580646514893\n",
      "Epoch 3: |          | 528/? [12:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 528, loss 4.470177173614502\n",
      "Epoch 3: |          | 529/? [12:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 529, loss 4.078024387359619\n",
      "Epoch 3: |          | 530/? [12:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 530, loss 4.667348384857178\n",
      "Epoch 3: |          | 531/? [12:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 531, loss 4.125174522399902\n",
      "Epoch 3: |          | 532/? [12:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 532, loss 4.421071529388428\n",
      "Epoch 3: |          | 533/? [12:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 533, loss 4.065423488616943\n",
      "Epoch 3: |          | 534/? [12:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 534, loss 3.786802291870117\n",
      "Epoch 3: |          | 535/? [12:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 535, loss 4.550341606140137\n",
      "Epoch 3: |          | 536/? [12:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 536, loss 4.737595558166504\n",
      "Epoch 3: |          | 537/? [12:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 537, loss 4.435128211975098\n",
      "Epoch 3: |          | 538/? [12:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 538, loss 4.088928699493408\n",
      "Epoch 3: |          | 539/? [12:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 539, loss 4.185367107391357\n",
      "Epoch 3: |          | 540/? [12:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 540, loss 4.653009414672852\n",
      "Epoch 3: |          | 541/? [12:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 541, loss 4.351806640625\n",
      "Epoch 3: |          | 542/? [12:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 542, loss 4.161517143249512\n",
      "Epoch 3: |          | 543/? [12:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 543, loss 4.508143424987793\n",
      "Epoch 3: |          | 544/? [12:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 544, loss 4.384629249572754\n",
      "Epoch 3: |          | 545/? [12:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 545, loss 3.6001534461975098\n",
      "Epoch 3: |          | 546/? [13:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 546, loss 4.435132026672363\n",
      "Epoch 3: |          | 547/? [13:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 547, loss 4.893809795379639\n",
      "Epoch 3: |          | 548/? [13:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 548, loss 4.5829596519470215\n",
      "Epoch 3: |          | 549/? [13:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 549, loss 4.404671669006348\n",
      "Epoch 3: |          | 550/? [13:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 550, loss 4.796490669250488\n",
      "Epoch 3: |          | 551/? [13:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 551, loss 4.413954257965088\n",
      "Epoch 3: |          | 552/? [13:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 552, loss 4.48079776763916\n",
      "Epoch 3: |          | 553/? [13:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 553, loss 3.8234927654266357\n",
      "Epoch 3: |          | 554/? [13:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 554, loss 4.460873603820801\n",
      "Epoch 3: |          | 555/? [13:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 555, loss 4.806937217712402\n",
      "Epoch 3: |          | 556/? [13:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 556, loss 4.437241554260254\n",
      "Epoch 3: |          | 557/? [13:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 557, loss 4.032407283782959\n",
      "Epoch 3: |          | 558/? [13:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 558, loss 4.171723365783691\n",
      "Epoch 3: |          | 559/? [13:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 559, loss 4.204914569854736\n",
      "Epoch 3: |          | 560/? [13:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 560, loss 3.632997989654541\n",
      "Epoch 3: |          | 561/? [13:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 561, loss 3.566540479660034\n",
      "Epoch 3: |          | 562/? [13:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 562, loss 4.590451717376709\n",
      "Epoch 3: |          | 563/? [13:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 563, loss 3.648385524749756\n",
      "Epoch 3: |          | 564/? [13:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 564, loss 4.172144889831543\n",
      "Epoch 3: |          | 565/? [13:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 565, loss 4.596227169036865\n",
      "Epoch 3: |          | 566/? [13:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 566, loss 4.609241485595703\n",
      "Epoch 3: |          | 567/? [13:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 567, loss 4.75853967666626\n",
      "Epoch 3: |          | 568/? [13:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 568, loss 3.833836317062378\n",
      "Epoch 3: |          | 569/? [13:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 569, loss 4.471124172210693\n",
      "Epoch 3: |          | 570/? [13:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 570, loss 4.5081892013549805\n",
      "Epoch 3: |          | 571/? [13:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 571, loss 4.052560329437256\n",
      "Epoch 3: |          | 572/? [13:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 572, loss 5.0157151222229\n",
      "Epoch 3: |          | 573/? [13:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 573, loss 3.377411365509033\n",
      "Epoch 3: |          | 574/? [13:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 574, loss 4.597848415374756\n",
      "Epoch 3: |          | 575/? [13:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 575, loss 3.9534175395965576\n",
      "Epoch 3: |          | 576/? [13:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 576, loss 4.163707256317139\n",
      "Epoch 3: |          | 577/? [13:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 577, loss 4.41571569442749\n",
      "Epoch 3: |          | 578/? [13:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 578, loss 4.628108024597168\n",
      "Epoch 3: |          | 579/? [13:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 579, loss 3.6575615406036377\n",
      "Epoch 3: |          | 580/? [13:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 580, loss 4.458987236022949\n",
      "Epoch 3: |          | 581/? [13:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 581, loss 4.486658573150635\n",
      "Epoch 3: |          | 582/? [13:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 582, loss 4.546048164367676\n",
      "Epoch 3: |          | 583/? [13:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 583, loss 4.270000457763672\n",
      "Epoch 3: |          | 584/? [13:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 584, loss 4.4609503746032715\n",
      "Epoch 3: |          | 585/? [13:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 585, loss 4.4876298904418945\n",
      "Epoch 3: |          | 586/? [13:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 586, loss 4.540676116943359\n",
      "Epoch 3: |          | 587/? [13:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 587, loss 4.462072372436523\n",
      "Epoch 3: |          | 588/? [14:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 588, loss 4.534863471984863\n",
      "Epoch 3: |          | 589/? [14:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 589, loss 3.9158904552459717\n",
      "Epoch 3: |          | 590/? [14:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 590, loss 4.595612525939941\n",
      "Epoch 3: |          | 591/? [14:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 591, loss 4.428427696228027\n",
      "Epoch 3: |          | 592/? [14:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 592, loss 4.129451751708984\n",
      "Epoch 3: |          | 593/? [14:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 593, loss 4.293356418609619\n",
      "Epoch 3: |          | 594/? [14:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 594, loss 5.180229187011719\n",
      "Epoch 3: |          | 595/? [14:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 595, loss 3.8308780193328857\n",
      "Epoch 3: |          | 596/? [14:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 596, loss 3.9314093589782715\n",
      "Epoch 3: |          | 597/? [14:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 597, loss 4.214666843414307\n",
      "Epoch 3: |          | 598/? [14:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 598, loss 4.697105407714844\n",
      "Epoch 3: |          | 599/? [14:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 599, loss 4.331408500671387\n",
      "Epoch 3: |          | 600/? [14:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 600, loss 4.048515319824219\n",
      "Epoch 3: |          | 601/? [14:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 601, loss 4.38435173034668\n",
      "Epoch 3: |          | 602/? [14:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 602, loss 3.8849544525146484\n",
      "Epoch 3: |          | 603/? [14:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 603, loss 4.037574768066406\n",
      "Epoch 3: |          | 604/? [14:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 604, loss 6.4036359786987305\n",
      "Epoch 3: |          | 605/? [14:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 605, loss 3.786416530609131\n",
      "Epoch 3: |          | 606/? [14:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 606, loss 4.0767927169799805\n",
      "Epoch 3: |          | 607/? [14:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 607, loss 4.455833435058594\n",
      "Epoch 3: |          | 608/? [14:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 608, loss 4.201749801635742\n",
      "Epoch 3: |          | 609/? [14:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 609, loss 4.110976219177246\n",
      "Epoch 3: |          | 610/? [14:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 610, loss 4.213050842285156\n",
      "Epoch 3: |          | 611/? [14:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 611, loss 4.35618782043457\n",
      "Epoch 3: |          | 612/? [14:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 612, loss 4.114092826843262\n",
      "Epoch 3: |          | 613/? [14:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 613, loss 4.422956943511963\n",
      "Epoch 3: |          | 614/? [14:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 614, loss 4.178576469421387\n",
      "Epoch 3: |          | 615/? [14:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 615, loss 4.733681678771973\n",
      "Epoch 3: |          | 616/? [14:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 616, loss 4.98367166519165\n",
      "Epoch 3: |          | 617/? [14:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 617, loss 3.3975467681884766\n",
      "Epoch 3: |          | 618/? [14:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 618, loss 4.466099739074707\n",
      "Epoch 3: |          | 619/? [14:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 619, loss 4.10307502746582\n",
      "Epoch 3: |          | 620/? [14:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 620, loss 4.582360744476318\n",
      "Epoch 3: |          | 621/? [14:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 621, loss 3.9864020347595215\n",
      "Epoch 3: |          | 622/? [14:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 622, loss 3.7649378776550293\n",
      "Epoch 3: |          | 623/? [14:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 623, loss 3.472290515899658\n",
      "Epoch 3: |          | 624/? [14:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 624, loss 3.13246488571167\n",
      "Epoch 3: |          | 625/? [14:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 625, loss 4.790536403656006\n",
      "Epoch 3: |          | 626/? [14:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 626, loss 4.265416145324707\n",
      "Epoch 3: |          | 627/? [14:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 627, loss 4.189976692199707\n",
      "Epoch 3: |          | 628/? [14:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 628, loss 4.129795074462891\n",
      "Epoch 3: |          | 629/? [14:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 629, loss 4.568630695343018\n",
      "Epoch 3: |          | 630/? [14:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 630, loss 4.29550838470459\n",
      "Epoch 3: |          | 631/? [15:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 631, loss 4.500768661499023\n",
      "Epoch 3: |          | 632/? [15:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 632, loss 3.657963991165161\n",
      "Epoch 3: |          | 633/? [15:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 633, loss 4.522110939025879\n",
      "Epoch 3: |          | 634/? [15:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 634, loss 4.09125280380249\n",
      "Epoch 3: |          | 635/? [15:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 635, loss 3.8491692543029785\n",
      "Epoch 3: |          | 636/? [15:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 636, loss 4.2970170974731445\n",
      "Epoch 3: |          | 637/? [15:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 637, loss 4.098414421081543\n",
      "Epoch 3: |          | 638/? [15:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 638, loss 4.383307456970215\n",
      "Epoch 3: |          | 639/? [15:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 639, loss 4.112627029418945\n",
      "Epoch 3: |          | 640/? [15:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 640, loss 4.723064422607422\n",
      "Epoch 3: |          | 641/? [15:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 641, loss 3.7973792552948\n",
      "Epoch 3: |          | 642/? [15:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 642, loss 4.465436935424805\n",
      "Epoch 3: |          | 643/? [15:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 643, loss 4.4486083984375\n",
      "Epoch 3: |          | 644/? [15:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 644, loss 4.292715549468994\n",
      "Epoch 3: |          | 645/? [15:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 645, loss 4.084245681762695\n",
      "Epoch 3: |          | 646/? [15:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 646, loss 4.080930233001709\n",
      "Epoch 3: |          | 647/? [15:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 647, loss 4.7209882736206055\n",
      "Epoch 3: |          | 648/? [15:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 648, loss 4.1002068519592285\n",
      "Epoch 3: |          | 649/? [15:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 649, loss 3.880127429962158\n",
      "Epoch 3: |          | 650/? [15:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 650, loss 4.623358249664307\n",
      "Epoch 3: |          | 651/? [15:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 651, loss 4.798795223236084\n",
      "Epoch 3: |          | 652/? [15:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 652, loss 4.1574602127075195\n",
      "Epoch 3: |          | 653/? [15:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 653, loss 4.297669887542725\n",
      "Epoch 3: |          | 654/? [15:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 654, loss 4.501938343048096\n",
      "Epoch 3: |          | 655/? [15:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 655, loss 4.283133506774902\n",
      "Epoch 3: |          | 656/? [15:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 656, loss 3.8578567504882812\n",
      "Epoch 3: |          | 657/? [15:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 657, loss 6.381352424621582\n",
      "Epoch 3: |          | 658/? [15:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 658, loss 3.9539084434509277\n",
      "Epoch 3: |          | 659/? [15:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 659, loss 4.2934675216674805\n",
      "Epoch 3: |          | 660/? [15:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 660, loss 4.644119739532471\n",
      "Epoch 3: |          | 661/? [15:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 661, loss 4.565277576446533\n",
      "Epoch 3: |          | 662/? [15:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 662, loss 4.445445537567139\n",
      "Epoch 3: |          | 663/? [15:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 663, loss 4.172296524047852\n",
      "Epoch 3: |          | 664/? [15:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 664, loss 4.087263107299805\n",
      "Epoch 3: |          | 665/? [15:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 665, loss 4.416866779327393\n",
      "Epoch 3: |          | 666/? [15:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 666, loss 4.1985859870910645\n",
      "Epoch 3: |          | 667/? [15:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 667, loss 5.090763092041016\n",
      "Epoch 3: |          | 668/? [15:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 668, loss 3.7726097106933594\n",
      "Epoch 3: |          | 669/? [15:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 669, loss 4.0424485206604\n",
      "Epoch 3: |          | 670/? [15:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 670, loss 4.738659858703613\n",
      "Epoch 3: |          | 671/? [15:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 671, loss 4.449665546417236\n",
      "Epoch 3: |          | 672/? [15:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 672, loss 4.451920509338379\n",
      "Epoch 3: |          | 673/? [16:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 673, loss 4.3055901527404785\n",
      "Epoch 3: |          | 674/? [16:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 674, loss 3.04148530960083\n",
      "Epoch 3: |          | 675/? [16:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 675, loss 1.5052212476730347\n",
      "Epoch 3: |          | 676/? [16:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 676, loss 1.2656503915786743\n",
      "Epoch 3: |          | 677/? [16:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 677, loss 0.9650498628616333\n",
      "Epoch 3: |          | 678/? [16:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 678, loss 2.0061984062194824\n",
      "Epoch 3: |          | 679/? [16:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 679, loss 3.7365734577178955\n",
      "Epoch 3: |          | 680/? [16:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 680, loss 4.262994766235352\n",
      "Epoch 3: |          | 681/? [16:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 681, loss 3.6684188842773438\n",
      "Epoch 3: |          | 682/? [16:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 682, loss 4.082485198974609\n",
      "Epoch 3: |          | 683/? [16:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 683, loss 3.755308151245117\n",
      "Epoch 3: |          | 684/? [16:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 684, loss 4.9128570556640625\n",
      "Epoch 3: |          | 685/? [16:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 685, loss 4.483320236206055\n",
      "Epoch 3: |          | 686/? [16:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 686, loss 4.001529216766357\n",
      "Epoch 3: |          | 687/? [16:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 687, loss 4.640058517456055\n",
      "Epoch 3: |          | 688/? [16:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 688, loss 4.197648048400879\n",
      "Epoch 3: |          | 689/? [16:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 689, loss 4.249207973480225\n",
      "Epoch 3: |          | 690/? [16:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 690, loss 4.793700695037842\n",
      "Epoch 3: |          | 691/? [16:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 691, loss 4.29804801940918\n",
      "Epoch 3: |          | 692/? [16:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 692, loss 4.285745143890381\n",
      "Epoch 3: |          | 693/? [16:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 693, loss 4.833949089050293\n",
      "Epoch 3: |          | 694/? [16:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 694, loss 4.185310363769531\n",
      "Epoch 3: |          | 695/? [16:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 695, loss 4.778959274291992\n",
      "Epoch 3: |          | 696/? [16:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 696, loss 3.9610180854797363\n",
      "Epoch 3: |          | 697/? [16:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 697, loss 4.246396541595459\n",
      "Epoch 3: |          | 698/? [16:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 698, loss 3.504848003387451\n",
      "Epoch 3: |          | 699/? [16:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 699, loss 4.402443885803223\n",
      "Epoch 3: |          | 700/? [16:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 700, loss 4.507508277893066\n",
      "Epoch 3: |          | 701/? [16:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 701, loss 4.050820827484131\n",
      "Epoch 3: |          | 702/? [16:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 702, loss 4.329207420349121\n",
      "Epoch 3: |          | 703/? [16:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 703, loss 4.4917216300964355\n",
      "Epoch 3: |          | 704/? [16:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 704, loss 4.381209373474121\n",
      "Epoch 3: |          | 705/? [16:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 705, loss 3.96107816696167\n",
      "Epoch 3: |          | 706/? [16:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 706, loss 4.042340278625488\n",
      "Epoch 3: |          | 707/? [16:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 707, loss 4.525609493255615\n",
      "Epoch 3: |          | 708/? [16:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 708, loss 4.264985084533691\n",
      "Epoch 3: |          | 709/? [16:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 709, loss 4.179469585418701\n",
      "Epoch 3: |          | 710/? [16:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 710, loss 4.7722907066345215\n",
      "Epoch 3: |          | 711/? [16:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 711, loss 4.9263739585876465\n",
      "Epoch 3: |          | 712/? [16:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 712, loss 4.544093132019043\n",
      "Epoch 3: |          | 713/? [16:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 713, loss 4.584257125854492\n",
      "Epoch 3: |          | 714/? [16:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 714, loss 4.730136871337891\n",
      "Epoch 3: |          | 715/? [17:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 715, loss 3.534337282180786\n",
      "Epoch 3: |          | 716/? [17:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 716, loss 4.33373498916626\n",
      "Epoch 3: |          | 717/? [17:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 717, loss 4.133703708648682\n",
      "Epoch 3: |          | 718/? [17:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 718, loss 3.6914238929748535\n",
      "Epoch 3: |          | 719/? [17:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 719, loss 4.259105682373047\n",
      "Epoch 3: |          | 720/? [17:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 720, loss 3.957827091217041\n",
      "Epoch 3: |          | 721/? [17:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 721, loss 4.661685466766357\n",
      "Epoch 3: |          | 722/? [17:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 722, loss 3.835197925567627\n",
      "Epoch 3: |          | 723/? [17:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 723, loss 4.446996212005615\n",
      "Epoch 3: |          | 724/? [17:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 724, loss 4.166440010070801\n",
      "Epoch 3: |          | 725/? [17:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 725, loss 3.9447102546691895\n",
      "Epoch 3: |          | 726/? [17:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 726, loss 4.186108589172363\n",
      "Epoch 3: |          | 727/? [17:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 727, loss 3.8886184692382812\n",
      "Epoch 3: |          | 728/? [17:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 728, loss 3.7360126972198486\n",
      "Epoch 3: |          | 729/? [17:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 729, loss 4.172555446624756\n",
      "Epoch 3: |          | 730/? [17:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 730, loss 4.153086185455322\n",
      "Epoch 3: |          | 731/? [17:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 731, loss 4.32927942276001\n",
      "Epoch 3: |          | 732/? [17:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 732, loss 4.625365257263184\n",
      "Epoch 3: |          | 733/? [17:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 733, loss 4.249074459075928\n",
      "Epoch 3: |          | 734/? [17:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 734, loss 4.5425028800964355\n",
      "Epoch 3: |          | 735/? [17:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 735, loss 4.417782783508301\n",
      "Epoch 3: |          | 736/? [17:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 736, loss 3.9080817699432373\n",
      "Epoch 3: |          | 737/? [17:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 737, loss 4.697264671325684\n",
      "Epoch 3: |          | 738/? [17:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 738, loss 3.8242859840393066\n",
      "Epoch 3: |          | 739/? [17:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 739, loss 4.420304775238037\n",
      "Epoch 3: |          | 740/? [17:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 740, loss 4.014267921447754\n",
      "Epoch 3: |          | 741/? [17:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 741, loss 4.277316570281982\n",
      "Epoch 3: |          | 742/? [17:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 742, loss 4.6600751876831055\n",
      "Epoch 3: |          | 743/? [17:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 743, loss 4.4531965255737305\n",
      "Epoch 3: |          | 744/? [17:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 744, loss 4.4164228439331055\n",
      "Epoch 3: |          | 745/? [17:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 745, loss 4.0276055335998535\n",
      "Epoch 3: |          | 746/? [17:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 746, loss 4.3389387130737305\n",
      "Epoch 3: |          | 747/? [17:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 747, loss 4.111269474029541\n",
      "Epoch 3: |          | 748/? [17:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 748, loss 3.4018192291259766\n",
      "Epoch 3: |          | 749/? [17:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 749, loss 4.293928623199463\n",
      "Epoch 3: |          | 750/? [17:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 750, loss 4.665696620941162\n",
      "Epoch 3: |          | 751/? [17:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 751, loss 2.855498790740967\n",
      "Epoch 3: |          | 752/? [17:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 752, loss 4.469814777374268\n",
      "Epoch 3: |          | 753/? [17:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 753, loss 3.689622402191162\n",
      "Epoch 3: |          | 754/? [17:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 754, loss 4.114275932312012\n",
      "Epoch 3: |          | 755/? [17:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 755, loss 3.8459014892578125\n",
      "Epoch 3: |          | 756/? [17:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 756, loss 4.238409996032715\n",
      "Epoch 3: |          | 757/? [18:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 757, loss 4.299558639526367\n",
      "Epoch 3: |          | 758/? [18:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 758, loss 3.985218048095703\n",
      "Epoch 3: |          | 759/? [18:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 759, loss 4.0406084060668945\n",
      "Epoch 3: |          | 760/? [18:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 760, loss 4.481031894683838\n",
      "Epoch 3: |          | 761/? [18:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 761, loss 4.537597179412842\n",
      "Epoch 3: |          | 762/? [18:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 762, loss 4.19338321685791\n",
      "Epoch 3: |          | 763/? [18:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 763, loss 4.49993371963501\n",
      "Epoch 3: |          | 764/? [18:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 764, loss 4.668206214904785\n",
      "Epoch 3: |          | 765/? [18:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 765, loss 4.3680524826049805\n",
      "Epoch 3: |          | 766/? [18:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 766, loss 4.783005237579346\n",
      "Epoch 3: |          | 767/? [18:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 767, loss 4.869291305541992\n",
      "Epoch 3: |          | 768/? [18:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 768, loss 4.386772632598877\n",
      "Epoch 3: |          | 769/? [18:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 769, loss 3.5100369453430176\n",
      "Epoch 3: |          | 770/? [18:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 770, loss 4.105999946594238\n",
      "Epoch 3: |          | 771/? [18:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 771, loss 4.871792793273926\n",
      "Epoch 3: |          | 772/? [18:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 772, loss 4.569226264953613\n",
      "Epoch 3: |          | 773/? [18:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 773, loss 4.238654136657715\n",
      "Epoch 3: |          | 774/? [18:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 774, loss 4.331560134887695\n",
      "Epoch 3: |          | 775/? [18:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 775, loss 4.8464860916137695\n",
      "Epoch 3: |          | 776/? [18:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 776, loss 4.234816551208496\n",
      "Epoch 3: |          | 777/? [18:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 777, loss 4.201457977294922\n",
      "Epoch 3: |          | 778/? [18:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 778, loss 4.585139274597168\n",
      "Epoch 3: |          | 779/? [18:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 779, loss 4.9752631187438965\n",
      "Epoch 3: |          | 780/? [18:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 780, loss 3.9071145057678223\n",
      "Epoch 3: |          | 781/? [18:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 781, loss 4.037197113037109\n",
      "Epoch 3: |          | 782/? [18:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 782, loss 4.451888084411621\n",
      "Epoch 3: |          | 783/? [18:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 783, loss 4.535841941833496\n",
      "Epoch 3: |          | 784/? [18:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 784, loss 4.0496602058410645\n",
      "Epoch 3: |          | 785/? [18:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 785, loss 3.924459457397461\n",
      "Epoch 3: |          | 786/? [18:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 786, loss 4.747832298278809\n",
      "Epoch 3: |          | 787/? [18:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 787, loss 4.747971534729004\n",
      "Epoch 3: |          | 788/? [18:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 788, loss 2.8701188564300537\n",
      "Epoch 3: |          | 789/? [18:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 789, loss 4.244206428527832\n",
      "Epoch 3: |          | 790/? [18:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 790, loss 5.06439733505249\n",
      "Epoch 3: |          | 791/? [18:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 791, loss 4.759393215179443\n",
      "Epoch 3: |          | 792/? [18:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 792, loss 3.896549940109253\n",
      "Epoch 3: |          | 793/? [18:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 793, loss 4.385453701019287\n",
      "Epoch 3: |          | 794/? [18:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 794, loss 4.792664527893066\n",
      "Epoch 3: |          | 795/? [18:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 795, loss 4.255317687988281\n",
      "Epoch 3: |          | 796/? [18:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 796, loss 4.660454750061035\n",
      "Epoch 3: |          | 797/? [18:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 797, loss 3.5559487342834473\n",
      "Epoch 3: |          | 798/? [18:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 798, loss 3.714312791824341\n",
      "Epoch 3: |          | 799/? [19:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 799, loss 4.697654724121094\n",
      "Epoch 3: |          | 800/? [19:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 800, loss 4.473158359527588\n",
      "Epoch 3: |          | 801/? [19:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 801, loss 4.009291172027588\n",
      "Epoch 3: |          | 802/? [19:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 802, loss 4.429162979125977\n",
      "Epoch 3: |          | 803/? [19:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 803, loss 4.218731880187988\n",
      "Epoch 3: |          | 804/? [19:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 804, loss 4.390212059020996\n",
      "Epoch 3: |          | 805/? [19:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 805, loss 4.687950134277344\n",
      "Epoch 3: |          | 806/? [19:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 806, loss 4.920754909515381\n",
      "Epoch 3: |          | 807/? [19:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 807, loss 4.367478847503662\n",
      "Epoch 3: |          | 808/? [19:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 808, loss 3.901395082473755\n",
      "Epoch 3: |          | 809/? [19:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 809, loss 4.473057270050049\n",
      "Epoch 3: |          | 810/? [19:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 810, loss 4.292213439941406\n",
      "Epoch 3: |          | 811/? [19:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 811, loss 4.587109565734863\n",
      "Epoch 3: |          | 812/? [19:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 812, loss 5.237519264221191\n",
      "Epoch 3: |          | 813/? [19:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 813, loss 4.912657737731934\n",
      "Epoch 3: |          | 814/? [19:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 814, loss 3.9023711681365967\n",
      "Epoch 3: |          | 815/? [19:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 815, loss 4.641437530517578\n",
      "Epoch 3: |          | 816/? [19:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 816, loss 4.46520471572876\n",
      "Epoch 3: |          | 817/? [19:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 817, loss 3.774956464767456\n",
      "Epoch 3: |          | 818/? [19:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 818, loss 4.84728479385376\n",
      "Epoch 3: |          | 819/? [19:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 819, loss 4.514695644378662\n",
      "Epoch 3: |          | 820/? [19:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 820, loss 4.361252784729004\n",
      "Epoch 3: |          | 821/? [19:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 821, loss 4.338071346282959\n",
      "Epoch 3: |          | 822/? [19:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 822, loss 3.917046070098877\n",
      "Epoch 3: |          | 823/? [19:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 823, loss 3.8835926055908203\n",
      "Epoch 3: |          | 824/? [19:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 824, loss 4.440883159637451\n",
      "Epoch 3: |          | 825/? [19:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 825, loss 3.9534378051757812\n",
      "Epoch 3: |          | 826/? [19:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 826, loss 4.4791951179504395\n",
      "Epoch 3: |          | 827/? [19:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 827, loss 4.09317684173584\n",
      "Epoch 3: |          | 828/? [19:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 828, loss 4.672491073608398\n",
      "Epoch 3: |          | 829/? [19:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 829, loss 4.267286777496338\n",
      "Epoch 3: |          | 830/? [19:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 830, loss 4.877735614776611\n",
      "Epoch 3: |          | 831/? [19:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 831, loss 2.70717191696167\n",
      "Epoch 3: |          | 832/? [19:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 832, loss 4.284250259399414\n",
      "Epoch 3: |          | 833/? [19:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 833, loss 4.129775047302246\n",
      "Epoch 3: |          | 834/? [19:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 834, loss 4.921517848968506\n",
      "Epoch 3: |          | 835/? [19:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 835, loss 4.286299705505371\n",
      "Epoch 3: |          | 836/? [19:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 836, loss 4.932994842529297\n",
      "Epoch 3: |          | 837/? [19:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 837, loss 4.390539646148682\n",
      "Epoch 3: |          | 838/? [19:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 838, loss 3.648754119873047\n",
      "Epoch 3: |          | 839/? [19:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 839, loss 4.029580593109131\n",
      "Epoch 3: |          | 840/? [19:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 840, loss 4.572140693664551\n",
      "Epoch 3: |          | 841/? [20:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 841, loss 4.645025730133057\n",
      "Epoch 3: |          | 842/? [20:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 842, loss 4.279940605163574\n",
      "Epoch 3: |          | 843/? [20:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 843, loss 4.667707920074463\n",
      "Epoch 3: |          | 844/? [20:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 844, loss 3.9991321563720703\n",
      "Epoch 3: |          | 845/? [20:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 845, loss 4.428654670715332\n",
      "Epoch 3: |          | 846/? [20:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 846, loss 4.919323921203613\n",
      "Epoch 3: |          | 847/? [20:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 847, loss 4.443506717681885\n",
      "Epoch 3: |          | 848/? [20:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 848, loss 3.933852434158325\n",
      "Epoch 3: |          | 849/? [20:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 849, loss 4.091540336608887\n",
      "Epoch 3: |          | 850/? [20:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 850, loss 4.168132305145264\n",
      "Epoch 3: |          | 851/? [20:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 851, loss 4.545664310455322\n",
      "Epoch 3: |          | 852/? [20:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 852, loss 4.570368766784668\n",
      "Epoch 3: |          | 853/? [20:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 853, loss 4.459123611450195\n",
      "Epoch 3: |          | 854/? [20:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 854, loss 3.6685261726379395\n",
      "Epoch 3: |          | 855/? [20:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 855, loss 4.046992301940918\n",
      "Epoch 3: |          | 856/? [20:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 856, loss 3.970902919769287\n",
      "Epoch 3: |          | 857/? [20:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 857, loss 4.559197902679443\n",
      "Epoch 3: |          | 858/? [20:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 858, loss 4.393620491027832\n",
      "Epoch 3: |          | 859/? [20:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 859, loss 4.456167221069336\n",
      "Epoch 3: |          | 860/? [20:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 860, loss 4.843147277832031\n",
      "Epoch 3: |          | 861/? [20:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 861, loss 4.038301944732666\n",
      "Epoch 3: |          | 862/? [20:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 862, loss 4.4903974533081055\n",
      "Epoch 3: |          | 863/? [20:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 863, loss 3.7318115234375\n",
      "Epoch 3: |          | 864/? [20:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 864, loss 4.4407854080200195\n",
      "Epoch 3: |          | 865/? [20:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 865, loss 4.328588485717773\n",
      "Epoch 3: |          | 866/? [20:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 866, loss 3.3139796257019043\n",
      "Epoch 3: |          | 867/? [20:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 867, loss 3.56134295463562\n",
      "Epoch 3: |          | 868/? [20:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 868, loss 4.48147439956665\n",
      "Epoch 3: |          | 869/? [20:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 869, loss 4.4616827964782715\n",
      "Epoch 3: |          | 870/? [20:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 870, loss 4.031068801879883\n",
      "Epoch 3: |          | 871/? [20:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 871, loss 4.438224792480469\n",
      "Epoch 3: |          | 872/? [20:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 872, loss 4.289360523223877\n",
      "Epoch 3: |          | 873/? [20:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 873, loss 4.264505863189697\n",
      "Epoch 3: |          | 874/? [20:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 874, loss 3.724898099899292\n",
      "Epoch 3: |          | 875/? [20:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 875, loss 4.468402862548828\n",
      "Epoch 3: |          | 876/? [20:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 876, loss 4.100726127624512\n",
      "Epoch 3: |          | 877/? [20:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 877, loss 4.403449058532715\n",
      "Epoch 3: |          | 878/? [20:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 878, loss 3.8148350715637207\n",
      "Epoch 3: |          | 879/? [20:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 879, loss 3.8967270851135254\n",
      "Epoch 3: |          | 880/? [20:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 880, loss 5.0319719314575195\n",
      "Epoch 3: |          | 881/? [20:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 881, loss 4.400850296020508\n",
      "Epoch 3: |          | 882/? [20:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 882, loss 4.167613983154297\n",
      "Epoch 3: |          | 883/? [21:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 883, loss 4.319447040557861\n",
      "Epoch 3: |          | 884/? [21:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 884, loss 4.404013633728027\n",
      "Epoch 3: |          | 885/? [21:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 885, loss 4.1444268226623535\n",
      "Epoch 3: |          | 886/? [21:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 886, loss 4.786152362823486\n",
      "Epoch 3: |          | 887/? [21:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 887, loss 4.839285850524902\n",
      "Epoch 3: |          | 888/? [21:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 888, loss 4.539872646331787\n",
      "Epoch 3: |          | 889/? [21:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 889, loss 4.099780559539795\n",
      "Epoch 3: |          | 890/? [21:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 890, loss 4.40897274017334\n",
      "Epoch 3: |          | 891/? [21:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 891, loss 4.0284247398376465\n",
      "Epoch 3: |          | 892/? [21:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 892, loss 4.7424726486206055\n",
      "Epoch 3: |          | 893/? [21:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 893, loss 4.141479969024658\n",
      "Epoch 3: |          | 894/? [21:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 894, loss 3.748708724975586\n",
      "Epoch 3: |          | 895/? [21:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 895, loss 4.857711315155029\n",
      "Epoch 3: |          | 896/? [21:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 896, loss 4.4471635818481445\n",
      "Epoch 3: |          | 897/? [21:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 897, loss 4.493946075439453\n",
      "Epoch 3: |          | 898/? [21:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 898, loss 4.47368860244751\n",
      "Epoch 3: |          | 899/? [21:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 899, loss 4.191437244415283\n",
      "Epoch 3: |          | 900/? [21:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 900, loss 4.08355712890625\n",
      "Epoch 3: |          | 901/? [21:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 901, loss 4.58548641204834\n",
      "Epoch 3: |          | 902/? [21:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 902, loss 4.6180830001831055\n",
      "Epoch 3: |          | 903/? [21:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 903, loss 3.896221160888672\n",
      "Epoch 3: |          | 904/? [21:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 904, loss 4.410571575164795\n",
      "Epoch 3: |          | 905/? [21:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 905, loss 4.6290283203125\n",
      "Epoch 3: |          | 906/? [21:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 906, loss 4.337824821472168\n",
      "Epoch 3: |          | 907/? [21:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 907, loss 4.39619779586792\n",
      "Epoch 3: |          | 908/? [21:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 908, loss 4.465764045715332\n",
      "Epoch 3: |          | 909/? [21:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 909, loss 4.483403205871582\n",
      "Epoch 3: |          | 910/? [21:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 910, loss 4.210367679595947\n",
      "Epoch 3: |          | 911/? [21:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 911, loss 4.266161918640137\n",
      "Epoch 3: |          | 912/? [21:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 912, loss 4.237250328063965\n",
      "Epoch 3: |          | 913/? [21:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 913, loss 4.24245548248291\n",
      "Epoch 3: |          | 914/? [21:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 914, loss 4.571237087249756\n",
      "Epoch 3: |          | 915/? [21:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 915, loss 4.421267986297607\n",
      "Epoch 3: |          | 916/? [21:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 916, loss 4.247860908508301\n",
      "Epoch 3: |          | 917/? [21:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 917, loss 4.286791801452637\n",
      "Epoch 3: |          | 918/? [21:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 918, loss 4.144773006439209\n",
      "Epoch 3: |          | 919/? [21:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 919, loss 4.129821300506592\n",
      "Epoch 3: |          | 920/? [21:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 920, loss 4.3039960861206055\n",
      "Epoch 3: |          | 921/? [21:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 921, loss 4.155252933502197\n",
      "Epoch 3: |          | 922/? [22:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 922, loss 4.322208404541016\n",
      "Epoch 3: |          | 923/? [22:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 923, loss 4.172520637512207\n",
      "Epoch 3: |          | 924/? [22:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 924, loss 4.098316192626953\n",
      "Epoch 3: |          | 925/? [22:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 925, loss 4.442843437194824\n",
      "Epoch 3: |          | 926/? [22:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 926, loss 4.240906715393066\n",
      "Epoch 3: |          | 927/? [22:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 927, loss 4.48017692565918\n",
      "Epoch 3: |          | 928/? [22:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 928, loss 3.9303412437438965\n",
      "Epoch 3: |          | 929/? [22:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 929, loss 4.077599048614502\n",
      "Epoch 3: |          | 930/? [22:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 930, loss 3.9688720703125\n",
      "Epoch 3: |          | 931/? [22:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 931, loss 3.670675277709961\n",
      "Epoch 3: |          | 932/? [22:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 932, loss 4.366885662078857\n",
      "Epoch 3: |          | 933/? [22:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 933, loss 4.147837162017822\n",
      "Epoch 3: |          | 934/? [22:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 934, loss 4.785671234130859\n",
      "Epoch 3: |          | 935/? [22:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 935, loss 5.033516883850098\n",
      "Epoch 3: |          | 936/? [22:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 936, loss 4.2593865394592285\n",
      "Epoch 3: |          | 937/? [22:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 937, loss 4.473262786865234\n",
      "Epoch 3: |          | 938/? [22:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 938, loss 4.158213138580322\n",
      "Epoch 3: |          | 939/? [22:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 939, loss 4.4382004737854\n",
      "Epoch 3: |          | 940/? [22:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 940, loss 4.677021026611328\n",
      "Epoch 3: |          | 941/? [22:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 941, loss 4.111305236816406\n",
      "Epoch 3: |          | 942/? [22:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 942, loss 3.6403212547302246\n",
      "Epoch 3: |          | 943/? [22:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 943, loss 4.584439277648926\n",
      "Epoch 3: |          | 944/? [22:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 944, loss 3.5269017219543457\n",
      "Epoch 3: |          | 945/? [22:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 945, loss 4.249939918518066\n",
      "Epoch 3: |          | 946/? [22:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 946, loss 4.2545671463012695\n",
      "Epoch 3: |          | 947/? [22:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 947, loss 4.178103923797607\n",
      "Epoch 3: |          | 948/? [22:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 948, loss 4.360953330993652\n",
      "Epoch 3: |          | 949/? [22:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 949, loss 4.2165398597717285\n",
      "Epoch 3: |          | 950/? [22:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 950, loss 4.0156073570251465\n",
      "Epoch 3: |          | 951/? [22:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 951, loss 4.689645767211914\n",
      "Epoch 3: |          | 952/? [22:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 952, loss 4.69088077545166\n",
      "Epoch 3: |          | 953/? [22:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 953, loss 5.178259372711182\n",
      "Epoch 3: |          | 954/? [22:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 954, loss 4.158034801483154\n",
      "Epoch 3: |          | 955/? [22:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 955, loss 4.885608196258545\n",
      "Epoch 3: |          | 956/? [22:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 956, loss 4.20352840423584\n",
      "Epoch 3: |          | 957/? [22:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 957, loss 4.504593849182129\n",
      "Epoch 3: |          | 958/? [22:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 958, loss 4.684754848480225\n",
      "Epoch 3: |          | 959/? [22:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 959, loss 4.419490337371826\n",
      "Epoch 3: |          | 960/? [22:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 960, loss 4.63746976852417\n",
      "Epoch 3: |          | 961/? [22:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 961, loss 4.80916690826416\n",
      "Epoch 3: |          | 962/? [22:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 962, loss 4.394962310791016\n",
      "Epoch 3: |          | 963/? [22:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 963, loss 4.08619499206543\n",
      "Epoch 3: |          | 964/? [22:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 964, loss 4.5712432861328125\n",
      "Epoch 3: |          | 965/? [23:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 965, loss 4.022987365722656\n",
      "Epoch 3: |          | 966/? [23:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 966, loss 3.8956756591796875\n",
      "Epoch 3: |          | 967/? [23:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 967, loss 4.175719261169434\n",
      "Epoch 3: |          | 968/? [23:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 968, loss 4.181271553039551\n",
      "Epoch 3: |          | 969/? [23:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 969, loss 3.9342315196990967\n",
      "Epoch 3: |          | 970/? [23:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 970, loss 4.492347240447998\n",
      "Epoch 3: |          | 971/? [23:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 971, loss 4.7999396324157715\n",
      "Epoch 3: |          | 972/? [23:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 972, loss 4.137038230895996\n",
      "Epoch 3: |          | 973/? [23:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 973, loss 4.35861873626709\n",
      "Epoch 3: |          | 974/? [23:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 974, loss 4.272019386291504\n",
      "Epoch 3: |          | 975/? [23:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 975, loss 4.302079200744629\n",
      "Epoch 3: |          | 976/? [23:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 976, loss 4.398880958557129\n",
      "Epoch 3: |          | 977/? [23:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 977, loss 4.998588562011719\n",
      "Epoch 3: |          | 978/? [23:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 978, loss 4.514793872833252\n",
      "Epoch 3: |          | 979/? [23:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 979, loss 4.795599937438965\n",
      "Epoch 3: |          | 980/? [23:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 980, loss 3.9189133644104004\n",
      "Epoch 3: |          | 981/? [23:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 981, loss 3.6508803367614746\n",
      "Epoch 3: |          | 982/? [23:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 982, loss 4.355740547180176\n",
      "Epoch 3: |          | 983/? [23:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 983, loss 4.765115261077881\n",
      "Epoch 3: |          | 984/? [23:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 984, loss 3.771846055984497\n",
      "Epoch 3: |          | 985/? [23:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 985, loss 4.087240219116211\n",
      "Epoch 3: |          | 986/? [23:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 986, loss 4.100255489349365\n",
      "Epoch 3: |          | 987/? [23:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 987, loss 3.6647632122039795\n",
      "Epoch 3: |          | 988/? [23:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 988, loss 4.659983158111572\n",
      "Epoch 3: |          | 989/? [23:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 989, loss 4.338367462158203\n",
      "Epoch 3: |          | 990/? [23:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 990, loss 3.5019779205322266\n",
      "Epoch 3: |          | 991/? [23:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 991, loss 4.300285816192627\n",
      "Epoch 3: |          | 992/? [23:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 992, loss 5.04647159576416\n",
      "Epoch 3: |          | 993/? [23:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 993, loss 4.118511199951172\n",
      "Epoch 3: |          | 994/? [23:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 994, loss 4.144234657287598\n",
      "Epoch 3: |          | 995/? [23:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 995, loss 4.582900524139404\n",
      "Epoch 3: |          | 996/? [23:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 996, loss 4.585105895996094\n",
      "Epoch 3: |          | 997/? [23:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 997, loss 4.146246910095215\n",
      "Epoch 3: |          | 998/? [23:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 998, loss 4.438483238220215\n",
      "Epoch 3: |          | 999/? [23:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 999, loss 4.517087936401367\n",
      "Epoch 3: |          | 1000/? [23:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1000, loss 4.005990028381348\n",
      "Epoch 3: |          | 1001/? [23:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1001, loss 4.620466232299805\n",
      "Epoch 3: |          | 1002/? [23:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1002, loss 4.590692043304443\n",
      "Epoch 3: |          | 1003/? [23:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1003, loss 4.720157146453857\n",
      "Epoch 3: |          | 1004/? [23:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1004, loss 3.590541362762451\n",
      "Epoch 3: |          | 1005/? [23:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1005, loss 4.23013973236084\n",
      "Epoch 3: |          | 1006/? [23:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1006, loss 4.611055850982666\n",
      "Epoch 3: |          | 1007/? [24:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1007, loss 4.1479668617248535\n",
      "Epoch 3: |          | 1008/? [24:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1008, loss 4.296069145202637\n",
      "Epoch 3: |          | 1009/? [24:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1009, loss 4.664658546447754\n",
      "Epoch 3: |          | 1010/? [24:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1010, loss 3.6697304248809814\n",
      "Epoch 3: |          | 1011/? [24:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1011, loss 4.329392910003662\n",
      "Epoch 3: |          | 1012/? [24:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1012, loss 4.071945667266846\n",
      "Epoch 3: |          | 1013/? [24:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1013, loss 4.280799388885498\n",
      "Epoch 3: |          | 1014/? [24:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1014, loss 4.7054338455200195\n",
      "Epoch 3: |          | 1015/? [24:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1015, loss 4.373935222625732\n",
      "Epoch 3: |          | 1016/? [24:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1016, loss 4.15080451965332\n",
      "Epoch 3: |          | 1017/? [24:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1017, loss 3.589298725128174\n",
      "Epoch 3: |          | 1018/? [24:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1018, loss 4.235652923583984\n",
      "Epoch 3: |          | 1019/? [24:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1019, loss 4.281490802764893\n",
      "Epoch 3: |          | 1020/? [24:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1020, loss 3.8587422370910645\n",
      "Epoch 3: |          | 1021/? [24:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1021, loss 4.092410087585449\n",
      "Epoch 3: |          | 1022/? [24:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1022, loss 3.8209195137023926\n",
      "Epoch 3: |          | 1023/? [24:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1023, loss 3.555267810821533\n",
      "Epoch 3: |          | 1024/? [24:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1024, loss 4.129632472991943\n",
      "Epoch 3: |          | 1025/? [24:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1025, loss 4.025987148284912\n",
      "Epoch 3: |          | 1026/? [24:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1026, loss 3.0644750595092773\n",
      "Epoch 3: |          | 1027/? [24:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1027, loss 4.343708038330078\n",
      "Epoch 3: |          | 1028/? [24:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1028, loss 4.200109004974365\n",
      "Epoch 3: |          | 1029/? [24:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1029, loss 4.117412567138672\n",
      "Epoch 3: |          | 1030/? [24:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1030, loss 3.8971664905548096\n",
      "Epoch 3: |          | 1031/? [24:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1031, loss 3.9671711921691895\n",
      "Epoch 3: |          | 1032/? [24:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1032, loss 4.57455587387085\n",
      "Epoch 3: |          | 1033/? [24:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1033, loss 4.657332897186279\n",
      "Epoch 3: |          | 1034/? [24:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1034, loss 4.031708240509033\n",
      "Epoch 3: |          | 1035/? [24:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1035, loss 4.060117721557617\n",
      "Epoch 3: |          | 1036/? [24:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1036, loss 3.931644916534424\n",
      "Epoch 3: |          | 1037/? [24:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1037, loss 4.627022743225098\n",
      "Epoch 3: |          | 1038/? [24:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1038, loss 4.787877082824707\n",
      "Epoch 3: |          | 1039/? [24:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1039, loss 5.034719467163086\n",
      "Epoch 3: |          | 1040/? [24:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1040, loss 4.347438812255859\n",
      "Epoch 3: |          | 1041/? [24:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1041, loss 4.697384357452393\n",
      "Epoch 3: |          | 1042/? [24:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1042, loss 4.265450477600098\n",
      "Epoch 3: |          | 1043/? [24:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1043, loss 4.628019332885742\n",
      "Epoch 3: |          | 1044/? [24:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1044, loss 4.186327934265137\n",
      "Epoch 3: |          | 1045/? [24:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1045, loss 3.7419967651367188\n",
      "Epoch 3: |          | 1046/? [24:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1046, loss 3.5458359718322754\n",
      "Epoch 3: |          | 1047/? [24:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1047, loss 4.823444366455078\n",
      "Epoch 3: |          | 1048/? [24:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1048, loss 4.1347150802612305\n",
      "Epoch 3: |          | 1049/? [25:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1049, loss 4.373065948486328\n",
      "Epoch 3: |          | 1050/? [25:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1050, loss 3.9394984245300293\n",
      "Epoch 3: |          | 1051/? [25:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1051, loss 3.994786024093628\n",
      "Epoch 3: |          | 1052/? [25:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1052, loss 4.592471122741699\n",
      "Epoch 3: |          | 1053/? [25:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1053, loss 4.711544990539551\n",
      "Epoch 3: |          | 1054/? [25:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1054, loss 4.0732622146606445\n",
      "Epoch 3: |          | 1055/? [25:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1055, loss 3.771467685699463\n",
      "Epoch 3: |          | 1056/? [25:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1056, loss 3.8033251762390137\n",
      "Epoch 3: |          | 1057/? [25:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1057, loss 4.472089767456055\n",
      "Epoch 3: |          | 1058/? [25:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1058, loss 4.024451732635498\n",
      "Epoch 3: |          | 1059/? [25:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1059, loss 4.6679558753967285\n",
      "Epoch 3: |          | 1060/? [25:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1060, loss 4.516135215759277\n",
      "Epoch 3: |          | 1061/? [25:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1061, loss 3.1735122203826904\n",
      "Epoch 3: |          | 1062/? [25:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1062, loss 4.254763603210449\n",
      "Epoch 3: |          | 1063/? [25:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1063, loss 4.246927261352539\n",
      "Epoch 3: |          | 1064/? [25:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1064, loss 4.474803924560547\n",
      "Epoch 3: |          | 1065/? [25:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1065, loss 3.052659273147583\n",
      "Epoch 3: |          | 1066/? [25:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1066, loss 4.3379225730896\n",
      "Epoch 3: |          | 1067/? [25:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1067, loss 3.820936918258667\n",
      "Epoch 3: |          | 1068/? [25:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1068, loss 3.993952989578247\n",
      "Epoch 3: |          | 1069/? [25:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1069, loss 4.398993015289307\n",
      "Epoch 3: |          | 1070/? [25:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1070, loss 4.155127048492432\n",
      "Epoch 3: |          | 1071/? [25:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1071, loss 4.525970935821533\n",
      "Epoch 3: |          | 1072/? [25:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1072, loss 4.568639755249023\n",
      "Epoch 3: |          | 1073/? [25:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1073, loss 4.8345232009887695\n",
      "Epoch 3: |          | 1074/? [25:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1074, loss 4.095210552215576\n",
      "Epoch 3: |          | 1075/? [25:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1075, loss 3.874143123626709\n",
      "Epoch 3: |          | 1076/? [25:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1076, loss 4.565140247344971\n",
      "Epoch 3: |          | 1077/? [25:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1077, loss 4.021676063537598\n",
      "Epoch 3: |          | 1078/? [25:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1078, loss 4.253878116607666\n",
      "Epoch 3: |          | 1079/? [25:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1079, loss 4.949734210968018\n",
      "Epoch 3: |          | 1080/? [25:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1080, loss 4.288165092468262\n",
      "Epoch 3: |          | 1081/? [25:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1081, loss 4.508288860321045\n",
      "Epoch 3: |          | 1082/? [25:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1082, loss 4.01768684387207\n",
      "Epoch 3: |          | 1083/? [25:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1083, loss 3.706282377243042\n",
      "Epoch 3: |          | 1084/? [25:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1084, loss 3.477830171585083\n",
      "Epoch 3: |          | 1085/? [25:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1085, loss 4.140988349914551\n",
      "Epoch 3: |          | 1086/? [25:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1086, loss 4.4219465255737305\n",
      "Epoch 3: |          | 1087/? [25:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1087, loss 4.9429521560668945\n",
      "Epoch 3: |          | 1088/? [25:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1088, loss 4.505311012268066\n",
      "Epoch 3: |          | 1089/? [25:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1089, loss 4.619297981262207\n",
      "Epoch 3: |          | 1090/? [25:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1090, loss 4.3243255615234375\n",
      "Epoch 3: |          | 1091/? [26:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1091, loss 4.059433937072754\n",
      "Epoch 3: |          | 1092/? [26:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1092, loss 4.322756767272949\n",
      "Epoch 3: |          | 1093/? [26:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1093, loss 3.779824733734131\n",
      "Epoch 3: |          | 1094/? [26:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1094, loss 4.328269958496094\n",
      "Epoch 3: |          | 1095/? [26:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1095, loss 4.390705108642578\n",
      "Epoch 3: |          | 1096/? [26:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1096, loss 4.60972261428833\n",
      "Epoch 3: |          | 1097/? [26:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1097, loss 4.183567523956299\n",
      "Epoch 3: |          | 1098/? [26:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1098, loss 3.4127063751220703\n",
      "Epoch 3: |          | 1099/? [26:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1099, loss 4.174149990081787\n",
      "Epoch 3: |          | 1100/? [26:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1100, loss 4.367946147918701\n",
      "Epoch 3: |          | 1101/? [26:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1101, loss 3.955540895462036\n",
      "Epoch 3: |          | 1102/? [26:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1102, loss 4.791394233703613\n",
      "Epoch 3: |          | 1103/? [26:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1103, loss 5.388329029083252\n",
      "Epoch 3: |          | 1104/? [26:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1104, loss 4.550597190856934\n",
      "Epoch 3: |          | 1105/? [26:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1105, loss 4.632007122039795\n",
      "Epoch 3: |          | 1106/? [26:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1106, loss 4.157049655914307\n",
      "Epoch 3: |          | 1107/? [26:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1107, loss 4.250985145568848\n",
      "Epoch 3: |          | 1108/? [26:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1108, loss 4.220442771911621\n",
      "Epoch 3: |          | 1109/? [26:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1109, loss 3.7789454460144043\n",
      "Epoch 3: |          | 1110/? [26:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1110, loss 4.871853828430176\n",
      "Epoch 3: |          | 1111/? [26:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1111, loss 4.4902143478393555\n",
      "Epoch 3: |          | 1112/? [26:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1112, loss 4.359925746917725\n",
      "Epoch 3: |          | 1113/? [26:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1113, loss 4.150786399841309\n",
      "Epoch 3: |          | 1114/? [26:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1114, loss 3.5998878479003906\n",
      "Epoch 3: |          | 1115/? [26:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1115, loss 3.3020882606506348\n",
      "Epoch 3: |          | 1116/? [26:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1116, loss 3.7819676399230957\n",
      "Epoch 3: |          | 1117/? [26:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1117, loss 3.993715763092041\n",
      "Epoch 3: |          | 1118/? [26:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1118, loss 4.066636085510254\n",
      "Epoch 3: |          | 1119/? [26:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1119, loss 4.757852077484131\n",
      "Epoch 3: |          | 1120/? [26:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1120, loss 4.231436729431152\n",
      "Epoch 3: |          | 1121/? [26:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1121, loss 4.549123287200928\n",
      "Epoch 3: |          | 1122/? [26:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1122, loss 4.180728435516357\n",
      "Epoch 3: |          | 1123/? [26:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1123, loss 4.320156097412109\n",
      "Epoch 3: |          | 1124/? [26:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1124, loss 4.673521041870117\n",
      "Epoch 3: |          | 1125/? [26:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1125, loss 3.918914318084717\n",
      "Epoch 3: |          | 1126/? [26:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1126, loss 3.836641311645508\n",
      "Epoch 3: |          | 1127/? [26:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1127, loss 4.191048622131348\n",
      "Epoch 3: |          | 1128/? [26:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1128, loss 4.208968162536621\n",
      "Epoch 3: |          | 1129/? [26:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1129, loss 4.321497440338135\n",
      "Epoch 3: |          | 1130/? [26:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1130, loss 4.510128498077393\n",
      "Epoch 3: |          | 1131/? [26:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1131, loss 4.587425231933594\n",
      "Epoch 3: |          | 1132/? [27:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1132, loss 3.274386167526245\n",
      "Epoch 3: |          | 1133/? [27:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1133, loss 4.240348815917969\n",
      "Epoch 3: |          | 1134/? [27:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1134, loss 3.9698092937469482\n",
      "Epoch 3: |          | 1135/? [27:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1135, loss 4.643494129180908\n",
      "Epoch 3: |          | 1136/? [27:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1136, loss 4.260233402252197\n",
      "Epoch 3: |          | 1137/? [27:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1137, loss 4.283333778381348\n",
      "Epoch 3: |          | 1138/? [27:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1138, loss 4.854568004608154\n",
      "Epoch 3: |          | 1139/? [27:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1139, loss 5.025883197784424\n",
      "Epoch 3: |          | 1140/? [27:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1140, loss 4.259467601776123\n",
      "Epoch 3: |          | 1141/? [27:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1141, loss 4.600385665893555\n",
      "Epoch 3: |          | 1142/? [27:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1142, loss 4.703312397003174\n",
      "Epoch 3: |          | 1143/? [27:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1143, loss 4.722945690155029\n",
      "Epoch 3: |          | 1144/? [27:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1144, loss 4.186572074890137\n",
      "Epoch 3: |          | 1145/? [27:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1145, loss 4.261757850646973\n",
      "Epoch 3: |          | 1146/? [27:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1146, loss 4.035258769989014\n",
      "Epoch 3: |          | 1147/? [27:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1147, loss 3.7714133262634277\n",
      "Epoch 3: |          | 1148/? [27:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1148, loss 4.047735691070557\n",
      "Epoch 3: |          | 1149/? [27:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1149, loss 5.161972999572754\n",
      "Epoch 3: |          | 1150/? [27:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1150, loss 4.459563732147217\n",
      "Epoch 3: |          | 1151/? [27:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1151, loss 4.810816764831543\n",
      "Epoch 3: |          | 1152/? [27:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1152, loss 3.9515960216522217\n",
      "Epoch 3: |          | 1153/? [27:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1153, loss 4.323135852813721\n",
      "Epoch 3: |          | 1154/? [27:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1154, loss 3.998199462890625\n",
      "Epoch 3: |          | 1155/? [27:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1155, loss 4.212578773498535\n",
      "Epoch 3: |          | 1156/? [27:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1156, loss 4.3010687828063965\n",
      "Epoch 3: |          | 1157/? [27:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1157, loss 4.4866228103637695\n",
      "Epoch 3: |          | 1158/? [27:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1158, loss 4.7876362800598145\n",
      "Epoch 3: |          | 1159/? [27:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1159, loss 3.4319775104522705\n",
      "Epoch 3: |          | 1160/? [27:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1160, loss 4.695838451385498\n",
      "Epoch 3: |          | 1161/? [27:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1161, loss 4.5684285163879395\n",
      "Epoch 3: |          | 1162/? [27:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1162, loss 4.449473857879639\n",
      "Epoch 3: |          | 1163/? [27:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1163, loss 4.984893321990967\n",
      "Epoch 3: |          | 1164/? [27:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1164, loss 4.79514217376709\n",
      "Epoch 3: |          | 1165/? [27:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1165, loss 3.897696018218994\n",
      "Epoch 3: |          | 1166/? [27:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1166, loss 4.470607280731201\n",
      "Epoch 3: |          | 1167/? [27:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1167, loss 4.579183101654053\n",
      "Epoch 3: |          | 1168/? [27:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1168, loss 4.979084491729736\n",
      "Epoch 3: |          | 1169/? [27:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1169, loss 3.9643466472625732\n",
      "Epoch 3: |          | 1170/? [27:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1170, loss 4.578577995300293\n",
      "Epoch 3: |          | 1171/? [27:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1171, loss 3.96392560005188\n",
      "Epoch 3: |          | 1172/? [27:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1172, loss 3.783052921295166\n",
      "Epoch 3: |          | 1173/? [27:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1173, loss 4.465658664703369\n",
      "Epoch 3: |          | 1174/? [28:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1174, loss 3.880652666091919\n",
      "Epoch 3: |          | 1175/? [28:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1175, loss 4.5755414962768555\n",
      "Epoch 3: |          | 1176/? [28:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1176, loss 4.6221184730529785\n",
      "Epoch 3: |          | 1177/? [28:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1177, loss 4.744302749633789\n",
      "Epoch 3: |          | 1178/? [28:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1178, loss 4.117783546447754\n",
      "Epoch 3: |          | 1179/? [28:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1179, loss 4.6424384117126465\n",
      "Epoch 3: |          | 1180/? [28:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1180, loss 4.454282760620117\n",
      "Epoch 3: |          | 1181/? [28:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1181, loss 4.459437370300293\n",
      "Epoch 3: |          | 1182/? [28:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1182, loss 4.250824928283691\n",
      "Epoch 3: |          | 1183/? [28:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1183, loss 3.979874849319458\n",
      "Epoch 3: |          | 1184/? [28:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1184, loss 4.26276969909668\n",
      "Epoch 3: |          | 1185/? [28:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1185, loss 4.14918851852417\n",
      "Epoch 3: |          | 1186/? [28:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1186, loss 4.369165897369385\n",
      "Epoch 3: |          | 1187/? [28:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1187, loss 4.1973557472229\n",
      "Epoch 3: |          | 1188/? [28:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1188, loss 4.595737457275391\n",
      "Epoch 3: |          | 1189/? [28:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1189, loss 4.659419536590576\n",
      "Epoch 3: |          | 1190/? [28:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1190, loss 4.16578483581543\n",
      "Epoch 3: |          | 1191/? [28:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1191, loss 4.24582576751709\n",
      "Epoch 3: |          | 1192/? [28:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1192, loss 4.566598892211914\n",
      "Epoch 3: |          | 1193/? [28:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1193, loss 4.041001319885254\n",
      "Epoch 3: |          | 1194/? [28:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1194, loss 3.641676664352417\n",
      "Epoch 3: |          | 1195/? [28:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1195, loss 4.32348108291626\n",
      "Epoch 3: |          | 1196/? [28:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1196, loss 4.518169403076172\n",
      "Epoch 3: |          | 1197/? [28:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1197, loss 4.326714992523193\n",
      "Epoch 3: |          | 1198/? [28:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1198, loss 4.400561809539795\n",
      "Epoch 3: |          | 1199/? [28:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1199, loss 4.621909141540527\n",
      "Epoch 3: |          | 1200/? [28:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1200, loss 3.823850631713867\n",
      "Epoch 3: |          | 1201/? [28:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1201, loss 4.47835636138916\n",
      "Epoch 3: |          | 1202/? [28:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1202, loss 4.120406150817871\n",
      "Epoch 3: |          | 1203/? [28:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1203, loss 4.139959812164307\n",
      "Epoch 3: |          | 1204/? [28:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1204, loss 3.5814242362976074\n",
      "Epoch 3: |          | 1205/? [28:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1205, loss 4.2276177406311035\n",
      "Epoch 3: |          | 1206/? [28:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1206, loss 4.218111991882324\n",
      "Epoch 3: |          | 1207/? [28:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1207, loss 4.620588779449463\n",
      "Epoch 3: |          | 1208/? [28:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1208, loss 4.754994869232178\n",
      "Epoch 3: |          | 1209/? [28:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1209, loss 4.308608055114746\n",
      "Epoch 3: |          | 1210/? [28:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1210, loss 4.628526210784912\n",
      "Epoch 3: |          | 1211/? [28:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1211, loss 4.616656303405762\n",
      "Epoch 3: |          | 1212/? [28:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1212, loss 4.3886895179748535\n",
      "Epoch 3: |          | 1213/? [28:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1213, loss 4.08864688873291\n",
      "Epoch 3: |          | 1214/? [28:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1214, loss 4.797273635864258\n",
      "Epoch 3: |          | 1215/? [29:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1215, loss 4.083010673522949\n",
      "Epoch 3: |          | 1216/? [29:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1216, loss 4.319774627685547\n",
      "Epoch 3: |          | 1217/? [29:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1217, loss 4.480954647064209\n",
      "Epoch 3: |          | 1218/? [29:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1218, loss 4.473938941955566\n",
      "Epoch 3: |          | 1219/? [29:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1219, loss 4.104658126831055\n",
      "Epoch 3: |          | 1220/? [29:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1220, loss 4.841702938079834\n",
      "Epoch 3: |          | 1221/? [29:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1221, loss 4.459560394287109\n",
      "Epoch 3: |          | 1222/? [29:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1222, loss 3.4143059253692627\n",
      "Epoch 3: |          | 1223/? [29:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1223, loss 3.569200038909912\n",
      "Epoch 3: |          | 1224/? [29:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1224, loss 3.9714443683624268\n",
      "Epoch 3: |          | 1225/? [29:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1225, loss 4.672486305236816\n",
      "Epoch 3: |          | 1226/? [29:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1226, loss 4.693350315093994\n",
      "Epoch 3: |          | 1227/? [29:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1227, loss 4.266532897949219\n",
      "Epoch 3: |          | 1228/? [29:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1228, loss 4.192195415496826\n",
      "Epoch 3: |          | 1229/? [29:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1229, loss 3.749324083328247\n",
      "Epoch 3: |          | 1230/? [29:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1230, loss 4.442816734313965\n",
      "Epoch 3: |          | 1231/? [29:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1231, loss 4.489992141723633\n",
      "Epoch 3: |          | 1232/? [29:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1232, loss 4.648256778717041\n",
      "Epoch 3: |          | 1233/? [29:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1233, loss 4.44243860244751\n",
      "Epoch 3: |          | 1234/? [29:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1234, loss 3.4039435386657715\n",
      "Epoch 3: |          | 1235/? [29:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1235, loss 4.499100208282471\n",
      "Epoch 3: |          | 1236/? [29:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1236, loss 3.869466781616211\n",
      "Epoch 3: |          | 1237/? [29:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1237, loss 4.2359700202941895\n",
      "Epoch 3: |          | 1238/? [29:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1238, loss 4.197272777557373\n",
      "Epoch 3: |          | 1239/? [29:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1239, loss 4.096704006195068\n",
      "Epoch 3: |          | 1240/? [29:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1240, loss 4.7738776206970215\n",
      "Epoch 3: |          | 1241/? [29:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1241, loss 4.291357517242432\n",
      "Epoch 3: |          | 1242/? [29:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1242, loss 4.135995864868164\n",
      "Epoch 3: |          | 1243/? [29:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1243, loss 3.948481798171997\n",
      "Epoch 3: |          | 1244/? [29:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1244, loss 4.135109901428223\n",
      "Epoch 3: |          | 1245/? [29:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1245, loss 3.6989688873291016\n",
      "Epoch 3: |          | 1246/? [29:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1246, loss 4.44279670715332\n",
      "Epoch 3: |          | 1247/? [29:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1247, loss 4.4687933921813965\n",
      "Epoch 3: |          | 1248/? [29:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1248, loss 3.9540085792541504\n",
      "Epoch 3: |          | 1249/? [29:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1249, loss 4.143288612365723\n",
      "Epoch 3: |          | 1250/? [29:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1250, loss 4.30429744720459\n",
      "Epoch 3: |          | 1251/? [29:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1251, loss 4.008799076080322\n",
      "Epoch 3: |          | 1252/? [29:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1252, loss 4.825644016265869\n",
      "Epoch 3: |          | 1253/? [29:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1253, loss 4.193349838256836\n",
      "Epoch 3: |          | 1254/? [29:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1254, loss 3.453826427459717\n",
      "Epoch 3: |          | 1255/? [29:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1255, loss 4.924493312835693\n",
      "Epoch 3: |          | 1256/? [29:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1256, loss 3.879509449005127\n",
      "Epoch 3: |          | 1257/? [30:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1257, loss 3.9248600006103516\n",
      "Epoch 3: |          | 1258/? [30:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1258, loss 4.637351036071777\n",
      "Epoch 3: |          | 1259/? [30:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1259, loss 4.345196723937988\n",
      "Epoch 3: |          | 1260/? [30:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1260, loss 4.783608913421631\n",
      "Epoch 3: |          | 1261/? [30:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1261, loss 4.130130290985107\n",
      "Epoch 3: |          | 1262/? [30:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1262, loss 4.052441596984863\n",
      "Epoch 3: |          | 1263/? [30:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1263, loss 4.506944179534912\n",
      "Epoch 3: |          | 1264/? [30:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1264, loss 4.75333833694458\n",
      "Epoch 3: |          | 1265/? [30:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1265, loss 4.547985076904297\n",
      "Epoch 3: |          | 1266/? [30:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1266, loss 4.176714897155762\n",
      "Epoch 3: |          | 1267/? [30:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1267, loss 4.290081024169922\n",
      "Epoch 3: |          | 1268/? [30:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1268, loss 4.158629417419434\n",
      "Epoch 3: |          | 1269/? [30:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1269, loss 3.7199363708496094\n",
      "Epoch 3: |          | 1270/? [30:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1270, loss 4.067493915557861\n",
      "Epoch 3: |          | 1271/? [30:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1271, loss 4.259276866912842\n",
      "Epoch 3: |          | 1272/? [30:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1272, loss 3.7925307750701904\n",
      "Epoch 3: |          | 1273/? [30:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1273, loss 4.551599979400635\n",
      "Epoch 3: |          | 1274/? [30:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1274, loss 3.4010589122772217\n",
      "Epoch 3: |          | 1275/? [30:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1275, loss 3.94769287109375\n",
      "Epoch 3: |          | 1276/? [30:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1276, loss 4.3046979904174805\n",
      "Epoch 3: |          | 1277/? [30:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1277, loss 3.9307847023010254\n",
      "Epoch 3: |          | 1278/? [30:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1278, loss 3.7616965770721436\n",
      "Epoch 3: |          | 1279/? [30:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1279, loss 4.404424667358398\n",
      "Epoch 3: |          | 1280/? [30:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1280, loss 3.553244113922119\n",
      "Epoch 3: |          | 1281/? [30:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1281, loss 4.045147895812988\n",
      "Epoch 3: |          | 1282/? [30:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1282, loss 3.752598524093628\n",
      "Epoch 3: |          | 1283/? [30:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1283, loss 4.546610355377197\n",
      "Epoch 3: |          | 1284/? [30:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1284, loss 3.5789794921875\n",
      "Epoch 3: |          | 1285/? [30:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1285, loss 4.755017280578613\n",
      "Epoch 3: |          | 1286/? [30:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1286, loss 3.1528141498565674\n",
      "Epoch 3: |          | 1287/? [30:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1287, loss 4.604760646820068\n",
      "Epoch 3: |          | 1288/? [30:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1288, loss 4.423537731170654\n",
      "Epoch 3: |          | 1289/? [30:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1289, loss 3.427424192428589\n",
      "Epoch 3: |          | 1290/? [30:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1290, loss 4.3451457023620605\n",
      "Epoch 3: |          | 1291/? [30:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1291, loss 5.238453388214111\n",
      "Epoch 3: |          | 1292/? [30:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1292, loss 4.5715742111206055\n",
      "Epoch 3: |          | 1293/? [30:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1293, loss 4.006411075592041\n",
      "Epoch 3: |          | 1294/? [30:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1294, loss 4.253890037536621\n",
      "Epoch 3: |          | 1295/? [30:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1295, loss 4.329841613769531\n",
      "Epoch 3: |          | 1296/? [30:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1296, loss 3.5506787300109863\n",
      "Epoch 3: |          | 1297/? [30:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1297, loss 4.529324054718018\n",
      "Epoch 3: |          | 1298/? [31:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1298, loss 4.257801532745361\n",
      "Epoch 3: |          | 1299/? [31:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1299, loss 3.306452989578247\n",
      "Epoch 3: |          | 1300/? [31:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1300, loss 4.289588928222656\n",
      "Epoch 3: |          | 1301/? [31:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1301, loss 4.0078229904174805\n",
      "Epoch 3: |          | 1302/? [31:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1302, loss 4.155312538146973\n",
      "Epoch 3: |          | 1303/? [31:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1303, loss 4.006075859069824\n",
      "Epoch 3: |          | 1304/? [31:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1304, loss 4.828400611877441\n",
      "Epoch 3: |          | 1305/? [31:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1305, loss 3.601973295211792\n",
      "Epoch 3: |          | 1306/? [31:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1306, loss 4.212355613708496\n",
      "Epoch 3: |          | 1307/? [31:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1307, loss 3.780238628387451\n",
      "Epoch 3: |          | 1308/? [31:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1308, loss 3.734126329421997\n",
      "Epoch 3: |          | 1309/? [31:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1309, loss 3.8959896564483643\n",
      "Epoch 3: |          | 1310/? [31:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1310, loss 4.3844709396362305\n",
      "Epoch 3: |          | 1311/? [31:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1311, loss 3.8145058155059814\n",
      "Epoch 3: |          | 1312/? [31:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1312, loss 3.543400287628174\n",
      "Epoch 3: |          | 1313/? [31:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1313, loss 4.752026081085205\n",
      "Epoch 3: |          | 1314/? [31:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1314, loss 3.9033827781677246\n",
      "Epoch 3: |          | 1315/? [31:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1315, loss 4.712210655212402\n",
      "Epoch 3: |          | 1316/? [31:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1316, loss 4.4745097160339355\n",
      "Epoch 3: |          | 1317/? [31:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1317, loss 4.08505392074585\n",
      "Epoch 3: |          | 1318/? [31:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1318, loss 4.287073135375977\n",
      "Epoch 3: |          | 1319/? [31:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1319, loss 4.483376502990723\n",
      "Epoch 3: |          | 1320/? [31:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1320, loss 4.063226222991943\n",
      "Epoch 3: |          | 1321/? [31:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1321, loss 4.504283905029297\n",
      "Epoch 3: |          | 1322/? [31:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1322, loss 4.442446708679199\n",
      "Epoch 3: |          | 1323/? [31:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1323, loss 3.8338637351989746\n",
      "Epoch 3: |          | 1324/? [31:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1324, loss 4.842350959777832\n",
      "Epoch 3: |          | 1325/? [31:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1325, loss 4.957386493682861\n",
      "Epoch 3: |          | 1326/? [31:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1326, loss 4.353201866149902\n",
      "Epoch 3: |          | 1327/? [31:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1327, loss 4.19809627532959\n",
      "Epoch 3: |          | 1328/? [31:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1328, loss 3.9132046699523926\n",
      "Epoch 3: |          | 1329/? [31:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1329, loss 4.55695915222168\n",
      "Epoch 3: |          | 1330/? [31:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1330, loss 4.251389980316162\n",
      "Epoch 3: |          | 1331/? [31:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1331, loss 4.329623222351074\n",
      "Epoch 3: |          | 1332/? [31:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1332, loss 4.132069110870361\n",
      "Epoch 3: |          | 1333/? [31:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1333, loss 4.033841133117676\n",
      "Epoch 3: |          | 1334/? [31:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1334, loss 4.202694892883301\n",
      "Epoch 3: |          | 1335/? [31:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1335, loss 4.11630392074585\n",
      "Epoch 3: |          | 1336/? [31:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1336, loss 3.7510600090026855\n",
      "Epoch 3: |          | 1337/? [31:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1337, loss 4.396147727966309\n",
      "Epoch 3: |          | 1338/? [31:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1338, loss 3.5220539569854736\n",
      "Epoch 3: |          | 1339/? [31:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1339, loss 4.235334873199463\n",
      "Epoch 3: |          | 1340/? [32:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1340, loss 3.5954761505126953\n",
      "Epoch 3: |          | 1341/? [32:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1341, loss 4.40666389465332\n",
      "Epoch 3: |          | 1342/? [32:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1342, loss 4.615357398986816\n",
      "Epoch 3: |          | 1343/? [32:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1343, loss 4.122431755065918\n",
      "Epoch 3: |          | 1344/? [32:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1344, loss 4.212244033813477\n",
      "Epoch 3: |          | 1345/? [32:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1345, loss 4.341670036315918\n",
      "Epoch 3: |          | 1346/? [32:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1346, loss 5.587524890899658\n",
      "Epoch 3: |          | 1347/? [32:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1347, loss 4.524951934814453\n",
      "Epoch 3: |          | 1348/? [32:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1348, loss 4.701136589050293\n",
      "Epoch 3: |          | 1349/? [32:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1349, loss 4.448602199554443\n",
      "Epoch 3: |          | 1350/? [32:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1350, loss 4.721135139465332\n",
      "Epoch 3: |          | 1351/? [32:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1351, loss 4.520419120788574\n",
      "Epoch 3: |          | 1352/? [32:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1352, loss 3.6467442512512207\n",
      "Epoch 3: |          | 1353/? [32:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1353, loss 3.990737199783325\n",
      "Epoch 3: |          | 1354/? [32:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1354, loss 4.492035865783691\n",
      "Epoch 3: |          | 1355/? [32:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1355, loss 4.655030250549316\n",
      "Epoch 3: |          | 1356/? [32:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1356, loss 4.316143035888672\n",
      "Epoch 3: |          | 1357/? [32:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1357, loss 4.07102108001709\n",
      "Epoch 3: |          | 1358/? [32:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1358, loss 4.296704292297363\n",
      "Epoch 3: |          | 1359/? [32:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1359, loss 4.151823997497559\n",
      "Epoch 3: |          | 1360/? [32:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1360, loss 4.334427833557129\n",
      "Epoch 3: |          | 1361/? [32:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1361, loss 4.283564567565918\n",
      "Epoch 3: |          | 1362/? [32:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1362, loss 4.111162185668945\n",
      "Epoch 3: |          | 1363/? [32:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1363, loss 3.5340735912323\n",
      "Epoch 3: |          | 1364/? [32:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1364, loss 4.0808186531066895\n",
      "Epoch 3: |          | 1365/? [32:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1365, loss 3.7414894104003906\n",
      "Epoch 3: |          | 1366/? [32:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1366, loss 4.502274513244629\n",
      "Epoch 3: |          | 1367/? [32:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1367, loss 3.7688300609588623\n",
      "Epoch 3: |          | 1368/? [32:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1368, loss 3.570042371749878\n",
      "Epoch 3: |          | 1369/? [32:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1369, loss 4.3022847175598145\n",
      "Epoch 3: |          | 1370/? [32:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1370, loss 3.75708270072937\n",
      "Epoch 3: |          | 1371/? [32:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1371, loss 4.730545997619629\n",
      "Epoch 3: |          | 1372/? [32:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1372, loss 4.1138505935668945\n",
      "Epoch 3: |          | 1373/? [32:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1373, loss 4.589358329772949\n",
      "Epoch 3: |          | 1374/? [32:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1374, loss 3.6510977745056152\n",
      "Epoch 3: |          | 1375/? [32:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1375, loss 4.318737983703613\n",
      "Epoch 3: |          | 1376/? [32:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1376, loss 4.204215049743652\n",
      "Epoch 3: |          | 1377/? [32:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1377, loss 4.15781307220459\n",
      "Epoch 3: |          | 1378/? [32:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1378, loss 4.245203971862793\n",
      "Epoch 3: |          | 1379/? [32:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1379, loss 4.236398696899414\n",
      "Epoch 3: |          | 1380/? [32:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1380, loss 4.327003002166748\n",
      "Epoch 3: |          | 1381/? [32:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1381, loss 4.4524335861206055\n",
      "Epoch 3: |          | 1382/? [33:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1382, loss 3.9795620441436768\n",
      "Epoch 3: |          | 1383/? [33:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1383, loss 4.242335319519043\n",
      "Epoch 3: |          | 1384/? [33:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1384, loss 4.231209754943848\n",
      "Epoch 3: |          | 1385/? [33:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1385, loss 4.1626739501953125\n",
      "Epoch 3: |          | 1386/? [33:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1386, loss 4.204232215881348\n",
      "Epoch 3: |          | 1387/? [33:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1387, loss 4.200032711029053\n",
      "Epoch 3: |          | 1388/? [33:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1388, loss 3.618471622467041\n",
      "Epoch 3: |          | 1389/? [33:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1389, loss 4.457795143127441\n",
      "Epoch 3: |          | 1390/? [33:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1390, loss 4.739254951477051\n",
      "Epoch 3: |          | 1391/? [33:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1391, loss 4.368280410766602\n",
      "Epoch 3: |          | 1392/? [33:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1392, loss 3.843749523162842\n",
      "Epoch 3: |          | 1393/? [33:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1393, loss 4.03149938583374\n",
      "Epoch 3: |          | 1394/? [33:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1394, loss 3.730036973953247\n",
      "Epoch 3: |          | 1395/? [33:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1395, loss 4.349678993225098\n",
      "Epoch 3: |          | 1396/? [33:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1396, loss 4.29977560043335\n",
      "Epoch 3: |          | 1397/? [33:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1397, loss 3.4255051612854004\n",
      "Epoch 3: |          | 1398/? [33:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1398, loss 4.685352325439453\n",
      "Epoch 3: |          | 1399/? [33:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1399, loss 4.751546382904053\n",
      "Epoch 3: |          | 1400/? [33:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1400, loss 3.7537314891815186\n",
      "Epoch 3: |          | 1401/? [33:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1401, loss 4.641170978546143\n",
      "Epoch 3: |          | 1402/? [33:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1402, loss 4.175509452819824\n",
      "Epoch 3: |          | 1403/? [33:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1403, loss 4.389174461364746\n",
      "Epoch 3: |          | 1404/? [33:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1404, loss 4.396349906921387\n",
      "Epoch 3: |          | 1405/? [33:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1405, loss 4.718789100646973\n",
      "Epoch 3: |          | 1406/? [33:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1406, loss 4.600849151611328\n",
      "Epoch 3: |          | 1407/? [33:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1407, loss 4.763826847076416\n",
      "Epoch 3: |          | 1408/? [33:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1408, loss 3.913302183151245\n",
      "Epoch 3: |          | 1409/? [33:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1409, loss 3.994325637817383\n",
      "Epoch 3: |          | 1410/? [33:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1410, loss 4.031686782836914\n",
      "Epoch 3: |          | 1411/? [33:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1411, loss 4.423283576965332\n",
      "Epoch 3: |          | 1412/? [33:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1412, loss 3.961019992828369\n",
      "Epoch 3: |          | 1413/? [33:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1413, loss 3.85772705078125\n",
      "Epoch 3: |          | 1414/? [33:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1414, loss 3.948558807373047\n",
      "Epoch 3: |          | 1415/? [33:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1415, loss 4.28220272064209\n",
      "Epoch 3: |          | 1416/? [33:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1416, loss 4.675442695617676\n",
      "Epoch 3: |          | 1417/? [33:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1417, loss 4.250139236450195\n",
      "Epoch 3: |          | 1418/? [33:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1418, loss 4.462597370147705\n",
      "Epoch 3: |          | 1419/? [33:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1419, loss 4.202000617980957\n",
      "Epoch 3: |          | 1420/? [33:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1420, loss 4.049350738525391\n",
      "Epoch 3: |          | 1421/? [33:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1421, loss 3.7174274921417236\n",
      "Epoch 3: |          | 1422/? [34:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1422, loss 4.540843963623047\n",
      "Epoch 3: |          | 1423/? [34:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1423, loss 4.575207710266113\n",
      "Epoch 3: |          | 1424/? [34:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1424, loss 4.164927959442139\n",
      "Epoch 3: |          | 1425/? [34:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1425, loss 4.429335117340088\n",
      "Epoch 3: |          | 1426/? [34:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1426, loss 3.8969969749450684\n",
      "Epoch 3: |          | 1427/? [34:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1427, loss 4.5563883781433105\n",
      "Epoch 3: |          | 1428/? [34:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1428, loss 4.527420997619629\n",
      "Epoch 3: |          | 1429/? [34:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1429, loss 4.447815895080566\n",
      "Epoch 3: |          | 1430/? [34:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1430, loss 4.5112409591674805\n",
      "Epoch 3: |          | 1431/? [34:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1431, loss 4.26956033706665\n",
      "Epoch 3: |          | 1432/? [34:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1432, loss 4.2947797775268555\n",
      "Epoch 3: |          | 1433/? [34:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1433, loss 4.153448581695557\n",
      "Epoch 3: |          | 1434/? [34:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1434, loss 4.343278884887695\n",
      "Epoch 3: |          | 1435/? [34:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1435, loss 3.905907392501831\n",
      "Epoch 3: |          | 1436/? [34:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1436, loss 4.283315181732178\n",
      "Epoch 3: |          | 1437/? [34:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1437, loss 3.5173840522766113\n",
      "Epoch 3: |          | 1438/? [34:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1438, loss 5.2562103271484375\n",
      "Epoch 3: |          | 1439/? [34:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1439, loss 4.404414176940918\n",
      "Epoch 3: |          | 1440/? [34:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1440, loss 4.4505839347839355\n",
      "Epoch 3: |          | 1441/? [34:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1441, loss 4.796448230743408\n",
      "Epoch 3: |          | 1442/? [34:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1442, loss 4.81204891204834\n",
      "Epoch 3: |          | 1443/? [34:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1443, loss 3.9020836353302\n",
      "Epoch 3: |          | 1444/? [34:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1444, loss 3.9792773723602295\n",
      "Epoch 3: |          | 1445/? [34:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1445, loss 4.5183820724487305\n",
      "Epoch 3: |          | 1446/? [34:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1446, loss 4.108349323272705\n",
      "Epoch 3: |          | 1447/? [34:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1447, loss 4.214329719543457\n",
      "Epoch 3: |          | 1448/? [34:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1448, loss 4.012877941131592\n",
      "Epoch 3: |          | 1449/? [34:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1449, loss 4.2989912033081055\n",
      "Epoch 3: |          | 1450/? [34:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1450, loss 4.374456405639648\n",
      "Epoch 3: |          | 1451/? [34:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1451, loss 4.697752475738525\n",
      "Epoch 3: |          | 1452/? [34:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1452, loss 4.319789409637451\n",
      "Epoch 3: |          | 1453/? [34:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1453, loss 3.6108450889587402\n",
      "Epoch 3: |          | 1454/? [34:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1454, loss 4.218960762023926\n",
      "Epoch 3: |          | 1455/? [34:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1455, loss 4.349827766418457\n",
      "Epoch 3: |          | 1456/? [34:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1456, loss 3.8975605964660645\n",
      "Epoch 3: |          | 1457/? [34:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1457, loss 4.071150779724121\n",
      "Epoch 3: |          | 1458/? [34:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1458, loss 4.235970497131348\n",
      "Epoch 3: |          | 1459/? [34:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1459, loss 4.460923194885254\n",
      "Epoch 3: |          | 1460/? [34:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1460, loss 4.320502758026123\n",
      "Epoch 3: |          | 1461/? [34:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1461, loss 4.320281982421875\n",
      "Epoch 3: |          | 1462/? [34:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1462, loss 4.62954044342041\n",
      "Epoch 3: |          | 1463/? [34:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1463, loss 4.548532009124756\n",
      "Epoch 3: |          | 1464/? [35:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1464, loss 3.8560187816619873\n",
      "Epoch 3: |          | 1465/? [35:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1465, loss 4.23096227645874\n",
      "Epoch 3: |          | 1466/? [35:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1466, loss 3.8695836067199707\n",
      "Epoch 3: |          | 1467/? [35:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1467, loss 4.565650463104248\n",
      "Epoch 3: |          | 1468/? [35:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1468, loss 4.147200107574463\n",
      "Epoch 3: |          | 1469/? [35:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1469, loss 3.798466205596924\n",
      "Epoch 3: |          | 1470/? [35:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1470, loss 4.404077529907227\n",
      "Epoch 3: |          | 1471/? [35:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1471, loss 4.604416370391846\n",
      "Epoch 3: |          | 1472/? [35:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1472, loss 4.327193260192871\n",
      "Epoch 3: |          | 1473/? [35:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1473, loss 4.1468400955200195\n",
      "Epoch 3: |          | 1474/? [35:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1474, loss 4.042761325836182\n",
      "Epoch 3: |          | 1475/? [35:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1475, loss 3.665515184402466\n",
      "Epoch 3: |          | 1476/? [35:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1476, loss 4.267655372619629\n",
      "Epoch 3: |          | 1477/? [35:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1477, loss 4.302264213562012\n",
      "Epoch 3: |          | 1478/? [35:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1478, loss 4.150208950042725\n",
      "Epoch 3: |          | 1479/? [35:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1479, loss 4.7336201667785645\n",
      "Epoch 3: |          | 1480/? [35:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1480, loss 4.423576354980469\n",
      "Epoch 3: |          | 1481/? [35:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1481, loss 4.19863224029541\n",
      "Epoch 3: |          | 1482/? [35:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1482, loss 4.308161735534668\n",
      "Epoch 3: |          | 1483/? [35:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1483, loss 3.9325127601623535\n",
      "Epoch 3: |          | 1484/? [35:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1484, loss 4.138051986694336\n",
      "Epoch 3: |          | 1485/? [35:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1485, loss 4.318385124206543\n",
      "Epoch 3: |          | 1486/? [35:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1486, loss 4.25684928894043\n",
      "Epoch 3: |          | 1487/? [35:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1487, loss 3.7144999504089355\n",
      "Epoch 3: |          | 1488/? [35:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1488, loss 4.414970874786377\n",
      "Epoch 3: |          | 1489/? [35:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1489, loss 4.3593668937683105\n",
      "Epoch 3: |          | 1490/? [35:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1490, loss 4.330597400665283\n",
      "Epoch 3: |          | 1491/? [35:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1491, loss 3.1553537845611572\n",
      "Epoch 3: |          | 1492/? [35:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1492, loss 3.7693698406219482\n",
      "Epoch 3: |          | 1493/? [35:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1493, loss 3.4474682807922363\n",
      "Epoch 3: |          | 1494/? [35:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1494, loss 4.180785179138184\n",
      "Epoch 3: |          | 1495/? [35:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1495, loss 4.079499244689941\n",
      "Epoch 3: |          | 1496/? [35:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1496, loss 4.436343193054199\n",
      "Epoch 3: |          | 1497/? [35:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1497, loss 3.605602264404297\n",
      "Epoch 3: |          | 1498/? [35:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1498, loss 3.9420864582061768\n",
      "Epoch 3: |          | 1499/? [35:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1499, loss 4.537612438201904\n",
      "Epoch 3: |          | 1500/? [35:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1500, loss 4.448081970214844\n",
      "Epoch 3: |          | 1501/? [35:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1501, loss 4.257505416870117\n",
      "Epoch 3: |          | 1502/? [35:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1502, loss 4.349508762359619\n",
      "Epoch 3: |          | 1503/? [35:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1503, loss 4.06990385055542\n",
      "Epoch 3: |          | 1504/? [35:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1504, loss 4.709610939025879\n",
      "Epoch 3: |          | 1505/? [36:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1505, loss 4.596718788146973\n",
      "Epoch 3: |          | 1506/? [36:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1506, loss 4.154871940612793\n",
      "Epoch 3: |          | 1507/? [36:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1507, loss 4.003381252288818\n",
      "Epoch 3: |          | 1508/? [36:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1508, loss 4.199886322021484\n",
      "Epoch 3: |          | 1509/? [36:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1509, loss 4.16654109954834\n",
      "Epoch 3: |          | 1510/? [36:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1510, loss 4.450407981872559\n",
      "Epoch 3: |          | 1511/? [36:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1511, loss 3.9850239753723145\n",
      "Epoch 3: |          | 1512/? [36:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1512, loss 4.7137675285339355\n",
      "Epoch 3: |          | 1513/? [36:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1513, loss 4.826601028442383\n",
      "Epoch 3: |          | 1514/? [36:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1514, loss 3.8928210735321045\n",
      "Epoch 3: |          | 1515/? [36:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1515, loss 4.842210292816162\n",
      "Epoch 3: |          | 1516/? [36:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1516, loss 4.632632255554199\n",
      "Epoch 3: |          | 1517/? [36:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1517, loss 4.0987868309021\n",
      "Epoch 3: |          | 1518/? [36:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1518, loss 3.89253568649292\n",
      "Epoch 3: |          | 1519/? [36:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1519, loss 4.464513301849365\n",
      "Epoch 3: |          | 1520/? [36:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1520, loss 4.747206687927246\n",
      "Epoch 3: |          | 1521/? [36:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1521, loss 4.227331161499023\n",
      "Epoch 3: |          | 1522/? [36:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1522, loss 3.9640021324157715\n",
      "Epoch 3: |          | 1523/? [36:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1523, loss 4.332249641418457\n",
      "Epoch 3: |          | 1524/? [36:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1524, loss 4.23920202255249\n",
      "Epoch 3: |          | 1525/? [36:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1525, loss 3.9899725914001465\n",
      "Epoch 3: |          | 1526/? [36:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1526, loss 4.504283428192139\n",
      "Epoch 3: |          | 1527/? [36:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1527, loss 4.511546611785889\n",
      "Epoch 3: |          | 1528/? [36:32<00:00,  0.70it/s, v_num=31]ERROR: Input has inproper shape\n",
      "Epoch 3: |          | 1529/? [36:32<00:00,  0.70it/s, v_num=31]   VALIDATION: Batch 0, loss 4.778925895690918\n",
      "   VALIDATION: Batch 1, loss 3.7348670959472656\n",
      "   VALIDATION: Batch 2, loss 4.96677303314209\n",
      "   VALIDATION: Batch 3, loss 4.602985382080078\n",
      "   VALIDATION: Batch 4, loss 4.198085784912109\n",
      "   VALIDATION: Batch 5, loss 3.836078643798828\n",
      "   VALIDATION: Batch 6, loss 4.142146587371826\n",
      "   VALIDATION: Batch 7, loss 4.811150550842285\n",
      "   VALIDATION: Batch 8, loss 4.657164096832275\n",
      "   VALIDATION: Batch 9, loss 4.782332420349121\n",
      "   VALIDATION: Batch 10, loss 4.484169960021973\n",
      "   VALIDATION: Batch 11, loss 4.128580093383789\n",
      "   VALIDATION: Batch 12, loss 4.36201286315918\n",
      "   VALIDATION: Batch 13, loss 4.850314140319824\n",
      "   VALIDATION: Batch 14, loss 4.23240852355957\n",
      "   VALIDATION: Batch 15, loss 4.109224319458008\n",
      "   VALIDATION: Batch 16, loss 4.774582862854004\n",
      "   VALIDATION: Batch 17, loss 4.398996353149414\n",
      "   VALIDATION: Batch 18, loss 3.6814606189727783\n",
      "   VALIDATION: Batch 19, loss 4.605859279632568\n",
      "   VALIDATION: Batch 20, loss 4.866515636444092\n",
      "   VALIDATION: Batch 21, loss 5.0995917320251465\n",
      "   VALIDATION: Batch 22, loss 4.742856979370117\n",
      "   VALIDATION: Batch 23, loss 4.274610996246338\n",
      "   VALIDATION: Batch 24, loss 4.109251499176025\n",
      "   VALIDATION: Batch 25, loss 4.549102306365967\n",
      "   VALIDATION: Batch 26, loss 4.733795642852783\n",
      "   VALIDATION: Batch 27, loss 4.608149528503418\n",
      "   VALIDATION: Batch 28, loss 4.363131999969482\n",
      "   VALIDATION: Batch 29, loss 4.620377063751221\n",
      "   VALIDATION: Batch 30, loss 4.203120231628418\n",
      "   VALIDATION: Batch 31, loss 4.5073323249816895\n",
      "   VALIDATION: Batch 32, loss 5.1070098876953125\n",
      "   VALIDATION: Batch 33, loss 3.275115489959717\n",
      "   VALIDATION: Batch 34, loss 4.454913139343262\n",
      "   VALIDATION: Batch 35, loss 4.698368072509766\n",
      "   VALIDATION: Batch 36, loss 4.091556549072266\n",
      "   VALIDATION: Batch 37, loss 3.9984798431396484\n",
      "   VALIDATION: Batch 38, loss 4.088835716247559\n",
      "   VALIDATION: Batch 39, loss 4.479569435119629\n",
      "   VALIDATION: Batch 40, loss 4.559149742126465\n",
      "   VALIDATION: Batch 41, loss 3.3742358684539795\n",
      "   VALIDATION: Batch 42, loss 4.647111415863037\n",
      "   VALIDATION: Batch 43, loss 4.71010684967041\n",
      "   VALIDATION: Batch 44, loss 4.282103538513184\n",
      "   VALIDATION: Batch 45, loss 4.729190826416016\n",
      "   VALIDATION: Batch 46, loss 3.8759989738464355\n",
      "   VALIDATION: Batch 47, loss 4.851868629455566\n",
      "   VALIDATION: Batch 48, loss 4.934445381164551\n",
      "   VALIDATION: Batch 49, loss 4.620935916900635\n",
      "   VALIDATION: Batch 50, loss 4.547661781311035\n",
      "   VALIDATION: Batch 51, loss 5.030072212219238\n",
      "   VALIDATION: Batch 52, loss 4.181807994842529\n",
      "   VALIDATION: Batch 53, loss 4.048959732055664\n",
      "   VALIDATION: Batch 54, loss 4.13694429397583\n",
      "   VALIDATION: Batch 55, loss 4.895798683166504\n",
      "   VALIDATION: Batch 56, loss 4.332169532775879\n",
      "   VALIDATION: Batch 57, loss 5.7312750816345215\n",
      "   VALIDATION: Batch 58, loss 4.415189266204834\n",
      "   VALIDATION: Batch 59, loss 4.0482916831970215\n",
      "   VALIDATION: Batch 60, loss 3.602752685546875\n",
      "   VALIDATION: Batch 61, loss 4.452977180480957\n",
      "   VALIDATION: Batch 62, loss 4.422868728637695\n",
      "   VALIDATION: Batch 63, loss 4.898333549499512\n",
      "   VALIDATION: Batch 64, loss 4.7323455810546875\n",
      "   VALIDATION: Batch 65, loss 3.9052467346191406\n",
      "   VALIDATION: Batch 66, loss 4.8285064697265625\n",
      "   VALIDATION: Batch 67, loss 4.231259822845459\n",
      "   VALIDATION: Batch 68, loss 4.408815860748291\n",
      "   VALIDATION: Batch 69, loss 4.65450382232666\n",
      "   VALIDATION: Batch 70, loss 4.83713960647583\n",
      "   VALIDATION: Batch 71, loss 4.311119079589844\n",
      "   VALIDATION: Batch 72, loss 5.192032337188721\n",
      "   VALIDATION: Batch 73, loss 4.038990497589111\n",
      "   VALIDATION: Batch 74, loss 4.637815475463867\n",
      "   VALIDATION: Batch 75, loss 4.659346580505371\n",
      "   VALIDATION: Batch 76, loss 4.516225337982178\n",
      "   VALIDATION: Batch 77, loss 4.744897365570068\n",
      "   VALIDATION: Batch 78, loss 4.559001445770264\n",
      "   VALIDATION: Batch 79, loss 4.522507667541504\n",
      "   VALIDATION: Batch 80, loss 4.606349468231201\n",
      "   VALIDATION: Batch 81, loss 4.351240634918213\n",
      "   VALIDATION: Batch 82, loss 4.6949462890625\n",
      "   VALIDATION: Batch 83, loss 3.996967315673828\n",
      "   VALIDATION: Batch 84, loss 4.6792707443237305\n",
      "   VALIDATION: Batch 85, loss 4.309616565704346\n",
      "   VALIDATION: Batch 86, loss 4.403672218322754\n",
      "   VALIDATION: Batch 87, loss 4.245522975921631\n",
      "   VALIDATION: Batch 88, loss 3.8505024909973145\n",
      "   VALIDATION: Batch 89, loss 4.182578086853027\n",
      "   VALIDATION: Batch 90, loss 4.39594841003418\n",
      "   VALIDATION: Batch 91, loss 4.539446830749512\n",
      "   VALIDATION: Batch 92, loss 4.216553688049316\n",
      "   VALIDATION: Batch 93, loss 4.916832447052002\n",
      "   VALIDATION: Batch 94, loss 4.416240692138672\n",
      "   VALIDATION: Batch 95, loss 3.8837552070617676\n",
      "   VALIDATION: Batch 96, loss 4.340937614440918\n",
      "   VALIDATION: Batch 97, loss 4.10565185546875\n",
      "   VALIDATION: Batch 98, loss 4.6530070304870605\n",
      "   VALIDATION: Batch 99, loss 4.773684978485107\n",
      "   VALIDATION: Batch 100, loss 5.090881824493408\n",
      "   VALIDATION: Batch 101, loss 3.691404342651367\n",
      "   VALIDATION: Batch 102, loss 5.106297492980957\n",
      "   VALIDATION: Batch 103, loss 5.043728828430176\n",
      "   VALIDATION: Batch 104, loss 3.9986557960510254\n",
      "   VALIDATION: Batch 105, loss 4.50380277633667\n",
      "   VALIDATION: Batch 106, loss 4.320728778839111\n",
      "   VALIDATION: Batch 107, loss 4.463250637054443\n",
      "   VALIDATION: Batch 108, loss 4.163685321807861\n",
      "   VALIDATION: Batch 109, loss 4.789733409881592\n",
      "   VALIDATION: Batch 110, loss 4.501351356506348\n",
      "   VALIDATION: Batch 111, loss 4.81471586227417\n",
      "   VALIDATION: Batch 112, loss 5.592970848083496\n",
      "   VALIDATION: Batch 113, loss 4.972686290740967\n",
      "   VALIDATION: Batch 114, loss 4.730679512023926\n",
      "   VALIDATION: Batch 115, loss 4.191740989685059\n",
      "   VALIDATION: Batch 116, loss 4.034538745880127\n",
      "   VALIDATION: Batch 117, loss 4.702394962310791\n",
      "   VALIDATION: Batch 118, loss 4.894344329833984\n",
      "   VALIDATION: Batch 119, loss 4.060795783996582\n",
      "   VALIDATION: Batch 120, loss 3.6525750160217285\n",
      "   VALIDATION: Batch 121, loss 3.9539971351623535\n",
      "   VALIDATION: Batch 122, loss 4.347115516662598\n",
      "   VALIDATION: Batch 123, loss 4.434045314788818\n",
      "   VALIDATION: Batch 124, loss 3.7294185161590576\n",
      "   VALIDATION: Batch 125, loss 4.4103593826293945\n",
      "   VALIDATION: Batch 126, loss 4.593118190765381\n",
      "   VALIDATION: Batch 127, loss 4.3999247550964355\n",
      "   VALIDATION: Batch 128, loss 4.530036449432373\n",
      "   VALIDATION: Batch 129, loss 4.162083625793457\n",
      "   VALIDATION: Batch 130, loss 3.8116111755371094\n",
      "   VALIDATION: Batch 131, loss 3.78798246383667\n",
      "   VALIDATION: Batch 132, loss 4.445651531219482\n",
      "   VALIDATION: Batch 133, loss 4.632028102874756\n",
      "   VALIDATION: Batch 134, loss 4.544318199157715\n",
      "   VALIDATION: Batch 135, loss 4.760523796081543\n",
      "   VALIDATION: Batch 136, loss 4.882384300231934\n",
      "   VALIDATION: Batch 137, loss 4.7177534103393555\n",
      "   VALIDATION: Batch 138, loss 4.430168151855469\n",
      "   VALIDATION: Batch 139, loss 4.81505823135376\n",
      "   VALIDATION: Batch 140, loss 3.900369167327881\n",
      "   VALIDATION: Batch 141, loss 4.840551376342773\n",
      "   VALIDATION: Batch 142, loss 3.567476272583008\n",
      "   VALIDATION: Batch 143, loss 4.3849382400512695\n",
      "   VALIDATION: Batch 144, loss 4.644683837890625\n",
      "   VALIDATION: Batch 145, loss 4.449204444885254\n",
      "   VALIDATION: Batch 146, loss 4.2469162940979\n",
      "   VALIDATION: Batch 147, loss 4.60547399520874\n",
      "   VALIDATION: Batch 148, loss 4.688908576965332\n",
      "   VALIDATION: Batch 149, loss 5.167281150817871\n",
      "   VALIDATION: Batch 150, loss 4.749555587768555\n",
      "   VALIDATION: Batch 151, loss 5.017804145812988\n",
      "   VALIDATION: Batch 152, loss 4.387422561645508\n",
      "   VALIDATION: Batch 153, loss 4.651484489440918\n",
      "   VALIDATION: Batch 154, loss 4.492363929748535\n",
      "   VALIDATION: Batch 155, loss 4.212250709533691\n",
      "   VALIDATION: Batch 156, loss 4.899832725524902\n",
      "   VALIDATION: Batch 157, loss 4.6083550453186035\n",
      "   VALIDATION: Batch 158, loss 3.977379322052002\n",
      "   VALIDATION: Batch 159, loss 4.4482316970825195\n",
      "   VALIDATION: Batch 160, loss 4.799038887023926\n",
      "   VALIDATION: Batch 161, loss 5.015515327453613\n",
      "   VALIDATION: Batch 162, loss 4.493973731994629\n",
      "   VALIDATION: Batch 163, loss 3.8967368602752686\n",
      "   VALIDATION: Batch 164, loss 4.408253192901611\n",
      "   VALIDATION: Batch 165, loss 4.836488723754883\n",
      "   VALIDATION: Batch 166, loss 4.301512241363525\n",
      "   VALIDATION: Batch 167, loss 4.7619500160217285\n",
      "   VALIDATION: Batch 168, loss 3.5776724815368652\n",
      "   VALIDATION: Batch 169, loss 4.247673511505127\n",
      "   VALIDATION: Batch 170, loss 4.470574855804443\n",
      "   VALIDATION: Batch 171, loss 4.620279312133789\n",
      "   VALIDATION: Batch 172, loss 4.4808783531188965\n",
      "   VALIDATION: Batch 173, loss 4.445960998535156\n",
      "   VALIDATION: Batch 174, loss 4.810570240020752\n",
      "   VALIDATION: Batch 175, loss 4.521871089935303\n",
      "   VALIDATION: Batch 176, loss 4.317975044250488\n",
      "   VALIDATION: Batch 177, loss 4.316592216491699\n",
      "   VALIDATION: Batch 178, loss 5.3192267417907715\n",
      "   VALIDATION: Batch 179, loss 4.627690315246582\n",
      "   VALIDATION: Batch 180, loss 4.131096363067627\n",
      "   VALIDATION: Batch 181, loss 4.3011674880981445\n",
      "   VALIDATION: Batch 182, loss 4.583625793457031\n",
      "   VALIDATION: Batch 183, loss 3.6506409645080566\n",
      "   VALIDATION: Batch 184, loss 3.416929244995117\n",
      "   VALIDATION: Batch 185, loss 4.177477836608887\n",
      "   VALIDATION: Batch 186, loss 4.081376075744629\n",
      "   VALIDATION: Batch 187, loss 4.291138648986816\n",
      "   VALIDATION: Batch 188, loss 4.671663761138916\n",
      "   VALIDATION: Batch 189, loss 4.02337646484375\n",
      "   VALIDATION: Batch 190, loss 4.067936897277832\n",
      "   VALIDATION: Batch 191, loss 4.597651481628418\n",
      "   VALIDATION: Batch 192, loss 4.968636512756348\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=31]              TRRAINING: Batch 0, loss 4.453548908233643\n",
      "Epoch 4: |          | 1/? [00:01<00:00,  0.57it/s, v_num=31]   TRRAINING: Batch 1, loss 3.9830727577209473\n",
      "Epoch 4: |          | 2/? [00:03<00:00,  0.63it/s, v_num=31]   TRRAINING: Batch 2, loss 4.094411373138428\n",
      "Epoch 4: |          | 3/? [00:04<00:00,  0.64it/s, v_num=31]   TRRAINING: Batch 3, loss 3.7510523796081543\n",
      "Epoch 4: |          | 4/? [00:06<00:00,  0.65it/s, v_num=31]   TRRAINING: Batch 4, loss 4.184926509857178\n",
      "Epoch 4: |          | 5/? [00:07<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 5, loss 4.993127346038818\n",
      "Epoch 4: |          | 6/? [00:08<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 6, loss 4.580225467681885\n",
      "Epoch 4: |          | 7/? [00:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 7, loss 3.8661930561065674\n",
      "Epoch 4: |          | 8/? [00:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 8, loss 3.93884539604187\n",
      "Epoch 4: |          | 9/? [00:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 9, loss 4.266141414642334\n",
      "Epoch 4: |          | 10/? [00:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 10, loss 4.495898246765137\n",
      "Epoch 4: |          | 11/? [00:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 11, loss 4.426249980926514\n",
      "Epoch 4: |          | 12/? [00:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 12, loss 5.252875328063965\n",
      "Epoch 4: |          | 13/? [00:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 13, loss 4.302985668182373\n",
      "Epoch 4: |          | 14/? [00:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 14, loss 4.518879413604736\n",
      "Epoch 4: |          | 15/? [00:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 15, loss 3.771608352661133\n",
      "Epoch 4: |          | 16/? [00:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 16, loss 3.561993360519409\n",
      "Epoch 4: |          | 17/? [00:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 17, loss 4.82021427154541\n",
      "Epoch 4: |          | 18/? [00:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 18, loss 4.257939338684082\n",
      "Epoch 4: |          | 19/? [00:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 19, loss 4.046743869781494\n",
      "Epoch 4: |          | 20/? [00:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 20, loss 4.353804588317871\n",
      "Epoch 4: |          | 21/? [00:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 21, loss 4.487030982971191\n",
      "Epoch 4: |          | 22/? [00:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 22, loss 4.359771728515625\n",
      "Epoch 4: |          | 23/? [00:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 23, loss 3.7166056632995605\n",
      "Epoch 4: |          | 24/? [00:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 24, loss 4.378472328186035\n",
      "Epoch 4: |          | 25/? [00:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 25, loss 4.234651565551758\n",
      "Epoch 4: |          | 26/? [00:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 26, loss 3.987290859222412\n",
      "Epoch 4: |          | 27/? [00:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 27, loss 3.8818371295928955\n",
      "Epoch 4: |          | 28/? [00:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 28, loss 4.812993049621582\n",
      "Epoch 4: |          | 29/? [00:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 29, loss 4.321592807769775\n",
      "Epoch 4: |          | 30/? [00:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 30, loss 4.195533275604248\n",
      "Epoch 4: |          | 31/? [00:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 31, loss 4.840130805969238\n",
      "Epoch 4: |          | 32/? [00:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 32, loss 4.321188449859619\n",
      "Epoch 4: |          | 33/? [00:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 33, loss 4.164687156677246\n",
      "Epoch 4: |          | 34/? [00:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 34, loss 4.0918097496032715\n",
      "Epoch 4: |          | 35/? [00:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 35, loss 3.5032799243927\n",
      "Epoch 4: |          | 36/? [00:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 36, loss 4.394865989685059\n",
      "Epoch 4: |          | 37/? [00:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 37, loss 4.55539608001709\n",
      "Epoch 4: |          | 38/? [00:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 38, loss 4.859712600708008\n",
      "Epoch 4: |          | 39/? [00:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 39, loss 4.735787868499756\n",
      "Epoch 4: |          | 40/? [00:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 40, loss 4.147146224975586\n",
      "Epoch 4: |          | 41/? [00:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 41, loss 4.163922309875488\n",
      "Epoch 4: |          | 42/? [01:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 42, loss 3.959824323654175\n",
      "Epoch 4: |          | 43/? [01:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 43, loss 4.124090194702148\n",
      "Epoch 4: |          | 44/? [01:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 44, loss 3.3641135692596436\n",
      "Epoch 4: |          | 45/? [01:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 45, loss 3.1024606227874756\n",
      "Epoch 4: |          | 46/? [01:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 46, loss 4.771546363830566\n",
      "Epoch 4: |          | 47/? [01:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 47, loss 3.9452157020568848\n",
      "Epoch 4: |          | 48/? [01:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 48, loss 3.7818641662597656\n",
      "Epoch 4: |          | 49/? [01:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 49, loss 4.2543559074401855\n",
      "Epoch 4: |          | 50/? [01:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 50, loss 4.118550777435303\n",
      "Epoch 4: |          | 51/? [01:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 51, loss 4.126789093017578\n",
      "Epoch 4: |          | 52/? [01:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 52, loss 4.744359493255615\n",
      "Epoch 4: |          | 53/? [01:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 53, loss 4.365042209625244\n",
      "Epoch 4: |          | 54/? [01:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 54, loss 4.174592018127441\n",
      "Epoch 4: |          | 55/? [01:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 55, loss 4.26206111907959\n",
      "Epoch 4: |          | 56/? [01:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 56, loss 4.378554344177246\n",
      "Epoch 4: |          | 57/? [01:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 57, loss 4.284728050231934\n",
      "Epoch 4: |          | 58/? [01:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 58, loss 5.525930404663086\n",
      "Epoch 4: |          | 59/? [01:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 59, loss 4.45327091217041\n",
      "Epoch 4: |          | 60/? [01:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 60, loss 4.565110206604004\n",
      "Epoch 4: |          | 61/? [01:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 61, loss 4.568404197692871\n",
      "Epoch 4: |          | 62/? [01:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 62, loss 4.136144161224365\n",
      "Epoch 4: |          | 63/? [01:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 63, loss 4.363266468048096\n",
      "Epoch 4: |          | 64/? [01:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 64, loss 4.186427116394043\n",
      "Epoch 4: |          | 65/? [01:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 65, loss 4.232961177825928\n",
      "Epoch 4: |          | 66/? [01:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 66, loss 3.6835989952087402\n",
      "Epoch 4: |          | 67/? [01:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 67, loss 4.329937934875488\n",
      "Epoch 4: |          | 68/? [01:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 68, loss 4.425318717956543\n",
      "Epoch 4: |          | 69/? [01:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 69, loss 4.2634968757629395\n",
      "Epoch 4: |          | 70/? [01:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 70, loss 3.917261838912964\n",
      "Epoch 4: |          | 71/? [01:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 71, loss 3.870882749557495\n",
      "Epoch 4: |          | 72/? [01:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 72, loss 4.205362319946289\n",
      "Epoch 4: |          | 73/? [01:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 73, loss 4.3780059814453125\n",
      "Epoch 4: |          | 74/? [01:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 74, loss 4.035773277282715\n",
      "Epoch 4: |          | 75/? [01:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 75, loss 4.172801971435547\n",
      "Epoch 4: |          | 76/? [01:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 76, loss 4.178734302520752\n",
      "Epoch 4: |          | 77/? [01:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 77, loss 4.210363864898682\n",
      "Epoch 4: |          | 78/? [01:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 78, loss 3.9800140857696533\n",
      "Epoch 4: |          | 79/? [01:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 79, loss 4.195578098297119\n",
      "Epoch 4: |          | 80/? [01:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 80, loss 4.121128559112549\n",
      "Epoch 4: |          | 81/? [01:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 81, loss 3.6196224689483643\n",
      "Epoch 4: |          | 82/? [01:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 82, loss 4.448398590087891\n",
      "Epoch 4: |          | 83/? [01:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 83, loss 3.809157133102417\n",
      "Epoch 4: |          | 84/? [02:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 84, loss 3.6827874183654785\n",
      "Epoch 4: |          | 85/? [02:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 85, loss 3.588111400604248\n",
      "Epoch 4: |          | 86/? [02:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 86, loss 3.6941440105438232\n",
      "Epoch 4: |          | 87/? [02:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 87, loss 3.8774497509002686\n",
      "Epoch 4: |          | 88/? [02:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 88, loss 4.643771171569824\n",
      "Epoch 4: |          | 89/? [02:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 89, loss 4.539032936096191\n",
      "Epoch 4: |          | 90/? [02:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 90, loss 4.348972320556641\n",
      "Epoch 4: |          | 91/? [02:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 91, loss 4.072052478790283\n",
      "Epoch 4: |          | 92/? [02:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 92, loss 4.51668643951416\n",
      "Epoch 4: |          | 93/? [02:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 93, loss 4.613614082336426\n",
      "Epoch 4: |          | 94/? [02:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 94, loss 4.493615627288818\n",
      "Epoch 4: |          | 95/? [02:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 95, loss 4.855493068695068\n",
      "Epoch 4: |          | 96/? [02:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 96, loss 4.003443717956543\n",
      "Epoch 4: |          | 97/? [02:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 97, loss 3.820523738861084\n",
      "Epoch 4: |          | 98/? [02:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 98, loss 4.341303825378418\n",
      "Epoch 4: |          | 99/? [02:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 99, loss 4.527728080749512\n",
      "Epoch 4: |          | 100/? [02:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 100, loss 4.514803409576416\n",
      "Epoch 4: |          | 101/? [02:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 101, loss 4.147303104400635\n",
      "Epoch 4: |          | 102/? [02:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 102, loss 4.154297828674316\n",
      "Epoch 4: |          | 103/? [02:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 103, loss 3.9133734703063965\n",
      "Epoch 4: |          | 104/? [02:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 104, loss 4.317011833190918\n",
      "Epoch 4: |          | 105/? [02:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 105, loss 4.186128616333008\n",
      "Epoch 4: |          | 106/? [02:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 106, loss 4.320004940032959\n",
      "Epoch 4: |          | 107/? [02:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 107, loss 4.389036655426025\n",
      "Epoch 4: |          | 108/? [02:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 108, loss 4.339045524597168\n",
      "Epoch 4: |          | 109/? [02:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 109, loss 3.99131441116333\n",
      "Epoch 4: |          | 110/? [02:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 110, loss 4.276337623596191\n",
      "Epoch 4: |          | 111/? [02:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 111, loss 4.907503604888916\n",
      "Epoch 4: |          | 112/? [02:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 112, loss 3.5907607078552246\n",
      "Epoch 4: |          | 113/? [02:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 113, loss 3.1406774520874023\n",
      "Epoch 4: |          | 114/? [02:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 114, loss 4.483333587646484\n",
      "Epoch 4: |          | 115/? [02:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 115, loss 4.6151123046875\n",
      "Epoch 4: |          | 116/? [02:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 116, loss 3.778136730194092\n",
      "Epoch 4: |          | 117/? [02:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 117, loss 3.737229585647583\n",
      "Epoch 4: |          | 118/? [02:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 118, loss 4.558804035186768\n",
      "Epoch 4: |          | 119/? [02:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 119, loss 4.768074035644531\n",
      "Epoch 4: |          | 120/? [02:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 120, loss 4.519424915313721\n",
      "Epoch 4: |          | 121/? [02:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 121, loss 4.268357276916504\n",
      "Epoch 4: |          | 122/? [02:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 122, loss 3.6880288124084473\n",
      "Epoch 4: |          | 123/? [02:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 123, loss 4.179536819458008\n",
      "Epoch 4: |          | 124/? [02:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 124, loss 4.400064468383789\n",
      "Epoch 4: |          | 125/? [02:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 125, loss 4.027586460113525\n",
      "Epoch 4: |          | 126/? [03:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 126, loss 4.49131441116333\n",
      "Epoch 4: |          | 127/? [03:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 127, loss 4.586350440979004\n",
      "Epoch 4: |          | 128/? [03:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 128, loss 3.660506010055542\n",
      "Epoch 4: |          | 129/? [03:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 129, loss 4.323874473571777\n",
      "Epoch 4: |          | 130/? [03:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 130, loss 3.3617382049560547\n",
      "Epoch 4: |          | 131/? [03:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 131, loss 4.233523368835449\n",
      "Epoch 4: |          | 132/? [03:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 132, loss 4.230291843414307\n",
      "Epoch 4: |          | 133/? [03:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 133, loss 4.171577453613281\n",
      "Epoch 4: |          | 134/? [03:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 134, loss 4.336701393127441\n",
      "Epoch 4: |          | 135/? [03:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 135, loss 4.460465431213379\n",
      "Epoch 4: |          | 136/? [03:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 136, loss 4.466152667999268\n",
      "Epoch 4: |          | 137/? [03:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 137, loss 3.4205336570739746\n",
      "Epoch 4: |          | 138/? [03:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 138, loss 4.064640998840332\n",
      "Epoch 4: |          | 139/? [03:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 139, loss 4.579249382019043\n",
      "Epoch 4: |          | 140/? [03:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 140, loss 3.711341381072998\n",
      "Epoch 4: |          | 141/? [03:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 141, loss 3.82391357421875\n",
      "Epoch 4: |          | 142/? [03:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 142, loss 5.4188714027404785\n",
      "Epoch 4: |          | 143/? [03:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 143, loss 5.045400619506836\n",
      "Epoch 4: |          | 144/? [03:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 144, loss 4.197087287902832\n",
      "Epoch 4: |          | 145/? [03:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 145, loss 3.69628643989563\n",
      "Epoch 4: |          | 146/? [03:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 146, loss 3.88970685005188\n",
      "Epoch 4: |          | 147/? [03:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 147, loss 4.221526145935059\n",
      "Epoch 4: |          | 148/? [03:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 148, loss 3.9588654041290283\n",
      "Epoch 4: |          | 149/? [03:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 149, loss 3.473733425140381\n",
      "Epoch 4: |          | 150/? [03:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 150, loss 4.351871490478516\n",
      "Epoch 4: |          | 151/? [03:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 151, loss 4.451151371002197\n",
      "Epoch 4: |          | 152/? [03:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 152, loss 4.434411525726318\n",
      "Epoch 4: |          | 153/? [03:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 153, loss 3.507155656814575\n",
      "Epoch 4: |          | 154/? [03:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 154, loss 4.765074729919434\n",
      "Epoch 4: |          | 155/? [03:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 155, loss 4.286344528198242\n",
      "Epoch 4: |          | 156/? [03:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 156, loss 3.6166253089904785\n",
      "Epoch 4: |          | 157/? [03:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 157, loss 4.341094017028809\n",
      "Epoch 4: |          | 158/? [03:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 158, loss 4.42575216293335\n",
      "Epoch 4: |          | 159/? [03:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 159, loss 4.153550148010254\n",
      "Epoch 4: |          | 160/? [03:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 160, loss 3.8925139904022217\n",
      "Epoch 4: |          | 161/? [03:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 161, loss 4.303946018218994\n",
      "Epoch 4: |          | 162/? [03:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 162, loss 4.348086833953857\n",
      "Epoch 4: |          | 163/? [03:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 163, loss 3.4293034076690674\n",
      "Epoch 4: |          | 164/? [03:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 164, loss 3.88904070854187\n",
      "Epoch 4: |          | 165/? [03:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 165, loss 4.6951799392700195\n",
      "Epoch 4: |          | 166/? [03:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 166, loss 4.409084320068359\n",
      "Epoch 4: |          | 167/? [03:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 167, loss 4.479206562042236\n",
      "Epoch 4: |          | 168/? [03:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 168, loss 4.070357799530029\n",
      "Epoch 4: |          | 169/? [04:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 169, loss 3.721980333328247\n",
      "Epoch 4: |          | 170/? [04:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 170, loss 3.940779447555542\n",
      "Epoch 4: |          | 171/? [04:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 171, loss 4.298431396484375\n",
      "Epoch 4: |          | 172/? [04:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 172, loss 4.082351207733154\n",
      "Epoch 4: |          | 173/? [04:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 173, loss 4.784256458282471\n",
      "Epoch 4: |          | 174/? [04:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 174, loss 4.529706001281738\n",
      "Epoch 4: |          | 175/? [04:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 175, loss 4.8544769287109375\n",
      "Epoch 4: |          | 176/? [04:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 176, loss 3.996497631072998\n",
      "Epoch 4: |          | 177/? [04:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 177, loss 4.042918682098389\n",
      "Epoch 4: |          | 178/? [04:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 178, loss 3.8802967071533203\n",
      "Epoch 4: |          | 179/? [04:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 179, loss 4.609700679779053\n",
      "Epoch 4: |          | 180/? [04:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 180, loss 4.125144004821777\n",
      "Epoch 4: |          | 181/? [04:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 181, loss 3.9064133167266846\n",
      "Epoch 4: |          | 182/? [04:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 182, loss 4.3309407234191895\n",
      "Epoch 4: |          | 183/? [04:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 183, loss 3.80433988571167\n",
      "Epoch 4: |          | 184/? [04:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 184, loss 3.9867827892303467\n",
      "Epoch 4: |          | 185/? [04:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 185, loss 4.619050025939941\n",
      "Epoch 4: |          | 186/? [04:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 186, loss 4.05792236328125\n",
      "Epoch 4: |          | 187/? [04:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 187, loss 4.57033634185791\n",
      "Epoch 4: |          | 188/? [04:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 188, loss 3.967531681060791\n",
      "Epoch 4: |          | 189/? [04:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 189, loss 4.608303070068359\n",
      "Epoch 4: |          | 190/? [04:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 190, loss 4.1143646240234375\n",
      "Epoch 4: |          | 191/? [04:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 191, loss 4.912925720214844\n",
      "Epoch 4: |          | 192/? [04:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 192, loss 4.70062255859375\n",
      "Epoch 4: |          | 193/? [04:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 193, loss 3.9682564735412598\n",
      "Epoch 4: |          | 194/? [04:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 194, loss 3.866112470626831\n",
      "Epoch 4: |          | 195/? [04:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 195, loss 4.5736846923828125\n",
      "Epoch 4: |          | 196/? [04:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 196, loss 4.443161964416504\n",
      "Epoch 4: |          | 197/? [04:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 197, loss 4.320404529571533\n",
      "Epoch 4: |          | 198/? [04:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 198, loss 3.630826234817505\n",
      "Epoch 4: |          | 199/? [04:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 199, loss 4.496053695678711\n",
      "Epoch 4: |          | 200/? [04:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 200, loss 4.01491641998291\n",
      "Epoch 4: |          | 201/? [04:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 201, loss 4.346373081207275\n",
      "Epoch 4: |          | 202/? [04:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 202, loss 4.393276214599609\n",
      "Epoch 4: |          | 203/? [04:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 203, loss 4.114087104797363\n",
      "Epoch 4: |          | 204/? [04:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 204, loss 4.189333438873291\n",
      "Epoch 4: |          | 205/? [04:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 205, loss 3.989109754562378\n",
      "Epoch 4: |          | 206/? [04:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 206, loss 3.808715343475342\n",
      "Epoch 4: |          | 207/? [04:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 207, loss 4.356495380401611\n",
      "Epoch 4: |          | 208/? [04:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 208, loss 4.277451515197754\n",
      "Epoch 4: |          | 209/? [04:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 209, loss 4.109169006347656\n",
      "Epoch 4: |          | 210/? [04:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 210, loss 4.780306339263916\n",
      "Epoch 4: |          | 211/? [05:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 211, loss 4.071457862854004\n",
      "Epoch 4: |          | 212/? [05:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 212, loss 4.347025394439697\n",
      "Epoch 4: |          | 213/? [05:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 213, loss 4.180515766143799\n",
      "Epoch 4: |          | 214/? [05:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 214, loss 4.055588245391846\n",
      "Epoch 4: |          | 215/? [05:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 215, loss 3.7170112133026123\n",
      "Epoch 4: |          | 216/? [05:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 216, loss 4.420366287231445\n",
      "Epoch 4: |          | 217/? [05:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 217, loss 4.291283130645752\n",
      "Epoch 4: |          | 218/? [05:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 218, loss 4.324366569519043\n",
      "Epoch 4: |          | 219/? [05:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 219, loss 4.232278347015381\n",
      "Epoch 4: |          | 220/? [05:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 220, loss 4.334467887878418\n",
      "Epoch 4: |          | 221/? [05:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 221, loss 4.149837017059326\n",
      "Epoch 4: |          | 222/? [05:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 222, loss 3.3721656799316406\n",
      "Epoch 4: |          | 223/? [05:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 223, loss 4.517428398132324\n",
      "Epoch 4: |          | 224/? [05:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 224, loss 4.493516445159912\n",
      "Epoch 4: |          | 225/? [05:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 225, loss 4.224258899688721\n",
      "Epoch 4: |          | 226/? [05:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 226, loss 4.130352020263672\n",
      "Epoch 4: |          | 227/? [05:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 227, loss 4.503069877624512\n",
      "Epoch 4: |          | 228/? [05:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 228, loss 4.18458890914917\n",
      "Epoch 4: |          | 229/? [05:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 229, loss 4.296669960021973\n",
      "Epoch 4: |          | 230/? [05:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 230, loss 4.193993091583252\n",
      "Epoch 4: |          | 231/? [05:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 231, loss 4.123957633972168\n",
      "Epoch 4: |          | 232/? [05:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 232, loss 3.9034526348114014\n",
      "Epoch 4: |          | 233/? [05:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 233, loss 4.5829644203186035\n",
      "Epoch 4: |          | 234/? [05:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 234, loss 4.615975379943848\n",
      "Epoch 4: |          | 235/? [05:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 235, loss 4.6272501945495605\n",
      "Epoch 4: |          | 236/? [05:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 236, loss 4.079941272735596\n",
      "Epoch 4: |          | 237/? [05:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 237, loss 4.261220455169678\n",
      "Epoch 4: |          | 238/? [05:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 238, loss 4.508275032043457\n",
      "Epoch 4: |          | 239/? [05:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 239, loss 4.074425220489502\n",
      "Epoch 4: |          | 240/? [05:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 240, loss 3.5517287254333496\n",
      "Epoch 4: |          | 241/? [05:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 241, loss 4.296126365661621\n",
      "Epoch 4: |          | 242/? [05:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 242, loss 4.600424766540527\n",
      "Epoch 4: |          | 243/? [05:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 243, loss 3.4083385467529297\n",
      "Epoch 4: |          | 244/? [05:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 244, loss 3.877117872238159\n",
      "Epoch 4: |          | 245/? [05:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 245, loss 4.228671550750732\n",
      "Epoch 4: |          | 246/? [05:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 246, loss 4.355927467346191\n",
      "Epoch 4: |          | 247/? [05:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 247, loss 4.381466865539551\n",
      "Epoch 4: |          | 248/? [05:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 248, loss 3.871530532836914\n",
      "Epoch 4: |          | 249/? [05:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 249, loss 3.6863009929656982\n",
      "Epoch 4: |          | 250/? [05:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 250, loss 4.302053451538086\n",
      "Epoch 4: |          | 251/? [05:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 251, loss 4.320182800292969\n",
      "Epoch 4: |          | 252/? [06:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 252, loss 4.084949970245361\n",
      "Epoch 4: |          | 253/? [06:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 253, loss 4.910100936889648\n",
      "Epoch 4: |          | 254/? [06:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 254, loss 4.626380920410156\n",
      "Epoch 4: |          | 255/? [06:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 255, loss 4.185260772705078\n",
      "Epoch 4: |          | 256/? [06:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 256, loss 5.500002384185791\n",
      "Epoch 4: |          | 257/? [06:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 257, loss 4.059497356414795\n",
      "Epoch 4: |          | 258/? [06:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 258, loss 4.162009239196777\n",
      "Epoch 4: |          | 259/? [06:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 259, loss 4.018245220184326\n",
      "Epoch 4: |          | 260/? [06:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 260, loss 3.9656474590301514\n",
      "Epoch 4: |          | 261/? [06:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 261, loss 3.8542938232421875\n",
      "Epoch 4: |          | 262/? [06:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 262, loss 4.482527256011963\n",
      "Epoch 4: |          | 263/? [06:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 263, loss 4.151591777801514\n",
      "Epoch 4: |          | 264/? [06:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 264, loss 4.301459312438965\n",
      "Epoch 4: |          | 265/? [06:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 265, loss 3.6969666481018066\n",
      "Epoch 4: |          | 266/? [06:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 266, loss 4.18049430847168\n",
      "Epoch 4: |          | 267/? [06:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 267, loss 3.7599449157714844\n",
      "Epoch 4: |          | 268/? [06:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 268, loss 4.129492282867432\n",
      "Epoch 4: |          | 269/? [06:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 269, loss 4.525488376617432\n",
      "Epoch 4: |          | 270/? [06:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 270, loss 4.105855941772461\n",
      "Epoch 4: |          | 271/? [06:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 271, loss 4.48767614364624\n",
      "Epoch 4: |          | 272/? [06:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 272, loss 4.691788196563721\n",
      "Epoch 4: |          | 273/? [06:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 273, loss 3.851332187652588\n",
      "Epoch 4: |          | 274/? [06:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 274, loss 4.78240966796875\n",
      "Epoch 4: |          | 275/? [06:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 275, loss 4.327980995178223\n",
      "Epoch 4: |          | 276/? [06:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 276, loss 3.602893114089966\n",
      "Epoch 4: |          | 277/? [06:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 277, loss 4.30275821685791\n",
      "Epoch 4: |          | 278/? [06:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 278, loss 3.283693790435791\n",
      "Epoch 4: |          | 279/? [06:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 279, loss 4.022612571716309\n",
      "Epoch 4: |          | 280/? [06:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 280, loss 3.707197666168213\n",
      "Epoch 4: |          | 281/? [06:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 281, loss 4.4326066970825195\n",
      "Epoch 4: |          | 282/? [06:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 282, loss 3.979815721511841\n",
      "Epoch 4: |          | 283/? [06:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 283, loss 4.093990802764893\n",
      "Epoch 4: |          | 284/? [06:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 284, loss 3.9950079917907715\n",
      "Epoch 4: |          | 285/? [06:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 285, loss 3.4483726024627686\n",
      "Epoch 4: |          | 286/? [06:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 286, loss 4.046656131744385\n",
      "Epoch 4: |          | 287/? [06:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 287, loss 3.816237211227417\n",
      "Epoch 4: |          | 288/? [06:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 288, loss 3.774195909500122\n",
      "Epoch 4: |          | 289/? [06:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 289, loss 3.6269469261169434\n",
      "Epoch 4: |          | 290/? [06:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 290, loss 3.1079611778259277\n",
      "Epoch 4: |          | 291/? [06:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 291, loss 4.2874908447265625\n",
      "Epoch 4: |          | 292/? [06:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 292, loss 3.916098117828369\n",
      "Epoch 4: |          | 293/? [06:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 293, loss 4.21430778503418\n",
      "Epoch 4: |          | 294/? [07:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 294, loss 4.078649520874023\n",
      "Epoch 4: |          | 295/? [07:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 295, loss 4.423907279968262\n",
      "Epoch 4: |          | 296/? [07:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 296, loss 3.842883348464966\n",
      "Epoch 4: |          | 297/? [07:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 297, loss 4.576842308044434\n",
      "Epoch 4: |          | 298/? [07:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 298, loss 4.3894362449646\n",
      "Epoch 4: |          | 299/? [07:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 299, loss 4.9088134765625\n",
      "Epoch 4: |          | 300/? [07:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 300, loss 4.176974296569824\n",
      "Epoch 4: |          | 301/? [07:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 301, loss 3.996537685394287\n",
      "Epoch 4: |          | 302/? [07:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 302, loss 4.4412407875061035\n",
      "Epoch 4: |          | 303/? [07:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 303, loss 4.211974143981934\n",
      "Epoch 4: |          | 304/? [07:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 304, loss 4.43077278137207\n",
      "Epoch 4: |          | 305/? [07:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 305, loss 4.464093208312988\n",
      "Epoch 4: |          | 306/? [07:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 306, loss 4.1056928634643555\n",
      "Epoch 4: |          | 307/? [07:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 307, loss 4.351891994476318\n",
      "Epoch 4: |          | 308/? [07:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 308, loss 4.505418300628662\n",
      "Epoch 4: |          | 309/? [07:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 309, loss 4.156041622161865\n",
      "Epoch 4: |          | 310/? [07:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 310, loss 4.5958733558654785\n",
      "Epoch 4: |          | 311/? [07:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 311, loss 4.163439750671387\n",
      "Epoch 4: |          | 312/? [07:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 312, loss 4.186768531799316\n",
      "Epoch 4: |          | 313/? [07:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 313, loss 3.9202983379364014\n",
      "Epoch 4: |          | 314/? [07:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 314, loss 4.301360130310059\n",
      "Epoch 4: |          | 315/? [07:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 315, loss 3.8974571228027344\n",
      "Epoch 4: |          | 316/? [07:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 316, loss 4.522871494293213\n",
      "Epoch 4: |          | 317/? [07:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 317, loss 4.314333915710449\n",
      "Epoch 4: |          | 318/? [07:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 318, loss 4.466897010803223\n",
      "Epoch 4: |          | 319/? [07:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 319, loss 3.666320323944092\n",
      "Epoch 4: |          | 320/? [07:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 320, loss 4.149120330810547\n",
      "Epoch 4: |          | 321/? [07:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 321, loss 3.9471282958984375\n",
      "Epoch 4: |          | 322/? [07:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 322, loss 4.604020118713379\n",
      "Epoch 4: |          | 323/? [07:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 323, loss 4.616555213928223\n",
      "Epoch 4: |          | 324/? [07:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 324, loss 4.259666919708252\n",
      "Epoch 4: |          | 325/? [07:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 325, loss 4.7272539138793945\n",
      "Epoch 4: |          | 326/? [07:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 326, loss 4.266856670379639\n",
      "Epoch 4: |          | 327/? [07:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 327, loss 4.003087043762207\n",
      "Epoch 4: |          | 328/? [07:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 328, loss 3.7394022941589355\n",
      "Epoch 4: |          | 329/? [07:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 329, loss 4.396923065185547\n",
      "Epoch 4: |          | 330/? [07:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 330, loss 4.8666181564331055\n",
      "Epoch 4: |          | 331/? [07:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 331, loss 3.0944156646728516\n",
      "Epoch 4: |          | 332/? [07:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 332, loss 4.1882548332214355\n",
      "Epoch 4: |          | 333/? [07:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 333, loss 4.049404144287109\n",
      "Epoch 4: |          | 334/? [07:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 334, loss 4.838473796844482\n",
      "Epoch 4: |          | 335/? [07:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 335, loss 4.762682914733887\n",
      "Epoch 4: |          | 336/? [08:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 336, loss 4.627881050109863\n",
      "Epoch 4: |          | 337/? [08:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 337, loss 5.295299530029297\n",
      "Epoch 4: |          | 338/? [08:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 338, loss 4.9693684577941895\n",
      "Epoch 4: |          | 339/? [08:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 339, loss 3.9526782035827637\n",
      "Epoch 4: |          | 340/? [08:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 340, loss 3.88399076461792\n",
      "Epoch 4: |          | 341/? [08:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 341, loss 3.616945266723633\n",
      "Epoch 4: |          | 342/? [08:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 342, loss 4.318281173706055\n",
      "Epoch 4: |          | 343/? [08:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 343, loss 3.9961235523223877\n",
      "Epoch 4: |          | 344/? [08:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 344, loss 4.855099678039551\n",
      "Epoch 4: |          | 345/? [08:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 345, loss 4.0506815910339355\n",
      "Epoch 4: |          | 346/? [08:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 346, loss 4.324912071228027\n",
      "Epoch 4: |          | 347/? [08:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 347, loss 4.069037437438965\n",
      "Epoch 4: |          | 348/? [08:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 348, loss 3.4281322956085205\n",
      "Epoch 4: |          | 349/? [08:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 349, loss 3.2510478496551514\n",
      "Epoch 4: |          | 350/? [08:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 350, loss 4.594322204589844\n",
      "Epoch 4: |          | 351/? [08:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 351, loss 4.6143951416015625\n",
      "Epoch 4: |          | 352/? [08:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 352, loss 3.869553804397583\n",
      "Epoch 4: |          | 353/? [08:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 353, loss 3.6080641746520996\n",
      "Epoch 4: |          | 354/? [08:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 354, loss 4.034896373748779\n",
      "Epoch 4: |          | 355/? [08:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 355, loss 4.338067531585693\n",
      "Epoch 4: |          | 356/? [08:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 356, loss 4.3703155517578125\n",
      "Epoch 4: |          | 357/? [08:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 357, loss 3.884913921356201\n",
      "Epoch 4: |          | 358/? [08:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 358, loss 3.821516513824463\n",
      "Epoch 4: |          | 359/? [08:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 359, loss 4.430178165435791\n",
      "Epoch 4: |          | 360/? [08:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 360, loss 4.007453441619873\n",
      "Epoch 4: |          | 361/? [08:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 361, loss 4.111418724060059\n",
      "Epoch 4: |          | 362/? [08:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 362, loss 3.9274895191192627\n",
      "Epoch 4: |          | 363/? [08:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 363, loss 3.829766035079956\n",
      "Epoch 4: |          | 364/? [08:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 364, loss 4.452593803405762\n",
      "Epoch 4: |          | 365/? [08:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 365, loss 4.495469093322754\n",
      "Epoch 4: |          | 366/? [08:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 366, loss 4.224238872528076\n",
      "Epoch 4: |          | 367/? [08:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 367, loss 4.227961540222168\n",
      "Epoch 4: |          | 368/? [08:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 368, loss 3.7280807495117188\n",
      "Epoch 4: |          | 369/? [08:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 369, loss 4.081254005432129\n",
      "Epoch 4: |          | 370/? [08:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 370, loss 3.6970772743225098\n",
      "Epoch 4: |          | 371/? [08:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 371, loss 4.681514263153076\n",
      "Epoch 4: |          | 372/? [08:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 372, loss 4.090533256530762\n",
      "Epoch 4: |          | 373/? [08:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 373, loss 4.3341383934021\n",
      "Epoch 4: |          | 374/? [08:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 374, loss 4.007077217102051\n",
      "Epoch 4: |          | 375/? [08:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 375, loss 4.7043914794921875\n",
      "Epoch 4: |          | 376/? [08:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 376, loss 4.072239875793457\n",
      "Epoch 4: |          | 377/? [08:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 377, loss 4.2871599197387695\n",
      "Epoch 4: |          | 378/? [09:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 378, loss 4.438973903656006\n",
      "Epoch 4: |          | 379/? [09:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 379, loss 4.276669502258301\n",
      "Epoch 4: |          | 380/? [09:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 380, loss 4.339885234832764\n",
      "Epoch 4: |          | 381/? [09:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 381, loss 4.3477349281311035\n",
      "Epoch 4: |          | 382/? [09:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 382, loss 4.074679374694824\n",
      "Epoch 4: |          | 383/? [09:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 383, loss 4.119454383850098\n",
      "Epoch 4: |          | 384/? [09:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 384, loss 4.612489700317383\n",
      "Epoch 4: |          | 385/? [09:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 385, loss 4.229946136474609\n",
      "Epoch 4: |          | 386/? [09:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 386, loss 3.1272354125976562\n",
      "Epoch 4: |          | 387/? [09:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 387, loss 4.005224227905273\n",
      "Epoch 4: |          | 388/? [09:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 388, loss 4.05031681060791\n",
      "Epoch 4: |          | 389/? [09:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 389, loss 4.566653251647949\n",
      "Epoch 4: |          | 390/? [09:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 390, loss 3.9968364238739014\n",
      "Epoch 4: |          | 391/? [09:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 391, loss 4.435960292816162\n",
      "Epoch 4: |          | 392/? [09:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 392, loss 4.489914894104004\n",
      "Epoch 4: |          | 393/? [09:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 393, loss 4.552210330963135\n",
      "Epoch 4: |          | 394/? [09:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 394, loss 4.18333625793457\n",
      "Epoch 4: |          | 395/? [09:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 395, loss 4.431190013885498\n",
      "Epoch 4: |          | 396/? [09:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 396, loss 4.350214958190918\n",
      "Epoch 4: |          | 397/? [09:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 397, loss 4.161876201629639\n",
      "Epoch 4: |          | 398/? [09:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 398, loss 4.041296005249023\n",
      "Epoch 4: |          | 399/? [09:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 399, loss 4.128505706787109\n",
      "Epoch 4: |          | 400/? [09:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 400, loss 4.086949348449707\n",
      "Epoch 4: |          | 401/? [09:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 401, loss 4.018065452575684\n",
      "Epoch 4: |          | 402/? [09:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 402, loss 4.452484607696533\n",
      "Epoch 4: |          | 403/? [09:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 403, loss 4.323716640472412\n",
      "Epoch 4: |          | 404/? [09:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 404, loss 3.8461081981658936\n",
      "Epoch 4: |          | 405/? [09:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 405, loss 3.854478120803833\n",
      "Epoch 4: |          | 406/? [09:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 406, loss 4.2008442878723145\n",
      "Epoch 4: |          | 407/? [09:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 407, loss 4.129724025726318\n",
      "Epoch 4: |          | 408/? [09:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 408, loss 4.534183025360107\n",
      "Epoch 4: |          | 409/? [09:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 409, loss 4.504258155822754\n",
      "Epoch 4: |          | 410/? [09:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 410, loss 4.194165229797363\n",
      "Epoch 4: |          | 411/? [09:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 411, loss 4.025140285491943\n",
      "Epoch 4: |          | 412/? [09:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 412, loss 3.537923812866211\n",
      "Epoch 4: |          | 413/? [09:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 413, loss 4.314277172088623\n",
      "Epoch 4: |          | 414/? [09:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 414, loss 3.883690595626831\n",
      "Epoch 4: |          | 415/? [09:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 415, loss 4.328324317932129\n",
      "Epoch 4: |          | 416/? [09:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 416, loss 4.755675315856934\n",
      "Epoch 4: |          | 417/? [09:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 417, loss 4.831233978271484\n",
      "Epoch 4: |          | 418/? [10:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 418, loss 4.393458366394043\n",
      "Epoch 4: |          | 419/? [10:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 419, loss 4.224681854248047\n",
      "Epoch 4: |          | 420/? [10:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 420, loss 4.309564113616943\n",
      "Epoch 4: |          | 421/? [10:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 421, loss 4.800048351287842\n",
      "Epoch 4: |          | 422/? [10:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 422, loss 4.313431739807129\n",
      "Epoch 4: |          | 423/? [10:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 423, loss 3.905363082885742\n",
      "Epoch 4: |          | 424/? [10:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 424, loss 4.593101978302002\n",
      "Epoch 4: |          | 425/? [10:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 425, loss 4.324648380279541\n",
      "Epoch 4: |          | 426/? [10:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 426, loss 3.9465503692626953\n",
      "Epoch 4: |          | 427/? [10:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 427, loss 4.013604640960693\n",
      "Epoch 4: |          | 428/? [10:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 428, loss 4.810366630554199\n",
      "Epoch 4: |          | 429/? [10:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 429, loss 3.672589063644409\n",
      "Epoch 4: |          | 430/? [10:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 430, loss 4.323437213897705\n",
      "Epoch 4: |          | 431/? [10:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 431, loss 4.190369606018066\n",
      "Epoch 4: |          | 432/? [10:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 432, loss 4.329597473144531\n",
      "Epoch 4: |          | 433/? [10:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 433, loss 4.304553031921387\n",
      "Epoch 4: |          | 434/? [10:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 434, loss 4.197409152984619\n",
      "Epoch 4: |          | 435/? [10:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 435, loss 3.875164747238159\n",
      "Epoch 4: |          | 436/? [10:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 436, loss 4.280867576599121\n",
      "Epoch 4: |          | 437/? [10:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 437, loss 4.422350883483887\n",
      "Epoch 4: |          | 438/? [10:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 438, loss 4.068638801574707\n",
      "Epoch 4: |          | 439/? [10:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 439, loss 4.026515483856201\n",
      "Epoch 4: |          | 440/? [10:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 440, loss 3.994856357574463\n",
      "Epoch 4: |          | 441/? [10:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 441, loss 4.343031406402588\n",
      "Epoch 4: |          | 442/? [10:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 442, loss 4.072318077087402\n",
      "Epoch 4: |          | 443/? [10:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 443, loss 4.250585556030273\n",
      "Epoch 4: |          | 444/? [10:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 444, loss 4.194910526275635\n",
      "Epoch 4: |          | 445/? [10:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 445, loss 5.193133354187012\n",
      "Epoch 4: |          | 446/? [10:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 446, loss 4.185625076293945\n",
      "Epoch 4: |          | 447/? [10:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 447, loss 4.740994453430176\n",
      "Epoch 4: |          | 448/? [10:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 448, loss 3.8197765350341797\n",
      "Epoch 4: |          | 449/? [10:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 449, loss 4.2015275955200195\n",
      "Epoch 4: |          | 450/? [10:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 450, loss 4.549570560455322\n",
      "Epoch 4: |          | 451/? [10:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 451, loss 4.135587215423584\n",
      "Epoch 4: |          | 452/? [10:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 452, loss 3.929858446121216\n",
      "Epoch 4: |          | 453/? [10:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 453, loss 4.634114742279053\n",
      "Epoch 4: |          | 454/? [10:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 454, loss 4.000962734222412\n",
      "Epoch 4: |          | 455/? [10:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 455, loss 4.389578819274902\n",
      "Epoch 4: |          | 456/? [10:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 456, loss 3.6979212760925293\n",
      "Epoch 4: |          | 457/? [10:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 457, loss 4.133314609527588\n",
      "Epoch 4: |          | 458/? [10:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 458, loss 4.606178283691406\n",
      "Epoch 4: |          | 459/? [10:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 459, loss 4.5625104904174805\n",
      "Epoch 4: |          | 460/? [11:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 460, loss 4.321069240570068\n",
      "Epoch 4: |          | 461/? [11:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 461, loss 4.248161315917969\n",
      "Epoch 4: |          | 462/? [11:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 462, loss 4.3571577072143555\n",
      "Epoch 4: |          | 463/? [11:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 463, loss 4.177736759185791\n",
      "Epoch 4: |          | 464/? [11:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 464, loss 3.681555986404419\n",
      "Epoch 4: |          | 465/? [11:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 465, loss 3.9642231464385986\n",
      "Epoch 4: |          | 466/? [11:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 466, loss 4.451296329498291\n",
      "Epoch 4: |          | 467/? [11:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 467, loss 4.240086555480957\n",
      "Epoch 4: |          | 468/? [11:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 468, loss 4.180283069610596\n",
      "Epoch 4: |          | 469/? [11:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 469, loss 4.316221714019775\n",
      "Epoch 4: |          | 470/? [11:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 470, loss 3.636600971221924\n",
      "Epoch 4: |          | 471/? [11:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 471, loss 4.432116985321045\n",
      "Epoch 4: |          | 472/? [11:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 472, loss 3.9650802612304688\n",
      "Epoch 4: |          | 473/? [11:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 473, loss 3.9711010456085205\n",
      "Epoch 4: |          | 474/? [11:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 474, loss 3.615321636199951\n",
      "Epoch 4: |          | 475/? [11:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 475, loss 4.935816764831543\n",
      "Epoch 4: |          | 476/? [11:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 476, loss 3.8936545848846436\n",
      "Epoch 4: |          | 477/? [11:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 477, loss 3.4138760566711426\n",
      "Epoch 4: |          | 478/? [11:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 478, loss 3.6160988807678223\n",
      "Epoch 4: |          | 479/? [11:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 479, loss 4.097609519958496\n",
      "Epoch 4: |          | 480/? [11:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 480, loss 4.048392295837402\n",
      "Epoch 4: |          | 481/? [11:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 481, loss 3.6806678771972656\n",
      "Epoch 4: |          | 482/? [11:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 482, loss 3.956303358078003\n",
      "Epoch 4: |          | 483/? [11:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 483, loss 3.649994373321533\n",
      "Epoch 4: |          | 484/? [11:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 484, loss 4.615655422210693\n",
      "Epoch 4: |          | 485/? [11:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 485, loss 4.436068534851074\n",
      "Epoch 4: |          | 486/? [11:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 486, loss 4.083495616912842\n",
      "Epoch 4: |          | 487/? [11:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 487, loss 4.36997127532959\n",
      "Epoch 4: |          | 488/? [11:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 488, loss 4.041572570800781\n",
      "Epoch 4: |          | 489/? [11:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 489, loss 3.5997700691223145\n",
      "Epoch 4: |          | 490/? [11:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 490, loss 4.253664016723633\n",
      "Epoch 4: |          | 491/? [11:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 491, loss 4.058210849761963\n",
      "Epoch 4: |          | 492/? [11:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 492, loss 3.386378049850464\n",
      "Epoch 4: |          | 493/? [11:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 493, loss 4.44375467300415\n",
      "Epoch 4: |          | 494/? [11:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 494, loss 4.292445182800293\n",
      "Epoch 4: |          | 495/? [11:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 495, loss 4.37460994720459\n",
      "Epoch 4: |          | 496/? [11:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 496, loss 3.952840805053711\n",
      "Epoch 4: |          | 497/? [11:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 497, loss 4.516842365264893\n",
      "Epoch 4: |          | 498/? [11:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 498, loss 4.150295257568359\n",
      "Epoch 4: |          | 499/? [11:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 499, loss 4.234449863433838\n",
      "Epoch 4: |          | 500/? [11:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 500, loss 4.044458389282227\n",
      "Epoch 4: |          | 501/? [11:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 501, loss 3.7751693725585938\n",
      "Epoch 4: |          | 502/? [11:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 502, loss 4.252425193786621\n",
      "Epoch 4: |          | 503/? [12:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 503, loss 4.24764347076416\n",
      "Epoch 4: |          | 504/? [12:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 504, loss 4.040712833404541\n",
      "Epoch 4: |          | 505/? [12:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 505, loss 3.4527688026428223\n",
      "Epoch 4: |          | 506/? [12:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 506, loss 4.121155738830566\n",
      "Epoch 4: |          | 507/? [12:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 507, loss 4.171685695648193\n",
      "Epoch 4: |          | 508/? [12:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 508, loss 4.524556636810303\n",
      "Epoch 4: |          | 509/? [12:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 509, loss 3.861557722091675\n",
      "Epoch 4: |          | 510/? [12:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 510, loss 4.385746955871582\n",
      "Epoch 4: |          | 511/? [12:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 511, loss 4.235958099365234\n",
      "Epoch 4: |          | 512/? [12:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 512, loss 3.6166954040527344\n",
      "Epoch 4: |          | 513/? [12:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 513, loss 3.8805301189422607\n",
      "Epoch 4: |          | 514/? [12:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 514, loss 4.013445854187012\n",
      "Epoch 4: |          | 515/? [12:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 515, loss 3.712871551513672\n",
      "Epoch 4: |          | 516/? [12:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 516, loss 3.971909284591675\n",
      "Epoch 4: |          | 517/? [12:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 517, loss 4.295041561126709\n",
      "Epoch 4: |          | 518/? [12:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 518, loss 3.791142702102661\n",
      "Epoch 4: |          | 519/? [12:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 519, loss 4.239710330963135\n",
      "Epoch 4: |          | 520/? [12:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 520, loss 4.090530872344971\n",
      "Epoch 4: |          | 521/? [12:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 521, loss 4.0835371017456055\n",
      "Epoch 4: |          | 522/? [12:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 522, loss 4.636282920837402\n",
      "Epoch 4: |          | 523/? [12:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 523, loss 4.705901145935059\n",
      "Epoch 4: |          | 524/? [12:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 524, loss 4.500854015350342\n",
      "Epoch 4: |          | 525/? [12:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 525, loss 4.063267707824707\n",
      "Epoch 4: |          | 526/? [12:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 526, loss 3.87359356880188\n",
      "Epoch 4: |          | 527/? [12:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 527, loss 4.54219913482666\n",
      "Epoch 4: |          | 528/? [12:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 528, loss 4.29037618637085\n",
      "Epoch 4: |          | 529/? [12:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 529, loss 3.90165376663208\n",
      "Epoch 4: |          | 530/? [12:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 530, loss 4.478266716003418\n",
      "Epoch 4: |          | 531/? [12:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 531, loss 3.9754748344421387\n",
      "Epoch 4: |          | 532/? [12:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 532, loss 4.2602338790893555\n",
      "Epoch 4: |          | 533/? [12:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 533, loss 3.8981940746307373\n",
      "Epoch 4: |          | 534/? [12:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 534, loss 3.575524091720581\n",
      "Epoch 4: |          | 535/? [12:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 535, loss 4.166741371154785\n",
      "Epoch 4: |          | 536/? [12:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 536, loss 4.561518669128418\n",
      "Epoch 4: |          | 537/? [12:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 537, loss 4.299580097198486\n",
      "Epoch 4: |          | 538/? [12:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 538, loss 3.9327049255371094\n",
      "Epoch 4: |          | 539/? [12:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 539, loss 4.027068138122559\n",
      "Epoch 4: |          | 540/? [12:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 540, loss 4.4533491134643555\n",
      "Epoch 4: |          | 541/? [12:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 541, loss 4.1971845626831055\n",
      "Epoch 4: |          | 542/? [12:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 542, loss 3.974573850631714\n",
      "Epoch 4: |          | 543/? [12:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 543, loss 4.338531017303467\n",
      "Epoch 4: |          | 544/? [12:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 544, loss 4.2570624351501465\n",
      "Epoch 4: |          | 545/? [13:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 545, loss 3.4685444831848145\n",
      "Epoch 4: |          | 546/? [13:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 546, loss 4.266167640686035\n",
      "Epoch 4: |          | 547/? [13:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 547, loss 4.766697406768799\n",
      "Epoch 4: |          | 548/? [13:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 548, loss 4.391151428222656\n",
      "Epoch 4: |          | 549/? [13:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 549, loss 4.258301734924316\n",
      "Epoch 4: |          | 550/? [13:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 550, loss 4.6154561042785645\n",
      "Epoch 4: |          | 551/? [13:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 551, loss 4.265321731567383\n",
      "Epoch 4: |          | 552/? [13:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 552, loss 4.279738903045654\n",
      "Epoch 4: |          | 553/? [13:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 553, loss 3.6760661602020264\n",
      "Epoch 4: |          | 554/? [13:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 554, loss 4.295370578765869\n",
      "Epoch 4: |          | 555/? [13:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 555, loss 4.613587379455566\n",
      "Epoch 4: |          | 556/? [13:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 556, loss 4.325848579406738\n",
      "Epoch 4: |          | 557/? [13:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 557, loss 3.851682662963867\n",
      "Epoch 4: |          | 558/? [13:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 558, loss 4.037379264831543\n",
      "Epoch 4: |          | 559/? [13:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 559, loss 4.0535078048706055\n",
      "Epoch 4: |          | 560/? [13:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 560, loss 3.5025088787078857\n",
      "Epoch 4: |          | 561/? [13:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 561, loss 3.5660483837127686\n",
      "Epoch 4: |          | 562/? [13:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 562, loss 4.441054344177246\n",
      "Epoch 4: |          | 563/? [13:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 563, loss 3.5107407569885254\n",
      "Epoch 4: |          | 564/? [13:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 564, loss 4.008426666259766\n",
      "Epoch 4: |          | 565/? [13:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 565, loss 4.449472904205322\n",
      "Epoch 4: |          | 566/? [13:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 566, loss 4.454553604125977\n",
      "Epoch 4: |          | 567/? [13:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 567, loss 4.610311508178711\n",
      "Epoch 4: |          | 568/? [13:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 568, loss 3.681201219558716\n",
      "Epoch 4: |          | 569/? [13:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 569, loss 4.3239359855651855\n",
      "Epoch 4: |          | 570/? [13:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 570, loss 4.369379043579102\n",
      "Epoch 4: |          | 571/? [13:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 571, loss 3.923269748687744\n",
      "Epoch 4: |          | 572/? [13:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 572, loss 4.875044822692871\n",
      "Epoch 4: |          | 573/? [13:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 573, loss 3.2544569969177246\n",
      "Epoch 4: |          | 574/? [13:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 574, loss 4.453059196472168\n",
      "Epoch 4: |          | 575/? [13:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 575, loss 3.786207914352417\n",
      "Epoch 4: |          | 576/? [13:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 576, loss 4.027500152587891\n",
      "Epoch 4: |          | 577/? [13:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 577, loss 4.250582695007324\n",
      "Epoch 4: |          | 578/? [13:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 578, loss 4.468402862548828\n",
      "Epoch 4: |          | 579/? [13:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 579, loss 3.5663437843322754\n",
      "Epoch 4: |          | 580/? [13:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 580, loss 4.285456657409668\n",
      "Epoch 4: |          | 581/? [13:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 581, loss 4.334866523742676\n",
      "Epoch 4: |          | 582/? [13:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 582, loss 4.391826152801514\n",
      "Epoch 4: |          | 583/? [13:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 583, loss 4.1215410232543945\n",
      "Epoch 4: |          | 584/? [13:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 584, loss 4.315149784088135\n",
      "Epoch 4: |          | 585/? [13:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 585, loss 4.328311443328857\n",
      "Epoch 4: |          | 586/? [13:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 586, loss 4.383028507232666\n",
      "Epoch 4: |          | 587/? [14:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 587, loss 4.336140155792236\n",
      "Epoch 4: |          | 588/? [14:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 588, loss 4.348650932312012\n",
      "Epoch 4: |          | 589/? [14:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 589, loss 3.7563087940216064\n",
      "Epoch 4: |          | 590/? [14:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 590, loss 4.403346538543701\n",
      "Epoch 4: |          | 591/? [14:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 591, loss 4.258338928222656\n",
      "Epoch 4: |          | 592/? [14:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 592, loss 3.941175937652588\n",
      "Epoch 4: |          | 593/? [14:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 593, loss 4.165476322174072\n",
      "Epoch 4: |          | 594/? [14:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 594, loss 4.997644901275635\n",
      "Epoch 4: |          | 595/? [14:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 595, loss 3.70527982711792\n",
      "Epoch 4: |          | 596/? [14:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 596, loss 3.7820658683776855\n",
      "Epoch 4: |          | 597/? [14:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 597, loss 4.034664630889893\n",
      "Epoch 4: |          | 598/? [14:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 598, loss 4.543893814086914\n",
      "Epoch 4: |          | 599/? [14:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 599, loss 4.182435035705566\n",
      "Epoch 4: |          | 600/? [14:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 600, loss 3.921225070953369\n",
      "Epoch 4: |          | 601/? [14:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 601, loss 4.222220420837402\n",
      "Epoch 4: |          | 602/? [14:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 602, loss 3.7600276470184326\n",
      "Epoch 4: |          | 603/? [14:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 603, loss 3.902251720428467\n",
      "Epoch 4: |          | 604/? [14:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 604, loss 5.911627769470215\n",
      "Epoch 4: |          | 605/? [14:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 605, loss 3.6331169605255127\n",
      "Epoch 4: |          | 606/? [14:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 606, loss 3.9282174110412598\n",
      "Epoch 4: |          | 607/? [14:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 607, loss 4.285417079925537\n",
      "Epoch 4: |          | 608/? [14:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 608, loss 4.05477237701416\n",
      "Epoch 4: |          | 609/? [14:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 609, loss 3.9568657875061035\n",
      "Epoch 4: |          | 610/? [14:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 610, loss 4.050856113433838\n",
      "Epoch 4: |          | 611/? [14:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 611, loss 4.195404052734375\n",
      "Epoch 4: |          | 612/? [14:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 612, loss 3.9376823902130127\n",
      "Epoch 4: |          | 613/? [14:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 613, loss 4.239059925079346\n",
      "Epoch 4: |          | 614/? [14:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 614, loss 4.051532745361328\n",
      "Epoch 4: |          | 615/? [14:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 615, loss 4.5841522216796875\n",
      "Epoch 4: |          | 616/? [14:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 616, loss 4.773309230804443\n",
      "Epoch 4: |          | 617/? [14:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 617, loss 3.32129168510437\n",
      "Epoch 4: |          | 618/? [14:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 618, loss 4.276486873626709\n",
      "Epoch 4: |          | 619/? [14:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 619, loss 3.8864097595214844\n",
      "Epoch 4: |          | 620/? [14:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 620, loss 4.397218704223633\n",
      "Epoch 4: |          | 621/? [14:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 621, loss 3.830406665802002\n",
      "Epoch 4: |          | 622/? [14:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 622, loss 3.620873212814331\n",
      "Epoch 4: |          | 623/? [14:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 623, loss 3.3203513622283936\n",
      "Epoch 4: |          | 624/? [14:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 624, loss 3.031613349914551\n",
      "Epoch 4: |          | 625/? [14:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 625, loss 4.651070594787598\n",
      "Epoch 4: |          | 626/? [14:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 626, loss 4.116095066070557\n",
      "Epoch 4: |          | 627/? [14:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 627, loss 4.011941432952881\n",
      "Epoch 4: |          | 628/? [14:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 628, loss 3.9534878730773926\n",
      "Epoch 4: |          | 629/? [14:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 629, loss 4.400826930999756\n",
      "Epoch 4: |          | 630/? [15:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 630, loss 4.140565872192383\n",
      "Epoch 4: |          | 631/? [15:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 631, loss 4.2935662269592285\n",
      "Epoch 4: |          | 632/? [15:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 632, loss 3.522939682006836\n",
      "Epoch 4: |          | 633/? [15:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 633, loss 4.362583637237549\n",
      "Epoch 4: |          | 634/? [15:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 634, loss 3.900522232055664\n",
      "Epoch 4: |          | 635/? [15:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 635, loss 3.7049121856689453\n",
      "Epoch 4: |          | 636/? [15:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 636, loss 4.153441429138184\n",
      "Epoch 4: |          | 637/? [15:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 637, loss 3.930738925933838\n",
      "Epoch 4: |          | 638/? [15:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 638, loss 4.205107688903809\n",
      "Epoch 4: |          | 639/? [15:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 639, loss 3.9552974700927734\n",
      "Epoch 4: |          | 640/? [15:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 640, loss 4.555254936218262\n",
      "Epoch 4: |          | 641/? [15:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 641, loss 3.5801424980163574\n",
      "Epoch 4: |          | 642/? [15:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 642, loss 4.3152689933776855\n",
      "Epoch 4: |          | 643/? [15:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 643, loss 4.29699182510376\n",
      "Epoch 4: |          | 644/? [15:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 644, loss 4.1520586013793945\n",
      "Epoch 4: |          | 645/? [15:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 645, loss 3.912006378173828\n",
      "Epoch 4: |          | 646/? [15:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 646, loss 3.9497570991516113\n",
      "Epoch 4: |          | 647/? [15:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 647, loss 4.5325446128845215\n",
      "Epoch 4: |          | 648/? [15:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 648, loss 3.937216281890869\n",
      "Epoch 4: |          | 649/? [15:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 649, loss 3.579462766647339\n",
      "Epoch 4: |          | 650/? [15:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 650, loss 4.469908237457275\n",
      "Epoch 4: |          | 651/? [15:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 651, loss 4.589000225067139\n",
      "Epoch 4: |          | 652/? [15:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 652, loss 3.9815335273742676\n",
      "Epoch 4: |          | 653/? [15:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 653, loss 4.149158000946045\n",
      "Epoch 4: |          | 654/? [15:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 654, loss 4.299229621887207\n",
      "Epoch 4: |          | 655/? [15:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 655, loss 4.105311870574951\n",
      "Epoch 4: |          | 656/? [15:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 656, loss 3.6951403617858887\n",
      "Epoch 4: |          | 657/? [15:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 657, loss 6.144939422607422\n",
      "Epoch 4: |          | 658/? [15:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 658, loss 3.7393100261688232\n",
      "Epoch 4: |          | 659/? [15:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 659, loss 4.130009651184082\n",
      "Epoch 4: |          | 660/? [15:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 660, loss 4.472074508666992\n",
      "Epoch 4: |          | 661/? [15:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 661, loss 4.38883113861084\n",
      "Epoch 4: |          | 662/? [15:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 662, loss 4.295446395874023\n",
      "Epoch 4: |          | 663/? [15:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 663, loss 4.033703804016113\n",
      "Epoch 4: |          | 664/? [15:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 664, loss 3.933178663253784\n",
      "Epoch 4: |          | 665/? [15:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 665, loss 4.250174045562744\n",
      "Epoch 4: |          | 666/? [15:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 666, loss 4.03603458404541\n",
      "Epoch 4: |          | 667/? [15:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 667, loss 4.918240547180176\n",
      "Epoch 4: |          | 668/? [15:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 668, loss 3.6268703937530518\n",
      "Epoch 4: |          | 669/? [15:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 669, loss 3.877770185470581\n",
      "Epoch 4: |          | 670/? [15:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 670, loss 4.56977653503418\n",
      "Epoch 4: |          | 671/? [15:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 671, loss 4.301001071929932\n",
      "Epoch 4: |          | 672/? [16:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 672, loss 4.304911136627197\n",
      "Epoch 4: |          | 673/? [16:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 673, loss 4.145692825317383\n",
      "Epoch 4: |          | 674/? [16:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 674, loss 2.6868069171905518\n",
      "Epoch 4: |          | 675/? [16:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 675, loss 1.1976940631866455\n",
      "Epoch 4: |          | 676/? [16:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 676, loss 0.991457462310791\n",
      "Epoch 4: |          | 677/? [16:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 677, loss 0.8068329095840454\n",
      "Epoch 4: |          | 678/? [16:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 678, loss 1.8775818347930908\n",
      "Epoch 4: |          | 679/? [16:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 679, loss 3.5882022380828857\n",
      "Epoch 4: |          | 680/? [16:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 680, loss 4.065810203552246\n",
      "Epoch 4: |          | 681/? [16:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 681, loss 3.5332534313201904\n",
      "Epoch 4: |          | 682/? [16:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 682, loss 3.8874621391296387\n",
      "Epoch 4: |          | 683/? [16:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 683, loss 3.592764377593994\n",
      "Epoch 4: |          | 684/? [16:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 684, loss 4.702818393707275\n",
      "Epoch 4: |          | 685/? [16:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 685, loss 4.304826259613037\n",
      "Epoch 4: |          | 686/? [16:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 686, loss 3.8495898246765137\n",
      "Epoch 4: |          | 687/? [16:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 687, loss 4.457339286804199\n",
      "Epoch 4: |          | 688/? [16:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 688, loss 4.001679420471191\n",
      "Epoch 4: |          | 689/? [16:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 689, loss 4.055064678192139\n",
      "Epoch 4: |          | 690/? [16:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 690, loss 4.648816108703613\n",
      "Epoch 4: |          | 691/? [16:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 691, loss 4.13492488861084\n",
      "Epoch 4: |          | 692/? [16:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 692, loss 4.1354522705078125\n",
      "Epoch 4: |          | 693/? [16:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 693, loss 4.6857709884643555\n",
      "Epoch 4: |          | 694/? [16:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 694, loss 4.017062187194824\n",
      "Epoch 4: |          | 695/? [16:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 695, loss 4.608028411865234\n",
      "Epoch 4: |          | 696/? [16:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 696, loss 3.8287150859832764\n",
      "Epoch 4: |          | 697/? [16:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 697, loss 4.0822343826293945\n",
      "Epoch 4: |          | 698/? [16:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 698, loss 3.3993430137634277\n",
      "Epoch 4: |          | 699/? [16:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 699, loss 4.243164539337158\n",
      "Epoch 4: |          | 700/? [16:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 700, loss 4.316563129425049\n",
      "Epoch 4: |          | 701/? [16:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 701, loss 3.9184112548828125\n",
      "Epoch 4: |          | 702/? [16:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 702, loss 4.200798988342285\n",
      "Epoch 4: |          | 703/? [16:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 703, loss 4.333043098449707\n",
      "Epoch 4: |          | 704/? [16:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 704, loss 4.193383693695068\n",
      "Epoch 4: |          | 705/? [16:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 705, loss 3.8308796882629395\n",
      "Epoch 4: |          | 706/? [16:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 706, loss 3.8926994800567627\n",
      "Epoch 4: |          | 707/? [16:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 707, loss 4.385328769683838\n",
      "Epoch 4: |          | 708/? [16:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 708, loss 4.111752510070801\n",
      "Epoch 4: |          | 709/? [16:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 709, loss 4.011033535003662\n",
      "Epoch 4: |          | 710/? [16:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 710, loss 4.589646339416504\n",
      "Epoch 4: |          | 711/? [16:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 711, loss 4.732555866241455\n",
      "Epoch 4: |          | 712/? [16:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 712, loss 4.364984035491943\n",
      "Epoch 4: |          | 713/? [16:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 713, loss 4.41909646987915\n",
      "Epoch 4: |          | 714/? [16:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 714, loss 4.5495285987854\n",
      "Epoch 4: |          | 715/? [17:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 715, loss 3.423494338989258\n",
      "Epoch 4: |          | 716/? [17:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 716, loss 4.168627738952637\n",
      "Epoch 4: |          | 717/? [17:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 717, loss 3.999948501586914\n",
      "Epoch 4: |          | 718/? [17:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 718, loss 3.549619197845459\n",
      "Epoch 4: |          | 719/? [17:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 719, loss 4.108120918273926\n",
      "Epoch 4: |          | 720/? [17:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 720, loss 3.7872605323791504\n",
      "Epoch 4: |          | 721/? [17:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 721, loss 4.479369163513184\n",
      "Epoch 4: |          | 722/? [17:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 722, loss 3.70892071723938\n",
      "Epoch 4: |          | 723/? [17:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 723, loss 4.310372352600098\n",
      "Epoch 4: |          | 724/? [17:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 724, loss 3.9554812908172607\n",
      "Epoch 4: |          | 725/? [17:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 725, loss 3.785977840423584\n",
      "Epoch 4: |          | 726/? [17:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 726, loss 4.023486137390137\n",
      "Epoch 4: |          | 727/? [17:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 727, loss 3.755223512649536\n",
      "Epoch 4: |          | 728/? [17:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 728, loss 3.5822315216064453\n",
      "Epoch 4: |          | 729/? [17:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 729, loss 4.0556721687316895\n",
      "Epoch 4: |          | 730/? [17:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 730, loss 4.018219947814941\n",
      "Epoch 4: |          | 731/? [17:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 731, loss 4.169045448303223\n",
      "Epoch 4: |          | 732/? [17:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 732, loss 4.463315486907959\n",
      "Epoch 4: |          | 733/? [17:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 733, loss 4.123744010925293\n",
      "Epoch 4: |          | 734/? [17:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 734, loss 4.38686466217041\n",
      "Epoch 4: |          | 735/? [17:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 735, loss 4.258072376251221\n",
      "Epoch 4: |          | 736/? [17:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 736, loss 3.7562637329101562\n",
      "Epoch 4: |          | 737/? [17:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 737, loss 4.554092884063721\n",
      "Epoch 4: |          | 738/? [17:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 738, loss 3.7000114917755127\n",
      "Epoch 4: |          | 739/? [17:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 739, loss 4.2498459815979\n",
      "Epoch 4: |          | 740/? [17:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 740, loss 3.8917057514190674\n",
      "Epoch 4: |          | 741/? [17:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 741, loss 4.118196487426758\n",
      "Epoch 4: |          | 742/? [17:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 742, loss 4.4866461753845215\n",
      "Epoch 4: |          | 743/? [17:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 743, loss 4.316748142242432\n",
      "Epoch 4: |          | 744/? [17:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 744, loss 4.283381462097168\n",
      "Epoch 4: |          | 745/? [17:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 745, loss 3.8813412189483643\n",
      "Epoch 4: |          | 746/? [17:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 746, loss 4.1811394691467285\n",
      "Epoch 4: |          | 747/? [17:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 747, loss 3.926779270172119\n",
      "Epoch 4: |          | 748/? [17:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 748, loss 3.083895444869995\n",
      "Epoch 4: |          | 749/? [17:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 749, loss 4.102829933166504\n",
      "Epoch 4: |          | 750/? [17:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 750, loss 4.475355625152588\n",
      "Epoch 4: |          | 751/? [17:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 751, loss 2.6603312492370605\n",
      "Epoch 4: |          | 752/? [17:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 752, loss 4.278599739074707\n",
      "Epoch 4: |          | 753/? [17:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 753, loss 3.485886335372925\n",
      "Epoch 4: |          | 754/? [17:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 754, loss 3.906053066253662\n",
      "Epoch 4: |          | 755/? [17:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 755, loss 3.6881611347198486\n",
      "Epoch 4: |          | 756/? [18:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 756, loss 4.087368011474609\n",
      "Epoch 4: |          | 757/? [18:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 757, loss 4.159771919250488\n",
      "Epoch 4: |          | 758/? [18:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 758, loss 3.8642890453338623\n",
      "Epoch 4: |          | 759/? [18:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 759, loss 3.8578243255615234\n",
      "Epoch 4: |          | 760/? [18:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 760, loss 4.347773551940918\n",
      "Epoch 4: |          | 761/? [18:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 761, loss 4.370254039764404\n",
      "Epoch 4: |          | 762/? [18:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 762, loss 4.028182029724121\n",
      "Epoch 4: |          | 763/? [18:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 763, loss 4.284178256988525\n",
      "Epoch 4: |          | 764/? [18:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 764, loss 4.488526344299316\n",
      "Epoch 4: |          | 765/? [18:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 765, loss 4.220240116119385\n",
      "Epoch 4: |          | 766/? [18:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 766, loss 4.614231109619141\n",
      "Epoch 4: |          | 767/? [18:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 767, loss 4.697749614715576\n",
      "Epoch 4: |          | 768/? [18:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 768, loss 4.222521781921387\n",
      "Epoch 4: |          | 769/? [18:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 769, loss 3.3700263500213623\n",
      "Epoch 4: |          | 770/? [18:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 770, loss 3.986429214477539\n",
      "Epoch 4: |          | 771/? [18:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 771, loss 4.731295585632324\n",
      "Epoch 4: |          | 772/? [18:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 772, loss 4.395178318023682\n",
      "Epoch 4: |          | 773/? [18:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 773, loss 4.086893081665039\n",
      "Epoch 4: |          | 774/? [18:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 774, loss 4.202747821807861\n",
      "Epoch 4: |          | 775/? [18:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 775, loss 4.683903694152832\n",
      "Epoch 4: |          | 776/? [18:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 776, loss 4.083810806274414\n",
      "Epoch 4: |          | 777/? [18:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 777, loss 4.01950740814209\n",
      "Epoch 4: |          | 778/? [18:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 778, loss 4.377313613891602\n",
      "Epoch 4: |          | 779/? [18:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 779, loss 4.816300868988037\n",
      "Epoch 4: |          | 780/? [18:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 780, loss 3.781406879425049\n",
      "Epoch 4: |          | 781/? [18:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 781, loss 3.9063327312469482\n",
      "Epoch 4: |          | 782/? [18:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 782, loss 4.2916154861450195\n",
      "Epoch 4: |          | 783/? [18:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 783, loss 4.392151832580566\n",
      "Epoch 4: |          | 784/? [18:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 784, loss 3.9068408012390137\n",
      "Epoch 4: |          | 785/? [18:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 785, loss 3.736957550048828\n",
      "Epoch 4: |          | 786/? [18:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 786, loss 4.595919609069824\n",
      "Epoch 4: |          | 787/? [18:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 787, loss 4.562291622161865\n",
      "Epoch 4: |          | 788/? [18:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 788, loss 2.732618808746338\n",
      "Epoch 4: |          | 789/? [18:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 789, loss 4.079583644866943\n",
      "Epoch 4: |          | 790/? [18:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 790, loss 4.912399768829346\n",
      "Epoch 4: |          | 791/? [18:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 791, loss 4.608797073364258\n",
      "Epoch 4: |          | 792/? [18:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 792, loss 3.766458511352539\n",
      "Epoch 4: |          | 793/? [18:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 793, loss 4.265151023864746\n",
      "Epoch 4: |          | 794/? [18:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 794, loss 4.642144203186035\n",
      "Epoch 4: |          | 795/? [18:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 795, loss 4.085056304931641\n",
      "Epoch 4: |          | 796/? [18:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 796, loss 4.5079545974731445\n",
      "Epoch 4: |          | 797/? [18:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 797, loss 3.447443723678589\n",
      "Epoch 4: |          | 798/? [19:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 798, loss 3.5606677532196045\n",
      "Epoch 4: |          | 799/? [19:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 799, loss 4.552610874176025\n",
      "Epoch 4: |          | 800/? [19:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 800, loss 4.316400051116943\n",
      "Epoch 4: |          | 801/? [19:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 801, loss 3.8782248497009277\n",
      "Epoch 4: |          | 802/? [19:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 802, loss 4.251997947692871\n",
      "Epoch 4: |          | 803/? [19:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 803, loss 4.059234142303467\n",
      "Epoch 4: |          | 804/? [19:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 804, loss 4.217376232147217\n",
      "Epoch 4: |          | 805/? [19:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 805, loss 4.479249000549316\n",
      "Epoch 4: |          | 806/? [19:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 806, loss 4.781027317047119\n",
      "Epoch 4: |          | 807/? [19:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 807, loss 4.1871490478515625\n",
      "Epoch 4: |          | 808/? [19:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 808, loss 3.7562942504882812\n",
      "Epoch 4: |          | 809/? [19:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 809, loss 4.336765766143799\n",
      "Epoch 4: |          | 810/? [19:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 810, loss 4.115937232971191\n",
      "Epoch 4: |          | 811/? [19:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 811, loss 4.3920793533325195\n",
      "Epoch 4: |          | 812/? [19:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 812, loss 5.029125213623047\n",
      "Epoch 4: |          | 813/? [19:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 813, loss 4.764395713806152\n",
      "Epoch 4: |          | 814/? [19:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 814, loss 3.7575576305389404\n",
      "Epoch 4: |          | 815/? [19:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 815, loss 4.500685691833496\n",
      "Epoch 4: |          | 816/? [19:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 816, loss 4.329422950744629\n",
      "Epoch 4: |          | 817/? [19:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 817, loss 3.6306557655334473\n",
      "Epoch 4: |          | 818/? [19:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 818, loss 4.678497314453125\n",
      "Epoch 4: |          | 819/? [19:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 819, loss 4.344801902770996\n",
      "Epoch 4: |          | 820/? [19:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 820, loss 4.183287620544434\n",
      "Epoch 4: |          | 821/? [19:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 821, loss 4.1604204177856445\n",
      "Epoch 4: |          | 822/? [19:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 822, loss 3.7681732177734375\n",
      "Epoch 4: |          | 823/? [19:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 823, loss 3.757546901702881\n",
      "Epoch 4: |          | 824/? [19:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 824, loss 4.29793119430542\n",
      "Epoch 4: |          | 825/? [19:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 825, loss 3.796557664871216\n",
      "Epoch 4: |          | 826/? [19:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 826, loss 4.3442864418029785\n",
      "Epoch 4: |          | 827/? [19:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 827, loss 3.9536101818084717\n",
      "Epoch 4: |          | 828/? [19:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 828, loss 4.511819839477539\n",
      "Epoch 4: |          | 829/? [19:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 829, loss 4.144396781921387\n",
      "Epoch 4: |          | 830/? [19:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 830, loss 4.718810558319092\n",
      "Epoch 4: |          | 831/? [19:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 831, loss 2.587938070297241\n",
      "Epoch 4: |          | 832/? [19:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 832, loss 4.147009372711182\n",
      "Epoch 4: |          | 833/? [19:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 833, loss 3.985278367996216\n",
      "Epoch 4: |          | 834/? [19:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 834, loss 4.766402244567871\n",
      "Epoch 4: |          | 835/? [19:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 835, loss 4.09908390045166\n",
      "Epoch 4: |          | 836/? [19:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 836, loss 4.816856384277344\n",
      "Epoch 4: |          | 837/? [19:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 837, loss 4.235832691192627\n",
      "Epoch 4: |          | 838/? [19:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 838, loss 3.4836106300354004\n",
      "Epoch 4: |          | 839/? [19:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 839, loss 3.892559766769409\n",
      "Epoch 4: |          | 840/? [20:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 840, loss 4.436388969421387\n",
      "Epoch 4: |          | 841/? [20:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 841, loss 4.482067108154297\n",
      "Epoch 4: |          | 842/? [20:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 842, loss 4.142407417297363\n",
      "Epoch 4: |          | 843/? [20:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 843, loss 4.507436752319336\n",
      "Epoch 4: |          | 844/? [20:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 844, loss 3.8501949310302734\n",
      "Epoch 4: |          | 845/? [20:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 845, loss 4.275289058685303\n",
      "Epoch 4: |          | 846/? [20:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 846, loss 4.730886459350586\n",
      "Epoch 4: |          | 847/? [20:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 847, loss 4.279232978820801\n",
      "Epoch 4: |          | 848/? [20:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 848, loss 3.803814649581909\n",
      "Epoch 4: |          | 849/? [20:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 849, loss 3.9315154552459717\n",
      "Epoch 4: |          | 850/? [20:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 850, loss 4.021413803100586\n",
      "Epoch 4: |          | 851/? [20:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 851, loss 4.379049777984619\n",
      "Epoch 4: |          | 852/? [20:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 852, loss 4.431380271911621\n",
      "Epoch 4: |          | 853/? [20:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 853, loss 4.296538352966309\n",
      "Epoch 4: |          | 854/? [20:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 854, loss 3.549988269805908\n",
      "Epoch 4: |          | 855/? [20:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 855, loss 3.8965232372283936\n",
      "Epoch 4: |          | 856/? [20:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 856, loss 3.812739610671997\n",
      "Epoch 4: |          | 857/? [20:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 857, loss 4.399961948394775\n",
      "Epoch 4: |          | 858/? [20:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 858, loss 4.257961273193359\n",
      "Epoch 4: |          | 859/? [20:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 859, loss 4.291719913482666\n",
      "Epoch 4: |          | 860/? [20:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 860, loss 4.6741719245910645\n",
      "Epoch 4: |          | 861/? [20:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 861, loss 3.9136557579040527\n",
      "Epoch 4: |          | 862/? [20:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 862, loss 4.3103179931640625\n",
      "Epoch 4: |          | 863/? [20:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 863, loss 3.624124050140381\n",
      "Epoch 4: |          | 864/? [20:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 864, loss 4.258944511413574\n",
      "Epoch 4: |          | 865/? [20:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 865, loss 4.177426338195801\n",
      "Epoch 4: |          | 866/? [20:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 866, loss 3.211977481842041\n",
      "Epoch 4: |          | 867/? [20:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 867, loss 3.4263203144073486\n",
      "Epoch 4: |          | 868/? [20:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 868, loss 4.316465377807617\n",
      "Epoch 4: |          | 869/? [20:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 869, loss 4.337601661682129\n",
      "Epoch 4: |          | 870/? [20:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 870, loss 3.912856340408325\n",
      "Epoch 4: |          | 871/? [20:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 871, loss 4.277606964111328\n",
      "Epoch 4: |          | 872/? [20:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 872, loss 4.113683223724365\n",
      "Epoch 4: |          | 873/? [20:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 873, loss 4.113823890686035\n",
      "Epoch 4: |          | 874/? [20:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 874, loss 3.5813705921173096\n",
      "Epoch 4: |          | 875/? [20:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 875, loss 4.340284824371338\n",
      "Epoch 4: |          | 876/? [20:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 876, loss 3.9142673015594482\n",
      "Epoch 4: |          | 877/? [20:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 877, loss 4.266026020050049\n",
      "Epoch 4: |          | 878/? [20:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 878, loss 3.6776509284973145\n",
      "Epoch 4: |          | 879/? [20:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 879, loss 3.7560031414031982\n",
      "Epoch 4: |          | 880/? [20:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 880, loss 4.897875785827637\n",
      "Epoch 4: |          | 881/? [20:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 881, loss 4.259274482727051\n",
      "Epoch 4: |          | 882/? [21:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 882, loss 4.094939708709717\n",
      "Epoch 4: |          | 883/? [21:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 883, loss 4.167613983154297\n",
      "Epoch 4: |          | 884/? [21:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 884, loss 4.2389421463012695\n",
      "Epoch 4: |          | 885/? [21:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 885, loss 3.987766742706299\n",
      "Epoch 4: |          | 886/? [21:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 886, loss 4.640291690826416\n",
      "Epoch 4: |          | 887/? [21:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 887, loss 4.674471855163574\n",
      "Epoch 4: |          | 888/? [21:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 888, loss 4.405918598175049\n",
      "Epoch 4: |          | 889/? [21:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 889, loss 3.945166826248169\n",
      "Epoch 4: |          | 890/? [21:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 890, loss 4.2333574295043945\n",
      "Epoch 4: |          | 891/? [21:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 891, loss 3.9195117950439453\n",
      "Epoch 4: |          | 892/? [21:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 892, loss 4.584500312805176\n",
      "Epoch 4: |          | 893/? [21:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 893, loss 4.00562047958374\n",
      "Epoch 4: |          | 894/? [21:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 894, loss 3.5793750286102295\n",
      "Epoch 4: |          | 895/? [21:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 895, loss 4.71513557434082\n",
      "Epoch 4: |          | 896/? [21:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 896, loss 4.305391788482666\n",
      "Epoch 4: |          | 897/? [21:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 897, loss 4.294183731079102\n",
      "Epoch 4: |          | 898/? [21:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 898, loss 4.297503471374512\n",
      "Epoch 4: |          | 899/? [21:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 899, loss 4.030191898345947\n",
      "Epoch 4: |          | 900/? [21:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 900, loss 3.9368700981140137\n",
      "Epoch 4: |          | 901/? [21:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 901, loss 4.407291889190674\n",
      "Epoch 4: |          | 902/? [21:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 902, loss 4.487124443054199\n",
      "Epoch 4: |          | 903/? [21:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 903, loss 3.7636303901672363\n",
      "Epoch 4: |          | 904/? [21:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 904, loss 4.257079124450684\n",
      "Epoch 4: |          | 905/? [21:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 905, loss 4.454948902130127\n",
      "Epoch 4: |          | 906/? [21:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 906, loss 4.207646369934082\n",
      "Epoch 4: |          | 907/? [21:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 907, loss 4.236268043518066\n",
      "Epoch 4: |          | 908/? [21:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 908, loss 4.309689521789551\n",
      "Epoch 4: |          | 909/? [21:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 909, loss 4.344876289367676\n",
      "Epoch 4: |          | 910/? [21:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 910, loss 4.113621711730957\n",
      "Epoch 4: |          | 911/? [21:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 911, loss 4.135398864746094\n",
      "Epoch 4: |          | 912/? [21:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 912, loss 4.089786052703857\n",
      "Epoch 4: |          | 913/? [21:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 913, loss 4.09733247756958\n",
      "Epoch 4: |          | 914/? [21:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 914, loss 4.410372734069824\n",
      "Epoch 4: |          | 915/? [21:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 915, loss 4.249805927276611\n",
      "Epoch 4: |          | 916/? [21:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 916, loss 4.107903957366943\n",
      "Epoch 4: |          | 917/? [21:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 917, loss 4.096901893615723\n",
      "Epoch 4: |          | 918/? [21:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 918, loss 4.011253356933594\n",
      "Epoch 4: |          | 919/? [21:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 919, loss 4.016541481018066\n",
      "Epoch 4: |          | 920/? [21:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 920, loss 4.168050765991211\n",
      "Epoch 4: |          | 921/? [22:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 921, loss 4.012164115905762\n",
      "Epoch 4: |          | 922/? [22:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 922, loss 4.167356967926025\n",
      "Epoch 4: |          | 923/? [22:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 923, loss 4.018200874328613\n",
      "Epoch 4: |          | 924/? [22:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 924, loss 3.959489345550537\n",
      "Epoch 4: |          | 925/? [22:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 925, loss 4.299365043640137\n",
      "Epoch 4: |          | 926/? [22:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 926, loss 4.091975212097168\n",
      "Epoch 4: |          | 927/? [22:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 927, loss 4.368225574493408\n",
      "Epoch 4: |          | 928/? [22:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 928, loss 3.8114402294158936\n",
      "Epoch 4: |          | 929/? [22:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 929, loss 3.9464383125305176\n",
      "Epoch 4: |          | 930/? [22:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 930, loss 3.851161241531372\n",
      "Epoch 4: |          | 931/? [22:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 931, loss 3.55761456489563\n",
      "Epoch 4: |          | 932/? [22:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 932, loss 4.218469142913818\n",
      "Epoch 4: |          | 933/? [22:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 933, loss 3.9876766204833984\n",
      "Epoch 4: |          | 934/? [22:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 934, loss 4.596644878387451\n",
      "Epoch 4: |          | 935/? [22:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 935, loss 4.890969276428223\n",
      "Epoch 4: |          | 936/? [22:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 936, loss 4.099310874938965\n",
      "Epoch 4: |          | 937/? [22:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 937, loss 4.210533618927002\n",
      "Epoch 4: |          | 938/? [22:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 938, loss 4.0173444747924805\n",
      "Epoch 4: |          | 939/? [22:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 939, loss 4.269538402557373\n",
      "Epoch 4: |          | 940/? [22:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 940, loss 4.502453804016113\n",
      "Epoch 4: |          | 941/? [22:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 941, loss 4.0036773681640625\n",
      "Epoch 4: |          | 942/? [22:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 942, loss 3.484448194503784\n",
      "Epoch 4: |          | 943/? [22:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 943, loss 4.412405967712402\n",
      "Epoch 4: |          | 944/? [22:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 944, loss 3.3897411823272705\n",
      "Epoch 4: |          | 945/? [22:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 945, loss 4.119497776031494\n",
      "Epoch 4: |          | 946/? [22:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 946, loss 4.07640266418457\n",
      "Epoch 4: |          | 947/? [22:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 947, loss 4.020745754241943\n",
      "Epoch 4: |          | 948/? [22:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 948, loss 4.22799015045166\n",
      "Epoch 4: |          | 949/? [22:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 949, loss 4.093837261199951\n",
      "Epoch 4: |          | 950/? [22:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 950, loss 3.8663172721862793\n",
      "Epoch 4: |          | 951/? [22:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 951, loss 4.523949146270752\n",
      "Epoch 4: |          | 952/? [22:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 952, loss 4.517875671386719\n",
      "Epoch 4: |          | 953/? [22:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 953, loss 5.035261154174805\n",
      "Epoch 4: |          | 954/? [22:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 954, loss 4.000314235687256\n",
      "Epoch 4: |          | 955/? [22:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 955, loss 4.677800178527832\n",
      "Epoch 4: |          | 956/? [22:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 956, loss 4.066982269287109\n",
      "Epoch 4: |          | 957/? [22:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 957, loss 4.308600425720215\n",
      "Epoch 4: |          | 958/? [22:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 958, loss 4.42985200881958\n",
      "Epoch 4: |          | 959/? [22:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 959, loss 4.0916571617126465\n",
      "Epoch 4: |          | 960/? [22:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 960, loss 4.427881717681885\n",
      "Epoch 4: |          | 961/? [22:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 961, loss 4.646782875061035\n",
      "Epoch 4: |          | 962/? [22:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 962, loss 4.20388126373291\n",
      "Epoch 4: |          | 963/? [22:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 963, loss 3.9293618202209473\n",
      "Epoch 4: |          | 964/? [23:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 964, loss 4.431304454803467\n",
      "Epoch 4: |          | 965/? [23:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 965, loss 3.8666439056396484\n",
      "Epoch 4: |          | 966/? [23:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 966, loss 3.741748809814453\n",
      "Epoch 4: |          | 967/? [23:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 967, loss 4.010851860046387\n",
      "Epoch 4: |          | 968/? [23:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 968, loss 3.974548816680908\n",
      "Epoch 4: |          | 969/? [23:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 969, loss 3.7864468097686768\n",
      "Epoch 4: |          | 970/? [23:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 970, loss 4.323891639709473\n",
      "Epoch 4: |          | 971/? [23:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 971, loss 4.5808000564575195\n",
      "Epoch 4: |          | 972/? [23:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 972, loss 3.9786477088928223\n",
      "Epoch 4: |          | 973/? [23:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 973, loss 4.1613264083862305\n",
      "Epoch 4: |          | 974/? [23:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 974, loss 4.113141059875488\n",
      "Epoch 4: |          | 975/? [23:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 975, loss 4.170108318328857\n",
      "Epoch 4: |          | 976/? [23:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 976, loss 4.259981632232666\n",
      "Epoch 4: |          | 977/? [23:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 977, loss 4.849660873413086\n",
      "Epoch 4: |          | 978/? [23:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 978, loss 4.320004940032959\n",
      "Epoch 4: |          | 979/? [23:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 979, loss 4.627912521362305\n",
      "Epoch 4: |          | 980/? [23:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 980, loss 3.7573280334472656\n",
      "Epoch 4: |          | 981/? [23:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 981, loss 3.522446870803833\n",
      "Epoch 4: |          | 982/? [23:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 982, loss 4.2139058113098145\n",
      "Epoch 4: |          | 983/? [23:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 983, loss 4.62941837310791\n",
      "Epoch 4: |          | 984/? [23:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 984, loss 3.6596412658691406\n",
      "Epoch 4: |          | 985/? [23:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 985, loss 3.929168224334717\n",
      "Epoch 4: |          | 986/? [23:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 986, loss 3.9710941314697266\n",
      "Epoch 4: |          | 987/? [23:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 987, loss 3.4816184043884277\n",
      "Epoch 4: |          | 988/? [23:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 988, loss 4.5079450607299805\n",
      "Epoch 4: |          | 989/? [23:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 989, loss 4.161530017852783\n",
      "Epoch 4: |          | 990/? [23:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 990, loss 3.4099278450012207\n",
      "Epoch 4: |          | 991/? [23:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 991, loss 4.183757305145264\n",
      "Epoch 4: |          | 992/? [23:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 992, loss 4.905445098876953\n",
      "Epoch 4: |          | 993/? [23:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 993, loss 3.993804931640625\n",
      "Epoch 4: |          | 994/? [23:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 994, loss 4.012260437011719\n",
      "Epoch 4: |          | 995/? [23:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 995, loss 4.407203674316406\n",
      "Epoch 4: |          | 996/? [23:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 996, loss 4.417798042297363\n",
      "Epoch 4: |          | 997/? [23:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 997, loss 4.003577709197998\n",
      "Epoch 4: |          | 998/? [23:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 998, loss 4.282414436340332\n",
      "Epoch 4: |          | 999/? [23:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 999, loss 4.323098182678223\n",
      "Epoch 4: |          | 1000/? [23:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1000, loss 3.808495283126831\n",
      "Epoch 4: |          | 1001/? [23:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1001, loss 4.468827247619629\n",
      "Epoch 4: |          | 1002/? [23:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1002, loss 4.391668796539307\n",
      "Epoch 4: |          | 1003/? [23:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1003, loss 4.561171531677246\n",
      "Epoch 4: |          | 1004/? [23:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1004, loss 3.4995789527893066\n",
      "Epoch 4: |          | 1005/? [23:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1005, loss 4.10244083404541\n",
      "Epoch 4: |          | 1006/? [24:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1006, loss 4.453183650970459\n",
      "Epoch 4: |          | 1007/? [24:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1007, loss 4.045226573944092\n",
      "Epoch 4: |          | 1008/? [24:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1008, loss 4.1666765213012695\n",
      "Epoch 4: |          | 1009/? [24:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1009, loss 4.468453407287598\n",
      "Epoch 4: |          | 1010/? [24:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1010, loss 3.5416932106018066\n",
      "Epoch 4: |          | 1011/? [24:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1011, loss 4.1764655113220215\n",
      "Epoch 4: |          | 1012/? [24:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1012, loss 3.9596030712127686\n",
      "Epoch 4: |          | 1013/? [24:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1013, loss 4.116578578948975\n",
      "Epoch 4: |          | 1014/? [24:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1014, loss 4.5621232986450195\n",
      "Epoch 4: |          | 1015/? [24:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1015, loss 4.226875305175781\n",
      "Epoch 4: |          | 1016/? [24:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1016, loss 4.008297920227051\n",
      "Epoch 4: |          | 1017/? [24:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1017, loss 3.5105080604553223\n",
      "Epoch 4: |          | 1018/? [24:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1018, loss 4.072145462036133\n",
      "Epoch 4: |          | 1019/? [24:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1019, loss 4.136223316192627\n",
      "Epoch 4: |          | 1020/? [24:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1020, loss 3.716247081756592\n",
      "Epoch 4: |          | 1021/? [24:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1021, loss 3.9381210803985596\n",
      "Epoch 4: |          | 1022/? [24:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1022, loss 3.702049732208252\n",
      "Epoch 4: |          | 1023/? [24:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1023, loss 3.4458858966827393\n",
      "Epoch 4: |          | 1024/? [24:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1024, loss 3.975435256958008\n",
      "Epoch 4: |          | 1025/? [24:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1025, loss 3.8774516582489014\n",
      "Epoch 4: |          | 1026/? [24:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1026, loss 2.9611687660217285\n",
      "Epoch 4: |          | 1027/? [24:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1027, loss 4.214199066162109\n",
      "Epoch 4: |          | 1028/? [24:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1028, loss 4.0474019050598145\n",
      "Epoch 4: |          | 1029/? [24:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1029, loss 3.931931257247925\n",
      "Epoch 4: |          | 1030/? [24:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1030, loss 3.773543119430542\n",
      "Epoch 4: |          | 1031/? [24:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1031, loss 3.8157401084899902\n",
      "Epoch 4: |          | 1032/? [24:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1032, loss 4.340775012969971\n",
      "Epoch 4: |          | 1033/? [24:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1033, loss 4.517178535461426\n",
      "Epoch 4: |          | 1034/? [24:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1034, loss 3.8771586418151855\n",
      "Epoch 4: |          | 1035/? [24:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1035, loss 3.896570920944214\n",
      "Epoch 4: |          | 1036/? [24:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1036, loss 3.8035552501678467\n",
      "Epoch 4: |          | 1037/? [24:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1037, loss 4.454615116119385\n",
      "Epoch 4: |          | 1038/? [24:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1038, loss 4.618210792541504\n",
      "Epoch 4: |          | 1039/? [24:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1039, loss 4.912283897399902\n",
      "Epoch 4: |          | 1040/? [24:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1040, loss 4.196013927459717\n",
      "Epoch 4: |          | 1041/? [24:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1041, loss 4.526316165924072\n",
      "Epoch 4: |          | 1042/? [24:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1042, loss 4.122957229614258\n",
      "Epoch 4: |          | 1043/? [24:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1043, loss 4.475740432739258\n",
      "Epoch 4: |          | 1044/? [24:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1044, loss 4.043365478515625\n",
      "Epoch 4: |          | 1045/? [24:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1045, loss 3.5945351123809814\n",
      "Epoch 4: |          | 1046/? [24:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1046, loss 3.4095778465270996\n",
      "Epoch 4: |          | 1047/? [25:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1047, loss 4.671011447906494\n",
      "Epoch 4: |          | 1048/? [25:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1048, loss 4.02632474899292\n",
      "Epoch 4: |          | 1049/? [25:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1049, loss 4.253123760223389\n",
      "Epoch 4: |          | 1050/? [25:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1050, loss 3.8232624530792236\n",
      "Epoch 4: |          | 1051/? [25:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1051, loss 3.8121085166931152\n",
      "Epoch 4: |          | 1052/? [25:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1052, loss 4.414046764373779\n",
      "Epoch 4: |          | 1053/? [25:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1053, loss 4.575396537780762\n",
      "Epoch 4: |          | 1054/? [25:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1054, loss 3.954681873321533\n",
      "Epoch 4: |          | 1055/? [25:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1055, loss 3.6379504203796387\n",
      "Epoch 4: |          | 1056/? [25:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1056, loss 3.6510326862335205\n",
      "Epoch 4: |          | 1057/? [25:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1057, loss 4.319777488708496\n",
      "Epoch 4: |          | 1058/? [25:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1058, loss 3.864262342453003\n",
      "Epoch 4: |          | 1059/? [25:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1059, loss 4.4775390625\n",
      "Epoch 4: |          | 1060/? [25:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1060, loss 4.368844032287598\n",
      "Epoch 4: |          | 1061/? [25:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1061, loss 3.077042579650879\n",
      "Epoch 4: |          | 1062/? [25:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1062, loss 4.039227485656738\n",
      "Epoch 4: |          | 1063/? [25:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1063, loss 4.107064247131348\n",
      "Epoch 4: |          | 1064/? [25:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1064, loss 4.2993011474609375\n",
      "Epoch 4: |          | 1065/? [25:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1065, loss 2.85295033454895\n",
      "Epoch 4: |          | 1066/? [25:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1066, loss 4.200475692749023\n",
      "Epoch 4: |          | 1067/? [25:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1067, loss 3.71073842048645\n",
      "Epoch 4: |          | 1068/? [25:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1068, loss 3.8664844036102295\n",
      "Epoch 4: |          | 1069/? [25:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1069, loss 4.2532196044921875\n",
      "Epoch 4: |          | 1070/? [25:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1070, loss 4.007302761077881\n",
      "Epoch 4: |          | 1071/? [25:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1071, loss 4.4041643142700195\n",
      "Epoch 4: |          | 1072/? [25:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1072, loss 4.435238838195801\n",
      "Epoch 4: |          | 1073/? [25:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1073, loss 4.652766227722168\n",
      "Epoch 4: |          | 1074/? [25:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1074, loss 3.9442667961120605\n",
      "Epoch 4: |          | 1075/? [25:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1075, loss 3.7469050884246826\n",
      "Epoch 4: |          | 1076/? [25:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1076, loss 4.374112606048584\n",
      "Epoch 4: |          | 1077/? [25:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1077, loss 3.8721389770507812\n",
      "Epoch 4: |          | 1078/? [25:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1078, loss 4.126561164855957\n",
      "Epoch 4: |          | 1079/? [25:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1079, loss 4.65497350692749\n",
      "Epoch 4: |          | 1080/? [25:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1080, loss 4.079546928405762\n",
      "Epoch 4: |          | 1081/? [25:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1081, loss 4.3522138595581055\n",
      "Epoch 4: |          | 1082/? [25:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1082, loss 3.9353060722351074\n",
      "Epoch 4: |          | 1083/? [25:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1083, loss 3.5503132343292236\n",
      "Epoch 4: |          | 1084/? [25:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1084, loss 3.3227124214172363\n",
      "Epoch 4: |          | 1085/? [25:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1085, loss 3.996098279953003\n",
      "Epoch 4: |          | 1086/? [25:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1086, loss 4.289621829986572\n",
      "Epoch 4: |          | 1087/? [25:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1087, loss 4.784812927246094\n",
      "Epoch 4: |          | 1088/? [25:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1088, loss 4.358468055725098\n",
      "Epoch 4: |          | 1089/? [26:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1089, loss 4.3680100440979\n",
      "Epoch 4: |          | 1090/? [26:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1090, loss 4.148535251617432\n",
      "Epoch 4: |          | 1091/? [26:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1091, loss 3.9189136028289795\n",
      "Epoch 4: |          | 1092/? [26:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1092, loss 4.17962646484375\n",
      "Epoch 4: |          | 1093/? [26:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1093, loss 3.6320621967315674\n",
      "Epoch 4: |          | 1094/? [26:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1094, loss 4.168930530548096\n",
      "Epoch 4: |          | 1095/? [26:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1095, loss 4.260460376739502\n",
      "Epoch 4: |          | 1096/? [26:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1096, loss 4.462263107299805\n",
      "Epoch 4: |          | 1097/? [26:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1097, loss 4.032458782196045\n",
      "Epoch 4: |          | 1098/? [26:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1098, loss 3.2951037883758545\n",
      "Epoch 4: |          | 1099/? [26:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1099, loss 4.016915798187256\n",
      "Epoch 4: |          | 1100/? [26:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1100, loss 4.217724800109863\n",
      "Epoch 4: |          | 1101/? [26:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1101, loss 3.832867383956909\n",
      "Epoch 4: |          | 1102/? [26:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1102, loss 4.616914749145508\n",
      "Epoch 4: |          | 1103/? [26:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1103, loss 5.160999774932861\n",
      "Epoch 4: |          | 1104/? [26:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1104, loss 4.368826866149902\n",
      "Epoch 4: |          | 1105/? [26:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1105, loss 4.511847496032715\n",
      "Epoch 4: |          | 1106/? [26:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1106, loss 4.01706600189209\n",
      "Epoch 4: |          | 1107/? [26:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1107, loss 4.116171836853027\n",
      "Epoch 4: |          | 1108/? [26:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1108, loss 4.109233379364014\n",
      "Epoch 4: |          | 1109/? [26:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1109, loss 3.6820883750915527\n",
      "Epoch 4: |          | 1110/? [26:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1110, loss 4.68523645401001\n",
      "Epoch 4: |          | 1111/? [26:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1111, loss 4.352399826049805\n",
      "Epoch 4: |          | 1112/? [26:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1112, loss 4.191290855407715\n",
      "Epoch 4: |          | 1113/? [26:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1113, loss 4.000881195068359\n",
      "Epoch 4: |          | 1114/? [26:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1114, loss 3.4358572959899902\n",
      "Epoch 4: |          | 1115/? [26:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1115, loss 3.192899703979492\n",
      "Epoch 4: |          | 1116/? [26:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1116, loss 3.6469645500183105\n",
      "Epoch 4: |          | 1117/? [26:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1117, loss 3.793041706085205\n",
      "Epoch 4: |          | 1118/? [26:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1118, loss 3.941953182220459\n",
      "Epoch 4: |          | 1119/? [26:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1119, loss 4.600761413574219\n",
      "Epoch 4: |          | 1120/? [26:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1120, loss 4.10013484954834\n",
      "Epoch 4: |          | 1121/? [26:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1121, loss 4.3729987144470215\n",
      "Epoch 4: |          | 1122/? [26:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1122, loss 3.9612293243408203\n",
      "Epoch 4: |          | 1123/? [26:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1123, loss 4.177170753479004\n",
      "Epoch 4: |          | 1124/? [26:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1124, loss 4.512509822845459\n",
      "Epoch 4: |          | 1125/? [26:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1125, loss 3.781275510787964\n",
      "Epoch 4: |          | 1126/? [26:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1126, loss 3.685786485671997\n",
      "Epoch 4: |          | 1127/? [26:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1127, loss 4.040039539337158\n",
      "Epoch 4: |          | 1128/? [26:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1128, loss 4.078580379486084\n",
      "Epoch 4: |          | 1129/? [26:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1129, loss 4.1925859451293945\n",
      "Epoch 4: |          | 1130/? [26:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1130, loss 4.371582508087158\n",
      "Epoch 4: |          | 1131/? [27:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1131, loss 4.461129188537598\n",
      "Epoch 4: |          | 1132/? [27:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1132, loss 3.186140775680542\n",
      "Epoch 4: |          | 1133/? [27:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1133, loss 4.072789192199707\n",
      "Epoch 4: |          | 1134/? [27:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1134, loss 3.8527679443359375\n",
      "Epoch 4: |          | 1135/? [27:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1135, loss 4.5001726150512695\n",
      "Epoch 4: |          | 1136/? [27:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1136, loss 4.124368190765381\n",
      "Epoch 4: |          | 1137/? [27:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1137, loss 4.136232852935791\n",
      "Epoch 4: |          | 1138/? [27:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1138, loss 4.687417507171631\n",
      "Epoch 4: |          | 1139/? [27:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1139, loss 5.027036190032959\n",
      "Epoch 4: |          | 1140/? [27:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1140, loss 4.014536380767822\n",
      "Epoch 4: |          | 1141/? [27:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1141, loss 4.488664150238037\n",
      "Epoch 4: |          | 1142/? [27:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1142, loss 4.568245887756348\n",
      "Epoch 4: |          | 1143/? [27:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1143, loss 4.586730003356934\n",
      "Epoch 4: |          | 1144/? [27:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1144, loss 4.043906211853027\n",
      "Epoch 4: |          | 1145/? [27:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1145, loss 4.134954929351807\n",
      "Epoch 4: |          | 1146/? [27:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1146, loss 3.8044376373291016\n",
      "Epoch 4: |          | 1147/? [27:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1147, loss 3.6673576831817627\n",
      "Epoch 4: |          | 1148/? [27:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1148, loss 3.919525146484375\n",
      "Epoch 4: |          | 1149/? [27:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1149, loss 5.273383140563965\n",
      "Epoch 4: |          | 1150/? [27:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1150, loss 4.314141273498535\n",
      "Epoch 4: |          | 1151/? [27:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1151, loss 4.6461381912231445\n",
      "Epoch 4: |          | 1152/? [27:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1152, loss 3.8067212104797363\n",
      "Epoch 4: |          | 1153/? [27:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1153, loss 4.219113349914551\n",
      "Epoch 4: |          | 1154/? [27:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1154, loss 3.889249324798584\n",
      "Epoch 4: |          | 1155/? [27:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1155, loss 4.08583402633667\n",
      "Epoch 4: |          | 1156/? [27:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1156, loss 4.121695518493652\n",
      "Epoch 4: |          | 1157/? [27:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1157, loss 4.348414421081543\n",
      "Epoch 4: |          | 1158/? [27:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1158, loss 4.615838050842285\n",
      "Epoch 4: |          | 1159/? [27:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1159, loss 3.3182525634765625\n",
      "Epoch 4: |          | 1160/? [27:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1160, loss 4.5568037033081055\n",
      "Epoch 4: |          | 1161/? [27:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1161, loss 4.399962425231934\n",
      "Epoch 4: |          | 1162/? [27:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1162, loss 4.305078506469727\n",
      "Epoch 4: |          | 1163/? [27:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1163, loss 4.8678669929504395\n",
      "Epoch 4: |          | 1164/? [27:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1164, loss 4.657530307769775\n",
      "Epoch 4: |          | 1165/? [27:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1165, loss 3.7766919136047363\n",
      "Epoch 4: |          | 1166/? [27:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1166, loss 4.3309221267700195\n",
      "Epoch 4: |          | 1167/? [27:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1167, loss 4.396044731140137\n",
      "Epoch 4: |          | 1168/? [27:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1168, loss 4.824885368347168\n",
      "Epoch 4: |          | 1169/? [27:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1169, loss 3.8337321281433105\n",
      "Epoch 4: |          | 1170/? [27:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1170, loss 4.432620525360107\n",
      "Epoch 4: |          | 1171/? [27:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1171, loss 3.8256397247314453\n",
      "Epoch 4: |          | 1172/? [27:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1172, loss 3.733140230178833\n",
      "Epoch 4: |          | 1173/? [28:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1173, loss 4.309835433959961\n",
      "Epoch 4: |          | 1174/? [28:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1174, loss 3.7430331707000732\n",
      "Epoch 4: |          | 1175/? [28:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1175, loss 4.3996100425720215\n",
      "Epoch 4: |          | 1176/? [28:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1176, loss 4.456849098205566\n",
      "Epoch 4: |          | 1177/? [28:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1177, loss 4.592490196228027\n",
      "Epoch 4: |          | 1178/? [28:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1178, loss 3.9915409088134766\n",
      "Epoch 4: |          | 1179/? [28:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1179, loss 4.510958194732666\n",
      "Epoch 4: |          | 1180/? [28:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1180, loss 4.305354118347168\n",
      "Epoch 4: |          | 1181/? [28:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1181, loss 4.290768623352051\n",
      "Epoch 4: |          | 1182/? [28:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1182, loss 4.103582382202148\n",
      "Epoch 4: |          | 1183/? [28:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1183, loss 3.8439815044403076\n",
      "Epoch 4: |          | 1184/? [28:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1184, loss 4.161181449890137\n",
      "Epoch 4: |          | 1185/? [28:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1185, loss 4.005115985870361\n",
      "Epoch 4: |          | 1186/? [28:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1186, loss 4.207974433898926\n",
      "Epoch 4: |          | 1187/? [28:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1187, loss 4.073126792907715\n",
      "Epoch 4: |          | 1188/? [28:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1188, loss 4.4292893409729\n",
      "Epoch 4: |          | 1189/? [28:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1189, loss 4.522044658660889\n",
      "Epoch 4: |          | 1190/? [28:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1190, loss 4.045254230499268\n",
      "Epoch 4: |          | 1191/? [28:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1191, loss 4.12738037109375\n",
      "Epoch 4: |          | 1192/? [28:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1192, loss 4.423060417175293\n",
      "Epoch 4: |          | 1193/? [28:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1193, loss 3.924417495727539\n",
      "Epoch 4: |          | 1194/? [28:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1194, loss 3.5621399879455566\n",
      "Epoch 4: |          | 1195/? [28:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1195, loss 4.172876834869385\n",
      "Epoch 4: |          | 1196/? [28:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1196, loss 4.369063377380371\n",
      "Epoch 4: |          | 1197/? [28:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1197, loss 4.162703990936279\n",
      "Epoch 4: |          | 1198/? [28:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1198, loss 4.259158134460449\n",
      "Epoch 4: |          | 1199/? [28:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1199, loss 4.498183250427246\n",
      "Epoch 4: |          | 1200/? [28:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1200, loss 3.714317798614502\n",
      "Epoch 4: |          | 1201/? [28:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1201, loss 4.336970329284668\n",
      "Epoch 4: |          | 1202/? [28:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1202, loss 3.994631290435791\n",
      "Epoch 4: |          | 1203/? [28:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1203, loss 3.9974493980407715\n",
      "Epoch 4: |          | 1204/? [28:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1204, loss 3.4784653186798096\n",
      "Epoch 4: |          | 1205/? [28:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1205, loss 4.088799476623535\n",
      "Epoch 4: |          | 1206/? [28:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1206, loss 4.085362434387207\n",
      "Epoch 4: |          | 1207/? [28:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1207, loss 4.460031509399414\n",
      "Epoch 4: |          | 1208/? [28:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1208, loss 4.625791549682617\n",
      "Epoch 4: |          | 1209/? [28:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1209, loss 4.163653373718262\n",
      "Epoch 4: |          | 1210/? [28:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1210, loss 4.469725131988525\n",
      "Epoch 4: |          | 1211/? [28:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1211, loss 4.462050914764404\n",
      "Epoch 4: |          | 1212/? [28:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1212, loss 4.246138095855713\n",
      "Epoch 4: |          | 1213/? [28:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1213, loss 3.9494762420654297\n",
      "Epoch 4: |          | 1214/? [29:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1214, loss 4.587378025054932\n",
      "Epoch 4: |          | 1215/? [29:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1215, loss 3.9671452045440674\n",
      "Epoch 4: |          | 1216/? [29:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1216, loss 4.162533283233643\n",
      "Epoch 4: |          | 1217/? [29:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1217, loss 4.318224906921387\n",
      "Epoch 4: |          | 1218/? [29:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1218, loss 4.314866542816162\n",
      "Epoch 4: |          | 1219/? [29:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1219, loss 3.9645493030548096\n",
      "Epoch 4: |          | 1220/? [29:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1220, loss 4.727646827697754\n",
      "Epoch 4: |          | 1221/? [29:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1221, loss 4.277538776397705\n",
      "Epoch 4: |          | 1222/? [29:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1222, loss 3.312321186065674\n",
      "Epoch 4: |          | 1223/? [29:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1223, loss 3.473101854324341\n",
      "Epoch 4: |          | 1224/? [29:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1224, loss 3.863020658493042\n",
      "Epoch 4: |          | 1225/? [29:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1225, loss 4.5443572998046875\n",
      "Epoch 4: |          | 1226/? [29:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1226, loss 4.528193473815918\n",
      "Epoch 4: |          | 1227/? [29:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1227, loss 4.143211841583252\n",
      "Epoch 4: |          | 1228/? [29:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1228, loss 4.04952335357666\n",
      "Epoch 4: |          | 1229/? [29:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1229, loss 3.6095993518829346\n",
      "Epoch 4: |          | 1230/? [29:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1230, loss 4.298688888549805\n",
      "Epoch 4: |          | 1231/? [29:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1231, loss 4.349760055541992\n",
      "Epoch 4: |          | 1232/? [29:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1232, loss 4.509097099304199\n",
      "Epoch 4: |          | 1233/? [29:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1233, loss 4.300090312957764\n",
      "Epoch 4: |          | 1234/? [29:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1234, loss 3.293213367462158\n",
      "Epoch 4: |          | 1235/? [29:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1235, loss 4.384324073791504\n",
      "Epoch 4: |          | 1236/? [29:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1236, loss 3.7570602893829346\n",
      "Epoch 4: |          | 1237/? [29:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1237, loss 4.101809024810791\n",
      "Epoch 4: |          | 1238/? [29:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1238, loss 4.092304229736328\n",
      "Epoch 4: |          | 1239/? [29:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1239, loss 3.9661355018615723\n",
      "Epoch 4: |          | 1240/? [29:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1240, loss 4.626744747161865\n",
      "Epoch 4: |          | 1241/? [29:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1241, loss 4.145731449127197\n",
      "Epoch 4: |          | 1242/? [29:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1242, loss 4.0010762214660645\n",
      "Epoch 4: |          | 1243/? [29:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1243, loss 3.8465237617492676\n",
      "Epoch 4: |          | 1244/? [29:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1244, loss 4.013631343841553\n",
      "Epoch 4: |          | 1245/? [29:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1245, loss 3.5897490978240967\n",
      "Epoch 4: |          | 1246/? [29:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1246, loss 4.304215431213379\n",
      "Epoch 4: |          | 1247/? [29:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1247, loss 4.311523914337158\n",
      "Epoch 4: |          | 1248/? [29:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1248, loss 3.846829652786255\n",
      "Epoch 4: |          | 1249/? [29:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1249, loss 4.015913009643555\n",
      "Epoch 4: |          | 1250/? [29:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1250, loss 4.173520565032959\n",
      "Epoch 4: |          | 1251/? [29:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1251, loss 3.882098436355591\n",
      "Epoch 4: |          | 1252/? [29:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1252, loss 4.678398609161377\n",
      "Epoch 4: |          | 1253/? [29:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1253, loss 4.076422691345215\n",
      "Epoch 4: |          | 1254/? [29:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1254, loss 3.3437716960906982\n",
      "Epoch 4: |          | 1255/? [30:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1255, loss 4.787209510803223\n",
      "Epoch 4: |          | 1256/? [30:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1256, loss 3.742997646331787\n",
      "Epoch 4: |          | 1257/? [30:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1257, loss 3.7941367626190186\n",
      "Epoch 4: |          | 1258/? [30:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1258, loss 4.507565498352051\n",
      "Epoch 4: |          | 1259/? [30:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1259, loss 4.183767795562744\n",
      "Epoch 4: |          | 1260/? [30:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1260, loss 4.640442848205566\n",
      "Epoch 4: |          | 1261/? [30:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1261, loss 3.970954179763794\n",
      "Epoch 4: |          | 1262/? [30:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1262, loss 3.9162497520446777\n",
      "Epoch 4: |          | 1263/? [30:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1263, loss 4.332269668579102\n",
      "Epoch 4: |          | 1264/? [30:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1264, loss 4.593680381774902\n",
      "Epoch 4: |          | 1265/? [30:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1265, loss 4.411622047424316\n",
      "Epoch 4: |          | 1266/? [30:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1266, loss 4.053293228149414\n",
      "Epoch 4: |          | 1267/? [30:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1267, loss 4.157386302947998\n",
      "Epoch 4: |          | 1268/? [30:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1268, loss 4.032402038574219\n",
      "Epoch 4: |          | 1269/? [30:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1269, loss 3.598109483718872\n",
      "Epoch 4: |          | 1270/? [30:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1270, loss 3.9248130321502686\n",
      "Epoch 4: |          | 1271/? [30:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1271, loss 4.114818572998047\n",
      "Epoch 4: |          | 1272/? [30:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1272, loss 3.668313503265381\n",
      "Epoch 4: |          | 1273/? [30:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1273, loss 4.385359764099121\n",
      "Epoch 4: |          | 1274/? [30:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1274, loss 3.269608974456787\n",
      "Epoch 4: |          | 1275/? [30:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1275, loss 3.8072876930236816\n",
      "Epoch 4: |          | 1276/? [30:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1276, loss 4.156148433685303\n",
      "Epoch 4: |          | 1277/? [30:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1277, loss 3.8267486095428467\n",
      "Epoch 4: |          | 1278/? [30:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1278, loss 3.5507569313049316\n",
      "Epoch 4: |          | 1279/? [30:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1279, loss 4.2569966316223145\n",
      "Epoch 4: |          | 1280/? [30:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1280, loss 3.4334301948547363\n",
      "Epoch 4: |          | 1281/? [30:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1281, loss 3.9361584186553955\n",
      "Epoch 4: |          | 1282/? [30:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1282, loss 3.6255054473876953\n",
      "Epoch 4: |          | 1283/? [30:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1283, loss 4.412570476531982\n",
      "Epoch 4: |          | 1284/? [30:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1284, loss 3.471836805343628\n",
      "Epoch 4: |          | 1285/? [30:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1285, loss 4.608614921569824\n",
      "Epoch 4: |          | 1286/? [30:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1286, loss 3.032514810562134\n",
      "Epoch 4: |          | 1287/? [30:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1287, loss 4.469271183013916\n",
      "Epoch 4: |          | 1288/? [30:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1288, loss 4.296144962310791\n",
      "Epoch 4: |          | 1289/? [30:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1289, loss 3.3444747924804688\n",
      "Epoch 4: |          | 1290/? [30:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1290, loss 4.199162006378174\n",
      "Epoch 4: |          | 1291/? [30:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1291, loss 5.09241247177124\n",
      "Epoch 4: |          | 1292/? [30:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1292, loss 4.391605854034424\n",
      "Epoch 4: |          | 1293/? [30:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1293, loss 3.8640689849853516\n",
      "Epoch 4: |          | 1294/? [30:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1294, loss 4.122097969055176\n",
      "Epoch 4: |          | 1295/? [30:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1295, loss 4.184483528137207\n",
      "Epoch 4: |          | 1296/? [30:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1296, loss 3.4375557899475098\n",
      "Epoch 4: |          | 1297/? [31:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1297, loss 4.376873970031738\n",
      "Epoch 4: |          | 1298/? [31:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1298, loss 4.112212181091309\n",
      "Epoch 4: |          | 1299/? [31:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1299, loss 3.206730365753174\n",
      "Epoch 4: |          | 1300/? [31:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1300, loss 4.144200801849365\n",
      "Epoch 4: |          | 1301/? [31:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1301, loss 3.8488376140594482\n",
      "Epoch 4: |          | 1302/? [31:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1302, loss 3.982938289642334\n",
      "Epoch 4: |          | 1303/? [31:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1303, loss 3.893622636795044\n",
      "Epoch 4: |          | 1304/? [31:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1304, loss 4.677204132080078\n",
      "Epoch 4: |          | 1305/? [31:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1305, loss 3.482715606689453\n",
      "Epoch 4: |          | 1306/? [31:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1306, loss 4.084082126617432\n",
      "Epoch 4: |          | 1307/? [31:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1307, loss 3.6718266010284424\n",
      "Epoch 4: |          | 1308/? [31:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1308, loss 3.6057727336883545\n",
      "Epoch 4: |          | 1309/? [31:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1309, loss 3.7344565391540527\n",
      "Epoch 4: |          | 1310/? [31:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1310, loss 4.26672887802124\n",
      "Epoch 4: |          | 1311/? [31:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1311, loss 3.701810359954834\n",
      "Epoch 4: |          | 1312/? [31:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1312, loss 3.4497170448303223\n",
      "Epoch 4: |          | 1313/? [31:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1313, loss 4.615341663360596\n",
      "Epoch 4: |          | 1314/? [31:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1314, loss 3.7839787006378174\n",
      "Epoch 4: |          | 1315/? [31:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1315, loss 4.565521240234375\n",
      "Epoch 4: |          | 1316/? [31:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1316, loss 4.360347747802734\n",
      "Epoch 4: |          | 1317/? [31:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1317, loss 3.9650771617889404\n",
      "Epoch 4: |          | 1318/? [31:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1318, loss 4.166250228881836\n",
      "Epoch 4: |          | 1319/? [31:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1319, loss 4.346457481384277\n",
      "Epoch 4: |          | 1320/? [31:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1320, loss 3.9466865062713623\n",
      "Epoch 4: |          | 1321/? [31:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1321, loss 4.368857383728027\n",
      "Epoch 4: |          | 1322/? [31:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1322, loss 4.2703423500061035\n",
      "Epoch 4: |          | 1323/? [31:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1323, loss 3.6987693309783936\n",
      "Epoch 4: |          | 1324/? [31:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1324, loss 4.683230876922607\n",
      "Epoch 4: |          | 1325/? [31:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1325, loss 4.8270697593688965\n",
      "Epoch 4: |          | 1326/? [31:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1326, loss 4.220963954925537\n",
      "Epoch 4: |          | 1327/? [31:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1327, loss 4.076000213623047\n",
      "Epoch 4: |          | 1328/? [31:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1328, loss 3.7863998413085938\n",
      "Epoch 4: |          | 1329/? [31:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1329, loss 4.3979692459106445\n",
      "Epoch 4: |          | 1330/? [31:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1330, loss 4.114433288574219\n",
      "Epoch 4: |          | 1331/? [31:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1331, loss 4.212981224060059\n",
      "Epoch 4: |          | 1332/? [31:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1332, loss 3.9513397216796875\n",
      "Epoch 4: |          | 1333/? [31:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1333, loss 3.9148011207580566\n",
      "Epoch 4: |          | 1334/? [31:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1334, loss 4.0563530921936035\n",
      "Epoch 4: |          | 1335/? [31:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1335, loss 3.963362216949463\n",
      "Epoch 4: |          | 1336/? [31:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1336, loss 3.579094409942627\n",
      "Epoch 4: |          | 1337/? [31:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1337, loss 4.242471218109131\n",
      "Epoch 4: |          | 1338/? [32:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1338, loss 3.416393995285034\n",
      "Epoch 4: |          | 1339/? [32:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1339, loss 4.110847473144531\n",
      "Epoch 4: |          | 1340/? [32:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1340, loss 3.488145112991333\n",
      "Epoch 4: |          | 1341/? [32:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1341, loss 4.23361349105835\n",
      "Epoch 4: |          | 1342/? [32:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1342, loss 4.492639541625977\n",
      "Epoch 4: |          | 1343/? [32:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1343, loss 3.9873898029327393\n",
      "Epoch 4: |          | 1344/? [32:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1344, loss 4.091244697570801\n",
      "Epoch 4: |          | 1345/? [32:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1345, loss 4.217139720916748\n",
      "Epoch 4: |          | 1346/? [32:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1346, loss 5.421131134033203\n",
      "Epoch 4: |          | 1347/? [32:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1347, loss 4.322929382324219\n",
      "Epoch 4: |          | 1348/? [32:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1348, loss 4.488228797912598\n",
      "Epoch 4: |          | 1349/? [32:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1349, loss 4.292803764343262\n",
      "Epoch 4: |          | 1350/? [32:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1350, loss 4.526796340942383\n",
      "Epoch 4: |          | 1351/? [32:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1351, loss 4.359034538269043\n",
      "Epoch 4: |          | 1352/? [32:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1352, loss 3.531035900115967\n",
      "Epoch 4: |          | 1353/? [32:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1353, loss 3.8746631145477295\n",
      "Epoch 4: |          | 1354/? [32:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1354, loss 4.32367467880249\n",
      "Epoch 4: |          | 1355/? [32:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1355, loss 4.479905128479004\n",
      "Epoch 4: |          | 1356/? [32:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1356, loss 4.189260005950928\n",
      "Epoch 4: |          | 1357/? [32:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1357, loss 3.9437179565429688\n",
      "Epoch 4: |          | 1358/? [32:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1358, loss 4.125092029571533\n",
      "Epoch 4: |          | 1359/? [32:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1359, loss 4.015310287475586\n",
      "Epoch 4: |          | 1360/? [32:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1360, loss 4.190669536590576\n",
      "Epoch 4: |          | 1361/? [32:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1361, loss 4.144649505615234\n",
      "Epoch 4: |          | 1362/? [32:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1362, loss 3.973593235015869\n",
      "Epoch 4: |          | 1363/? [32:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1363, loss 3.4208405017852783\n",
      "Epoch 4: |          | 1364/? [32:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1364, loss 3.9166531562805176\n",
      "Epoch 4: |          | 1365/? [32:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1365, loss 3.6167595386505127\n",
      "Epoch 4: |          | 1366/? [32:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1366, loss 4.37603759765625\n",
      "Epoch 4: |          | 1367/? [32:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1367, loss 3.637956142425537\n",
      "Epoch 4: |          | 1368/? [32:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1368, loss 3.4323277473449707\n",
      "Epoch 4: |          | 1369/? [32:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1369, loss 4.1566691398620605\n",
      "Epoch 4: |          | 1370/? [32:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1370, loss 3.634181499481201\n",
      "Epoch 4: |          | 1371/? [32:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1371, loss 4.593576431274414\n",
      "Epoch 4: |          | 1372/? [32:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1372, loss 4.006342887878418\n",
      "Epoch 4: |          | 1373/? [32:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1373, loss 4.449131011962891\n",
      "Epoch 4: |          | 1374/? [32:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1374, loss 3.5163044929504395\n",
      "Epoch 4: |          | 1375/? [32:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1375, loss 4.173396110534668\n",
      "Epoch 4: |          | 1376/? [32:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1376, loss 4.040248394012451\n",
      "Epoch 4: |          | 1377/? [32:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1377, loss 4.023085594177246\n",
      "Epoch 4: |          | 1378/? [32:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1378, loss 4.026206970214844\n",
      "Epoch 4: |          | 1379/? [32:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1379, loss 4.065701484680176\n",
      "Epoch 4: |          | 1380/? [32:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1380, loss 4.180933475494385\n",
      "Epoch 4: |          | 1381/? [33:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1381, loss 4.301487922668457\n",
      "Epoch 4: |          | 1382/? [33:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1382, loss 3.836700439453125\n",
      "Epoch 4: |          | 1383/? [33:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1383, loss 4.096563339233398\n",
      "Epoch 4: |          | 1384/? [33:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1384, loss 4.086897850036621\n",
      "Epoch 4: |          | 1385/? [33:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1385, loss 4.029610633850098\n",
      "Epoch 4: |          | 1386/? [33:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1386, loss 4.078975200653076\n",
      "Epoch 4: |          | 1387/? [33:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1387, loss 4.051880836486816\n",
      "Epoch 4: |          | 1388/? [33:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1388, loss 3.5136513710021973\n",
      "Epoch 4: |          | 1389/? [33:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1389, loss 4.285336971282959\n",
      "Epoch 4: |          | 1390/? [33:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1390, loss 4.624019622802734\n",
      "Epoch 4: |          | 1391/? [33:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1391, loss 4.218089580535889\n",
      "Epoch 4: |          | 1392/? [33:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1392, loss 3.699939250946045\n",
      "Epoch 4: |          | 1393/? [33:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1393, loss 3.8617796897888184\n",
      "Epoch 4: |          | 1394/? [33:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1394, loss 3.5464553833007812\n",
      "Epoch 4: |          | 1395/? [33:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1395, loss 4.210291385650635\n",
      "Epoch 4: |          | 1396/? [33:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1396, loss 4.142682075500488\n",
      "Epoch 4: |          | 1397/? [33:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1397, loss 3.2977538108825684\n",
      "Epoch 4: |          | 1398/? [33:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1398, loss 4.548949718475342\n",
      "Epoch 4: |          | 1399/? [33:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1399, loss 4.605693817138672\n",
      "Epoch 4: |          | 1400/? [33:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1400, loss 3.6219844818115234\n",
      "Epoch 4: |          | 1401/? [33:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1401, loss 4.488051414489746\n",
      "Epoch 4: |          | 1402/? [33:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1402, loss 4.038236141204834\n",
      "Epoch 4: |          | 1403/? [33:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1403, loss 4.249927997589111\n",
      "Epoch 4: |          | 1404/? [33:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1404, loss 4.237198352813721\n",
      "Epoch 4: |          | 1405/? [33:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1405, loss 4.573863506317139\n",
      "Epoch 4: |          | 1406/? [33:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1406, loss 4.46152925491333\n",
      "Epoch 4: |          | 1407/? [33:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1407, loss 4.595717430114746\n",
      "Epoch 4: |          | 1408/? [33:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1408, loss 3.761889696121216\n",
      "Epoch 4: |          | 1409/? [33:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1409, loss 3.8526389598846436\n",
      "Epoch 4: |          | 1410/? [33:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1410, loss 3.888780117034912\n",
      "Epoch 4: |          | 1411/? [33:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1411, loss 4.219438552856445\n",
      "Epoch 4: |          | 1412/? [33:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1412, loss 3.831085681915283\n",
      "Epoch 4: |          | 1413/? [33:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1413, loss 3.729682445526123\n",
      "Epoch 4: |          | 1414/? [33:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1414, loss 3.83674693107605\n",
      "Epoch 4: |          | 1415/? [33:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1415, loss 4.135694980621338\n",
      "Epoch 4: |          | 1416/? [33:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1416, loss 4.537521839141846\n",
      "Epoch 4: |          | 1417/? [33:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1417, loss 4.120955944061279\n",
      "Epoch 4: |          | 1418/? [33:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1418, loss 4.3233771324157715\n",
      "Epoch 4: |          | 1419/? [33:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1419, loss 4.075882911682129\n",
      "Epoch 4: |          | 1420/? [34:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1420, loss 3.9071688652038574\n",
      "Epoch 4: |          | 1421/? [34:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1421, loss 3.630315065383911\n",
      "Epoch 4: |          | 1422/? [34:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1422, loss 4.37587833404541\n",
      "Epoch 4: |          | 1423/? [34:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1423, loss 4.420250415802002\n",
      "Epoch 4: |          | 1424/? [34:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1424, loss 4.049684047698975\n",
      "Epoch 4: |          | 1425/? [34:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1425, loss 4.280252933502197\n",
      "Epoch 4: |          | 1426/? [34:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1426, loss 3.7730541229248047\n",
      "Epoch 4: |          | 1427/? [34:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1427, loss 4.426084518432617\n",
      "Epoch 4: |          | 1428/? [34:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1428, loss 4.402008533477783\n",
      "Epoch 4: |          | 1429/? [34:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1429, loss 4.303466796875\n",
      "Epoch 4: |          | 1430/? [34:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1430, loss 4.3524885177612305\n",
      "Epoch 4: |          | 1431/? [34:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1431, loss 4.113487243652344\n",
      "Epoch 4: |          | 1432/? [34:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1432, loss 4.156348705291748\n",
      "Epoch 4: |          | 1433/? [34:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1433, loss 4.025771141052246\n",
      "Epoch 4: |          | 1434/? [34:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1434, loss 4.209383964538574\n",
      "Epoch 4: |          | 1435/? [34:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1435, loss 3.786452531814575\n",
      "Epoch 4: |          | 1436/? [34:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1436, loss 4.160458087921143\n",
      "Epoch 4: |          | 1437/? [34:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1437, loss 3.3943068981170654\n",
      "Epoch 4: |          | 1438/? [34:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1438, loss 5.0944013595581055\n",
      "Epoch 4: |          | 1439/? [34:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1439, loss 4.22186279296875\n",
      "Epoch 4: |          | 1440/? [34:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1440, loss 4.319149971008301\n",
      "Epoch 4: |          | 1441/? [34:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1441, loss 4.666457176208496\n",
      "Epoch 4: |          | 1442/? [34:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1442, loss 4.67985725402832\n",
      "Epoch 4: |          | 1443/? [34:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1443, loss 3.7765889167785645\n",
      "Epoch 4: |          | 1444/? [34:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1444, loss 3.839907169342041\n",
      "Epoch 4: |          | 1445/? [34:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1445, loss 4.364840507507324\n",
      "Epoch 4: |          | 1446/? [34:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1446, loss 3.951277494430542\n",
      "Epoch 4: |          | 1447/? [34:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1447, loss 4.063422679901123\n",
      "Epoch 4: |          | 1448/? [34:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1448, loss 3.875073194503784\n",
      "Epoch 4: |          | 1449/? [34:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1449, loss 4.127072334289551\n",
      "Epoch 4: |          | 1450/? [34:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1450, loss 4.185880661010742\n",
      "Epoch 4: |          | 1451/? [34:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1451, loss 4.522643089294434\n",
      "Epoch 4: |          | 1452/? [34:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1452, loss 4.1794962882995605\n",
      "Epoch 4: |          | 1453/? [34:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1453, loss 3.502694606781006\n",
      "Epoch 4: |          | 1454/? [34:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1454, loss 4.102021217346191\n",
      "Epoch 4: |          | 1455/? [34:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1455, loss 4.198641300201416\n",
      "Epoch 4: |          | 1456/? [34:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1456, loss 3.7668585777282715\n",
      "Epoch 4: |          | 1457/? [34:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1457, loss 3.9416587352752686\n",
      "Epoch 4: |          | 1458/? [34:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1458, loss 4.131474494934082\n",
      "Epoch 4: |          | 1459/? [34:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1459, loss 4.3230061531066895\n",
      "Epoch 4: |          | 1460/? [34:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1460, loss 4.204986095428467\n",
      "Epoch 4: |          | 1461/? [34:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1461, loss 4.173111438751221\n",
      "Epoch 4: |          | 1462/? [35:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1462, loss 4.472936153411865\n",
      "Epoch 4: |          | 1463/? [35:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1463, loss 4.41803503036499\n",
      "Epoch 4: |          | 1464/? [35:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1464, loss 3.693079710006714\n",
      "Epoch 4: |          | 1465/? [35:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1465, loss 4.090205669403076\n",
      "Epoch 4: |          | 1466/? [35:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1466, loss 3.738953113555908\n",
      "Epoch 4: |          | 1467/? [35:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1467, loss 4.4154133796691895\n",
      "Epoch 4: |          | 1468/? [35:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1468, loss 3.9891140460968018\n",
      "Epoch 4: |          | 1469/? [35:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1469, loss 3.6841557025909424\n",
      "Epoch 4: |          | 1470/? [35:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1470, loss 4.260484218597412\n",
      "Epoch 4: |          | 1471/? [35:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1471, loss 4.492824554443359\n",
      "Epoch 4: |          | 1472/? [35:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1472, loss 4.218723297119141\n",
      "Epoch 4: |          | 1473/? [35:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1473, loss 4.028160095214844\n",
      "Epoch 4: |          | 1474/? [35:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1474, loss 3.9222774505615234\n",
      "Epoch 4: |          | 1475/? [35:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1475, loss 3.542562961578369\n",
      "Epoch 4: |          | 1476/? [35:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1476, loss 4.140799522399902\n",
      "Epoch 4: |          | 1477/? [35:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1477, loss 4.156664848327637\n",
      "Epoch 4: |          | 1478/? [35:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1478, loss 4.019166469573975\n",
      "Epoch 4: |          | 1479/? [35:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1479, loss 4.5947394371032715\n",
      "Epoch 4: |          | 1480/? [35:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1480, loss 4.285305976867676\n",
      "Epoch 4: |          | 1481/? [35:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1481, loss 4.054936408996582\n",
      "Epoch 4: |          | 1482/? [35:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1482, loss 4.166825771331787\n",
      "Epoch 4: |          | 1483/? [35:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1483, loss 3.8137402534484863\n",
      "Epoch 4: |          | 1484/? [35:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1484, loss 4.025075435638428\n",
      "Epoch 4: |          | 1485/? [35:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1485, loss 4.173214435577393\n",
      "Epoch 4: |          | 1486/? [35:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1486, loss 4.1494550704956055\n",
      "Epoch 4: |          | 1487/? [35:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1487, loss 3.5990099906921387\n",
      "Epoch 4: |          | 1488/? [35:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1488, loss 4.279043197631836\n",
      "Epoch 4: |          | 1489/? [35:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1489, loss 4.226534843444824\n",
      "Epoch 4: |          | 1490/? [35:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1490, loss 4.166537761688232\n",
      "Epoch 4: |          | 1491/? [35:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1491, loss 3.074761152267456\n",
      "Epoch 4: |          | 1492/? [35:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1492, loss 3.6389553546905518\n",
      "Epoch 4: |          | 1493/? [35:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1493, loss 3.2717270851135254\n",
      "Epoch 4: |          | 1494/? [35:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1494, loss 4.050660610198975\n",
      "Epoch 4: |          | 1495/? [35:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1495, loss 3.921558380126953\n",
      "Epoch 4: |          | 1496/? [35:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1496, loss 4.3008222579956055\n",
      "Epoch 4: |          | 1497/? [35:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1497, loss 3.547694683074951\n",
      "Epoch 4: |          | 1498/? [35:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1498, loss 3.792322874069214\n",
      "Epoch 4: |          | 1499/? [35:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1499, loss 4.371308326721191\n",
      "Epoch 4: |          | 1500/? [35:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1500, loss 4.320094108581543\n",
      "Epoch 4: |          | 1501/? [35:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1501, loss 4.131080627441406\n",
      "Epoch 4: |          | 1502/? [35:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1502, loss 4.213883399963379\n",
      "Epoch 4: |          | 1503/? [35:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1503, loss 3.957920551300049\n",
      "Epoch 4: |          | 1504/? [36:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1504, loss 4.5949177742004395\n",
      "Epoch 4: |          | 1505/? [36:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1505, loss 4.443329334259033\n",
      "Epoch 4: |          | 1506/? [36:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1506, loss 4.037069320678711\n",
      "Epoch 4: |          | 1507/? [36:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1507, loss 3.874957323074341\n",
      "Epoch 4: |          | 1508/? [36:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1508, loss 4.059357643127441\n",
      "Epoch 4: |          | 1509/? [36:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1509, loss 4.044930458068848\n",
      "Epoch 4: |          | 1510/? [36:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1510, loss 4.323417663574219\n",
      "Epoch 4: |          | 1511/? [36:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1511, loss 3.8801963329315186\n",
      "Epoch 4: |          | 1512/? [36:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1512, loss 4.540614128112793\n",
      "Epoch 4: |          | 1513/? [36:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1513, loss 4.707427024841309\n",
      "Epoch 4: |          | 1514/? [36:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1514, loss 3.7644829750061035\n",
      "Epoch 4: |          | 1515/? [36:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1515, loss 4.694288730621338\n",
      "Epoch 4: |          | 1516/? [36:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1516, loss 4.516656398773193\n",
      "Epoch 4: |          | 1517/? [36:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1517, loss 3.9893746376037598\n",
      "Epoch 4: |          | 1518/? [36:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1518, loss 3.7719993591308594\n",
      "Epoch 4: |          | 1519/? [36:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1519, loss 4.329634189605713\n",
      "Epoch 4: |          | 1520/? [36:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1520, loss 4.448054790496826\n",
      "Epoch 4: |          | 1521/? [36:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1521, loss 4.1172261238098145\n",
      "Epoch 4: |          | 1522/? [36:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1522, loss 3.8501079082489014\n",
      "Epoch 4: |          | 1523/? [36:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1523, loss 4.19693660736084\n",
      "Epoch 4: |          | 1524/? [36:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1524, loss 4.098656177520752\n",
      "Epoch 4: |          | 1525/? [36:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1525, loss 3.8174922466278076\n",
      "Epoch 4: |          | 1526/? [36:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1526, loss 4.357396602630615\n",
      "Epoch 4: |          | 1527/? [36:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1527, loss 4.358214378356934\n",
      "Epoch 4: |          | 1528/? [36:34<00:00,  0.70it/s, v_num=31]ERROR: Input has inproper shape\n",
      "Epoch 4: |          | 1529/? [36:34<00:00,  0.70it/s, v_num=31]   VALIDATION: Batch 0, loss 4.693024635314941\n",
      "   VALIDATION: Batch 1, loss 3.655355453491211\n",
      "   VALIDATION: Batch 2, loss 4.871747970581055\n",
      "   VALIDATION: Batch 3, loss 4.514957427978516\n",
      "   VALIDATION: Batch 4, loss 4.11275577545166\n",
      "   VALIDATION: Batch 5, loss 3.7724876403808594\n",
      "   VALIDATION: Batch 6, loss 4.067714691162109\n",
      "   VALIDATION: Batch 7, loss 4.715044975280762\n",
      "   VALIDATION: Batch 8, loss 4.558718681335449\n",
      "   VALIDATION: Batch 9, loss 4.713788032531738\n",
      "   VALIDATION: Batch 10, loss 4.3925251960754395\n",
      "   VALIDATION: Batch 11, loss 4.047593116760254\n",
      "   VALIDATION: Batch 12, loss 4.241988182067871\n",
      "   VALIDATION: Batch 13, loss 4.795910358428955\n",
      "   VALIDATION: Batch 14, loss 4.086320877075195\n",
      "   VALIDATION: Batch 15, loss 4.021909236907959\n",
      "   VALIDATION: Batch 16, loss 4.670126914978027\n",
      "   VALIDATION: Batch 17, loss 4.3706560134887695\n",
      "   VALIDATION: Batch 18, loss 3.607722043991089\n",
      "   VALIDATION: Batch 19, loss 4.509914398193359\n",
      "   VALIDATION: Batch 20, loss 4.7870564460754395\n",
      "   VALIDATION: Batch 21, loss 5.012373924255371\n",
      "   VALIDATION: Batch 22, loss 4.660643577575684\n",
      "   VALIDATION: Batch 23, loss 4.187958717346191\n",
      "   VALIDATION: Batch 24, loss 4.0325422286987305\n",
      "   VALIDATION: Batch 25, loss 4.437502384185791\n",
      "   VALIDATION: Batch 26, loss 4.664865016937256\n",
      "   VALIDATION: Batch 27, loss 4.510984897613525\n",
      "   VALIDATION: Batch 28, loss 4.300093650817871\n",
      "   VALIDATION: Batch 29, loss 4.527798652648926\n",
      "   VALIDATION: Batch 30, loss 4.137568473815918\n",
      "   VALIDATION: Batch 31, loss 4.444336414337158\n",
      "   VALIDATION: Batch 32, loss 4.970402717590332\n",
      "   VALIDATION: Batch 33, loss 3.2054107189178467\n",
      "   VALIDATION: Batch 34, loss 4.391556739807129\n",
      "   VALIDATION: Batch 35, loss 4.630400657653809\n",
      "   VALIDATION: Batch 36, loss 3.959866762161255\n",
      "   VALIDATION: Batch 37, loss 3.922785997390747\n",
      "   VALIDATION: Batch 38, loss 4.018889904022217\n",
      "   VALIDATION: Batch 39, loss 4.386880874633789\n",
      "   VALIDATION: Batch 40, loss 4.450521469116211\n",
      "   VALIDATION: Batch 41, loss 3.3214969635009766\n",
      "   VALIDATION: Batch 42, loss 4.62308406829834\n",
      "   VALIDATION: Batch 43, loss 4.617522239685059\n",
      "   VALIDATION: Batch 44, loss 4.226017475128174\n",
      "   VALIDATION: Batch 45, loss 4.640769958496094\n",
      "   VALIDATION: Batch 46, loss 3.765247344970703\n",
      "   VALIDATION: Batch 47, loss 4.774572849273682\n",
      "   VALIDATION: Batch 48, loss 4.856354236602783\n",
      "   VALIDATION: Batch 49, loss 4.521148204803467\n",
      "   VALIDATION: Batch 50, loss 4.469681739807129\n",
      "   VALIDATION: Batch 51, loss 4.93550968170166\n",
      "   VALIDATION: Batch 52, loss 4.108184337615967\n",
      "   VALIDATION: Batch 53, loss 3.9687886238098145\n",
      "   VALIDATION: Batch 54, loss 4.071047782897949\n",
      "   VALIDATION: Batch 55, loss 4.791778564453125\n",
      "   VALIDATION: Batch 56, loss 4.2215375900268555\n",
      "   VALIDATION: Batch 57, loss 5.658271789550781\n",
      "   VALIDATION: Batch 58, loss 4.3272385597229\n",
      "   VALIDATION: Batch 59, loss 3.962867259979248\n",
      "   VALIDATION: Batch 60, loss 3.508244752883911\n",
      "   VALIDATION: Batch 61, loss 4.3721208572387695\n",
      "   VALIDATION: Batch 62, loss 4.3381547927856445\n",
      "   VALIDATION: Batch 63, loss 4.848536968231201\n",
      "   VALIDATION: Batch 64, loss 4.667766094207764\n",
      "   VALIDATION: Batch 65, loss 3.8282675743103027\n",
      "   VALIDATION: Batch 66, loss 4.758738994598389\n",
      "   VALIDATION: Batch 67, loss 4.149013996124268\n",
      "   VALIDATION: Batch 68, loss 4.327868461608887\n",
      "   VALIDATION: Batch 69, loss 4.570670127868652\n",
      "   VALIDATION: Batch 70, loss 4.721668720245361\n",
      "   VALIDATION: Batch 71, loss 4.236849308013916\n",
      "   VALIDATION: Batch 72, loss 5.1086320877075195\n",
      "   VALIDATION: Batch 73, loss 3.9577629566192627\n",
      "   VALIDATION: Batch 74, loss 4.560732841491699\n",
      "   VALIDATION: Batch 75, loss 4.538214683532715\n",
      "   VALIDATION: Batch 76, loss 4.405811786651611\n",
      "   VALIDATION: Batch 77, loss 4.659564971923828\n",
      "   VALIDATION: Batch 78, loss 4.467843055725098\n",
      "   VALIDATION: Batch 79, loss 4.446310043334961\n",
      "   VALIDATION: Batch 80, loss 4.517036437988281\n",
      "   VALIDATION: Batch 81, loss 4.254916667938232\n",
      "   VALIDATION: Batch 82, loss 4.609045028686523\n",
      "   VALIDATION: Batch 83, loss 3.897113084793091\n",
      "   VALIDATION: Batch 84, loss 4.584654808044434\n",
      "   VALIDATION: Batch 85, loss 4.241422176361084\n",
      "   VALIDATION: Batch 86, loss 4.31734037399292\n",
      "   VALIDATION: Batch 87, loss 4.164821624755859\n",
      "   VALIDATION: Batch 88, loss 3.766965389251709\n",
      "   VALIDATION: Batch 89, loss 4.108579635620117\n",
      "   VALIDATION: Batch 90, loss 4.319289684295654\n",
      "   VALIDATION: Batch 91, loss 4.414135932922363\n",
      "   VALIDATION: Batch 92, loss 4.14983606338501\n",
      "   VALIDATION: Batch 93, loss 4.835358142852783\n",
      "   VALIDATION: Batch 94, loss 4.3160271644592285\n",
      "   VALIDATION: Batch 95, loss 3.7999203205108643\n",
      "   VALIDATION: Batch 96, loss 4.265158653259277\n",
      "   VALIDATION: Batch 97, loss 4.004141807556152\n",
      "   VALIDATION: Batch 98, loss 4.583913803100586\n",
      "   VALIDATION: Batch 99, loss 4.700612545013428\n",
      "   VALIDATION: Batch 100, loss 5.003537178039551\n",
      "   VALIDATION: Batch 101, loss 3.636826992034912\n",
      "   VALIDATION: Batch 102, loss 5.0538763999938965\n",
      "   VALIDATION: Batch 103, loss 4.96162223815918\n",
      "   VALIDATION: Batch 104, loss 3.924124240875244\n",
      "   VALIDATION: Batch 105, loss 4.4127068519592285\n",
      "   VALIDATION: Batch 106, loss 4.233791351318359\n",
      "   VALIDATION: Batch 107, loss 4.381199836730957\n",
      "   VALIDATION: Batch 108, loss 4.074234485626221\n",
      "   VALIDATION: Batch 109, loss 4.702679634094238\n",
      "   VALIDATION: Batch 110, loss 4.377232551574707\n",
      "   VALIDATION: Batch 111, loss 4.7096848487854\n",
      "   VALIDATION: Batch 112, loss 5.519411087036133\n",
      "   VALIDATION: Batch 113, loss 4.876956462860107\n",
      "   VALIDATION: Batch 114, loss 4.664524555206299\n",
      "   VALIDATION: Batch 115, loss 4.126387596130371\n",
      "   VALIDATION: Batch 116, loss 3.9723148345947266\n",
      "   VALIDATION: Batch 117, loss 4.6235575675964355\n",
      "   VALIDATION: Batch 118, loss 4.814766883850098\n",
      "   VALIDATION: Batch 119, loss 3.993776321411133\n",
      "   VALIDATION: Batch 120, loss 3.6031994819641113\n",
      "   VALIDATION: Batch 121, loss 3.867427349090576\n",
      "   VALIDATION: Batch 122, loss 4.250068187713623\n",
      "   VALIDATION: Batch 123, loss 4.323695182800293\n",
      "   VALIDATION: Batch 124, loss 3.6828243732452393\n",
      "   VALIDATION: Batch 125, loss 4.3099236488342285\n",
      "   VALIDATION: Batch 126, loss 4.522281169891357\n",
      "   VALIDATION: Batch 127, loss 4.276744365692139\n",
      "   VALIDATION: Batch 128, loss 4.4483561515808105\n",
      "   VALIDATION: Batch 129, loss 4.068059921264648\n",
      "   VALIDATION: Batch 130, loss 3.727783679962158\n",
      "   VALIDATION: Batch 131, loss 3.722568988800049\n",
      "   VALIDATION: Batch 132, loss 4.369505405426025\n",
      "   VALIDATION: Batch 133, loss 4.549383163452148\n",
      "   VALIDATION: Batch 134, loss 4.434927940368652\n",
      "   VALIDATION: Batch 135, loss 4.701883792877197\n",
      "   VALIDATION: Batch 136, loss 4.806639194488525\n",
      "   VALIDATION: Batch 137, loss 4.623615741729736\n",
      "   VALIDATION: Batch 138, loss 4.342895030975342\n",
      "   VALIDATION: Batch 139, loss 4.716607570648193\n",
      "   VALIDATION: Batch 140, loss 3.827028751373291\n",
      "   VALIDATION: Batch 141, loss 4.784266471862793\n",
      "   VALIDATION: Batch 142, loss 3.504973888397217\n",
      "   VALIDATION: Batch 143, loss 4.318917751312256\n",
      "   VALIDATION: Batch 144, loss 4.565803050994873\n",
      "   VALIDATION: Batch 145, loss 4.36120080947876\n",
      "   VALIDATION: Batch 146, loss 4.15627908706665\n",
      "   VALIDATION: Batch 147, loss 4.520808219909668\n",
      "   VALIDATION: Batch 148, loss 4.603180408477783\n",
      "   VALIDATION: Batch 149, loss 5.098860263824463\n",
      "   VALIDATION: Batch 150, loss 4.653743743896484\n",
      "   VALIDATION: Batch 151, loss 4.949745178222656\n",
      "   VALIDATION: Batch 152, loss 4.310960292816162\n",
      "   VALIDATION: Batch 153, loss 4.576931953430176\n",
      "   VALIDATION: Batch 154, loss 4.405876636505127\n",
      "   VALIDATION: Batch 155, loss 4.152928352355957\n",
      "   VALIDATION: Batch 156, loss 4.814271926879883\n",
      "   VALIDATION: Batch 157, loss 4.5037126541137695\n",
      "   VALIDATION: Batch 158, loss 3.903701066970825\n",
      "   VALIDATION: Batch 159, loss 4.364421844482422\n",
      "   VALIDATION: Batch 160, loss 4.734765529632568\n",
      "   VALIDATION: Batch 161, loss 4.904688835144043\n",
      "   VALIDATION: Batch 162, loss 4.354846000671387\n",
      "   VALIDATION: Batch 163, loss 3.8089771270751953\n",
      "   VALIDATION: Batch 164, loss 4.322757720947266\n",
      "   VALIDATION: Batch 165, loss 4.737519264221191\n",
      "   VALIDATION: Batch 166, loss 4.210948467254639\n",
      "   VALIDATION: Batch 167, loss 4.665217399597168\n",
      "   VALIDATION: Batch 168, loss 3.495041608810425\n",
      "   VALIDATION: Batch 169, loss 4.178757190704346\n",
      "   VALIDATION: Batch 170, loss 4.368612289428711\n",
      "   VALIDATION: Batch 171, loss 4.550070762634277\n",
      "   VALIDATION: Batch 172, loss 4.3947224617004395\n",
      "   VALIDATION: Batch 173, loss 4.339508056640625\n",
      "   VALIDATION: Batch 174, loss 4.713675498962402\n",
      "   VALIDATION: Batch 175, loss 4.45491886138916\n",
      "   VALIDATION: Batch 176, loss 4.22653341293335\n",
      "   VALIDATION: Batch 177, loss 4.206231117248535\n",
      "   VALIDATION: Batch 178, loss 5.242592811584473\n",
      "   VALIDATION: Batch 179, loss 4.545413494110107\n",
      "   VALIDATION: Batch 180, loss 4.055339336395264\n",
      "   VALIDATION: Batch 181, loss 4.175225734710693\n",
      "   VALIDATION: Batch 182, loss 4.4920654296875\n",
      "   VALIDATION: Batch 183, loss 3.561741590499878\n",
      "   VALIDATION: Batch 184, loss 3.3092918395996094\n",
      "   VALIDATION: Batch 185, loss 4.09798526763916\n",
      "   VALIDATION: Batch 186, loss 3.981311082839966\n",
      "   VALIDATION: Batch 187, loss 4.1883697509765625\n",
      "   VALIDATION: Batch 188, loss 4.587895393371582\n",
      "   VALIDATION: Batch 189, loss 3.93296480178833\n",
      "   VALIDATION: Batch 190, loss 4.014261245727539\n",
      "   VALIDATION: Batch 191, loss 4.516177177429199\n",
      "   VALIDATION: Batch 192, loss 4.8261284828186035\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=31]              TRRAINING: Batch 0, loss 4.328951835632324\n",
      "Epoch 5: |          | 1/? [00:01<00:00,  0.57it/s, v_num=31]   TRRAINING: Batch 1, loss 3.8681042194366455\n",
      "Epoch 5: |          | 2/? [00:03<00:00,  0.63it/s, v_num=31]   TRRAINING: Batch 2, loss 3.9685044288635254\n",
      "Epoch 5: |          | 3/? [00:04<00:00,  0.64it/s, v_num=31]   TRRAINING: Batch 3, loss 3.644747495651245\n",
      "Epoch 5: |          | 4/? [00:06<00:00,  0.65it/s, v_num=31]   TRRAINING: Batch 4, loss 4.057555198669434\n",
      "Epoch 5: |          | 5/? [00:07<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 5, loss 4.844237327575684\n",
      "Epoch 5: |          | 6/? [00:08<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 6, loss 4.385793209075928\n",
      "Epoch 5: |          | 7/? [00:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 7, loss 3.703749895095825\n",
      "Epoch 5: |          | 8/? [00:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 8, loss 3.7717678546905518\n",
      "Epoch 5: |          | 9/? [00:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 9, loss 4.114193916320801\n",
      "Epoch 5: |          | 10/? [00:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 10, loss 4.334027290344238\n",
      "Epoch 5: |          | 11/? [00:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 11, loss 4.255387306213379\n",
      "Epoch 5: |          | 12/? [00:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 12, loss 4.942385196685791\n",
      "Epoch 5: |          | 13/? [00:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 13, loss 4.187962532043457\n",
      "Epoch 5: |          | 14/? [00:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 14, loss 4.400040626525879\n",
      "Epoch 5: |          | 15/? [00:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 15, loss 3.644904375076294\n",
      "Epoch 5: |          | 16/? [00:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 16, loss 3.45570707321167\n",
      "Epoch 5: |          | 17/? [00:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 17, loss 4.670011043548584\n",
      "Epoch 5: |          | 18/? [00:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 18, loss 4.140148162841797\n",
      "Epoch 5: |          | 19/? [00:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 19, loss 3.910741090774536\n",
      "Epoch 5: |          | 20/? [00:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 20, loss 4.214126110076904\n",
      "Epoch 5: |          | 21/? [00:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 21, loss 4.332266330718994\n",
      "Epoch 5: |          | 22/? [00:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 22, loss 4.238674163818359\n",
      "Epoch 5: |          | 23/? [00:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 23, loss 3.586796283721924\n",
      "Epoch 5: |          | 24/? [00:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 24, loss 4.239485740661621\n",
      "Epoch 5: |          | 25/? [00:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 25, loss 4.054497718811035\n",
      "Epoch 5: |          | 26/? [00:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 26, loss 3.8283965587615967\n",
      "Epoch 5: |          | 27/? [00:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 27, loss 3.7525627613067627\n",
      "Epoch 5: |          | 28/? [00:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 28, loss 4.69148588180542\n",
      "Epoch 5: |          | 29/? [00:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 29, loss 4.198189735412598\n",
      "Epoch 5: |          | 30/? [00:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 30, loss 4.063014030456543\n",
      "Epoch 5: |          | 31/? [00:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 31, loss 4.670669078826904\n",
      "Epoch 5: |          | 32/? [00:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 32, loss 4.198486328125\n",
      "Epoch 5: |          | 33/? [00:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 33, loss 4.012072563171387\n",
      "Epoch 5: |          | 34/? [00:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 34, loss 3.971480131149292\n",
      "Epoch 5: |          | 35/? [00:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 35, loss 3.402111530303955\n",
      "Epoch 5: |          | 36/? [00:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 36, loss 4.24413537979126\n",
      "Epoch 5: |          | 37/? [00:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 37, loss 4.406975269317627\n",
      "Epoch 5: |          | 38/? [00:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 38, loss 4.71158504486084\n",
      "Epoch 5: |          | 39/? [00:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 39, loss 4.589938163757324\n",
      "Epoch 5: |          | 40/? [00:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 40, loss 4.006745338439941\n",
      "Epoch 5: |          | 41/? [00:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 41, loss 4.041179180145264\n",
      "Epoch 5: |          | 42/? [01:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 42, loss 3.815155506134033\n",
      "Epoch 5: |          | 43/? [01:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 43, loss 4.005677700042725\n",
      "Epoch 5: |          | 44/? [01:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 44, loss 3.1879446506500244\n",
      "Epoch 5: |          | 45/? [01:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 45, loss 2.811427593231201\n",
      "Epoch 5: |          | 46/? [01:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 46, loss 4.610224723815918\n",
      "Epoch 5: |          | 47/? [01:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 47, loss 3.7975401878356934\n",
      "Epoch 5: |          | 48/? [01:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 48, loss 3.6467461585998535\n",
      "Epoch 5: |          | 49/? [01:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 49, loss 4.130334377288818\n",
      "Epoch 5: |          | 50/? [01:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 50, loss 3.983046054840088\n",
      "Epoch 5: |          | 51/? [01:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 51, loss 3.973158597946167\n",
      "Epoch 5: |          | 52/? [01:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 52, loss 4.626429557800293\n",
      "Epoch 5: |          | 53/? [01:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 53, loss 4.226014614105225\n",
      "Epoch 5: |          | 54/? [01:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 54, loss 4.050496578216553\n",
      "Epoch 5: |          | 55/? [01:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 55, loss 4.123225212097168\n",
      "Epoch 5: |          | 56/? [01:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 56, loss 4.227903842926025\n",
      "Epoch 5: |          | 57/? [01:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 57, loss 4.1514892578125\n",
      "Epoch 5: |          | 58/? [01:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 58, loss 5.376294136047363\n",
      "Epoch 5: |          | 59/? [01:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 59, loss 4.310656547546387\n",
      "Epoch 5: |          | 60/? [01:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 60, loss 4.45586633682251\n",
      "Epoch 5: |          | 61/? [01:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 61, loss 4.427586555480957\n",
      "Epoch 5: |          | 62/? [01:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 62, loss 4.004077434539795\n",
      "Epoch 5: |          | 63/? [01:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 63, loss 4.23288631439209\n",
      "Epoch 5: |          | 64/? [01:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 64, loss 4.050775527954102\n",
      "Epoch 5: |          | 65/? [01:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 65, loss 4.083509922027588\n",
      "Epoch 5: |          | 66/? [01:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 66, loss 3.5414276123046875\n",
      "Epoch 5: |          | 67/? [01:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 67, loss 4.214164733886719\n",
      "Epoch 5: |          | 68/? [01:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 68, loss 4.2642011642456055\n",
      "Epoch 5: |          | 69/? [01:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 69, loss 4.147818565368652\n",
      "Epoch 5: |          | 70/? [01:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 70, loss 3.8089332580566406\n",
      "Epoch 5: |          | 71/? [01:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 71, loss 3.747241973876953\n",
      "Epoch 5: |          | 72/? [01:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 72, loss 4.043182849884033\n",
      "Epoch 5: |          | 73/? [01:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 73, loss 4.202069282531738\n",
      "Epoch 5: |          | 74/? [01:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 74, loss 3.9150893688201904\n",
      "Epoch 5: |          | 75/? [01:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 75, loss 4.032784461975098\n",
      "Epoch 5: |          | 76/? [01:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 76, loss 4.045307159423828\n",
      "Epoch 5: |          | 77/? [01:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 77, loss 4.084317207336426\n",
      "Epoch 5: |          | 78/? [01:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 78, loss 3.8788695335388184\n",
      "Epoch 5: |          | 79/? [01:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 79, loss 4.0743584632873535\n",
      "Epoch 5: |          | 80/? [01:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 80, loss 4.021598815917969\n",
      "Epoch 5: |          | 81/? [01:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 81, loss 3.4805855751037598\n",
      "Epoch 5: |          | 82/? [01:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 82, loss 4.296844005584717\n",
      "Epoch 5: |          | 83/? [01:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 83, loss 3.7072880268096924\n",
      "Epoch 5: |          | 84/? [02:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 84, loss 3.583136796951294\n",
      "Epoch 5: |          | 85/? [02:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 85, loss 3.4513614177703857\n",
      "Epoch 5: |          | 86/? [02:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 86, loss 3.599898099899292\n",
      "Epoch 5: |          | 87/? [02:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 87, loss 3.7717647552490234\n",
      "Epoch 5: |          | 88/? [02:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 88, loss 4.474454402923584\n",
      "Epoch 5: |          | 89/? [02:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 89, loss 4.42020845413208\n",
      "Epoch 5: |          | 90/? [02:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 90, loss 4.201611518859863\n",
      "Epoch 5: |          | 91/? [02:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 91, loss 3.9135124683380127\n",
      "Epoch 5: |          | 92/? [02:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 92, loss 4.383641242980957\n",
      "Epoch 5: |          | 93/? [02:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 93, loss 4.493508815765381\n",
      "Epoch 5: |          | 94/? [02:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 94, loss 4.347721099853516\n",
      "Epoch 5: |          | 95/? [02:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 95, loss 4.260562896728516\n",
      "Epoch 5: |          | 96/? [02:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 96, loss 3.8875203132629395\n",
      "Epoch 5: |          | 97/? [02:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 97, loss 3.68544340133667\n",
      "Epoch 5: |          | 98/? [02:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 98, loss 4.197572231292725\n",
      "Epoch 5: |          | 99/? [02:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 99, loss 4.409830570220947\n",
      "Epoch 5: |          | 100/? [02:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 100, loss 4.342031478881836\n",
      "Epoch 5: |          | 101/? [02:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 101, loss 4.000701904296875\n",
      "Epoch 5: |          | 102/? [02:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 102, loss 4.013552665710449\n",
      "Epoch 5: |          | 103/? [02:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 103, loss 3.7882938385009766\n",
      "Epoch 5: |          | 104/? [02:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 104, loss 4.1676554679870605\n",
      "Epoch 5: |          | 105/? [02:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 105, loss 4.0611772537231445\n",
      "Epoch 5: |          | 106/? [02:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 106, loss 4.198476314544678\n",
      "Epoch 5: |          | 107/? [02:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 107, loss 4.248144626617432\n",
      "Epoch 5: |          | 108/? [02:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 108, loss 4.18395471572876\n",
      "Epoch 5: |          | 109/? [02:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 109, loss 3.892328977584839\n",
      "Epoch 5: |          | 110/? [02:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 110, loss 4.125156879425049\n",
      "Epoch 5: |          | 111/? [02:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 111, loss 4.762145042419434\n",
      "Epoch 5: |          | 112/? [02:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 112, loss 3.438192367553711\n",
      "Epoch 5: |          | 113/? [02:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 113, loss 3.007284641265869\n",
      "Epoch 5: |          | 114/? [02:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 114, loss 4.343714237213135\n",
      "Epoch 5: |          | 115/? [02:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 115, loss 4.475354194641113\n",
      "Epoch 5: |          | 116/? [02:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 116, loss 3.5914618968963623\n",
      "Epoch 5: |          | 117/? [02:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 117, loss 3.567753553390503\n",
      "Epoch 5: |          | 118/? [02:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 118, loss 4.410632133483887\n",
      "Epoch 5: |          | 119/? [02:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 119, loss 4.635614395141602\n",
      "Epoch 5: |          | 120/? [02:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 120, loss 4.397496223449707\n",
      "Epoch 5: |          | 121/? [02:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 121, loss 4.125417232513428\n",
      "Epoch 5: |          | 122/? [02:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 122, loss 3.5771191120147705\n",
      "Epoch 5: |          | 123/? [02:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 123, loss 4.066643238067627\n",
      "Epoch 5: |          | 124/? [02:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 124, loss 4.257773399353027\n",
      "Epoch 5: |          | 125/? [02:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 125, loss 3.861567735671997\n",
      "Epoch 5: |          | 126/? [03:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 126, loss 4.348443984985352\n",
      "Epoch 5: |          | 127/? [03:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 127, loss 4.453332424163818\n",
      "Epoch 5: |          | 128/? [03:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 128, loss 3.550225019454956\n",
      "Epoch 5: |          | 129/? [03:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 129, loss 4.184228420257568\n",
      "Epoch 5: |          | 130/? [03:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 130, loss 3.260892152786255\n",
      "Epoch 5: |          | 131/? [03:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 131, loss 4.110741138458252\n",
      "Epoch 5: |          | 132/? [03:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 132, loss 4.138922691345215\n",
      "Epoch 5: |          | 133/? [03:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 133, loss 4.0367937088012695\n",
      "Epoch 5: |          | 134/? [03:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 134, loss 4.221175670623779\n",
      "Epoch 5: |          | 135/? [03:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 135, loss 4.316519737243652\n",
      "Epoch 5: |          | 136/? [03:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 136, loss 4.348906517028809\n",
      "Epoch 5: |          | 137/? [03:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 137, loss 3.318920850753784\n",
      "Epoch 5: |          | 138/? [03:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 138, loss 3.9320998191833496\n",
      "Epoch 5: |          | 139/? [03:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 139, loss 4.4284257888793945\n",
      "Epoch 5: |          | 140/? [03:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 140, loss 3.6280674934387207\n",
      "Epoch 5: |          | 141/? [03:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 141, loss 3.7088515758514404\n",
      "Epoch 5: |          | 142/? [03:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 142, loss 5.239026069641113\n",
      "Epoch 5: |          | 143/? [03:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 143, loss 4.888922691345215\n",
      "Epoch 5: |          | 144/? [03:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 144, loss 4.082919597625732\n",
      "Epoch 5: |          | 145/? [03:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 145, loss 3.5753493309020996\n",
      "Epoch 5: |          | 146/? [03:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 146, loss 3.750936508178711\n",
      "Epoch 5: |          | 147/? [03:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 147, loss 4.0683393478393555\n",
      "Epoch 5: |          | 148/? [03:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 148, loss 3.839057207107544\n",
      "Epoch 5: |          | 149/? [03:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 149, loss 3.367682695388794\n",
      "Epoch 5: |          | 150/? [03:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 150, loss 4.230023384094238\n",
      "Epoch 5: |          | 151/? [03:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 151, loss 4.3327555656433105\n",
      "Epoch 5: |          | 152/? [03:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 152, loss 4.30043888092041\n",
      "Epoch 5: |          | 153/? [03:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 153, loss 3.390909194946289\n",
      "Epoch 5: |          | 154/? [03:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 154, loss 4.645095348358154\n",
      "Epoch 5: |          | 155/? [03:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 155, loss 4.1695075035095215\n",
      "Epoch 5: |          | 156/? [03:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 156, loss 3.5398402214050293\n",
      "Epoch 5: |          | 157/? [03:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 157, loss 4.225436210632324\n",
      "Epoch 5: |          | 158/? [03:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 158, loss 4.3035078048706055\n",
      "Epoch 5: |          | 159/? [03:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 159, loss 4.040827751159668\n",
      "Epoch 5: |          | 160/? [03:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 160, loss 3.7865958213806152\n",
      "Epoch 5: |          | 161/? [03:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 161, loss 4.181939125061035\n",
      "Epoch 5: |          | 162/? [03:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 162, loss 4.219204902648926\n",
      "Epoch 5: |          | 163/? [03:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 163, loss 3.349569320678711\n",
      "Epoch 5: |          | 164/? [03:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 164, loss 3.7938003540039062\n",
      "Epoch 5: |          | 165/? [03:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 165, loss 4.580647945404053\n",
      "Epoch 5: |          | 166/? [03:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 166, loss 4.261179447174072\n",
      "Epoch 5: |          | 167/? [03:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 167, loss 4.361771106719971\n",
      "Epoch 5: |          | 168/? [03:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 168, loss 3.9496893882751465\n",
      "Epoch 5: |          | 169/? [04:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 169, loss 3.596518039703369\n",
      "Epoch 5: |          | 170/? [04:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 170, loss 3.8069019317626953\n",
      "Epoch 5: |          | 171/? [04:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 171, loss 4.167415618896484\n",
      "Epoch 5: |          | 172/? [04:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 172, loss 3.9723706245422363\n",
      "Epoch 5: |          | 173/? [04:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 173, loss 4.658835411071777\n",
      "Epoch 5: |          | 174/? [04:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 174, loss 4.291974067687988\n",
      "Epoch 5: |          | 175/? [04:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 175, loss 4.733981609344482\n",
      "Epoch 5: |          | 176/? [04:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 176, loss 3.8815364837646484\n",
      "Epoch 5: |          | 177/? [04:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 177, loss 3.9301905632019043\n",
      "Epoch 5: |          | 178/? [04:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 178, loss 3.765143632888794\n",
      "Epoch 5: |          | 179/? [04:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 179, loss 4.486201286315918\n",
      "Epoch 5: |          | 180/? [04:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 180, loss 3.9984893798828125\n",
      "Epoch 5: |          | 181/? [04:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 181, loss 3.7794241905212402\n",
      "Epoch 5: |          | 182/? [04:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 182, loss 4.210944652557373\n",
      "Epoch 5: |          | 183/? [04:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 183, loss 3.6799285411834717\n",
      "Epoch 5: |          | 184/? [04:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 184, loss 3.880434036254883\n",
      "Epoch 5: |          | 185/? [04:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 185, loss 4.488601207733154\n",
      "Epoch 5: |          | 186/? [04:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 186, loss 3.941479444503784\n",
      "Epoch 5: |          | 187/? [04:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 187, loss 4.459958553314209\n",
      "Epoch 5: |          | 188/? [04:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 188, loss 3.843125104904175\n",
      "Epoch 5: |          | 189/? [04:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 189, loss 4.473730564117432\n",
      "Epoch 5: |          | 190/? [04:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 190, loss 3.993328094482422\n",
      "Epoch 5: |          | 191/? [04:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 191, loss 4.78450870513916\n",
      "Epoch 5: |          | 192/? [04:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 192, loss 4.551126480102539\n",
      "Epoch 5: |          | 193/? [04:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 193, loss 3.8610382080078125\n",
      "Epoch 5: |          | 194/? [04:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 194, loss 3.7518951892852783\n",
      "Epoch 5: |          | 195/? [04:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 195, loss 4.445320129394531\n",
      "Epoch 5: |          | 196/? [04:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 196, loss 4.324945449829102\n",
      "Epoch 5: |          | 197/? [04:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 197, loss 4.1872944831848145\n",
      "Epoch 5: |          | 198/? [04:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 198, loss 3.5108444690704346\n",
      "Epoch 5: |          | 199/? [04:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 199, loss 4.404520511627197\n",
      "Epoch 5: |          | 200/? [04:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 200, loss 3.8935039043426514\n",
      "Epoch 5: |          | 201/? [04:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 201, loss 4.191897869110107\n",
      "Epoch 5: |          | 202/? [04:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 202, loss 4.2770538330078125\n",
      "Epoch 5: |          | 203/? [04:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 203, loss 3.9575729370117188\n",
      "Epoch 5: |          | 204/? [04:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 204, loss 4.035877704620361\n",
      "Epoch 5: |          | 205/? [04:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 205, loss 3.8588573932647705\n",
      "Epoch 5: |          | 206/? [04:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 206, loss 3.691704273223877\n",
      "Epoch 5: |          | 207/? [04:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 207, loss 4.223122596740723\n",
      "Epoch 5: |          | 208/? [04:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 208, loss 4.1321516036987305\n",
      "Epoch 5: |          | 209/? [04:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 209, loss 3.9773106575012207\n",
      "Epoch 5: |          | 210/? [05:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 210, loss 4.6688618659973145\n",
      "Epoch 5: |          | 211/? [05:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 211, loss 3.959094285964966\n",
      "Epoch 5: |          | 212/? [05:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 212, loss 4.221891403198242\n",
      "Epoch 5: |          | 213/? [05:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 213, loss 4.063900470733643\n",
      "Epoch 5: |          | 214/? [05:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 214, loss 3.9428653717041016\n",
      "Epoch 5: |          | 215/? [05:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 215, loss 3.596588134765625\n",
      "Epoch 5: |          | 216/? [05:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 216, loss 4.285917282104492\n",
      "Epoch 5: |          | 217/? [05:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 217, loss 4.167462348937988\n",
      "Epoch 5: |          | 218/? [05:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 218, loss 4.2083940505981445\n",
      "Epoch 5: |          | 219/? [05:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 219, loss 4.119563579559326\n",
      "Epoch 5: |          | 220/? [05:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 220, loss 4.2222371101379395\n",
      "Epoch 5: |          | 221/? [05:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 221, loss 4.024295806884766\n",
      "Epoch 5: |          | 222/? [05:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 222, loss 3.2705941200256348\n",
      "Epoch 5: |          | 223/? [05:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 223, loss 4.327702522277832\n",
      "Epoch 5: |          | 224/? [05:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 224, loss 4.382826328277588\n",
      "Epoch 5: |          | 225/? [05:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 225, loss 4.106354236602783\n",
      "Epoch 5: |          | 226/? [05:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 226, loss 4.031898021697998\n",
      "Epoch 5: |          | 227/? [05:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 227, loss 4.386209011077881\n",
      "Epoch 5: |          | 228/? [05:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 228, loss 4.072301387786865\n",
      "Epoch 5: |          | 229/? [05:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 229, loss 4.1551103591918945\n",
      "Epoch 5: |          | 230/? [05:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 230, loss 4.086718559265137\n",
      "Epoch 5: |          | 231/? [05:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 231, loss 4.014429569244385\n",
      "Epoch 5: |          | 232/? [05:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 232, loss 3.78971529006958\n",
      "Epoch 5: |          | 233/? [05:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 233, loss 4.453268051147461\n",
      "Epoch 5: |          | 234/? [05:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 234, loss 4.461613655090332\n",
      "Epoch 5: |          | 235/? [05:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 235, loss 4.495894432067871\n",
      "Epoch 5: |          | 236/? [05:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 236, loss 3.9523682594299316\n",
      "Epoch 5: |          | 237/? [05:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 237, loss 4.116854667663574\n",
      "Epoch 5: |          | 238/? [05:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 238, loss 4.385196685791016\n",
      "Epoch 5: |          | 239/? [05:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 239, loss 3.9523653984069824\n",
      "Epoch 5: |          | 240/? [05:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 240, loss 3.4553654193878174\n",
      "Epoch 5: |          | 241/? [05:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 241, loss 4.128195285797119\n",
      "Epoch 5: |          | 242/? [05:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 242, loss 4.468360900878906\n",
      "Epoch 5: |          | 243/? [05:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 243, loss 3.3079476356506348\n",
      "Epoch 5: |          | 244/? [05:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 244, loss 3.7649002075195312\n",
      "Epoch 5: |          | 245/? [05:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 245, loss 4.0886430740356445\n",
      "Epoch 5: |          | 246/? [05:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 246, loss 4.205504417419434\n",
      "Epoch 5: |          | 247/? [05:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 247, loss 4.232437610626221\n",
      "Epoch 5: |          | 248/? [05:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 248, loss 3.742870330810547\n",
      "Epoch 5: |          | 249/? [05:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 249, loss 3.5870308876037598\n",
      "Epoch 5: |          | 250/? [05:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 250, loss 4.190025329589844\n",
      "Epoch 5: |          | 251/? [05:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 251, loss 4.198781490325928\n",
      "Epoch 5: |          | 252/? [06:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 252, loss 3.9665145874023438\n",
      "Epoch 5: |          | 253/? [06:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 253, loss 4.790841102600098\n",
      "Epoch 5: |          | 254/? [06:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 254, loss 4.487477779388428\n",
      "Epoch 5: |          | 255/? [06:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 255, loss 4.06845760345459\n",
      "Epoch 5: |          | 256/? [06:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 256, loss 5.238470554351807\n",
      "Epoch 5: |          | 257/? [06:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 257, loss 3.9441516399383545\n",
      "Epoch 5: |          | 258/? [06:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 258, loss 4.038094997406006\n",
      "Epoch 5: |          | 259/? [06:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 259, loss 3.9129767417907715\n",
      "Epoch 5: |          | 260/? [06:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 260, loss 3.851706027984619\n",
      "Epoch 5: |          | 261/? [06:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 261, loss 3.691561460494995\n",
      "Epoch 5: |          | 262/? [06:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 262, loss 4.338255882263184\n",
      "Epoch 5: |          | 263/? [06:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 263, loss 4.034793853759766\n",
      "Epoch 5: |          | 264/? [06:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 264, loss 4.198887825012207\n",
      "Epoch 5: |          | 265/? [06:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 265, loss 3.5976333618164062\n",
      "Epoch 5: |          | 266/? [06:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 266, loss 4.0784196853637695\n",
      "Epoch 5: |          | 267/? [06:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 267, loss 3.6324462890625\n",
      "Epoch 5: |          | 268/? [06:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 268, loss 4.011746406555176\n",
      "Epoch 5: |          | 269/? [06:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 269, loss 4.404257774353027\n",
      "Epoch 5: |          | 270/? [06:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 270, loss 3.9892776012420654\n",
      "Epoch 5: |          | 271/? [06:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 271, loss 4.330955505371094\n",
      "Epoch 5: |          | 272/? [06:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 272, loss 4.582449436187744\n",
      "Epoch 5: |          | 273/? [06:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 273, loss 3.720952272415161\n",
      "Epoch 5: |          | 274/? [06:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 274, loss 4.6398491859436035\n",
      "Epoch 5: |          | 275/? [06:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 275, loss 4.209327697753906\n",
      "Epoch 5: |          | 276/? [06:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 276, loss 3.518275022506714\n",
      "Epoch 5: |          | 277/? [06:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 277, loss 4.185886383056641\n",
      "Epoch 5: |          | 278/? [06:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 278, loss 3.1772124767303467\n",
      "Epoch 5: |          | 279/? [06:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 279, loss 3.9076473712921143\n",
      "Epoch 5: |          | 280/? [06:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 280, loss 3.6097283363342285\n",
      "Epoch 5: |          | 281/? [06:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 281, loss 4.307682037353516\n",
      "Epoch 5: |          | 282/? [06:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 282, loss 3.8609976768493652\n",
      "Epoch 5: |          | 283/? [06:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 283, loss 3.9736862182617188\n",
      "Epoch 5: |          | 284/? [06:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 284, loss 3.871405839920044\n",
      "Epoch 5: |          | 285/? [06:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 285, loss 3.34851336479187\n",
      "Epoch 5: |          | 286/? [06:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 286, loss 3.9471993446350098\n",
      "Epoch 5: |          | 287/? [06:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 287, loss 3.70971417427063\n",
      "Epoch 5: |          | 288/? [06:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 288, loss 3.6543972492218018\n",
      "Epoch 5: |          | 289/? [06:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 289, loss 3.502063751220703\n",
      "Epoch 5: |          | 290/? [06:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 290, loss 3.0404860973358154\n",
      "Epoch 5: |          | 291/? [06:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 291, loss 4.175250053405762\n",
      "Epoch 5: |          | 292/? [06:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 292, loss 3.8016555309295654\n",
      "Epoch 5: |          | 293/? [06:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 293, loss 4.1069464683532715\n",
      "Epoch 5: |          | 294/? [07:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 294, loss 3.976311206817627\n",
      "Epoch 5: |          | 295/? [07:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 295, loss 4.282804012298584\n",
      "Epoch 5: |          | 296/? [07:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 296, loss 3.752265453338623\n",
      "Epoch 5: |          | 297/? [07:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 297, loss 4.463678359985352\n",
      "Epoch 5: |          | 298/? [07:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 298, loss 4.273318290710449\n",
      "Epoch 5: |          | 299/? [07:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 299, loss 4.726507663726807\n",
      "Epoch 5: |          | 300/? [07:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 300, loss 4.045164108276367\n",
      "Epoch 5: |          | 301/? [07:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 301, loss 3.893603801727295\n",
      "Epoch 5: |          | 302/? [07:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 302, loss 4.311095237731934\n",
      "Epoch 5: |          | 303/? [07:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 303, loss 4.102898597717285\n",
      "Epoch 5: |          | 304/? [07:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 304, loss 4.296380519866943\n",
      "Epoch 5: |          | 305/? [07:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 305, loss 4.349229335784912\n",
      "Epoch 5: |          | 306/? [07:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 306, loss 3.9875354766845703\n",
      "Epoch 5: |          | 307/? [07:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 307, loss 4.215498924255371\n",
      "Epoch 5: |          | 308/? [07:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 308, loss 4.399857521057129\n",
      "Epoch 5: |          | 309/? [07:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 309, loss 4.029069900512695\n",
      "Epoch 5: |          | 310/? [07:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 310, loss 4.449514865875244\n",
      "Epoch 5: |          | 311/? [07:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 311, loss 4.027689456939697\n",
      "Epoch 5: |          | 312/? [07:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 312, loss 4.028138637542725\n",
      "Epoch 5: |          | 313/? [07:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 313, loss 3.8366189002990723\n",
      "Epoch 5: |          | 314/? [07:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 314, loss 4.193299770355225\n",
      "Epoch 5: |          | 315/? [07:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 315, loss 3.7991816997528076\n",
      "Epoch 5: |          | 316/? [07:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 316, loss 4.391056537628174\n",
      "Epoch 5: |          | 317/? [07:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 317, loss 4.199684143066406\n",
      "Epoch 5: |          | 318/? [07:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 318, loss 4.336714267730713\n",
      "Epoch 5: |          | 319/? [07:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 319, loss 3.56099271774292\n",
      "Epoch 5: |          | 320/? [07:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 320, loss 4.044467449188232\n",
      "Epoch 5: |          | 321/? [07:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 321, loss 3.845787525177002\n",
      "Epoch 5: |          | 322/? [07:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 322, loss 4.482346057891846\n",
      "Epoch 5: |          | 323/? [07:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 323, loss 4.487880706787109\n",
      "Epoch 5: |          | 324/? [07:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 324, loss 4.142394065856934\n",
      "Epoch 5: |          | 325/? [07:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 325, loss 4.581804275512695\n",
      "Epoch 5: |          | 326/? [07:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 326, loss 4.143538475036621\n",
      "Epoch 5: |          | 327/? [07:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 327, loss 3.853424072265625\n",
      "Epoch 5: |          | 328/? [07:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 328, loss 3.623816728591919\n",
      "Epoch 5: |          | 329/? [07:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 329, loss 4.261724948883057\n",
      "Epoch 5: |          | 330/? [07:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 330, loss 4.737381935119629\n",
      "Epoch 5: |          | 331/? [07:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 331, loss 2.998366594314575\n",
      "Epoch 5: |          | 332/? [07:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 332, loss 4.0778489112854\n",
      "Epoch 5: |          | 333/? [07:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 333, loss 3.9445929527282715\n",
      "Epoch 5: |          | 334/? [07:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 334, loss 4.671181678771973\n",
      "Epoch 5: |          | 335/? [08:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 335, loss 4.5630340576171875\n",
      "Epoch 5: |          | 336/? [08:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 336, loss 4.497603416442871\n",
      "Epoch 5: |          | 337/? [08:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 337, loss 5.118520259857178\n",
      "Epoch 5: |          | 338/? [08:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 338, loss 4.838465690612793\n",
      "Epoch 5: |          | 339/? [08:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 339, loss 3.8366475105285645\n",
      "Epoch 5: |          | 340/? [08:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 340, loss 3.723346710205078\n",
      "Epoch 5: |          | 341/? [08:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 341, loss 3.5122718811035156\n",
      "Epoch 5: |          | 342/? [08:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 342, loss 4.186094760894775\n",
      "Epoch 5: |          | 343/? [08:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 343, loss 3.885448455810547\n",
      "Epoch 5: |          | 344/? [08:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 344, loss 4.728146553039551\n",
      "Epoch 5: |          | 345/? [08:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 345, loss 3.9277472496032715\n",
      "Epoch 5: |          | 346/? [08:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 346, loss 4.194598197937012\n",
      "Epoch 5: |          | 347/? [08:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 347, loss 3.953671932220459\n",
      "Epoch 5: |          | 348/? [08:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 348, loss 3.337040424346924\n",
      "Epoch 5: |          | 349/? [08:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 349, loss 3.1762325763702393\n",
      "Epoch 5: |          | 350/? [08:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 350, loss 4.4455952644348145\n",
      "Epoch 5: |          | 351/? [08:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 351, loss 4.475921630859375\n",
      "Epoch 5: |          | 352/? [08:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 352, loss 3.7502102851867676\n",
      "Epoch 5: |          | 353/? [08:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 353, loss 3.502028703689575\n",
      "Epoch 5: |          | 354/? [08:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 354, loss 3.9194750785827637\n",
      "Epoch 5: |          | 355/? [08:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 355, loss 4.2227091789245605\n",
      "Epoch 5: |          | 356/? [08:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 356, loss 4.263800621032715\n",
      "Epoch 5: |          | 357/? [08:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 357, loss 3.7627532482147217\n",
      "Epoch 5: |          | 358/? [08:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 358, loss 3.7039477825164795\n",
      "Epoch 5: |          | 359/? [08:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 359, loss 4.298970699310303\n",
      "Epoch 5: |          | 360/? [08:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 360, loss 3.898197889328003\n",
      "Epoch 5: |          | 361/? [08:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 361, loss 3.992145538330078\n",
      "Epoch 5: |          | 362/? [08:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 362, loss 3.8080801963806152\n",
      "Epoch 5: |          | 363/? [08:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 363, loss 3.707524061203003\n",
      "Epoch 5: |          | 364/? [08:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 364, loss 4.325253486633301\n",
      "Epoch 5: |          | 365/? [08:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 365, loss 4.349970817565918\n",
      "Epoch 5: |          | 366/? [08:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 366, loss 4.119963645935059\n",
      "Epoch 5: |          | 367/? [08:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 367, loss 4.103204250335693\n",
      "Epoch 5: |          | 368/? [08:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 368, loss 3.6392581462860107\n",
      "Epoch 5: |          | 369/? [08:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 369, loss 4.003697872161865\n",
      "Epoch 5: |          | 370/? [08:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 370, loss 3.602431535720825\n",
      "Epoch 5: |          | 371/? [08:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 371, loss 4.5501708984375\n",
      "Epoch 5: |          | 372/? [08:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 372, loss 3.928678035736084\n",
      "Epoch 5: |          | 373/? [08:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 373, loss 4.225245952606201\n",
      "Epoch 5: |          | 374/? [08:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 374, loss 3.8972268104553223\n",
      "Epoch 5: |          | 375/? [09:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 375, loss 4.589902400970459\n",
      "Epoch 5: |          | 376/? [09:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 376, loss 3.9699935913085938\n",
      "Epoch 5: |          | 377/? [09:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 377, loss 4.152926921844482\n",
      "Epoch 5: |          | 378/? [09:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 378, loss 4.32356071472168\n",
      "Epoch 5: |          | 379/? [09:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 379, loss 4.148892402648926\n",
      "Epoch 5: |          | 380/? [09:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 380, loss 4.2054243087768555\n",
      "Epoch 5: |          | 381/? [09:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 381, loss 4.247814655303955\n",
      "Epoch 5: |          | 382/? [09:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 382, loss 3.9625747203826904\n",
      "Epoch 5: |          | 383/? [09:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 383, loss 4.028257369995117\n",
      "Epoch 5: |          | 384/? [09:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 384, loss 4.480207920074463\n",
      "Epoch 5: |          | 385/? [09:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 385, loss 4.110777854919434\n",
      "Epoch 5: |          | 386/? [09:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 386, loss 3.0366790294647217\n",
      "Epoch 5: |          | 387/? [09:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 387, loss 3.8872122764587402\n",
      "Epoch 5: |          | 388/? [09:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 388, loss 3.837686538696289\n",
      "Epoch 5: |          | 389/? [09:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 389, loss 4.431492805480957\n",
      "Epoch 5: |          | 390/? [09:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 390, loss 3.8852810859680176\n",
      "Epoch 5: |          | 391/? [09:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 391, loss 4.296114921569824\n",
      "Epoch 5: |          | 392/? [09:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 392, loss 4.389712333679199\n",
      "Epoch 5: |          | 393/? [09:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 393, loss 4.415327548980713\n",
      "Epoch 5: |          | 394/? [09:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 394, loss 4.064906120300293\n",
      "Epoch 5: |          | 395/? [09:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 395, loss 4.282567977905273\n",
      "Epoch 5: |          | 396/? [09:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 396, loss 4.229492664337158\n",
      "Epoch 5: |          | 397/? [09:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 397, loss 4.0382819175720215\n",
      "Epoch 5: |          | 398/? [09:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 398, loss 3.926562547683716\n",
      "Epoch 5: |          | 399/? [09:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 399, loss 3.999218463897705\n",
      "Epoch 5: |          | 400/? [09:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 400, loss 3.9580864906311035\n",
      "Epoch 5: |          | 401/? [09:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 401, loss 3.9116554260253906\n",
      "Epoch 5: |          | 402/? [09:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 402, loss 4.315129280090332\n",
      "Epoch 5: |          | 403/? [09:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 403, loss 4.219456195831299\n",
      "Epoch 5: |          | 404/? [09:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 404, loss 3.7225871086120605\n",
      "Epoch 5: |          | 405/? [09:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 405, loss 3.754415988922119\n",
      "Epoch 5: |          | 406/? [09:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 406, loss 4.0949907302856445\n",
      "Epoch 5: |          | 407/? [09:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 407, loss 4.035479545593262\n",
      "Epoch 5: |          | 408/? [09:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 408, loss 4.434274196624756\n",
      "Epoch 5: |          | 409/? [09:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 409, loss 4.378620147705078\n",
      "Epoch 5: |          | 410/? [09:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 410, loss 4.071837425231934\n",
      "Epoch 5: |          | 411/? [09:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 411, loss 3.9168472290039062\n",
      "Epoch 5: |          | 412/? [09:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 412, loss 3.45412015914917\n",
      "Epoch 5: |          | 413/? [09:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 413, loss 4.1898603439331055\n",
      "Epoch 5: |          | 414/? [09:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 414, loss 3.788909912109375\n",
      "Epoch 5: |          | 415/? [09:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 415, loss 4.230076789855957\n",
      "Epoch 5: |          | 416/? [09:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 416, loss 4.640347003936768\n",
      "Epoch 5: |          | 417/? [10:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 417, loss 4.6841959953308105\n",
      "Epoch 5: |          | 418/? [10:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 418, loss 4.255496501922607\n",
      "Epoch 5: |          | 419/? [10:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 419, loss 4.072118759155273\n",
      "Epoch 5: |          | 420/? [10:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 420, loss 4.1894755363464355\n",
      "Epoch 5: |          | 421/? [10:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 421, loss 4.683785915374756\n",
      "Epoch 5: |          | 422/? [10:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 422, loss 4.209268569946289\n",
      "Epoch 5: |          | 423/? [10:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 423, loss 3.8068275451660156\n",
      "Epoch 5: |          | 424/? [10:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 424, loss 4.478934288024902\n",
      "Epoch 5: |          | 425/? [10:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 425, loss 4.321698188781738\n",
      "Epoch 5: |          | 426/? [10:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 426, loss 3.8263099193573\n",
      "Epoch 5: |          | 427/? [10:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 427, loss 3.909330368041992\n",
      "Epoch 5: |          | 428/? [10:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 428, loss 4.656275749206543\n",
      "Epoch 5: |          | 429/? [10:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 429, loss 3.5544381141662598\n",
      "Epoch 5: |          | 430/? [10:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 430, loss 4.204479694366455\n",
      "Epoch 5: |          | 431/? [10:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 431, loss 4.065950393676758\n",
      "Epoch 5: |          | 432/? [10:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 432, loss 4.217690467834473\n",
      "Epoch 5: |          | 433/? [10:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 433, loss 4.202765941619873\n",
      "Epoch 5: |          | 434/? [10:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 434, loss 4.069084644317627\n",
      "Epoch 5: |          | 435/? [10:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 435, loss 3.755305051803589\n",
      "Epoch 5: |          | 436/? [10:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 436, loss 4.1337571144104\n",
      "Epoch 5: |          | 437/? [10:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 437, loss 4.317691326141357\n",
      "Epoch 5: |          | 438/? [10:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 438, loss 3.957179546356201\n",
      "Epoch 5: |          | 439/? [10:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 439, loss 3.8718674182891846\n",
      "Epoch 5: |          | 440/? [10:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 440, loss 3.8226253986358643\n",
      "Epoch 5: |          | 441/? [10:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 441, loss 4.189842700958252\n",
      "Epoch 5: |          | 442/? [10:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 442, loss 3.9393150806427\n",
      "Epoch 5: |          | 443/? [10:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 443, loss 4.112343788146973\n",
      "Epoch 5: |          | 444/? [10:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 444, loss 4.0967936515808105\n",
      "Epoch 5: |          | 445/? [10:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 445, loss 5.0481767654418945\n",
      "Epoch 5: |          | 446/? [10:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 446, loss 4.069633960723877\n",
      "Epoch 5: |          | 447/? [10:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 447, loss 4.605472087860107\n",
      "Epoch 5: |          | 448/? [10:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 448, loss 3.6692261695861816\n",
      "Epoch 5: |          | 449/? [10:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 449, loss 4.094364166259766\n",
      "Epoch 5: |          | 450/? [10:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 450, loss 4.410323619842529\n",
      "Epoch 5: |          | 451/? [10:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 451, loss 4.018372535705566\n",
      "Epoch 5: |          | 452/? [10:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 452, loss 3.808765411376953\n",
      "Epoch 5: |          | 453/? [10:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 453, loss 4.502367973327637\n",
      "Epoch 5: |          | 454/? [10:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 454, loss 3.8911430835723877\n",
      "Epoch 5: |          | 455/? [10:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 455, loss 4.259328365325928\n",
      "Epoch 5: |          | 456/? [10:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 456, loss 3.555903673171997\n",
      "Epoch 5: |          | 457/? [10:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 457, loss 4.0076398849487305\n",
      "Epoch 5: |          | 458/? [10:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 458, loss 4.4921159744262695\n",
      "Epoch 5: |          | 459/? [10:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 459, loss 4.430171966552734\n",
      "Epoch 5: |          | 460/? [11:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 460, loss 4.203074932098389\n",
      "Epoch 5: |          | 461/? [11:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 461, loss 4.1048502922058105\n",
      "Epoch 5: |          | 462/? [11:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 462, loss 4.237452983856201\n",
      "Epoch 5: |          | 463/? [11:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 463, loss 4.060523986816406\n",
      "Epoch 5: |          | 464/? [11:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 464, loss 3.5549557209014893\n",
      "Epoch 5: |          | 465/? [11:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 465, loss 3.8484721183776855\n",
      "Epoch 5: |          | 466/? [11:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 466, loss 4.3164873123168945\n",
      "Epoch 5: |          | 467/? [11:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 467, loss 4.133261203765869\n",
      "Epoch 5: |          | 468/? [11:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 468, loss 4.043607711791992\n",
      "Epoch 5: |          | 469/? [11:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 469, loss 4.184625625610352\n",
      "Epoch 5: |          | 470/? [11:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 470, loss 3.5573017597198486\n",
      "Epoch 5: |          | 471/? [11:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 471, loss 4.33551549911499\n",
      "Epoch 5: |          | 472/? [11:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 472, loss 3.8458709716796875\n",
      "Epoch 5: |          | 473/? [11:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 473, loss 3.8484625816345215\n",
      "Epoch 5: |          | 474/? [11:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 474, loss 3.5431625843048096\n",
      "Epoch 5: |          | 475/? [11:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 475, loss 4.8018598556518555\n",
      "Epoch 5: |          | 476/? [11:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 476, loss 3.789886474609375\n",
      "Epoch 5: |          | 477/? [11:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 477, loss 3.3229095935821533\n",
      "Epoch 5: |          | 478/? [11:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 478, loss 3.5267624855041504\n",
      "Epoch 5: |          | 479/? [11:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 479, loss 3.989642381668091\n",
      "Epoch 5: |          | 480/? [11:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 480, loss 3.9758713245391846\n",
      "Epoch 5: |          | 481/? [11:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 481, loss 3.57932710647583\n",
      "Epoch 5: |          | 482/? [11:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 482, loss 3.8438796997070312\n",
      "Epoch 5: |          | 483/? [11:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 483, loss 3.5244743824005127\n",
      "Epoch 5: |          | 484/? [11:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 484, loss 4.472709655761719\n",
      "Epoch 5: |          | 485/? [11:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 485, loss 4.326504230499268\n",
      "Epoch 5: |          | 486/? [11:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 486, loss 3.950047016143799\n",
      "Epoch 5: |          | 487/? [11:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 487, loss 4.253127098083496\n",
      "Epoch 5: |          | 488/? [11:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 488, loss 3.928828477859497\n",
      "Epoch 5: |          | 489/? [11:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 489, loss 3.5049259662628174\n",
      "Epoch 5: |          | 490/? [11:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 490, loss 4.133409023284912\n",
      "Epoch 5: |          | 491/? [11:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 491, loss 3.9505252838134766\n",
      "Epoch 5: |          | 492/? [11:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 492, loss 3.287216901779175\n",
      "Epoch 5: |          | 493/? [11:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 493, loss 4.323659896850586\n",
      "Epoch 5: |          | 494/? [11:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 494, loss 4.157575607299805\n",
      "Epoch 5: |          | 495/? [11:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 495, loss 4.242975234985352\n",
      "Epoch 5: |          | 496/? [11:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 496, loss 3.826096296310425\n",
      "Epoch 5: |          | 497/? [11:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 497, loss 4.398369312286377\n",
      "Epoch 5: |          | 498/? [11:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 498, loss 4.017119407653809\n",
      "Epoch 5: |          | 499/? [11:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 499, loss 4.133352756500244\n",
      "Epoch 5: |          | 500/? [11:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 500, loss 3.9060611724853516\n",
      "Epoch 5: |          | 501/? [11:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 501, loss 3.6566669940948486\n",
      "Epoch 5: |          | 502/? [12:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 502, loss 4.10992431640625\n",
      "Epoch 5: |          | 503/? [12:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 503, loss 4.0822601318359375\n",
      "Epoch 5: |          | 504/? [12:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 504, loss 3.947783946990967\n",
      "Epoch 5: |          | 505/? [12:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 505, loss 3.3782310485839844\n",
      "Epoch 5: |          | 506/? [12:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 506, loss 4.0113205909729\n",
      "Epoch 5: |          | 507/? [12:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 507, loss 4.050746440887451\n",
      "Epoch 5: |          | 508/? [12:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 508, loss 4.412747383117676\n",
      "Epoch 5: |          | 509/? [12:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 509, loss 3.745009660720825\n",
      "Epoch 5: |          | 510/? [12:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 510, loss 4.270392894744873\n",
      "Epoch 5: |          | 511/? [12:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 511, loss 4.101749420166016\n",
      "Epoch 5: |          | 512/? [12:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 512, loss 3.492452621459961\n",
      "Epoch 5: |          | 513/? [12:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 513, loss 3.7434515953063965\n",
      "Epoch 5: |          | 514/? [12:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 514, loss 3.9058170318603516\n",
      "Epoch 5: |          | 515/? [12:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 515, loss 3.5788986682891846\n",
      "Epoch 5: |          | 516/? [12:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 516, loss 3.848550319671631\n",
      "Epoch 5: |          | 517/? [12:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 517, loss 4.152377128601074\n",
      "Epoch 5: |          | 518/? [12:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 518, loss 3.6635448932647705\n",
      "Epoch 5: |          | 519/? [12:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 519, loss 4.124777793884277\n",
      "Epoch 5: |          | 520/? [12:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 520, loss 3.9737708568573\n",
      "Epoch 5: |          | 521/? [12:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 521, loss 3.9770896434783936\n",
      "Epoch 5: |          | 522/? [12:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 522, loss 4.517385005950928\n",
      "Epoch 5: |          | 523/? [12:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 523, loss 4.589412689208984\n",
      "Epoch 5: |          | 524/? [12:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 524, loss 4.377390384674072\n",
      "Epoch 5: |          | 525/? [12:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 525, loss 3.960894823074341\n",
      "Epoch 5: |          | 526/? [12:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 526, loss 3.749464511871338\n",
      "Epoch 5: |          | 527/? [12:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 527, loss 4.434844970703125\n",
      "Epoch 5: |          | 528/? [12:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 528, loss 4.162801742553711\n",
      "Epoch 5: |          | 529/? [12:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 529, loss 3.7967636585235596\n",
      "Epoch 5: |          | 530/? [12:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 530, loss 4.365511894226074\n",
      "Epoch 5: |          | 531/? [12:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 531, loss 3.8773722648620605\n",
      "Epoch 5: |          | 532/? [12:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 532, loss 4.161187648773193\n",
      "Epoch 5: |          | 533/? [12:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 533, loss 3.7944278717041016\n",
      "Epoch 5: |          | 534/? [12:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 534, loss 3.4565062522888184\n",
      "Epoch 5: |          | 535/? [12:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 535, loss 3.9220833778381348\n",
      "Epoch 5: |          | 536/? [12:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 536, loss 4.4320783615112305\n",
      "Epoch 5: |          | 537/? [12:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 537, loss 4.186905860900879\n",
      "Epoch 5: |          | 538/? [12:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 538, loss 3.8176162242889404\n",
      "Epoch 5: |          | 539/? [12:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 539, loss 3.912506103515625\n",
      "Epoch 5: |          | 540/? [12:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 540, loss 4.3062334060668945\n",
      "Epoch 5: |          | 541/? [12:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 541, loss 4.093716144561768\n",
      "Epoch 5: |          | 542/? [12:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 542, loss 3.840273380279541\n",
      "Epoch 5: |          | 543/? [12:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 543, loss 4.228763103485107\n",
      "Epoch 5: |          | 544/? [12:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 544, loss 4.138028621673584\n",
      "Epoch 5: |          | 545/? [13:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 545, loss 3.3815078735351562\n",
      "Epoch 5: |          | 546/? [13:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 546, loss 4.147620677947998\n",
      "Epoch 5: |          | 547/? [13:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 547, loss 4.663320541381836\n",
      "Epoch 5: |          | 548/? [13:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 548, loss 4.284909248352051\n",
      "Epoch 5: |          | 549/? [13:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 549, loss 4.152630805969238\n",
      "Epoch 5: |          | 550/? [13:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 550, loss 4.505305290222168\n",
      "Epoch 5: |          | 551/? [13:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 551, loss 4.154914379119873\n",
      "Epoch 5: |          | 552/? [13:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 552, loss 4.148940563201904\n",
      "Epoch 5: |          | 553/? [13:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 553, loss 3.585165500640869\n",
      "Epoch 5: |          | 554/? [13:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 554, loss 4.166577339172363\n",
      "Epoch 5: |          | 555/? [13:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 555, loss 4.480007171630859\n",
      "Epoch 5: |          | 556/? [13:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 556, loss 4.229104518890381\n",
      "Epoch 5: |          | 557/? [13:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 557, loss 3.7223129272460938\n",
      "Epoch 5: |          | 558/? [13:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 558, loss 3.954049587249756\n",
      "Epoch 5: |          | 559/? [13:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 559, loss 3.9483909606933594\n",
      "Epoch 5: |          | 560/? [13:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 560, loss 3.4149792194366455\n",
      "Epoch 5: |          | 561/? [13:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 561, loss 3.4096176624298096\n",
      "Epoch 5: |          | 562/? [13:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 562, loss 4.341185569763184\n",
      "Epoch 5: |          | 563/? [13:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 563, loss 3.4104526042938232\n",
      "Epoch 5: |          | 564/? [13:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 564, loss 3.8879833221435547\n",
      "Epoch 5: |          | 565/? [13:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 565, loss 4.29816198348999\n",
      "Epoch 5: |          | 566/? [13:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 566, loss 4.333993911743164\n",
      "Epoch 5: |          | 567/? [13:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 567, loss 4.475063323974609\n",
      "Epoch 5: |          | 568/? [13:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 568, loss 3.5611279010772705\n",
      "Epoch 5: |          | 569/? [13:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 569, loss 4.189539909362793\n",
      "Epoch 5: |          | 570/? [13:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 570, loss 4.258727073669434\n",
      "Epoch 5: |          | 571/? [13:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 571, loss 3.8287785053253174\n",
      "Epoch 5: |          | 572/? [13:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 572, loss 4.761487007141113\n",
      "Epoch 5: |          | 573/? [13:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 573, loss 3.1725058555603027\n",
      "Epoch 5: |          | 574/? [13:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 574, loss 4.348642349243164\n",
      "Epoch 5: |          | 575/? [13:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 575, loss 3.669620990753174\n",
      "Epoch 5: |          | 576/? [13:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 576, loss 3.903097629547119\n",
      "Epoch 5: |          | 577/? [13:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 577, loss 4.1440534591674805\n",
      "Epoch 5: |          | 578/? [13:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 578, loss 4.342471599578857\n",
      "Epoch 5: |          | 579/? [13:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 579, loss 3.46958589553833\n",
      "Epoch 5: |          | 580/? [13:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 580, loss 4.163632869720459\n",
      "Epoch 5: |          | 581/? [13:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 581, loss 4.200068950653076\n",
      "Epoch 5: |          | 582/? [13:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 582, loss 4.263100624084473\n",
      "Epoch 5: |          | 583/? [13:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 583, loss 4.01120138168335\n",
      "Epoch 5: |          | 584/? [13:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 584, loss 4.205286026000977\n",
      "Epoch 5: |          | 585/? [13:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 585, loss 4.22182559967041\n",
      "Epoch 5: |          | 586/? [13:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 586, loss 4.25522518157959\n",
      "Epoch 5: |          | 587/? [14:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 587, loss 4.249112606048584\n",
      "Epoch 5: |          | 588/? [14:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 588, loss 4.231837272644043\n",
      "Epoch 5: |          | 589/? [14:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 589, loss 3.640655040740967\n",
      "Epoch 5: |          | 590/? [14:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 590, loss 4.2714643478393555\n",
      "Epoch 5: |          | 591/? [14:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 591, loss 4.143008232116699\n",
      "Epoch 5: |          | 592/? [14:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 592, loss 3.819392442703247\n",
      "Epoch 5: |          | 593/? [14:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 593, loss 4.066230297088623\n",
      "Epoch 5: |          | 594/? [14:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 594, loss 4.8765459060668945\n",
      "Epoch 5: |          | 595/? [14:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 595, loss 3.6149730682373047\n",
      "Epoch 5: |          | 596/? [14:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 596, loss 3.666318893432617\n",
      "Epoch 5: |          | 597/? [14:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 597, loss 3.9099743366241455\n",
      "Epoch 5: |          | 598/? [14:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 598, loss 4.428926467895508\n",
      "Epoch 5: |          | 599/? [14:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 599, loss 4.040137767791748\n",
      "Epoch 5: |          | 600/? [14:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 600, loss 3.8190104961395264\n",
      "Epoch 5: |          | 601/? [14:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 601, loss 4.098822593688965\n",
      "Epoch 5: |          | 602/? [14:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 602, loss 3.6638572216033936\n",
      "Epoch 5: |          | 603/? [14:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 603, loss 3.758643388748169\n",
      "Epoch 5: |          | 604/? [14:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 604, loss 5.553365230560303\n",
      "Epoch 5: |          | 605/? [14:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 605, loss 3.5143074989318848\n",
      "Epoch 5: |          | 606/? [14:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 606, loss 3.8134117126464844\n",
      "Epoch 5: |          | 607/? [14:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 607, loss 4.157421112060547\n",
      "Epoch 5: |          | 608/? [14:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 608, loss 3.945631504058838\n",
      "Epoch 5: |          | 609/? [14:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 609, loss 3.84635853767395\n",
      "Epoch 5: |          | 610/? [14:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 610, loss 3.9304633140563965\n",
      "Epoch 5: |          | 611/? [14:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 611, loss 4.089829444885254\n",
      "Epoch 5: |          | 612/? [14:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 612, loss 3.8294436931610107\n",
      "Epoch 5: |          | 613/? [14:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 613, loss 4.118010520935059\n",
      "Epoch 5: |          | 614/? [14:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 614, loss 3.9481449127197266\n",
      "Epoch 5: |          | 615/? [14:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 615, loss 4.470026016235352\n",
      "Epoch 5: |          | 616/? [14:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 616, loss 4.6561503410339355\n",
      "Epoch 5: |          | 617/? [14:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 617, loss 3.1967334747314453\n",
      "Epoch 5: |          | 618/? [14:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 618, loss 4.17288064956665\n",
      "Epoch 5: |          | 619/? [14:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 619, loss 3.7284321784973145\n",
      "Epoch 5: |          | 620/? [14:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 620, loss 4.27874231338501\n",
      "Epoch 5: |          | 621/? [14:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 621, loss 3.7263665199279785\n",
      "Epoch 5: |          | 622/? [14:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 622, loss 3.507521152496338\n",
      "Epoch 5: |          | 623/? [14:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 623, loss 3.258288860321045\n",
      "Epoch 5: |          | 624/? [14:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 624, loss 2.9739298820495605\n",
      "Epoch 5: |          | 625/? [14:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 625, loss 4.526484966278076\n",
      "Epoch 5: |          | 626/? [14:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 626, loss 3.9929728507995605\n",
      "Epoch 5: |          | 627/? [14:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 627, loss 3.900524854660034\n",
      "Epoch 5: |          | 628/? [14:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 628, loss 3.893195629119873\n",
      "Epoch 5: |          | 629/? [15:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 629, loss 4.276527404785156\n",
      "Epoch 5: |          | 630/? [15:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 630, loss 4.035999298095703\n",
      "Epoch 5: |          | 631/? [15:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 631, loss 4.168335914611816\n",
      "Epoch 5: |          | 632/? [15:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 632, loss 3.4518094062805176\n",
      "Epoch 5: |          | 633/? [15:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 633, loss 4.263387680053711\n",
      "Epoch 5: |          | 634/? [15:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 634, loss 3.799783229827881\n",
      "Epoch 5: |          | 635/? [15:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 635, loss 3.6076228618621826\n",
      "Epoch 5: |          | 636/? [15:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 636, loss 4.040818214416504\n",
      "Epoch 5: |          | 637/? [15:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 637, loss 3.829401731491089\n",
      "Epoch 5: |          | 638/? [15:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 638, loss 4.092784881591797\n",
      "Epoch 5: |          | 639/? [15:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 639, loss 3.8560409545898438\n",
      "Epoch 5: |          | 640/? [15:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 640, loss 4.433129787445068\n",
      "Epoch 5: |          | 641/? [15:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 641, loss 3.442410945892334\n",
      "Epoch 5: |          | 642/? [15:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 642, loss 4.197609901428223\n",
      "Epoch 5: |          | 643/? [15:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 643, loss 4.137864589691162\n",
      "Epoch 5: |          | 644/? [15:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 644, loss 4.057101249694824\n",
      "Epoch 5: |          | 645/? [15:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 645, loss 3.7917068004608154\n",
      "Epoch 5: |          | 646/? [15:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 646, loss 3.8349509239196777\n",
      "Epoch 5: |          | 647/? [15:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 647, loss 4.420685768127441\n",
      "Epoch 5: |          | 648/? [15:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 648, loss 3.8134608268737793\n",
      "Epoch 5: |          | 649/? [15:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 649, loss 3.3409981727600098\n",
      "Epoch 5: |          | 650/? [15:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 650, loss 4.355169773101807\n",
      "Epoch 5: |          | 651/? [15:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 651, loss 4.469174861907959\n",
      "Epoch 5: |          | 652/? [15:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 652, loss 3.892199993133545\n",
      "Epoch 5: |          | 653/? [15:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 653, loss 4.0391740798950195\n",
      "Epoch 5: |          | 654/? [15:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 654, loss 4.164290428161621\n",
      "Epoch 5: |          | 655/? [15:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 655, loss 3.989391326904297\n",
      "Epoch 5: |          | 656/? [15:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 656, loss 3.588017702102661\n",
      "Epoch 5: |          | 657/? [15:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 657, loss 5.986948490142822\n",
      "Epoch 5: |          | 658/? [15:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 658, loss 3.5825748443603516\n",
      "Epoch 5: |          | 659/? [15:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 659, loss 4.031971454620361\n",
      "Epoch 5: |          | 660/? [15:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 660, loss 4.361912727355957\n",
      "Epoch 5: |          | 661/? [15:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 661, loss 4.285636901855469\n",
      "Epoch 5: |          | 662/? [15:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 662, loss 4.195578575134277\n",
      "Epoch 5: |          | 663/? [15:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 663, loss 3.921318769454956\n",
      "Epoch 5: |          | 664/? [15:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 664, loss 3.8397789001464844\n",
      "Epoch 5: |          | 665/? [15:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 665, loss 4.153183937072754\n",
      "Epoch 5: |          | 666/? [15:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 666, loss 3.956261396408081\n",
      "Epoch 5: |          | 667/? [15:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 667, loss 4.807284355163574\n",
      "Epoch 5: |          | 668/? [15:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 668, loss 3.541491985321045\n",
      "Epoch 5: |          | 669/? [15:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 669, loss 3.767609119415283\n",
      "Epoch 5: |          | 670/? [15:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 670, loss 4.46713399887085\n",
      "Epoch 5: |          | 671/? [15:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 671, loss 4.175387382507324\n",
      "Epoch 5: |          | 672/? [16:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 672, loss 4.1859025955200195\n",
      "Epoch 5: |          | 673/? [16:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 673, loss 4.03454065322876\n",
      "Epoch 5: |          | 674/? [16:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 674, loss 2.496215581893921\n",
      "Epoch 5: |          | 675/? [16:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 675, loss 1.0140697956085205\n",
      "Epoch 5: |          | 676/? [16:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 676, loss 0.8505169153213501\n",
      "Epoch 5: |          | 677/? [16:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 677, loss 0.6770154237747192\n",
      "Epoch 5: |          | 678/? [16:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 678, loss 1.8516464233398438\n",
      "Epoch 5: |          | 679/? [16:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 679, loss 3.4725966453552246\n",
      "Epoch 5: |          | 680/? [16:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 680, loss 3.9518208503723145\n",
      "Epoch 5: |          | 681/? [16:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 681, loss 3.478666305541992\n",
      "Epoch 5: |          | 682/? [16:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 682, loss 3.7860589027404785\n",
      "Epoch 5: |          | 683/? [16:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 683, loss 3.5149848461151123\n",
      "Epoch 5: |          | 684/? [16:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 684, loss 4.568642616271973\n",
      "Epoch 5: |          | 685/? [16:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 685, loss 4.2040228843688965\n",
      "Epoch 5: |          | 686/? [16:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 686, loss 3.7531561851501465\n",
      "Epoch 5: |          | 687/? [16:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 687, loss 4.299757480621338\n",
      "Epoch 5: |          | 688/? [16:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 688, loss 3.892638683319092\n",
      "Epoch 5: |          | 689/? [16:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 689, loss 3.9082446098327637\n",
      "Epoch 5: |          | 690/? [16:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 690, loss 4.538022518157959\n",
      "Epoch 5: |          | 691/? [16:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 691, loss 4.0078816413879395\n",
      "Epoch 5: |          | 692/? [16:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 692, loss 4.028110980987549\n",
      "Epoch 5: |          | 693/? [16:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 693, loss 4.57279634475708\n",
      "Epoch 5: |          | 694/? [16:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 694, loss 3.9152934551239014\n",
      "Epoch 5: |          | 695/? [16:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 695, loss 4.487553119659424\n",
      "Epoch 5: |          | 696/? [16:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 696, loss 3.7472763061523438\n",
      "Epoch 5: |          | 697/? [16:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 697, loss 3.9725260734558105\n",
      "Epoch 5: |          | 698/? [16:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 698, loss 3.337311267852783\n",
      "Epoch 5: |          | 699/? [16:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 699, loss 4.13010835647583\n",
      "Epoch 5: |          | 700/? [16:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 700, loss 4.206847667694092\n",
      "Epoch 5: |          | 701/? [16:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 701, loss 3.8117785453796387\n",
      "Epoch 5: |          | 702/? [16:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 702, loss 4.092283725738525\n",
      "Epoch 5: |          | 703/? [16:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 703, loss 4.232797622680664\n",
      "Epoch 5: |          | 704/? [16:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 704, loss 4.082343578338623\n",
      "Epoch 5: |          | 705/? [16:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 705, loss 3.717644453048706\n",
      "Epoch 5: |          | 706/? [16:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 706, loss 3.78942608833313\n",
      "Epoch 5: |          | 707/? [16:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 707, loss 4.276933193206787\n",
      "Epoch 5: |          | 708/? [16:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 708, loss 3.9987387657165527\n",
      "Epoch 5: |          | 709/? [16:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 709, loss 3.915541172027588\n",
      "Epoch 5: |          | 710/? [16:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 710, loss 4.470775604248047\n",
      "Epoch 5: |          | 711/? [16:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 711, loss 4.602561950683594\n",
      "Epoch 5: |          | 712/? [16:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 712, loss 4.246005058288574\n",
      "Epoch 5: |          | 713/? [16:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 713, loss 4.324443817138672\n",
      "Epoch 5: |          | 714/? [17:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 714, loss 4.423219203948975\n",
      "Epoch 5: |          | 715/? [17:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 715, loss 3.320613384246826\n",
      "Epoch 5: |          | 716/? [17:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 716, loss 4.040445804595947\n",
      "Epoch 5: |          | 717/? [17:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 717, loss 3.9118247032165527\n",
      "Epoch 5: |          | 718/? [17:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 718, loss 3.4437713623046875\n",
      "Epoch 5: |          | 719/? [17:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 719, loss 3.9998958110809326\n",
      "Epoch 5: |          | 720/? [17:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 720, loss 3.668329954147339\n",
      "Epoch 5: |          | 721/? [17:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 721, loss 4.351276397705078\n",
      "Epoch 5: |          | 722/? [17:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 722, loss 3.6236610412597656\n",
      "Epoch 5: |          | 723/? [17:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 723, loss 4.206884384155273\n",
      "Epoch 5: |          | 724/? [17:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 724, loss 3.8014068603515625\n",
      "Epoch 5: |          | 725/? [17:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 725, loss 3.6900672912597656\n",
      "Epoch 5: |          | 726/? [17:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 726, loss 3.892669200897217\n",
      "Epoch 5: |          | 727/? [17:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 727, loss 3.6573052406311035\n",
      "Epoch 5: |          | 728/? [17:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 728, loss 3.4837212562561035\n",
      "Epoch 5: |          | 729/? [17:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 729, loss 3.9692931175231934\n",
      "Epoch 5: |          | 730/? [17:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 730, loss 3.910761594772339\n",
      "Epoch 5: |          | 731/? [17:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 731, loss 4.064680576324463\n",
      "Epoch 5: |          | 732/? [17:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 732, loss 4.340034484863281\n",
      "Epoch 5: |          | 733/? [17:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 733, loss 4.036220073699951\n",
      "Epoch 5: |          | 734/? [17:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 734, loss 4.2646284103393555\n",
      "Epoch 5: |          | 735/? [17:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 735, loss 4.149539470672607\n",
      "Epoch 5: |          | 736/? [17:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 736, loss 3.664757490158081\n",
      "Epoch 5: |          | 737/? [17:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 737, loss 4.451776027679443\n",
      "Epoch 5: |          | 738/? [17:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 738, loss 3.610196590423584\n",
      "Epoch 5: |          | 739/? [17:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 739, loss 4.130043983459473\n",
      "Epoch 5: |          | 740/? [17:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 740, loss 3.79716420173645\n",
      "Epoch 5: |          | 741/? [17:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 741, loss 3.991687774658203\n",
      "Epoch 5: |          | 742/? [17:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 742, loss 4.382336616516113\n",
      "Epoch 5: |          | 743/? [17:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 743, loss 4.205416679382324\n",
      "Epoch 5: |          | 744/? [17:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 744, loss 4.178030967712402\n",
      "Epoch 5: |          | 745/? [17:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 745, loss 3.7982044219970703\n",
      "Epoch 5: |          | 746/? [17:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 746, loss 4.0724592208862305\n",
      "Epoch 5: |          | 747/? [17:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 747, loss 3.791637897491455\n",
      "Epoch 5: |          | 748/? [17:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 748, loss 2.8841216564178467\n",
      "Epoch 5: |          | 749/? [17:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 749, loss 3.9944405555725098\n",
      "Epoch 5: |          | 750/? [17:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 750, loss 4.310287952423096\n",
      "Epoch 5: |          | 751/? [17:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 751, loss 2.5940322875976562\n",
      "Epoch 5: |          | 752/? [17:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 752, loss 4.156384468078613\n",
      "Epoch 5: |          | 753/? [17:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 753, loss 3.366809368133545\n",
      "Epoch 5: |          | 754/? [17:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 754, loss 3.794813871383667\n",
      "Epoch 5: |          | 755/? [17:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 755, loss 3.589825391769409\n",
      "Epoch 5: |          | 756/? [18:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 756, loss 3.9721553325653076\n",
      "Epoch 5: |          | 757/? [18:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 757, loss 4.051623821258545\n",
      "Epoch 5: |          | 758/? [18:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 758, loss 3.761871337890625\n",
      "Epoch 5: |          | 759/? [18:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 759, loss 3.7383906841278076\n",
      "Epoch 5: |          | 760/? [18:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 760, loss 4.2528252601623535\n",
      "Epoch 5: |          | 761/? [18:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 761, loss 4.255475044250488\n",
      "Epoch 5: |          | 762/? [18:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 762, loss 3.905412197113037\n",
      "Epoch 5: |          | 763/? [18:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 763, loss 4.140527248382568\n",
      "Epoch 5: |          | 764/? [18:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 764, loss 4.368221282958984\n",
      "Epoch 5: |          | 765/? [18:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 765, loss 4.099316596984863\n",
      "Epoch 5: |          | 766/? [18:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 766, loss 4.507672309875488\n",
      "Epoch 5: |          | 767/? [18:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 767, loss 4.565102577209473\n",
      "Epoch 5: |          | 768/? [18:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 768, loss 4.120188236236572\n",
      "Epoch 5: |          | 769/? [18:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 769, loss 3.2924537658691406\n",
      "Epoch 5: |          | 770/? [18:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 770, loss 3.867089033126831\n",
      "Epoch 5: |          | 771/? [18:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 771, loss 4.596292972564697\n",
      "Epoch 5: |          | 772/? [18:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 772, loss 4.271174430847168\n",
      "Epoch 5: |          | 773/? [18:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 773, loss 3.9808170795440674\n",
      "Epoch 5: |          | 774/? [18:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 774, loss 4.095489025115967\n",
      "Epoch 5: |          | 775/? [18:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 775, loss 4.573362350463867\n",
      "Epoch 5: |          | 776/? [18:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 776, loss 3.9873909950256348\n",
      "Epoch 5: |          | 777/? [18:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 777, loss 3.878134250640869\n",
      "Epoch 5: |          | 778/? [18:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 778, loss 4.265848159790039\n",
      "Epoch 5: |          | 779/? [18:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 779, loss 4.702618598937988\n",
      "Epoch 5: |          | 780/? [18:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 780, loss 3.6915268898010254\n",
      "Epoch 5: |          | 781/? [18:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 781, loss 3.8069446086883545\n",
      "Epoch 5: |          | 782/? [18:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 782, loss 4.195679187774658\n",
      "Epoch 5: |          | 783/? [18:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 783, loss 4.2704854011535645\n",
      "Epoch 5: |          | 784/? [18:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 784, loss 3.822608470916748\n",
      "Epoch 5: |          | 785/? [18:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 785, loss 3.625274181365967\n",
      "Epoch 5: |          | 786/? [18:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 786, loss 4.480360507965088\n",
      "Epoch 5: |          | 787/? [18:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 787, loss 4.435647010803223\n",
      "Epoch 5: |          | 788/? [18:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 788, loss 2.659802198410034\n",
      "Epoch 5: |          | 789/? [18:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 789, loss 3.950654983520508\n",
      "Epoch 5: |          | 790/? [18:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 790, loss 4.79292106628418\n",
      "Epoch 5: |          | 791/? [18:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 791, loss 4.485007286071777\n",
      "Epoch 5: |          | 792/? [18:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 792, loss 3.622282028198242\n",
      "Epoch 5: |          | 793/? [18:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 793, loss 4.166210174560547\n",
      "Epoch 5: |          | 794/? [18:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 794, loss 4.525788307189941\n",
      "Epoch 5: |          | 795/? [18:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 795, loss 3.972264528274536\n",
      "Epoch 5: |          | 796/? [18:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 796, loss 4.380764484405518\n",
      "Epoch 5: |          | 797/? [18:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 797, loss 3.345252513885498\n",
      "Epoch 5: |          | 798/? [19:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 798, loss 3.470240831375122\n",
      "Epoch 5: |          | 799/? [19:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 799, loss 4.432629585266113\n",
      "Epoch 5: |          | 800/? [19:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 800, loss 4.222370624542236\n",
      "Epoch 5: |          | 801/? [19:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 801, loss 3.798945188522339\n",
      "Epoch 5: |          | 802/? [19:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 802, loss 4.131241798400879\n",
      "Epoch 5: |          | 803/? [19:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 803, loss 3.942655086517334\n",
      "Epoch 5: |          | 804/? [19:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 804, loss 4.111096382141113\n",
      "Epoch 5: |          | 805/? [19:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 805, loss 4.291886806488037\n",
      "Epoch 5: |          | 806/? [19:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 806, loss 4.681975364685059\n",
      "Epoch 5: |          | 807/? [19:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 807, loss 4.058377265930176\n",
      "Epoch 5: |          | 808/? [19:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 808, loss 3.6737678050994873\n",
      "Epoch 5: |          | 809/? [19:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 809, loss 4.242269515991211\n",
      "Epoch 5: |          | 810/? [19:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 810, loss 3.998927593231201\n",
      "Epoch 5: |          | 811/? [19:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 811, loss 4.2712602615356445\n",
      "Epoch 5: |          | 812/? [19:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 812, loss 4.898533821105957\n",
      "Epoch 5: |          | 813/? [19:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 813, loss 4.658350944519043\n",
      "Epoch 5: |          | 814/? [19:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 814, loss 3.667917251586914\n",
      "Epoch 5: |          | 815/? [19:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 815, loss 4.39777946472168\n",
      "Epoch 5: |          | 816/? [19:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 816, loss 4.20977783203125\n",
      "Epoch 5: |          | 817/? [19:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 817, loss 3.5344550609588623\n",
      "Epoch 5: |          | 818/? [19:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 818, loss 4.530235767364502\n",
      "Epoch 5: |          | 819/? [19:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 819, loss 4.2219767570495605\n",
      "Epoch 5: |          | 820/? [19:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 820, loss 4.075406074523926\n",
      "Epoch 5: |          | 821/? [19:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 821, loss 4.051560401916504\n",
      "Epoch 5: |          | 822/? [19:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 822, loss 3.6796889305114746\n",
      "Epoch 5: |          | 823/? [19:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 823, loss 3.6880805492401123\n",
      "Epoch 5: |          | 824/? [19:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 824, loss 4.189101696014404\n",
      "Epoch 5: |          | 825/? [19:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 825, loss 3.699941635131836\n",
      "Epoch 5: |          | 826/? [19:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 826, loss 4.245064735412598\n",
      "Epoch 5: |          | 827/? [19:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 827, loss 3.854583740234375\n",
      "Epoch 5: |          | 828/? [19:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 828, loss 4.408280849456787\n",
      "Epoch 5: |          | 829/? [19:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 829, loss 4.062704563140869\n",
      "Epoch 5: |          | 830/? [19:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 830, loss 4.605710506439209\n",
      "Epoch 5: |          | 831/? [19:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 831, loss 2.500897169113159\n",
      "Epoch 5: |          | 832/? [19:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 832, loss 4.034863471984863\n",
      "Epoch 5: |          | 833/? [19:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 833, loss 3.89062237739563\n",
      "Epoch 5: |          | 834/? [19:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 834, loss 4.669183254241943\n",
      "Epoch 5: |          | 835/? [19:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 835, loss 3.9840874671936035\n",
      "Epoch 5: |          | 836/? [19:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 836, loss 4.685323715209961\n",
      "Epoch 5: |          | 837/? [19:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 837, loss 4.127518653869629\n",
      "Epoch 5: |          | 838/? [19:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 838, loss 3.3892879486083984\n",
      "Epoch 5: |          | 839/? [20:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 839, loss 3.793936252593994\n",
      "Epoch 5: |          | 840/? [20:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 840, loss 4.336119651794434\n",
      "Epoch 5: |          | 841/? [20:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 841, loss 4.369888782501221\n",
      "Epoch 5: |          | 842/? [20:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 842, loss 4.035747051239014\n",
      "Epoch 5: |          | 843/? [20:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 843, loss 4.373555660247803\n",
      "Epoch 5: |          | 844/? [20:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 844, loss 3.746736526489258\n",
      "Epoch 5: |          | 845/? [20:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 845, loss 4.163642406463623\n",
      "Epoch 5: |          | 846/? [20:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 846, loss 4.609791278839111\n",
      "Epoch 5: |          | 847/? [20:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 847, loss 4.165058135986328\n",
      "Epoch 5: |          | 848/? [20:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 848, loss 3.7131340503692627\n",
      "Epoch 5: |          | 849/? [20:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 849, loss 3.807654857635498\n",
      "Epoch 5: |          | 850/? [20:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 850, loss 3.918198347091675\n",
      "Epoch 5: |          | 851/? [20:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 851, loss 4.244680881500244\n",
      "Epoch 5: |          | 852/? [20:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 852, loss 4.339555263519287\n",
      "Epoch 5: |          | 853/? [20:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 853, loss 4.1831464767456055\n",
      "Epoch 5: |          | 854/? [20:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 854, loss 3.474045991897583\n",
      "Epoch 5: |          | 855/? [20:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 855, loss 3.7898669242858887\n",
      "Epoch 5: |          | 856/? [20:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 856, loss 3.722773313522339\n",
      "Epoch 5: |          | 857/? [20:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 857, loss 4.292061805725098\n",
      "Epoch 5: |          | 858/? [20:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 858, loss 4.158743858337402\n",
      "Epoch 5: |          | 859/? [20:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 859, loss 4.161325931549072\n",
      "Epoch 5: |          | 860/? [20:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 860, loss 4.560233116149902\n",
      "Epoch 5: |          | 861/? [20:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 861, loss 3.825490951538086\n",
      "Epoch 5: |          | 862/? [20:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 862, loss 4.186701774597168\n",
      "Epoch 5: |          | 863/? [20:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 863, loss 3.543879985809326\n",
      "Epoch 5: |          | 864/? [20:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 864, loss 4.133370399475098\n",
      "Epoch 5: |          | 865/? [20:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 865, loss 4.074187278747559\n",
      "Epoch 5: |          | 866/? [20:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 866, loss 3.125541925430298\n",
      "Epoch 5: |          | 867/? [20:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 867, loss 3.3198776245117188\n",
      "Epoch 5: |          | 868/? [20:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 868, loss 4.2084527015686035\n",
      "Epoch 5: |          | 869/? [20:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 869, loss 4.245409965515137\n",
      "Epoch 5: |          | 870/? [20:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 870, loss 3.8289923667907715\n",
      "Epoch 5: |          | 871/? [20:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 871, loss 4.1728997230529785\n",
      "Epoch 5: |          | 872/? [20:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 872, loss 4.009991645812988\n",
      "Epoch 5: |          | 873/? [20:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 873, loss 4.005921363830566\n",
      "Epoch 5: |          | 874/? [20:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 874, loss 3.4893221855163574\n",
      "Epoch 5: |          | 875/? [20:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 875, loss 4.2491350173950195\n",
      "Epoch 5: |          | 876/? [20:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 876, loss 3.8113155364990234\n",
      "Epoch 5: |          | 877/? [20:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 877, loss 4.167365074157715\n",
      "Epoch 5: |          | 878/? [21:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 878, loss 3.5817127227783203\n",
      "Epoch 5: |          | 879/? [21:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 879, loss 3.649937152862549\n",
      "Epoch 5: |          | 880/? [21:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 880, loss 4.76723575592041\n",
      "Epoch 5: |          | 881/? [21:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 881, loss 4.153305530548096\n",
      "Epoch 5: |          | 882/? [21:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 882, loss 3.9275410175323486\n",
      "Epoch 5: |          | 883/? [21:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 883, loss 4.066226005554199\n",
      "Epoch 5: |          | 884/? [21:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 884, loss 4.137057304382324\n",
      "Epoch 5: |          | 885/? [21:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 885, loss 3.899512767791748\n",
      "Epoch 5: |          | 886/? [21:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 886, loss 4.546038627624512\n",
      "Epoch 5: |          | 887/? [21:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 887, loss 4.568059921264648\n",
      "Epoch 5: |          | 888/? [21:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 888, loss 4.289050102233887\n",
      "Epoch 5: |          | 889/? [21:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 889, loss 3.8111491203308105\n",
      "Epoch 5: |          | 890/? [21:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 890, loss 4.09883975982666\n",
      "Epoch 5: |          | 891/? [21:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 891, loss 3.847235918045044\n",
      "Epoch 5: |          | 892/? [21:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 892, loss 4.468457221984863\n",
      "Epoch 5: |          | 893/? [21:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 893, loss 3.919344663619995\n",
      "Epoch 5: |          | 894/? [21:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 894, loss 3.454479694366455\n",
      "Epoch 5: |          | 895/? [21:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 895, loss 4.578662395477295\n",
      "Epoch 5: |          | 896/? [21:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 896, loss 4.192296504974365\n",
      "Epoch 5: |          | 897/? [21:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 897, loss 4.180899620056152\n",
      "Epoch 5: |          | 898/? [21:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 898, loss 4.1851420402526855\n",
      "Epoch 5: |          | 899/? [21:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 899, loss 3.914890766143799\n",
      "Epoch 5: |          | 900/? [21:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 900, loss 3.8363471031188965\n",
      "Epoch 5: |          | 901/? [21:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 901, loss 4.28048038482666\n",
      "Epoch 5: |          | 902/? [21:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 902, loss 4.374207973480225\n",
      "Epoch 5: |          | 903/? [21:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 903, loss 3.6660149097442627\n",
      "Epoch 5: |          | 904/? [21:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 904, loss 4.151578903198242\n",
      "Epoch 5: |          | 905/? [21:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 905, loss 4.340790748596191\n",
      "Epoch 5: |          | 906/? [21:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 906, loss 4.106326103210449\n",
      "Epoch 5: |          | 907/? [21:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 907, loss 4.1458516120910645\n",
      "Epoch 5: |          | 908/? [21:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 908, loss 4.225264072418213\n",
      "Epoch 5: |          | 909/? [21:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 909, loss 4.204199314117432\n",
      "Epoch 5: |          | 910/? [21:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 910, loss 3.9907288551330566\n",
      "Epoch 5: |          | 911/? [21:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 911, loss 4.026648998260498\n",
      "Epoch 5: |          | 912/? [21:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 912, loss 3.979342222213745\n",
      "Epoch 5: |          | 913/? [21:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 913, loss 3.9923534393310547\n",
      "Epoch 5: |          | 914/? [21:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 914, loss 4.300553798675537\n",
      "Epoch 5: |          | 915/? [21:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 915, loss 4.12570858001709\n",
      "Epoch 5: |          | 916/? [21:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 916, loss 4.004366397857666\n",
      "Epoch 5: |          | 917/? [21:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 917, loss 3.9730305671691895\n",
      "Epoch 5: |          | 918/? [21:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 918, loss 3.9043407440185547\n",
      "Epoch 5: |          | 919/? [21:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 919, loss 3.9010894298553467\n",
      "Epoch 5: |          | 920/? [21:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 920, loss 4.069773197174072\n",
      "Epoch 5: |          | 921/? [22:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 921, loss 3.8908896446228027\n",
      "Epoch 5: |          | 922/? [22:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 922, loss 4.0712080001831055\n",
      "Epoch 5: |          | 923/? [22:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 923, loss 3.908634901046753\n",
      "Epoch 5: |          | 924/? [22:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 924, loss 3.872194766998291\n",
      "Epoch 5: |          | 925/? [22:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 925, loss 4.197574138641357\n",
      "Epoch 5: |          | 926/? [22:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 926, loss 3.9909510612487793\n",
      "Epoch 5: |          | 927/? [22:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 927, loss 4.271134853363037\n",
      "Epoch 5: |          | 928/? [22:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 928, loss 3.7308051586151123\n",
      "Epoch 5: |          | 929/? [22:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 929, loss 3.8666698932647705\n",
      "Epoch 5: |          | 930/? [22:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 930, loss 3.7560863494873047\n",
      "Epoch 5: |          | 931/? [22:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 931, loss 3.4711997509002686\n",
      "Epoch 5: |          | 932/? [22:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 932, loss 4.127808094024658\n",
      "Epoch 5: |          | 933/? [22:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 933, loss 3.861525297164917\n",
      "Epoch 5: |          | 934/? [22:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 934, loss 4.459993839263916\n",
      "Epoch 5: |          | 935/? [22:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 935, loss 4.798028469085693\n",
      "Epoch 5: |          | 936/? [22:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 936, loss 3.9966437816619873\n",
      "Epoch 5: |          | 937/? [22:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 937, loss 4.022409439086914\n",
      "Epoch 5: |          | 938/? [22:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 938, loss 3.914710283279419\n",
      "Epoch 5: |          | 939/? [22:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 939, loss 4.164449214935303\n",
      "Epoch 5: |          | 940/? [22:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 940, loss 4.376434326171875\n",
      "Epoch 5: |          | 941/? [22:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 941, loss 3.9170265197753906\n",
      "Epoch 5: |          | 942/? [22:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 942, loss 3.397113800048828\n",
      "Epoch 5: |          | 943/? [22:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 943, loss 4.281148433685303\n",
      "Epoch 5: |          | 944/? [22:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 944, loss 3.268578290939331\n",
      "Epoch 5: |          | 945/? [22:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 945, loss 4.0322489738464355\n",
      "Epoch 5: |          | 946/? [22:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 946, loss 3.9658043384552\n",
      "Epoch 5: |          | 947/? [22:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 947, loss 3.902184247970581\n",
      "Epoch 5: |          | 948/? [22:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 948, loss 4.136393070220947\n",
      "Epoch 5: |          | 949/? [22:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 949, loss 3.989128589630127\n",
      "Epoch 5: |          | 950/? [22:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 950, loss 3.7649147510528564\n",
      "Epoch 5: |          | 951/? [22:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 951, loss 4.416139602661133\n",
      "Epoch 5: |          | 952/? [22:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 952, loss 4.391889572143555\n",
      "Epoch 5: |          | 953/? [22:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 953, loss 4.937448501586914\n",
      "Epoch 5: |          | 954/? [22:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 954, loss 3.907081127166748\n",
      "Epoch 5: |          | 955/? [22:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 955, loss 4.541965007781982\n",
      "Epoch 5: |          | 956/? [22:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 956, loss 3.956333875656128\n",
      "Epoch 5: |          | 957/? [22:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 957, loss 4.189490795135498\n",
      "Epoch 5: |          | 958/? [22:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 958, loss 4.282472610473633\n",
      "Epoch 5: |          | 959/? [22:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 959, loss 3.897592067718506\n",
      "Epoch 5: |          | 960/? [22:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 960, loss 4.307065010070801\n",
      "Epoch 5: |          | 961/? [22:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 961, loss 4.540709495544434\n",
      "Epoch 5: |          | 962/? [23:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 962, loss 4.068569183349609\n",
      "Epoch 5: |          | 963/? [23:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 963, loss 3.8403754234313965\n",
      "Epoch 5: |          | 964/? [23:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 964, loss 4.319024085998535\n",
      "Epoch 5: |          | 965/? [23:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 965, loss 3.764759063720703\n",
      "Epoch 5: |          | 966/? [23:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 966, loss 3.6352837085723877\n",
      "Epoch 5: |          | 967/? [23:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 967, loss 3.9105896949768066\n",
      "Epoch 5: |          | 968/? [23:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 968, loss 3.8456692695617676\n",
      "Epoch 5: |          | 969/? [23:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 969, loss 3.6778197288513184\n",
      "Epoch 5: |          | 970/? [23:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 970, loss 4.208209037780762\n",
      "Epoch 5: |          | 971/? [23:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 971, loss 4.449512004852295\n",
      "Epoch 5: |          | 972/? [23:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 972, loss 3.860196352005005\n",
      "Epoch 5: |          | 973/? [23:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 973, loss 4.024941444396973\n",
      "Epoch 5: |          | 974/? [23:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 974, loss 4.022583961486816\n",
      "Epoch 5: |          | 975/? [23:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 975, loss 4.06087589263916\n",
      "Epoch 5: |          | 976/? [23:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 976, loss 4.15155553817749\n",
      "Epoch 5: |          | 977/? [23:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 977, loss 4.754245281219482\n",
      "Epoch 5: |          | 978/? [23:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 978, loss 4.193747043609619\n",
      "Epoch 5: |          | 979/? [23:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 979, loss 4.515091896057129\n",
      "Epoch 5: |          | 980/? [23:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 980, loss 3.6180419921875\n",
      "Epoch 5: |          | 981/? [23:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 981, loss 3.433197498321533\n",
      "Epoch 5: |          | 982/? [23:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 982, loss 4.092688083648682\n",
      "Epoch 5: |          | 983/? [23:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 983, loss 4.521594047546387\n",
      "Epoch 5: |          | 984/? [23:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 984, loss 3.5721702575683594\n",
      "Epoch 5: |          | 985/? [23:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 985, loss 3.815729856491089\n",
      "Epoch 5: |          | 986/? [23:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 986, loss 3.8577492237091064\n",
      "Epoch 5: |          | 987/? [23:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 987, loss 3.4159560203552246\n",
      "Epoch 5: |          | 988/? [23:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 988, loss 4.400221824645996\n",
      "Epoch 5: |          | 989/? [23:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 989, loss 4.038670539855957\n",
      "Epoch 5: |          | 990/? [23:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 990, loss 3.338270664215088\n",
      "Epoch 5: |          | 991/? [23:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 991, loss 4.097421169281006\n",
      "Epoch 5: |          | 992/? [23:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 992, loss 4.79324197769165\n",
      "Epoch 5: |          | 993/? [23:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 993, loss 3.9058146476745605\n",
      "Epoch 5: |          | 994/? [23:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 994, loss 3.9206604957580566\n",
      "Epoch 5: |          | 995/? [23:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 995, loss 4.3058319091796875\n",
      "Epoch 5: |          | 996/? [23:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 996, loss 4.307643413543701\n",
      "Epoch 5: |          | 997/? [23:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 997, loss 3.9172720909118652\n",
      "Epoch 5: |          | 998/? [23:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 998, loss 4.171680927276611\n",
      "Epoch 5: |          | 999/? [23:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 999, loss 4.171684741973877\n",
      "Epoch 5: |          | 1000/? [23:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1000, loss 3.666804790496826\n",
      "Epoch 5: |          | 1001/? [23:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1001, loss 4.356117248535156\n",
      "Epoch 5: |          | 1002/? [23:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1002, loss 4.274745464324951\n",
      "Epoch 5: |          | 1003/? [23:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1003, loss 4.462818622589111\n",
      "Epoch 5: |          | 1004/? [24:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1004, loss 3.4487006664276123\n",
      "Epoch 5: |          | 1005/? [24:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1005, loss 4.00757360458374\n",
      "Epoch 5: |          | 1006/? [24:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1006, loss 4.342259407043457\n",
      "Epoch 5: |          | 1007/? [24:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1007, loss 3.8988349437713623\n",
      "Epoch 5: |          | 1008/? [24:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1008, loss 4.067042350769043\n",
      "Epoch 5: |          | 1009/? [24:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1009, loss 4.375243186950684\n",
      "Epoch 5: |          | 1010/? [24:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1010, loss 3.4519855976104736\n",
      "Epoch 5: |          | 1011/? [24:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1011, loss 4.07206392288208\n",
      "Epoch 5: |          | 1012/? [24:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1012, loss 3.860044479370117\n",
      "Epoch 5: |          | 1013/? [24:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1013, loss 4.025362968444824\n",
      "Epoch 5: |          | 1014/? [24:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1014, loss 4.463468551635742\n",
      "Epoch 5: |          | 1015/? [24:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1015, loss 4.136633396148682\n",
      "Epoch 5: |          | 1016/? [24:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1016, loss 3.870793104171753\n",
      "Epoch 5: |          | 1017/? [24:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1017, loss 3.2854838371276855\n",
      "Epoch 5: |          | 1018/? [24:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1018, loss 3.958888530731201\n",
      "Epoch 5: |          | 1019/? [24:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1019, loss 4.02870512008667\n",
      "Epoch 5: |          | 1020/? [24:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1020, loss 3.6270370483398438\n",
      "Epoch 5: |          | 1021/? [24:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1021, loss 3.8628287315368652\n",
      "Epoch 5: |          | 1022/? [24:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1022, loss 3.6053359508514404\n",
      "Epoch 5: |          | 1023/? [24:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1023, loss 3.3557162284851074\n",
      "Epoch 5: |          | 1024/? [24:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1024, loss 3.8649024963378906\n",
      "Epoch 5: |          | 1025/? [24:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1025, loss 3.7637596130371094\n",
      "Epoch 5: |          | 1026/? [24:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1026, loss 2.868677854537964\n",
      "Epoch 5: |          | 1027/? [24:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1027, loss 4.116925239562988\n",
      "Epoch 5: |          | 1028/? [24:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1028, loss 3.937469959259033\n",
      "Epoch 5: |          | 1029/? [24:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1029, loss 3.818032741546631\n",
      "Epoch 5: |          | 1030/? [24:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1030, loss 3.664323091506958\n",
      "Epoch 5: |          | 1031/? [24:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1031, loss 3.718024730682373\n",
      "Epoch 5: |          | 1032/? [24:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1032, loss 4.158231258392334\n",
      "Epoch 5: |          | 1033/? [24:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1033, loss 4.395058631896973\n",
      "Epoch 5: |          | 1034/? [24:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1034, loss 3.764418363571167\n",
      "Epoch 5: |          | 1035/? [24:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1035, loss 3.785548448562622\n",
      "Epoch 5: |          | 1036/? [24:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1036, loss 3.702657699584961\n",
      "Epoch 5: |          | 1037/? [24:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1037, loss 4.320343971252441\n",
      "Epoch 5: |          | 1038/? [24:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1038, loss 4.502292156219482\n",
      "Epoch 5: |          | 1039/? [24:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1039, loss 4.818282604217529\n",
      "Epoch 5: |          | 1040/? [24:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1040, loss 4.088013172149658\n",
      "Epoch 5: |          | 1041/? [24:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1041, loss 4.405884265899658\n",
      "Epoch 5: |          | 1042/? [24:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1042, loss 4.003715515136719\n",
      "Epoch 5: |          | 1043/? [24:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1043, loss 4.3750901222229\n",
      "Epoch 5: |          | 1044/? [24:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1044, loss 3.94352650642395\n",
      "Epoch 5: |          | 1045/? [24:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1045, loss 3.5099005699157715\n",
      "Epoch 5: |          | 1046/? [25:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1046, loss 3.324345350265503\n",
      "Epoch 5: |          | 1047/? [25:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1047, loss 4.544912815093994\n",
      "Epoch 5: |          | 1048/? [25:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1048, loss 3.936108112335205\n",
      "Epoch 5: |          | 1049/? [25:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1049, loss 4.16321325302124\n",
      "Epoch 5: |          | 1050/? [25:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1050, loss 3.708418607711792\n",
      "Epoch 5: |          | 1051/? [25:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1051, loss 3.682436466217041\n",
      "Epoch 5: |          | 1052/? [25:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1052, loss 4.2901411056518555\n",
      "Epoch 5: |          | 1053/? [25:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1053, loss 4.486047267913818\n",
      "Epoch 5: |          | 1054/? [25:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1054, loss 3.853151321411133\n",
      "Epoch 5: |          | 1055/? [25:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1055, loss 3.5418448448181152\n",
      "Epoch 5: |          | 1056/? [25:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1056, loss 3.5515666007995605\n",
      "Epoch 5: |          | 1057/? [25:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1057, loss 4.205365180969238\n",
      "Epoch 5: |          | 1058/? [25:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1058, loss 3.7512879371643066\n",
      "Epoch 5: |          | 1059/? [25:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1059, loss 4.3585710525512695\n",
      "Epoch 5: |          | 1060/? [25:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1060, loss 4.261595726013184\n",
      "Epoch 5: |          | 1061/? [25:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1061, loss 2.964003324508667\n",
      "Epoch 5: |          | 1062/? [25:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1062, loss 3.8808047771453857\n",
      "Epoch 5: |          | 1063/? [25:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1063, loss 3.9865806102752686\n",
      "Epoch 5: |          | 1064/? [25:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1064, loss 4.182208061218262\n",
      "Epoch 5: |          | 1065/? [25:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1065, loss 2.826066732406616\n",
      "Epoch 5: |          | 1066/? [25:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1066, loss 4.062991142272949\n",
      "Epoch 5: |          | 1067/? [25:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1067, loss 3.595048427581787\n",
      "Epoch 5: |          | 1068/? [25:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1068, loss 3.753786563873291\n",
      "Epoch 5: |          | 1069/? [25:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1069, loss 4.127484321594238\n",
      "Epoch 5: |          | 1070/? [25:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1070, loss 3.9054222106933594\n",
      "Epoch 5: |          | 1071/? [25:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1071, loss 4.3087663650512695\n",
      "Epoch 5: |          | 1072/? [25:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1072, loss 4.334999084472656\n",
      "Epoch 5: |          | 1073/? [25:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1073, loss 4.533097267150879\n",
      "Epoch 5: |          | 1074/? [25:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1074, loss 3.8291690349578857\n",
      "Epoch 5: |          | 1075/? [25:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1075, loss 3.6226913928985596\n",
      "Epoch 5: |          | 1076/? [25:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1076, loss 4.238025665283203\n",
      "Epoch 5: |          | 1077/? [25:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1077, loss 3.7663140296936035\n",
      "Epoch 5: |          | 1078/? [25:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1078, loss 4.034137725830078\n",
      "Epoch 5: |          | 1079/? [25:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1079, loss 4.495809555053711\n",
      "Epoch 5: |          | 1080/? [25:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1080, loss 3.955754518508911\n",
      "Epoch 5: |          | 1081/? [25:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1081, loss 4.265200614929199\n",
      "Epoch 5: |          | 1082/? [25:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1082, loss 3.825585126876831\n",
      "Epoch 5: |          | 1083/? [25:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1083, loss 3.4316978454589844\n",
      "Epoch 5: |          | 1084/? [25:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1084, loss 3.209010601043701\n",
      "Epoch 5: |          | 1085/? [25:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1085, loss 3.8920814990997314\n",
      "Epoch 5: |          | 1086/? [25:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1086, loss 4.195509433746338\n",
      "Epoch 5: |          | 1087/? [25:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1087, loss 4.686776161193848\n",
      "Epoch 5: |          | 1088/? [26:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1088, loss 4.2529191970825195\n",
      "Epoch 5: |          | 1089/? [26:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1089, loss 4.211366176605225\n",
      "Epoch 5: |          | 1090/? [26:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1090, loss 4.039487838745117\n",
      "Epoch 5: |          | 1091/? [26:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1091, loss 3.8040547370910645\n",
      "Epoch 5: |          | 1092/? [26:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1092, loss 4.0825910568237305\n",
      "Epoch 5: |          | 1093/? [26:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1093, loss 3.5474441051483154\n",
      "Epoch 5: |          | 1094/? [26:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1094, loss 4.046192169189453\n",
      "Epoch 5: |          | 1095/? [26:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1095, loss 4.160713195800781\n",
      "Epoch 5: |          | 1096/? [26:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1096, loss 4.363968849182129\n",
      "Epoch 5: |          | 1097/? [26:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1097, loss 3.9284942150115967\n",
      "Epoch 5: |          | 1098/? [26:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1098, loss 3.203005313873291\n",
      "Epoch 5: |          | 1099/? [26:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1099, loss 3.9008171558380127\n",
      "Epoch 5: |          | 1100/? [26:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1100, loss 4.105854511260986\n",
      "Epoch 5: |          | 1101/? [26:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1101, loss 3.7432937622070312\n",
      "Epoch 5: |          | 1102/? [26:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1102, loss 4.4993181228637695\n",
      "Epoch 5: |          | 1103/? [26:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1103, loss 4.989290714263916\n",
      "Epoch 5: |          | 1104/? [26:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1104, loss 4.260401725769043\n",
      "Epoch 5: |          | 1105/? [26:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1105, loss 4.402143955230713\n",
      "Epoch 5: |          | 1106/? [26:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1106, loss 3.9166340827941895\n",
      "Epoch 5: |          | 1107/? [26:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1107, loss 4.017311096191406\n",
      "Epoch 5: |          | 1108/? [26:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1108, loss 3.9968318939208984\n",
      "Epoch 5: |          | 1109/? [26:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1109, loss 3.593376874923706\n",
      "Epoch 5: |          | 1110/? [26:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1110, loss 4.559384346008301\n",
      "Epoch 5: |          | 1111/? [26:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1111, loss 4.241147994995117\n",
      "Epoch 5: |          | 1112/? [26:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1112, loss 4.085171699523926\n",
      "Epoch 5: |          | 1113/? [26:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1113, loss 3.9112517833709717\n",
      "Epoch 5: |          | 1114/? [26:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1114, loss 3.3376197814941406\n",
      "Epoch 5: |          | 1115/? [26:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1115, loss 3.127080202102661\n",
      "Epoch 5: |          | 1116/? [26:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1116, loss 3.5369248390197754\n",
      "Epoch 5: |          | 1117/? [26:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1117, loss 3.685593843460083\n",
      "Epoch 5: |          | 1118/? [26:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1118, loss 3.8429858684539795\n",
      "Epoch 5: |          | 1119/? [26:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1119, loss 4.48293399810791\n",
      "Epoch 5: |          | 1120/? [26:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1120, loss 3.989868640899658\n",
      "Epoch 5: |          | 1121/? [26:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1121, loss 4.267322540283203\n",
      "Epoch 5: |          | 1122/? [26:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1122, loss 3.810971736907959\n",
      "Epoch 5: |          | 1123/? [26:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1123, loss 4.0703349113464355\n",
      "Epoch 5: |          | 1124/? [26:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1124, loss 4.4060869216918945\n",
      "Epoch 5: |          | 1125/? [26:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1125, loss 3.6887142658233643\n",
      "Epoch 5: |          | 1126/? [26:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1126, loss 3.5859618186950684\n",
      "Epoch 5: |          | 1127/? [26:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1127, loss 3.900960922241211\n",
      "Epoch 5: |          | 1128/? [26:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1128, loss 3.984502077102661\n",
      "Epoch 5: |          | 1129/? [26:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1129, loss 4.100436210632324\n",
      "Epoch 5: |          | 1130/? [27:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1130, loss 4.263605117797852\n",
      "Epoch 5: |          | 1131/? [27:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1131, loss 4.360991477966309\n",
      "Epoch 5: |          | 1132/? [27:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1132, loss 3.112109899520874\n",
      "Epoch 5: |          | 1133/? [27:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1133, loss 3.9752249717712402\n",
      "Epoch 5: |          | 1134/? [27:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1134, loss 3.7641239166259766\n",
      "Epoch 5: |          | 1135/? [27:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1135, loss 4.395643711090088\n",
      "Epoch 5: |          | 1136/? [27:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1136, loss 4.033148765563965\n",
      "Epoch 5: |          | 1137/? [27:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1137, loss 4.03105354309082\n",
      "Epoch 5: |          | 1138/? [27:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1138, loss 4.57509708404541\n",
      "Epoch 5: |          | 1139/? [27:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1139, loss 4.303713321685791\n",
      "Epoch 5: |          | 1140/? [27:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1140, loss 3.9280972480773926\n",
      "Epoch 5: |          | 1141/? [27:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1141, loss 4.369647979736328\n",
      "Epoch 5: |          | 1142/? [27:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1142, loss 4.453217029571533\n",
      "Epoch 5: |          | 1143/? [27:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1143, loss 4.471100807189941\n",
      "Epoch 5: |          | 1144/? [27:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1144, loss 3.9012062549591064\n",
      "Epoch 5: |          | 1145/? [27:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1145, loss 4.016732215881348\n",
      "Epoch 5: |          | 1146/? [27:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1146, loss 3.6678504943847656\n",
      "Epoch 5: |          | 1147/? [27:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1147, loss 3.5818417072296143\n",
      "Epoch 5: |          | 1148/? [27:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1148, loss 3.7716517448425293\n",
      "Epoch 5: |          | 1149/? [27:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1149, loss 4.8705830574035645\n",
      "Epoch 5: |          | 1150/? [27:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1150, loss 4.2066731452941895\n",
      "Epoch 5: |          | 1151/? [27:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1151, loss 4.5199384689331055\n",
      "Epoch 5: |          | 1152/? [27:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1152, loss 3.701725482940674\n",
      "Epoch 5: |          | 1153/? [27:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1153, loss 4.117180347442627\n",
      "Epoch 5: |          | 1154/? [27:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1154, loss 3.786403179168701\n",
      "Epoch 5: |          | 1155/? [27:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1155, loss 3.981053113937378\n",
      "Epoch 5: |          | 1156/? [27:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1156, loss 3.9743239879608154\n",
      "Epoch 5: |          | 1157/? [27:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1157, loss 4.236780643463135\n",
      "Epoch 5: |          | 1158/? [27:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1158, loss 4.470003604888916\n",
      "Epoch 5: |          | 1159/? [27:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1159, loss 3.22428560256958\n",
      "Epoch 5: |          | 1160/? [27:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1160, loss 4.432389259338379\n",
      "Epoch 5: |          | 1161/? [27:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1161, loss 4.2756876945495605\n",
      "Epoch 5: |          | 1162/? [27:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1162, loss 4.183793067932129\n",
      "Epoch 5: |          | 1163/? [27:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1163, loss 4.761045455932617\n",
      "Epoch 5: |          | 1164/? [27:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1164, loss 4.553521156311035\n",
      "Epoch 5: |          | 1165/? [27:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1165, loss 3.693925380706787\n",
      "Epoch 5: |          | 1166/? [27:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1166, loss 4.236566543579102\n",
      "Epoch 5: |          | 1167/? [27:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1167, loss 4.2704668045043945\n",
      "Epoch 5: |          | 1168/? [27:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1168, loss 4.71710205078125\n",
      "Epoch 5: |          | 1169/? [27:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1169, loss 3.7417895793914795\n",
      "Epoch 5: |          | 1170/? [27:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1170, loss 4.323556900024414\n",
      "Epoch 5: |          | 1171/? [27:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1171, loss 3.688857316970825\n",
      "Epoch 5: |          | 1172/? [28:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1172, loss 3.6088099479675293\n",
      "Epoch 5: |          | 1173/? [28:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1173, loss 4.203200340270996\n",
      "Epoch 5: |          | 1174/? [28:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1174, loss 3.6294236183166504\n",
      "Epoch 5: |          | 1175/? [28:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1175, loss 4.294304847717285\n",
      "Epoch 5: |          | 1176/? [28:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1176, loss 4.324891090393066\n",
      "Epoch 5: |          | 1177/? [28:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1177, loss 4.491608142852783\n",
      "Epoch 5: |          | 1178/? [28:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1178, loss 3.8933563232421875\n",
      "Epoch 5: |          | 1179/? [28:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1179, loss 4.419562339782715\n",
      "Epoch 5: |          | 1180/? [28:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1180, loss 4.215765953063965\n",
      "Epoch 5: |          | 1181/? [28:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1181, loss 4.182552814483643\n",
      "Epoch 5: |          | 1182/? [28:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1182, loss 3.9971721172332764\n",
      "Epoch 5: |          | 1183/? [28:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1183, loss 3.710191249847412\n",
      "Epoch 5: |          | 1184/? [28:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1184, loss 4.080399513244629\n",
      "Epoch 5: |          | 1185/? [28:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1185, loss 3.889557361602783\n",
      "Epoch 5: |          | 1186/? [28:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1186, loss 4.0947465896606445\n",
      "Epoch 5: |          | 1187/? [28:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1187, loss 3.9840075969696045\n",
      "Epoch 5: |          | 1188/? [28:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1188, loss 4.332334995269775\n",
      "Epoch 5: |          | 1189/? [28:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1189, loss 4.433686256408691\n",
      "Epoch 5: |          | 1190/? [28:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1190, loss 3.9511818885803223\n",
      "Epoch 5: |          | 1191/? [28:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1191, loss 4.021657943725586\n",
      "Epoch 5: |          | 1192/? [28:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1192, loss 4.335214138031006\n",
      "Epoch 5: |          | 1193/? [28:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1193, loss 3.8399550914764404\n",
      "Epoch 5: |          | 1194/? [28:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1194, loss 3.4931883811950684\n",
      "Epoch 5: |          | 1195/? [28:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1195, loss 4.068984031677246\n",
      "Epoch 5: |          | 1196/? [28:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1196, loss 4.256394386291504\n",
      "Epoch 5: |          | 1197/? [28:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1197, loss 4.051138877868652\n",
      "Epoch 5: |          | 1198/? [28:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1198, loss 4.151952266693115\n",
      "Epoch 5: |          | 1199/? [28:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1199, loss 4.401984214782715\n",
      "Epoch 5: |          | 1200/? [28:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1200, loss 3.6253199577331543\n",
      "Epoch 5: |          | 1201/? [28:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1201, loss 4.230801582336426\n",
      "Epoch 5: |          | 1202/? [28:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1202, loss 3.8989856243133545\n",
      "Epoch 5: |          | 1203/? [28:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1203, loss 3.8923354148864746\n",
      "Epoch 5: |          | 1204/? [28:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1204, loss 3.3906986713409424\n",
      "Epoch 5: |          | 1205/? [28:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1205, loss 3.990630626678467\n",
      "Epoch 5: |          | 1206/? [28:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1206, loss 3.9992244243621826\n",
      "Epoch 5: |          | 1207/? [28:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1207, loss 4.358708381652832\n",
      "Epoch 5: |          | 1208/? [28:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1208, loss 4.5206427574157715\n",
      "Epoch 5: |          | 1209/? [28:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1209, loss 4.0707292556762695\n",
      "Epoch 5: |          | 1210/? [28:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1210, loss 4.36545467376709\n",
      "Epoch 5: |          | 1211/? [28:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1211, loss 4.350052833557129\n",
      "Epoch 5: |          | 1212/? [28:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1212, loss 4.153970718383789\n",
      "Epoch 5: |          | 1213/? [29:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1213, loss 3.852332353591919\n",
      "Epoch 5: |          | 1214/? [29:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1214, loss 4.470071315765381\n",
      "Epoch 5: |          | 1215/? [29:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1215, loss 3.883800983428955\n",
      "Epoch 5: |          | 1216/? [29:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1216, loss 4.0457611083984375\n",
      "Epoch 5: |          | 1217/? [29:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1217, loss 4.188126564025879\n",
      "Epoch 5: |          | 1218/? [29:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1218, loss 4.209921836853027\n",
      "Epoch 5: |          | 1219/? [29:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1219, loss 3.8682265281677246\n",
      "Epoch 5: |          | 1220/? [29:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1220, loss 4.572171211242676\n",
      "Epoch 5: |          | 1221/? [29:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1221, loss 4.1351776123046875\n",
      "Epoch 5: |          | 1222/? [29:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1222, loss 3.241441011428833\n",
      "Epoch 5: |          | 1223/? [29:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1223, loss 3.4029269218444824\n",
      "Epoch 5: |          | 1224/? [29:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1224, loss 3.7694175243377686\n",
      "Epoch 5: |          | 1225/? [29:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1225, loss 4.433138370513916\n",
      "Epoch 5: |          | 1226/? [29:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1226, loss 4.420714378356934\n",
      "Epoch 5: |          | 1227/? [29:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1227, loss 4.044665336608887\n",
      "Epoch 5: |          | 1228/? [29:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1228, loss 3.9306559562683105\n",
      "Epoch 5: |          | 1229/? [29:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1229, loss 3.515341281890869\n",
      "Epoch 5: |          | 1230/? [29:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1230, loss 4.20731258392334\n",
      "Epoch 5: |          | 1231/? [29:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1231, loss 4.257180213928223\n",
      "Epoch 5: |          | 1232/? [29:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1232, loss 4.411909103393555\n",
      "Epoch 5: |          | 1233/? [29:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1233, loss 4.19031286239624\n",
      "Epoch 5: |          | 1234/? [29:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1234, loss 3.214899778366089\n",
      "Epoch 5: |          | 1235/? [29:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1235, loss 4.302597999572754\n",
      "Epoch 5: |          | 1236/? [29:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1236, loss 3.672405958175659\n",
      "Epoch 5: |          | 1237/? [29:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1237, loss 3.980677843093872\n",
      "Epoch 5: |          | 1238/? [29:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1238, loss 3.9983623027801514\n",
      "Epoch 5: |          | 1239/? [29:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1239, loss 3.8607017993927\n",
      "Epoch 5: |          | 1240/? [29:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1240, loss 4.513593673706055\n",
      "Epoch 5: |          | 1241/? [29:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1241, loss 4.049395561218262\n",
      "Epoch 5: |          | 1242/? [29:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1242, loss 3.8744006156921387\n",
      "Epoch 5: |          | 1243/? [29:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1243, loss 3.767381191253662\n",
      "Epoch 5: |          | 1244/? [29:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1244, loss 3.926647901535034\n",
      "Epoch 5: |          | 1245/? [29:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1245, loss 3.511979579925537\n",
      "Epoch 5: |          | 1246/? [29:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1246, loss 4.2142486572265625\n",
      "Epoch 5: |          | 1247/? [29:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1247, loss 4.210771083831787\n",
      "Epoch 5: |          | 1248/? [29:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1248, loss 3.7566235065460205\n",
      "Epoch 5: |          | 1249/? [29:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1249, loss 3.920706272125244\n",
      "Epoch 5: |          | 1250/? [29:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1250, loss 4.081101417541504\n",
      "Epoch 5: |          | 1251/? [29:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1251, loss 3.7858924865722656\n",
      "Epoch 5: |          | 1252/? [29:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1252, loss 4.582005977630615\n",
      "Epoch 5: |          | 1253/? [29:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1253, loss 3.9691338539123535\n",
      "Epoch 5: |          | 1254/? [29:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1254, loss 3.268251895904541\n",
      "Epoch 5: |          | 1255/? [30:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1255, loss 4.6760945320129395\n",
      "Epoch 5: |          | 1256/? [30:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1256, loss 3.649437427520752\n",
      "Epoch 5: |          | 1257/? [30:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1257, loss 3.70243763923645\n",
      "Epoch 5: |          | 1258/? [30:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1258, loss 4.405882835388184\n",
      "Epoch 5: |          | 1259/? [30:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1259, loss 4.073734760284424\n",
      "Epoch 5: |          | 1260/? [30:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1260, loss 4.527741432189941\n",
      "Epoch 5: |          | 1261/? [30:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1261, loss 3.8635716438293457\n",
      "Epoch 5: |          | 1262/? [30:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1262, loss 3.8255774974823\n",
      "Epoch 5: |          | 1263/? [30:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1263, loss 4.201234817504883\n",
      "Epoch 5: |          | 1264/? [30:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1264, loss 4.483022212982178\n",
      "Epoch 5: |          | 1265/? [30:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1265, loss 4.3009748458862305\n",
      "Epoch 5: |          | 1266/? [30:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1266, loss 3.9626681804656982\n",
      "Epoch 5: |          | 1267/? [30:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1267, loss 4.058365821838379\n",
      "Epoch 5: |          | 1268/? [30:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1268, loss 3.943519115447998\n",
      "Epoch 5: |          | 1269/? [30:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1269, loss 3.502236843109131\n",
      "Epoch 5: |          | 1270/? [30:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1270, loss 3.8307785987854004\n",
      "Epoch 5: |          | 1271/? [30:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1271, loss 4.001860618591309\n",
      "Epoch 5: |          | 1272/? [30:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1272, loss 3.590935468673706\n",
      "Epoch 5: |          | 1273/? [30:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1273, loss 4.286788463592529\n",
      "Epoch 5: |          | 1274/? [30:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1274, loss 3.175022840499878\n",
      "Epoch 5: |          | 1275/? [30:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1275, loss 3.71274995803833\n",
      "Epoch 5: |          | 1276/? [30:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1276, loss 4.049158573150635\n",
      "Epoch 5: |          | 1277/? [30:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1277, loss 3.7408759593963623\n",
      "Epoch 5: |          | 1278/? [30:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1278, loss 3.437732696533203\n",
      "Epoch 5: |          | 1279/? [30:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1279, loss 4.157095432281494\n",
      "Epoch 5: |          | 1280/? [30:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1280, loss 3.333247661590576\n",
      "Epoch 5: |          | 1281/? [30:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1281, loss 3.844611644744873\n",
      "Epoch 5: |          | 1282/? [30:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1282, loss 3.521378993988037\n",
      "Epoch 5: |          | 1283/? [30:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1283, loss 4.30670166015625\n",
      "Epoch 5: |          | 1284/? [30:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1284, loss 3.391770601272583\n",
      "Epoch 5: |          | 1285/? [30:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1285, loss 4.506341457366943\n",
      "Epoch 5: |          | 1286/? [30:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1286, loss 2.9369678497314453\n",
      "Epoch 5: |          | 1287/? [30:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1287, loss 4.380311965942383\n",
      "Epoch 5: |          | 1288/? [30:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1288, loss 4.1948137283325195\n",
      "Epoch 5: |          | 1289/? [30:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1289, loss 3.2769980430603027\n",
      "Epoch 5: |          | 1290/? [30:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1290, loss 4.096834182739258\n",
      "Epoch 5: |          | 1291/? [30:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1291, loss 4.950536727905273\n",
      "Epoch 5: |          | 1292/? [30:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1292, loss 4.278885841369629\n",
      "Epoch 5: |          | 1293/? [30:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1293, loss 3.7715694904327393\n",
      "Epoch 5: |          | 1294/? [30:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1294, loss 4.029669761657715\n",
      "Epoch 5: |          | 1295/? [30:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1295, loss 4.08434534072876\n",
      "Epoch 5: |          | 1296/? [31:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1296, loss 3.3627514839172363\n",
      "Epoch 5: |          | 1297/? [31:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1297, loss 4.278100967407227\n",
      "Epoch 5: |          | 1298/? [31:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1298, loss 4.005672931671143\n",
      "Epoch 5: |          | 1299/? [31:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1299, loss 3.130183219909668\n",
      "Epoch 5: |          | 1300/? [31:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1300, loss 4.014740467071533\n",
      "Epoch 5: |          | 1301/? [31:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1301, loss 3.7438480854034424\n",
      "Epoch 5: |          | 1302/? [31:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1302, loss 3.8725502490997314\n",
      "Epoch 5: |          | 1303/? [31:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1303, loss 3.8058300018310547\n",
      "Epoch 5: |          | 1304/? [31:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1304, loss 4.5720014572143555\n",
      "Epoch 5: |          | 1305/? [31:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1305, loss 3.376387119293213\n",
      "Epoch 5: |          | 1306/? [31:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1306, loss 3.98207426071167\n",
      "Epoch 5: |          | 1307/? [31:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1307, loss 3.5981154441833496\n",
      "Epoch 5: |          | 1308/? [31:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1308, loss 3.527653455734253\n",
      "Epoch 5: |          | 1309/? [31:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1309, loss 3.6353397369384766\n",
      "Epoch 5: |          | 1310/? [31:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1310, loss 4.159106254577637\n",
      "Epoch 5: |          | 1311/? [31:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1311, loss 3.605950117111206\n",
      "Epoch 5: |          | 1312/? [31:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1312, loss 3.342141628265381\n",
      "Epoch 5: |          | 1313/? [31:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1313, loss 4.51113748550415\n",
      "Epoch 5: |          | 1314/? [31:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1314, loss 3.7003331184387207\n",
      "Epoch 5: |          | 1315/? [31:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1315, loss 4.484004974365234\n",
      "Epoch 5: |          | 1316/? [31:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1316, loss 4.255319595336914\n",
      "Epoch 5: |          | 1317/? [31:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1317, loss 3.8596718311309814\n",
      "Epoch 5: |          | 1318/? [31:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1318, loss 4.077576637268066\n",
      "Epoch 5: |          | 1319/? [31:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1319, loss 4.2497878074646\n",
      "Epoch 5: |          | 1320/? [31:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1320, loss 3.8460536003112793\n",
      "Epoch 5: |          | 1321/? [31:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1321, loss 4.269238471984863\n",
      "Epoch 5: |          | 1322/? [31:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1322, loss 4.13802433013916\n",
      "Epoch 5: |          | 1323/? [31:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1323, loss 3.595557451248169\n",
      "Epoch 5: |          | 1324/? [31:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1324, loss 4.567385673522949\n",
      "Epoch 5: |          | 1325/? [31:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1325, loss 4.723175048828125\n",
      "Epoch 5: |          | 1326/? [31:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1326, loss 4.127396106719971\n",
      "Epoch 5: |          | 1327/? [31:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1327, loss 3.9929728507995605\n",
      "Epoch 5: |          | 1328/? [31:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1328, loss 3.6830780506134033\n",
      "Epoch 5: |          | 1329/? [31:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1329, loss 4.28390645980835\n",
      "Epoch 5: |          | 1330/? [31:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1330, loss 4.0263166427612305\n",
      "Epoch 5: |          | 1331/? [31:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1331, loss 4.113806247711182\n",
      "Epoch 5: |          | 1332/? [31:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1332, loss 3.847686767578125\n",
      "Epoch 5: |          | 1333/? [31:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1333, loss 3.819157361984253\n",
      "Epoch 5: |          | 1334/? [31:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1334, loss 3.954868793487549\n",
      "Epoch 5: |          | 1335/? [31:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1335, loss 3.8647780418395996\n",
      "Epoch 5: |          | 1336/? [31:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1336, loss 3.4891581535339355\n",
      "Epoch 5: |          | 1337/? [31:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1337, loss 4.123307228088379\n",
      "Epoch 5: |          | 1338/? [32:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1338, loss 3.333679676055908\n",
      "Epoch 5: |          | 1339/? [32:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1339, loss 4.030117988586426\n",
      "Epoch 5: |          | 1340/? [32:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1340, loss 3.4138286113739014\n",
      "Epoch 5: |          | 1341/? [32:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1341, loss 4.134586334228516\n",
      "Epoch 5: |          | 1342/? [32:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1342, loss 4.399936676025391\n",
      "Epoch 5: |          | 1343/? [32:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1343, loss 3.8935084342956543\n",
      "Epoch 5: |          | 1344/? [32:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1344, loss 3.9979209899902344\n",
      "Epoch 5: |          | 1345/? [32:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1345, loss 4.123035430908203\n",
      "Epoch 5: |          | 1346/? [32:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1346, loss 5.299899101257324\n",
      "Epoch 5: |          | 1347/? [32:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1347, loss 4.194300174713135\n",
      "Epoch 5: |          | 1348/? [32:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1348, loss 4.3725690841674805\n",
      "Epoch 5: |          | 1349/? [32:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1349, loss 4.187431812286377\n",
      "Epoch 5: |          | 1350/? [32:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1350, loss 4.368987083435059\n",
      "Epoch 5: |          | 1351/? [32:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1351, loss 4.239734172821045\n",
      "Epoch 5: |          | 1352/? [32:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1352, loss 3.4376258850097656\n",
      "Epoch 5: |          | 1353/? [32:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1353, loss 3.790463924407959\n",
      "Epoch 5: |          | 1354/? [32:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1354, loss 4.211182594299316\n",
      "Epoch 5: |          | 1355/? [32:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1355, loss 4.375212669372559\n",
      "Epoch 5: |          | 1356/? [32:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1356, loss 4.075215816497803\n",
      "Epoch 5: |          | 1357/? [32:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1357, loss 3.8440284729003906\n",
      "Epoch 5: |          | 1358/? [32:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1358, loss 4.022038459777832\n",
      "Epoch 5: |          | 1359/? [32:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1359, loss 3.9054629802703857\n",
      "Epoch 5: |          | 1360/? [32:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1360, loss 4.093399524688721\n",
      "Epoch 5: |          | 1361/? [32:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1361, loss 4.050408840179443\n",
      "Epoch 5: |          | 1362/? [32:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1362, loss 3.875011920928955\n",
      "Epoch 5: |          | 1363/? [32:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1363, loss 3.3222756385803223\n",
      "Epoch 5: |          | 1364/? [32:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1364, loss 3.8205795288085938\n",
      "Epoch 5: |          | 1365/? [32:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1365, loss 3.5155341625213623\n",
      "Epoch 5: |          | 1366/? [32:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1366, loss 4.258494853973389\n",
      "Epoch 5: |          | 1367/? [32:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1367, loss 3.547227144241333\n",
      "Epoch 5: |          | 1368/? [32:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1368, loss 3.3439090251922607\n",
      "Epoch 5: |          | 1369/? [32:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1369, loss 4.057163238525391\n",
      "Epoch 5: |          | 1370/? [32:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1370, loss 3.5604300498962402\n",
      "Epoch 5: |          | 1371/? [32:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1371, loss 4.498173713684082\n",
      "Epoch 5: |          | 1372/? [32:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1372, loss 3.9172911643981934\n",
      "Epoch 5: |          | 1373/? [32:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1373, loss 4.35905122756958\n",
      "Epoch 5: |          | 1374/? [32:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1374, loss 3.4236671924591064\n",
      "Epoch 5: |          | 1375/? [32:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1375, loss 4.0699005126953125\n",
      "Epoch 5: |          | 1376/? [32:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1376, loss 3.9362823963165283\n",
      "Epoch 5: |          | 1377/? [33:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1377, loss 3.9320273399353027\n",
      "Epoch 5: |          | 1378/? [33:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1378, loss 3.930393695831299\n",
      "Epoch 5: |          | 1379/? [33:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1379, loss 3.9653587341308594\n",
      "Epoch 5: |          | 1380/? [33:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1380, loss 4.05881404876709\n",
      "Epoch 5: |          | 1381/? [33:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1381, loss 4.19290018081665\n",
      "Epoch 5: |          | 1382/? [33:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1382, loss 3.754223346710205\n",
      "Epoch 5: |          | 1383/? [33:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1383, loss 3.9829788208007812\n",
      "Epoch 5: |          | 1384/? [33:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1384, loss 3.992326259613037\n",
      "Epoch 5: |          | 1385/? [33:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1385, loss 3.941105604171753\n",
      "Epoch 5: |          | 1386/? [33:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1386, loss 4.001490592956543\n",
      "Epoch 5: |          | 1387/? [33:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1387, loss 3.9398181438446045\n",
      "Epoch 5: |          | 1388/? [33:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1388, loss 3.440075635910034\n",
      "Epoch 5: |          | 1389/? [33:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1389, loss 4.181912422180176\n",
      "Epoch 5: |          | 1390/? [33:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1390, loss 4.512975215911865\n",
      "Epoch 5: |          | 1391/? [33:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1391, loss 4.110717296600342\n",
      "Epoch 5: |          | 1392/? [33:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1392, loss 3.608142137527466\n",
      "Epoch 5: |          | 1393/? [33:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1393, loss 3.7655367851257324\n",
      "Epoch 5: |          | 1394/? [33:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1394, loss 3.430035352706909\n",
      "Epoch 5: |          | 1395/? [33:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1395, loss 4.10939884185791\n",
      "Epoch 5: |          | 1396/? [33:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1396, loss 4.027307033538818\n",
      "Epoch 5: |          | 1397/? [33:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1397, loss 3.2251319885253906\n",
      "Epoch 5: |          | 1398/? [33:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1398, loss 4.4426445960998535\n",
      "Epoch 5: |          | 1399/? [33:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1399, loss 4.4835524559021\n",
      "Epoch 5: |          | 1400/? [33:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1400, loss 3.5444228649139404\n",
      "Epoch 5: |          | 1401/? [33:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1401, loss 4.370747089385986\n",
      "Epoch 5: |          | 1402/? [33:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1402, loss 3.935487747192383\n",
      "Epoch 5: |          | 1403/? [33:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1403, loss 4.159023284912109\n",
      "Epoch 5: |          | 1404/? [33:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1404, loss 4.135922431945801\n",
      "Epoch 5: |          | 1405/? [33:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1405, loss 4.460641860961914\n",
      "Epoch 5: |          | 1406/? [33:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1406, loss 4.378814697265625\n",
      "Epoch 5: |          | 1407/? [33:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1407, loss 4.471958637237549\n",
      "Epoch 5: |          | 1408/? [33:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1408, loss 3.6572177410125732\n",
      "Epoch 5: |          | 1409/? [33:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1409, loss 3.7474873065948486\n",
      "Epoch 5: |          | 1410/? [33:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1410, loss 3.7846016883850098\n",
      "Epoch 5: |          | 1411/? [33:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1411, loss 4.090043067932129\n",
      "Epoch 5: |          | 1412/? [33:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1412, loss 3.7430005073547363\n",
      "Epoch 5: |          | 1413/? [33:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1413, loss 3.639813184738159\n",
      "Epoch 5: |          | 1414/? [33:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1414, loss 3.7635436058044434\n",
      "Epoch 5: |          | 1415/? [33:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1415, loss 4.043557167053223\n",
      "Epoch 5: |          | 1416/? [33:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1416, loss 4.422606468200684\n",
      "Epoch 5: |          | 1417/? [33:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1417, loss 4.039181232452393\n",
      "Epoch 5: |          | 1418/? [33:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1418, loss 4.212284564971924\n",
      "Epoch 5: |          | 1419/? [34:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1419, loss 3.9839744567871094\n",
      "Epoch 5: |          | 1420/? [34:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1420, loss 3.8105902671813965\n",
      "Epoch 5: |          | 1421/? [34:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1421, loss 3.5468990802764893\n",
      "Epoch 5: |          | 1422/? [34:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1422, loss 4.249536037445068\n",
      "Epoch 5: |          | 1423/? [34:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1423, loss 4.297243595123291\n",
      "Epoch 5: |          | 1424/? [34:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1424, loss 3.9598193168640137\n",
      "Epoch 5: |          | 1425/? [34:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1425, loss 4.176688194274902\n",
      "Epoch 5: |          | 1426/? [34:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1426, loss 3.688152313232422\n",
      "Epoch 5: |          | 1427/? [34:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1427, loss 4.324996471405029\n",
      "Epoch 5: |          | 1428/? [34:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1428, loss 4.297951698303223\n",
      "Epoch 5: |          | 1429/? [34:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1429, loss 4.202892303466797\n",
      "Epoch 5: |          | 1430/? [34:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1430, loss 4.261269569396973\n",
      "Epoch 5: |          | 1431/? [34:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1431, loss 4.007029056549072\n",
      "Epoch 5: |          | 1432/? [34:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1432, loss 4.058099269866943\n",
      "Epoch 5: |          | 1433/? [34:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1433, loss 3.944272994995117\n",
      "Epoch 5: |          | 1434/? [34:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1434, loss 4.1226091384887695\n",
      "Epoch 5: |          | 1435/? [34:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1435, loss 3.6975650787353516\n",
      "Epoch 5: |          | 1436/? [34:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1436, loss 4.060861110687256\n",
      "Epoch 5: |          | 1437/? [34:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1437, loss 3.307706356048584\n",
      "Epoch 5: |          | 1438/? [34:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1438, loss 4.9861650466918945\n",
      "Epoch 5: |          | 1439/? [34:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1439, loss 4.099750995635986\n",
      "Epoch 5: |          | 1440/? [34:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1440, loss 4.225401401519775\n",
      "Epoch 5: |          | 1441/? [34:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1441, loss 4.572938442230225\n",
      "Epoch 5: |          | 1442/? [34:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1442, loss 4.58632755279541\n",
      "Epoch 5: |          | 1443/? [34:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1443, loss 3.7027645111083984\n",
      "Epoch 5: |          | 1444/? [34:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1444, loss 3.751190662384033\n",
      "Epoch 5: |          | 1445/? [34:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1445, loss 4.25272274017334\n",
      "Epoch 5: |          | 1446/? [34:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1446, loss 3.8310794830322266\n",
      "Epoch 5: |          | 1447/? [34:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1447, loss 3.9768295288085938\n",
      "Epoch 5: |          | 1448/? [34:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1448, loss 3.7901196479797363\n",
      "Epoch 5: |          | 1449/? [34:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1449, loss 4.052339553833008\n",
      "Epoch 5: |          | 1450/? [34:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1450, loss 4.075934410095215\n",
      "Epoch 5: |          | 1451/? [34:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1451, loss 4.422201633453369\n",
      "Epoch 5: |          | 1452/? [34:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1452, loss 4.076338768005371\n",
      "Epoch 5: |          | 1453/? [34:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1453, loss 3.402409315109253\n",
      "Epoch 5: |          | 1454/? [34:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1454, loss 4.014049530029297\n",
      "Epoch 5: |          | 1455/? [34:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1455, loss 4.108410835266113\n",
      "Epoch 5: |          | 1456/? [34:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1456, loss 3.677669048309326\n",
      "Epoch 5: |          | 1457/? [34:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1457, loss 3.8533496856689453\n",
      "Epoch 5: |          | 1458/? [34:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1458, loss 4.056788444519043\n",
      "Epoch 5: |          | 1459/? [34:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1459, loss 4.21532678604126\n",
      "Epoch 5: |          | 1460/? [34:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1460, loss 4.124167442321777\n",
      "Epoch 5: |          | 1461/? [35:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1461, loss 4.068338394165039\n",
      "Epoch 5: |          | 1462/? [35:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1462, loss 4.3718461990356445\n",
      "Epoch 5: |          | 1463/? [35:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1463, loss 4.308787822723389\n",
      "Epoch 5: |          | 1464/? [35:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1464, loss 3.5918571949005127\n",
      "Epoch 5: |          | 1465/? [35:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1465, loss 3.978476047515869\n",
      "Epoch 5: |          | 1466/? [35:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1466, loss 3.6358211040496826\n",
      "Epoch 5: |          | 1467/? [35:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1467, loss 4.305057048797607\n",
      "Epoch 5: |          | 1468/? [35:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1468, loss 3.8766162395477295\n",
      "Epoch 5: |          | 1469/? [35:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1469, loss 3.5945873260498047\n",
      "Epoch 5: |          | 1470/? [35:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1470, loss 4.152795791625977\n",
      "Epoch 5: |          | 1471/? [35:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1471, loss 4.4070916175842285\n",
      "Epoch 5: |          | 1472/? [35:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1472, loss 4.115725994110107\n",
      "Epoch 5: |          | 1473/? [35:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1473, loss 3.925377607345581\n",
      "Epoch 5: |          | 1474/? [35:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1474, loss 3.839785099029541\n",
      "Epoch 5: |          | 1475/? [35:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1475, loss 3.450345516204834\n",
      "Epoch 5: |          | 1476/? [35:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1476, loss 4.054824352264404\n",
      "Epoch 5: |          | 1477/? [35:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1477, loss 4.052058219909668\n",
      "Epoch 5: |          | 1478/? [35:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1478, loss 3.929292678833008\n",
      "Epoch 5: |          | 1479/? [35:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1479, loss 4.478219509124756\n",
      "Epoch 5: |          | 1480/? [35:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1480, loss 4.182843208312988\n",
      "Epoch 5: |          | 1481/? [35:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1481, loss 3.9458889961242676\n",
      "Epoch 5: |          | 1482/? [35:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1482, loss 4.075249671936035\n",
      "Epoch 5: |          | 1483/? [35:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1483, loss 3.7274105548858643\n",
      "Epoch 5: |          | 1484/? [35:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1484, loss 3.9295973777770996\n",
      "Epoch 5: |          | 1485/? [35:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1485, loss 4.069615364074707\n",
      "Epoch 5: |          | 1486/? [35:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1486, loss 4.0516862869262695\n",
      "Epoch 5: |          | 1487/? [35:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1487, loss 3.514735460281372\n",
      "Epoch 5: |          | 1488/? [35:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1488, loss 4.178735733032227\n",
      "Epoch 5: |          | 1489/? [35:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1489, loss 4.127617359161377\n",
      "Epoch 5: |          | 1490/? [35:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1490, loss 4.051238059997559\n",
      "Epoch 5: |          | 1491/? [35:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1491, loss 2.9879870414733887\n",
      "Epoch 5: |          | 1492/? [35:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1492, loss 3.553804874420166\n",
      "Epoch 5: |          | 1493/? [35:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1493, loss 3.1739115715026855\n",
      "Epoch 5: |          | 1494/? [35:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1494, loss 3.9334514141082764\n",
      "Epoch 5: |          | 1495/? [35:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1495, loss 3.8201096057891846\n",
      "Epoch 5: |          | 1496/? [35:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1496, loss 4.17289924621582\n",
      "Epoch 5: |          | 1497/? [35:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1497, loss 3.416508436203003\n",
      "Epoch 5: |          | 1498/? [35:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1498, loss 3.6994521617889404\n",
      "Epoch 5: |          | 1499/? [35:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1499, loss 4.264443397521973\n",
      "Epoch 5: |          | 1500/? [35:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1500, loss 4.209290981292725\n",
      "Epoch 5: |          | 1501/? [35:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1501, loss 4.03658390045166\n",
      "Epoch 5: |          | 1502/? [36:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1502, loss 4.118005275726318\n",
      "Epoch 5: |          | 1503/? [36:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1503, loss 3.861830234527588\n",
      "Epoch 5: |          | 1504/? [36:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1504, loss 4.501482963562012\n",
      "Epoch 5: |          | 1505/? [36:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1505, loss 4.33606481552124\n",
      "Epoch 5: |          | 1506/? [36:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1506, loss 3.9459128379821777\n",
      "Epoch 5: |          | 1507/? [36:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1507, loss 3.7834270000457764\n",
      "Epoch 5: |          | 1508/? [36:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1508, loss 3.9441540241241455\n",
      "Epoch 5: |          | 1509/? [36:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1509, loss 3.946537733078003\n",
      "Epoch 5: |          | 1510/? [36:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1510, loss 4.23819637298584\n",
      "Epoch 5: |          | 1511/? [36:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1511, loss 3.8072268962860107\n",
      "Epoch 5: |          | 1512/? [36:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1512, loss 4.4233245849609375\n",
      "Epoch 5: |          | 1513/? [36:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1513, loss 4.626267433166504\n",
      "Epoch 5: |          | 1514/? [36:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1514, loss 3.680798053741455\n",
      "Epoch 5: |          | 1515/? [36:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1515, loss 4.584155082702637\n",
      "Epoch 5: |          | 1516/? [36:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1516, loss 4.430069923400879\n",
      "Epoch 5: |          | 1517/? [36:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1517, loss 3.897622585296631\n",
      "Epoch 5: |          | 1518/? [36:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1518, loss 3.677211284637451\n",
      "Epoch 5: |          | 1519/? [36:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1519, loss 4.246150016784668\n",
      "Epoch 5: |          | 1520/? [36:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1520, loss 4.354170322418213\n",
      "Epoch 5: |          | 1521/? [36:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1521, loss 4.017820358276367\n",
      "Epoch 5: |          | 1522/? [36:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1522, loss 3.764708995819092\n",
      "Epoch 5: |          | 1523/? [36:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1523, loss 4.112447738647461\n",
      "Epoch 5: |          | 1524/? [36:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1524, loss 3.999901533126831\n",
      "Epoch 5: |          | 1525/? [36:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1525, loss 3.697664260864258\n",
      "Epoch 5: |          | 1526/? [36:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1526, loss 4.24873685836792\n",
      "Epoch 5: |          | 1527/? [36:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1527, loss 4.265225410461426\n",
      "Epoch 5: |          | 1528/? [36:36<00:00,  0.70it/s, v_num=31]ERROR: Input has inproper shape\n",
      "Epoch 5: |          | 1529/? [36:36<00:00,  0.70it/s, v_num=31]   VALIDATION: Batch 0, loss 4.639483451843262\n",
      "   VALIDATION: Batch 1, loss 3.6215176582336426\n",
      "   VALIDATION: Batch 2, loss 4.825244426727295\n",
      "   VALIDATION: Batch 3, loss 4.475186347961426\n",
      "   VALIDATION: Batch 4, loss 4.060762882232666\n",
      "   VALIDATION: Batch 5, loss 3.7228598594665527\n",
      "   VALIDATION: Batch 6, loss 4.023364067077637\n",
      "   VALIDATION: Batch 7, loss 4.698145389556885\n",
      "   VALIDATION: Batch 8, loss 4.5284647941589355\n",
      "   VALIDATION: Batch 9, loss 4.681426048278809\n",
      "   VALIDATION: Batch 10, loss 4.338707447052002\n",
      "   VALIDATION: Batch 11, loss 4.016578197479248\n",
      "   VALIDATION: Batch 12, loss 4.203171730041504\n",
      "   VALIDATION: Batch 13, loss 4.7618408203125\n",
      "   VALIDATION: Batch 14, loss 4.0226945877075195\n",
      "   VALIDATION: Batch 15, loss 3.9775853157043457\n",
      "   VALIDATION: Batch 16, loss 4.625216007232666\n",
      "   VALIDATION: Batch 17, loss 4.323962211608887\n",
      "   VALIDATION: Batch 18, loss 3.560147762298584\n",
      "   VALIDATION: Batch 19, loss 4.464052677154541\n",
      "   VALIDATION: Batch 20, loss 4.756862640380859\n",
      "   VALIDATION: Batch 21, loss 4.978053569793701\n",
      "   VALIDATION: Batch 22, loss 4.633449554443359\n",
      "   VALIDATION: Batch 23, loss 4.164228916168213\n",
      "   VALIDATION: Batch 24, loss 3.992144823074341\n",
      "   VALIDATION: Batch 25, loss 4.396842956542969\n",
      "   VALIDATION: Batch 26, loss 4.622403144836426\n",
      "   VALIDATION: Batch 27, loss 4.495267868041992\n",
      "   VALIDATION: Batch 28, loss 4.281678676605225\n",
      "   VALIDATION: Batch 29, loss 4.451800346374512\n",
      "   VALIDATION: Batch 30, loss 4.101532936096191\n",
      "   VALIDATION: Batch 31, loss 4.405109405517578\n",
      "   VALIDATION: Batch 32, loss 4.928950786590576\n",
      "   VALIDATION: Batch 33, loss 3.1694295406341553\n",
      "   VALIDATION: Batch 34, loss 4.354876518249512\n",
      "   VALIDATION: Batch 35, loss 4.567916393280029\n",
      "   VALIDATION: Batch 36, loss 3.9171090126037598\n",
      "   VALIDATION: Batch 37, loss 3.8926918506622314\n",
      "   VALIDATION: Batch 38, loss 3.9736695289611816\n",
      "   VALIDATION: Batch 39, loss 4.357651710510254\n",
      "   VALIDATION: Batch 40, loss 4.3854217529296875\n",
      "   VALIDATION: Batch 41, loss 3.19636869430542\n",
      "   VALIDATION: Batch 42, loss 4.381522178649902\n",
      "   VALIDATION: Batch 43, loss 4.5941548347473145\n",
      "   VALIDATION: Batch 44, loss 4.180423736572266\n",
      "   VALIDATION: Batch 45, loss 4.596367835998535\n",
      "   VALIDATION: Batch 46, loss 3.7346789836883545\n",
      "   VALIDATION: Batch 47, loss 4.727577209472656\n",
      "   VALIDATION: Batch 48, loss 4.826657295227051\n",
      "   VALIDATION: Batch 49, loss 4.444205284118652\n",
      "   VALIDATION: Batch 50, loss 4.4527482986450195\n",
      "   VALIDATION: Batch 51, loss 4.906580924987793\n",
      "   VALIDATION: Batch 52, loss 4.0771613121032715\n",
      "   VALIDATION: Batch 53, loss 3.9351577758789062\n",
      "   VALIDATION: Batch 54, loss 4.029107570648193\n",
      "   VALIDATION: Batch 55, loss 4.745412349700928\n",
      "   VALIDATION: Batch 56, loss 4.182644367218018\n",
      "   VALIDATION: Batch 57, loss 5.657154083251953\n",
      "   VALIDATION: Batch 58, loss 4.286870002746582\n",
      "   VALIDATION: Batch 59, loss 3.920682907104492\n",
      "   VALIDATION: Batch 60, loss 3.459451675415039\n",
      "   VALIDATION: Batch 61, loss 4.324493885040283\n",
      "   VALIDATION: Batch 62, loss 4.303613662719727\n",
      "   VALIDATION: Batch 63, loss 4.820067882537842\n",
      "   VALIDATION: Batch 64, loss 4.613926887512207\n",
      "   VALIDATION: Batch 65, loss 3.7888712882995605\n",
      "   VALIDATION: Batch 66, loss 4.720663070678711\n",
      "   VALIDATION: Batch 67, loss 4.120877742767334\n",
      "   VALIDATION: Batch 68, loss 4.290970802307129\n",
      "   VALIDATION: Batch 69, loss 4.524016857147217\n",
      "   VALIDATION: Batch 70, loss 4.682845115661621\n",
      "   VALIDATION: Batch 71, loss 4.206732749938965\n",
      "   VALIDATION: Batch 72, loss 5.073993682861328\n",
      "   VALIDATION: Batch 73, loss 3.910130023956299\n",
      "   VALIDATION: Batch 74, loss 4.50629186630249\n",
      "   VALIDATION: Batch 75, loss 4.482202529907227\n",
      "   VALIDATION: Batch 76, loss 4.3400654792785645\n",
      "   VALIDATION: Batch 77, loss 4.623743057250977\n",
      "   VALIDATION: Batch 78, loss 4.423529148101807\n",
      "   VALIDATION: Batch 79, loss 4.414813041687012\n",
      "   VALIDATION: Batch 80, loss 4.476864814758301\n",
      "   VALIDATION: Batch 81, loss 4.197824954986572\n",
      "   VALIDATION: Batch 82, loss 4.5642595291137695\n",
      "   VALIDATION: Batch 83, loss 3.857138156890869\n",
      "   VALIDATION: Batch 84, loss 4.543038845062256\n",
      "   VALIDATION: Batch 85, loss 4.2012152671813965\n",
      "   VALIDATION: Batch 86, loss 4.266723155975342\n",
      "   VALIDATION: Batch 87, loss 4.132981300354004\n",
      "   VALIDATION: Batch 88, loss 3.7326767444610596\n",
      "   VALIDATION: Batch 89, loss 4.063179969787598\n",
      "   VALIDATION: Batch 90, loss 4.291651725769043\n",
      "   VALIDATION: Batch 91, loss 4.364506721496582\n",
      "   VALIDATION: Batch 92, loss 4.056844711303711\n",
      "   VALIDATION: Batch 93, loss 4.8010735511779785\n",
      "   VALIDATION: Batch 94, loss 4.252526760101318\n",
      "   VALIDATION: Batch 95, loss 3.7626121044158936\n",
      "   VALIDATION: Batch 96, loss 4.213168144226074\n",
      "   VALIDATION: Batch 97, loss 3.9517548084259033\n",
      "   VALIDATION: Batch 98, loss 4.543654441833496\n",
      "   VALIDATION: Batch 99, loss 4.670678615570068\n",
      "   VALIDATION: Batch 100, loss 4.981445789337158\n",
      "   VALIDATION: Batch 101, loss 3.596965789794922\n",
      "   VALIDATION: Batch 102, loss 5.04466438293457\n",
      "   VALIDATION: Batch 103, loss 4.921053886413574\n",
      "   VALIDATION: Batch 104, loss 3.8827407360076904\n",
      "   VALIDATION: Batch 105, loss 4.355801582336426\n",
      "   VALIDATION: Batch 106, loss 4.245382308959961\n",
      "   VALIDATION: Batch 107, loss 4.349270820617676\n",
      "   VALIDATION: Batch 108, loss 4.043819427490234\n",
      "   VALIDATION: Batch 109, loss 4.666224479675293\n",
      "   VALIDATION: Batch 110, loss 4.336089611053467\n",
      "   VALIDATION: Batch 111, loss 4.6672139167785645\n",
      "   VALIDATION: Batch 112, loss 5.496788501739502\n",
      "   VALIDATION: Batch 113, loss 4.830491542816162\n",
      "   VALIDATION: Batch 114, loss 4.629324913024902\n",
      "   VALIDATION: Batch 115, loss 4.073119640350342\n",
      "   VALIDATION: Batch 116, loss 3.9318301677703857\n",
      "   VALIDATION: Batch 117, loss 4.581486225128174\n",
      "   VALIDATION: Batch 118, loss 4.784367084503174\n",
      "   VALIDATION: Batch 119, loss 3.944037675857544\n",
      "   VALIDATION: Batch 120, loss 3.5770962238311768\n",
      "   VALIDATION: Batch 121, loss 3.8320116996765137\n",
      "   VALIDATION: Batch 122, loss 4.209490776062012\n",
      "   VALIDATION: Batch 123, loss 4.264387130737305\n",
      "   VALIDATION: Batch 124, loss 3.6438393592834473\n",
      "   VALIDATION: Batch 125, loss 4.2667951583862305\n",
      "   VALIDATION: Batch 126, loss 4.472808361053467\n",
      "   VALIDATION: Batch 127, loss 4.228089809417725\n",
      "   VALIDATION: Batch 128, loss 4.415263652801514\n",
      "   VALIDATION: Batch 129, loss 4.0413713455200195\n",
      "   VALIDATION: Batch 130, loss 3.6923320293426514\n",
      "   VALIDATION: Batch 131, loss 3.6923909187316895\n",
      "   VALIDATION: Batch 132, loss 4.337592124938965\n",
      "   VALIDATION: Batch 133, loss 4.509982585906982\n",
      "   VALIDATION: Batch 134, loss 4.384289264678955\n",
      "   VALIDATION: Batch 135, loss 4.6614789962768555\n",
      "   VALIDATION: Batch 136, loss 4.74758243560791\n",
      "   VALIDATION: Batch 137, loss 4.56667423248291\n",
      "   VALIDATION: Batch 138, loss 4.301579475402832\n",
      "   VALIDATION: Batch 139, loss 4.686430931091309\n",
      "   VALIDATION: Batch 140, loss 3.7967782020568848\n",
      "   VALIDATION: Batch 141, loss 4.738174915313721\n",
      "   VALIDATION: Batch 142, loss 3.470289707183838\n",
      "   VALIDATION: Batch 143, loss 4.258768558502197\n",
      "   VALIDATION: Batch 144, loss 4.522997856140137\n",
      "   VALIDATION: Batch 145, loss 4.310497760772705\n",
      "   VALIDATION: Batch 146, loss 4.1270341873168945\n",
      "   VALIDATION: Batch 147, loss 4.472531795501709\n",
      "   VALIDATION: Batch 148, loss 4.555405616760254\n",
      "   VALIDATION: Batch 149, loss 5.052723407745361\n",
      "   VALIDATION: Batch 150, loss 4.6106953620910645\n",
      "   VALIDATION: Batch 151, loss 4.921247959136963\n",
      "   VALIDATION: Batch 152, loss 4.278957366943359\n",
      "   VALIDATION: Batch 153, loss 4.527196884155273\n",
      "   VALIDATION: Batch 154, loss 4.369656085968018\n",
      "   VALIDATION: Batch 155, loss 4.118443012237549\n",
      "   VALIDATION: Batch 156, loss 4.762352466583252\n",
      "   VALIDATION: Batch 157, loss 4.443856716156006\n",
      "   VALIDATION: Batch 158, loss 3.884531021118164\n",
      "   VALIDATION: Batch 159, loss 4.330704689025879\n",
      "   VALIDATION: Batch 160, loss 4.6751484870910645\n",
      "   VALIDATION: Batch 161, loss 4.864457607269287\n",
      "   VALIDATION: Batch 162, loss 4.306992053985596\n",
      "   VALIDATION: Batch 163, loss 3.7644810676574707\n",
      "   VALIDATION: Batch 164, loss 4.288219928741455\n",
      "   VALIDATION: Batch 165, loss 4.702605247497559\n",
      "   VALIDATION: Batch 166, loss 4.165163516998291\n",
      "   VALIDATION: Batch 167, loss 4.613003730773926\n",
      "   VALIDATION: Batch 168, loss 3.460629940032959\n",
      "   VALIDATION: Batch 169, loss 4.137970924377441\n",
      "   VALIDATION: Batch 170, loss 4.319417953491211\n",
      "   VALIDATION: Batch 171, loss 4.483017921447754\n",
      "   VALIDATION: Batch 172, loss 4.338569641113281\n",
      "   VALIDATION: Batch 173, loss 4.284262180328369\n",
      "   VALIDATION: Batch 174, loss 4.664328575134277\n",
      "   VALIDATION: Batch 175, loss 4.417308330535889\n",
      "   VALIDATION: Batch 176, loss 4.18527889251709\n",
      "   VALIDATION: Batch 177, loss 4.1238226890563965\n",
      "   VALIDATION: Batch 178, loss 5.205935478210449\n",
      "   VALIDATION: Batch 179, loss 4.492184162139893\n",
      "   VALIDATION: Batch 180, loss 4.003556251525879\n",
      "   VALIDATION: Batch 181, loss 4.194474697113037\n",
      "   VALIDATION: Batch 182, loss 4.450092315673828\n",
      "   VALIDATION: Batch 183, loss 3.546787977218628\n",
      "   VALIDATION: Batch 184, loss 3.2950401306152344\n",
      "   VALIDATION: Batch 185, loss 4.056517601013184\n",
      "   VALIDATION: Batch 186, loss 3.945645809173584\n",
      "   VALIDATION: Batch 187, loss 4.144037246704102\n",
      "   VALIDATION: Batch 188, loss 4.553043842315674\n",
      "   VALIDATION: Batch 189, loss 3.9054081439971924\n",
      "   VALIDATION: Batch 190, loss 3.981606960296631\n",
      "   VALIDATION: Batch 191, loss 4.462677955627441\n",
      "   VALIDATION: Batch 192, loss 4.776492118835449\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=31]              TRRAINING: Batch 0, loss 4.254074573516846\n",
      "Epoch 6: |          | 1/? [00:01<00:00,  0.57it/s, v_num=31]   TRRAINING: Batch 1, loss 3.7927467823028564\n",
      "Epoch 6: |          | 2/? [00:03<00:00,  0.63it/s, v_num=31]   TRRAINING: Batch 2, loss 3.8710389137268066\n",
      "Epoch 6: |          | 3/? [00:04<00:00,  0.64it/s, v_num=31]   TRRAINING: Batch 3, loss 3.5537402629852295\n",
      "Epoch 6: |          | 4/? [00:06<00:00,  0.65it/s, v_num=31]   TRRAINING: Batch 4, loss 3.968325138092041\n",
      "Epoch 6: |          | 5/? [00:07<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 5, loss 4.754838466644287\n",
      "Epoch 6: |          | 6/? [00:08<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 6, loss 4.276185512542725\n",
      "Epoch 6: |          | 7/? [00:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 7, loss 3.5866646766662598\n",
      "Epoch 6: |          | 8/? [00:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 8, loss 3.680515766143799\n",
      "Epoch 6: |          | 9/? [00:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 9, loss 3.9911186695098877\n",
      "Epoch 6: |          | 10/? [00:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 10, loss 4.224852561950684\n",
      "Epoch 6: |          | 11/? [00:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 11, loss 4.145304203033447\n",
      "Epoch 6: |          | 12/? [00:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 12, loss 4.743653297424316\n",
      "Epoch 6: |          | 13/? [00:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 13, loss 4.095871925354004\n",
      "Epoch 6: |          | 14/? [00:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 14, loss 4.296942710876465\n",
      "Epoch 6: |          | 15/? [00:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 15, loss 3.5673489570617676\n",
      "Epoch 6: |          | 16/? [00:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 16, loss 3.369647979736328\n",
      "Epoch 6: |          | 17/? [00:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 17, loss 4.549454212188721\n",
      "Epoch 6: |          | 18/? [00:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 18, loss 4.051551818847656\n",
      "Epoch 6: |          | 19/? [00:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 19, loss 3.8222248554229736\n",
      "Epoch 6: |          | 20/? [00:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 20, loss 4.112053871154785\n",
      "Epoch 6: |          | 21/? [00:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 21, loss 4.236268997192383\n",
      "Epoch 6: |          | 22/? [00:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 22, loss 4.134152412414551\n",
      "Epoch 6: |          | 23/? [00:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 23, loss 3.5029263496398926\n",
      "Epoch 6: |          | 24/? [00:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 24, loss 4.1435136795043945\n",
      "Epoch 6: |          | 25/? [00:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 25, loss 3.9616096019744873\n",
      "Epoch 6: |          | 26/? [00:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 26, loss 3.7272861003875732\n",
      "Epoch 6: |          | 27/? [00:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 27, loss 3.669296979904175\n",
      "Epoch 6: |          | 28/? [00:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 28, loss 4.555217742919922\n",
      "Epoch 6: |          | 29/? [00:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 29, loss 4.106292247772217\n",
      "Epoch 6: |          | 30/? [00:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 30, loss 3.9510390758514404\n",
      "Epoch 6: |          | 31/? [00:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 31, loss 4.563932418823242\n",
      "Epoch 6: |          | 32/? [00:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 32, loss 4.087852478027344\n",
      "Epoch 6: |          | 33/? [00:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 33, loss 3.918515682220459\n",
      "Epoch 6: |          | 34/? [00:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 34, loss 3.888127088546753\n",
      "Epoch 6: |          | 35/? [00:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 35, loss 3.31489634513855\n",
      "Epoch 6: |          | 36/? [00:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 36, loss 4.137303352355957\n",
      "Epoch 6: |          | 37/? [00:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 37, loss 4.303320407867432\n",
      "Epoch 6: |          | 38/? [00:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 38, loss 4.613564968109131\n",
      "Epoch 6: |          | 39/? [00:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 39, loss 4.477948188781738\n",
      "Epoch 6: |          | 40/? [00:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 40, loss 3.897773027420044\n",
      "Epoch 6: |          | 41/? [00:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 41, loss 3.959954023361206\n",
      "Epoch 6: |          | 42/? [01:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 42, loss 3.724295139312744\n",
      "Epoch 6: |          | 43/? [01:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 43, loss 3.917426347732544\n",
      "Epoch 6: |          | 44/? [01:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 44, loss 3.09283185005188\n",
      "Epoch 6: |          | 45/? [01:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 45, loss 2.69830584526062\n",
      "Epoch 6: |          | 46/? [01:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 46, loss 4.503861427307129\n",
      "Epoch 6: |          | 47/? [01:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 47, loss 3.704477310180664\n",
      "Epoch 6: |          | 48/? [01:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 48, loss 3.5691211223602295\n",
      "Epoch 6: |          | 49/? [01:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 49, loss 4.037541389465332\n",
      "Epoch 6: |          | 50/? [01:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 50, loss 3.8815219402313232\n",
      "Epoch 6: |          | 51/? [01:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 51, loss 3.8649165630340576\n",
      "Epoch 6: |          | 52/? [01:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 52, loss 4.543353080749512\n",
      "Epoch 6: |          | 53/? [01:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 53, loss 4.149056911468506\n",
      "Epoch 6: |          | 54/? [01:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 54, loss 3.9641013145446777\n",
      "Epoch 6: |          | 55/? [01:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 55, loss 4.0268874168396\n",
      "Epoch 6: |          | 56/? [01:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 56, loss 4.152319431304932\n",
      "Epoch 6: |          | 57/? [01:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 57, loss 4.05775785446167\n",
      "Epoch 6: |          | 58/? [01:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 58, loss 5.252874374389648\n",
      "Epoch 6: |          | 59/? [01:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 59, loss 4.2163496017456055\n",
      "Epoch 6: |          | 60/? [01:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 60, loss 4.3608245849609375\n",
      "Epoch 6: |          | 61/? [01:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 61, loss 4.31635046005249\n",
      "Epoch 6: |          | 62/? [01:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 62, loss 3.920325756072998\n",
      "Epoch 6: |          | 63/? [01:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 63, loss 4.126723289489746\n",
      "Epoch 6: |          | 64/? [01:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 64, loss 3.952754259109497\n",
      "Epoch 6: |          | 65/? [01:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 65, loss 3.9870376586914062\n",
      "Epoch 6: |          | 66/? [01:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 66, loss 3.452948808670044\n",
      "Epoch 6: |          | 67/? [01:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 67, loss 4.121181011199951\n",
      "Epoch 6: |          | 68/? [01:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 68, loss 4.156075477600098\n",
      "Epoch 6: |          | 69/? [01:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 69, loss 4.061250686645508\n",
      "Epoch 6: |          | 70/? [01:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 70, loss 3.7093513011932373\n",
      "Epoch 6: |          | 71/? [01:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 71, loss 3.6652209758758545\n",
      "Epoch 6: |          | 72/? [01:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 72, loss 3.9122040271759033\n",
      "Epoch 6: |          | 73/? [01:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 73, loss 4.104206562042236\n",
      "Epoch 6: |          | 74/? [01:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 74, loss 3.8405139446258545\n",
      "Epoch 6: |          | 75/? [01:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 75, loss 3.960022449493408\n",
      "Epoch 6: |          | 76/? [01:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 76, loss 3.957974672317505\n",
      "Epoch 6: |          | 77/? [01:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 77, loss 3.979153871536255\n",
      "Epoch 6: |          | 78/? [01:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 78, loss 3.8144149780273438\n",
      "Epoch 6: |          | 79/? [01:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 79, loss 3.995568037033081\n",
      "Epoch 6: |          | 80/? [01:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 80, loss 3.936049222946167\n",
      "Epoch 6: |          | 81/? [01:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 81, loss 3.401840925216675\n",
      "Epoch 6: |          | 82/? [01:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 82, loss 4.192083835601807\n",
      "Epoch 6: |          | 83/? [01:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 83, loss 3.6383392810821533\n",
      "Epoch 6: |          | 84/? [02:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 84, loss 3.4969565868377686\n",
      "Epoch 6: |          | 85/? [02:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 85, loss 3.364622116088867\n",
      "Epoch 6: |          | 86/? [02:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 86, loss 3.5211524963378906\n",
      "Epoch 6: |          | 87/? [02:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 87, loss 3.6963448524475098\n",
      "Epoch 6: |          | 88/? [02:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 88, loss 4.379199504852295\n",
      "Epoch 6: |          | 89/? [02:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 89, loss 4.3347368240356445\n",
      "Epoch 6: |          | 90/? [02:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 90, loss 4.089519023895264\n",
      "Epoch 6: |          | 91/? [02:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 91, loss 3.8124027252197266\n",
      "Epoch 6: |          | 92/? [02:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 92, loss 4.280129909515381\n",
      "Epoch 6: |          | 93/? [02:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 93, loss 4.4042744636535645\n",
      "Epoch 6: |          | 94/? [02:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 94, loss 4.255197525024414\n",
      "Epoch 6: |          | 95/? [02:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 95, loss 4.088170051574707\n",
      "Epoch 6: |          | 96/? [02:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 96, loss 3.796882152557373\n",
      "Epoch 6: |          | 97/? [02:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 97, loss 3.6415512561798096\n",
      "Epoch 6: |          | 98/? [02:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 98, loss 4.122325897216797\n",
      "Epoch 6: |          | 99/? [02:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 99, loss 4.317072868347168\n",
      "Epoch 6: |          | 100/? [02:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 100, loss 4.25250768661499\n",
      "Epoch 6: |          | 101/? [02:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 101, loss 3.920175552368164\n",
      "Epoch 6: |          | 102/? [02:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 102, loss 3.924161195755005\n",
      "Epoch 6: |          | 103/? [02:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 103, loss 3.6917519569396973\n",
      "Epoch 6: |          | 104/? [02:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 104, loss 4.065642833709717\n",
      "Epoch 6: |          | 105/? [02:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 105, loss 3.9763267040252686\n",
      "Epoch 6: |          | 106/? [02:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 106, loss 4.127298355102539\n",
      "Epoch 6: |          | 107/? [02:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 107, loss 4.151729106903076\n",
      "Epoch 6: |          | 108/? [02:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 108, loss 4.10590934753418\n",
      "Epoch 6: |          | 109/? [02:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 109, loss 3.8255741596221924\n",
      "Epoch 6: |          | 110/? [02:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 110, loss 4.031283378601074\n",
      "Epoch 6: |          | 111/? [02:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 111, loss 4.660319805145264\n",
      "Epoch 6: |          | 112/? [02:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 112, loss 3.3601322174072266\n",
      "Epoch 6: |          | 113/? [02:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 113, loss 2.9378674030303955\n",
      "Epoch 6: |          | 114/? [02:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 114, loss 4.247957706451416\n",
      "Epoch 6: |          | 115/? [02:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 115, loss 4.375229835510254\n",
      "Epoch 6: |          | 116/? [02:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 116, loss 3.4909064769744873\n",
      "Epoch 6: |          | 117/? [02:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 117, loss 3.489192247390747\n",
      "Epoch 6: |          | 118/? [02:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 118, loss 4.322507381439209\n",
      "Epoch 6: |          | 119/? [02:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 119, loss 4.549521446228027\n",
      "Epoch 6: |          | 120/? [02:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 120, loss 4.299860954284668\n",
      "Epoch 6: |          | 121/? [02:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 121, loss 4.025412559509277\n",
      "Epoch 6: |          | 122/? [02:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 122, loss 3.4656403064727783\n",
      "Epoch 6: |          | 123/? [02:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 123, loss 3.969595432281494\n",
      "Epoch 6: |          | 124/? [02:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 124, loss 4.153284072875977\n",
      "Epoch 6: |          | 125/? [02:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 125, loss 3.7784371376037598\n",
      "Epoch 6: |          | 126/? [03:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 126, loss 4.229857444763184\n",
      "Epoch 6: |          | 127/? [03:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 127, loss 4.355260848999023\n",
      "Epoch 6: |          | 128/? [03:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 128, loss 3.466331958770752\n",
      "Epoch 6: |          | 129/? [03:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 129, loss 4.087656497955322\n",
      "Epoch 6: |          | 130/? [03:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 130, loss 3.189713478088379\n",
      "Epoch 6: |          | 131/? [03:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 131, loss 4.021190166473389\n",
      "Epoch 6: |          | 132/? [03:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 132, loss 4.052265167236328\n",
      "Epoch 6: |          | 133/? [03:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 133, loss 3.955063581466675\n",
      "Epoch 6: |          | 134/? [03:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 134, loss 4.123626232147217\n",
      "Epoch 6: |          | 135/? [03:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 135, loss 4.232516765594482\n",
      "Epoch 6: |          | 136/? [03:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 136, loss 4.255498886108398\n",
      "Epoch 6: |          | 137/? [03:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 137, loss 3.247173309326172\n",
      "Epoch 6: |          | 138/? [03:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 138, loss 3.8348922729492188\n",
      "Epoch 6: |          | 139/? [03:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 139, loss 4.316677570343018\n",
      "Epoch 6: |          | 140/? [03:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 140, loss 3.5284061431884766\n",
      "Epoch 6: |          | 141/? [03:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 141, loss 3.628427028656006\n",
      "Epoch 6: |          | 142/? [03:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 142, loss 5.11653995513916\n",
      "Epoch 6: |          | 143/? [03:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 143, loss 4.773591041564941\n",
      "Epoch 6: |          | 144/? [03:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 144, loss 3.9965553283691406\n",
      "Epoch 6: |          | 145/? [03:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 145, loss 3.483224391937256\n",
      "Epoch 6: |          | 146/? [03:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 146, loss 3.6713263988494873\n",
      "Epoch 6: |          | 147/? [03:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 147, loss 3.990596055984497\n",
      "Epoch 6: |          | 148/? [03:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 148, loss 3.7362308502197266\n",
      "Epoch 6: |          | 149/? [03:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 149, loss 3.268616199493408\n",
      "Epoch 6: |          | 150/? [03:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 150, loss 4.135018348693848\n",
      "Epoch 6: |          | 151/? [03:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 151, loss 4.26585578918457\n",
      "Epoch 6: |          | 152/? [03:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 152, loss 4.1858344078063965\n",
      "Epoch 6: |          | 153/? [03:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 153, loss 3.3008170127868652\n",
      "Epoch 6: |          | 154/? [03:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 154, loss 4.553441524505615\n",
      "Epoch 6: |          | 155/? [03:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 155, loss 4.084760665893555\n",
      "Epoch 6: |          | 156/? [03:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 156, loss 3.422177791595459\n",
      "Epoch 6: |          | 157/? [03:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 157, loss 4.130727291107178\n",
      "Epoch 6: |          | 158/? [03:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 158, loss 4.222926139831543\n",
      "Epoch 6: |          | 159/? [03:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 159, loss 3.9526023864746094\n",
      "Epoch 6: |          | 160/? [03:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 160, loss 3.7015538215637207\n",
      "Epoch 6: |          | 161/? [03:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 161, loss 4.126350402832031\n",
      "Epoch 6: |          | 162/? [03:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 162, loss 4.108363151550293\n",
      "Epoch 6: |          | 163/? [03:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 163, loss 3.2377877235412598\n",
      "Epoch 6: |          | 164/? [03:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 164, loss 3.6872825622558594\n",
      "Epoch 6: |          | 165/? [03:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 165, loss 4.490585803985596\n",
      "Epoch 6: |          | 166/? [03:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 166, loss 4.163792610168457\n",
      "Epoch 6: |          | 167/? [03:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 167, loss 4.256556034088135\n",
      "Epoch 6: |          | 168/? [04:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 168, loss 3.8625950813293457\n",
      "Epoch 6: |          | 169/? [04:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 169, loss 3.513190507888794\n",
      "Epoch 6: |          | 170/? [04:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 170, loss 3.703608989715576\n",
      "Epoch 6: |          | 171/? [04:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 171, loss 4.076455116271973\n",
      "Epoch 6: |          | 172/? [04:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 172, loss 3.8870208263397217\n",
      "Epoch 6: |          | 173/? [04:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 173, loss 4.521967887878418\n",
      "Epoch 6: |          | 174/? [04:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 174, loss 4.154476165771484\n",
      "Epoch 6: |          | 175/? [04:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 175, loss 4.611506462097168\n",
      "Epoch 6: |          | 176/? [04:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 176, loss 3.798581600189209\n",
      "Epoch 6: |          | 177/? [04:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 177, loss 3.859609603881836\n",
      "Epoch 6: |          | 178/? [04:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 178, loss 3.677760362625122\n",
      "Epoch 6: |          | 179/? [04:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 179, loss 4.390069484710693\n",
      "Epoch 6: |          | 180/? [04:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 180, loss 3.9130260944366455\n",
      "Epoch 6: |          | 181/? [04:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 181, loss 3.7083282470703125\n",
      "Epoch 6: |          | 182/? [04:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 182, loss 4.109949111938477\n",
      "Epoch 6: |          | 183/? [04:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 183, loss 3.5953564643859863\n",
      "Epoch 6: |          | 184/? [04:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 184, loss 3.8120198249816895\n",
      "Epoch 6: |          | 185/? [04:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 185, loss 4.384893417358398\n",
      "Epoch 6: |          | 186/? [04:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 186, loss 3.846536159515381\n",
      "Epoch 6: |          | 187/? [04:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 187, loss 4.362645626068115\n",
      "Epoch 6: |          | 188/? [04:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 188, loss 3.7557759284973145\n",
      "Epoch 6: |          | 189/? [04:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 189, loss 4.380293846130371\n",
      "Epoch 6: |          | 190/? [04:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 190, loss 3.895063877105713\n",
      "Epoch 6: |          | 191/? [04:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 191, loss 4.673871040344238\n",
      "Epoch 6: |          | 192/? [04:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 192, loss 4.433730125427246\n",
      "Epoch 6: |          | 193/? [04:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 193, loss 3.769087314605713\n",
      "Epoch 6: |          | 194/? [04:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 194, loss 3.6501879692077637\n",
      "Epoch 6: |          | 195/? [04:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 195, loss 4.367558479309082\n",
      "Epoch 6: |          | 196/? [04:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 196, loss 4.222439289093018\n",
      "Epoch 6: |          | 197/? [04:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 197, loss 4.0861711502075195\n",
      "Epoch 6: |          | 198/? [04:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 198, loss 3.4183106422424316\n",
      "Epoch 6: |          | 199/? [04:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 199, loss 4.323633670806885\n",
      "Epoch 6: |          | 200/? [04:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 200, loss 3.797170639038086\n",
      "Epoch 6: |          | 201/? [04:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 201, loss 4.076635360717773\n",
      "Epoch 6: |          | 202/? [04:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 202, loss 4.191778182983398\n",
      "Epoch 6: |          | 203/? [04:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 203, loss 3.8618550300598145\n",
      "Epoch 6: |          | 204/? [04:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 204, loss 3.9192276000976562\n",
      "Epoch 6: |          | 205/? [04:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 205, loss 3.7699134349823\n",
      "Epoch 6: |          | 206/? [04:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 206, loss 3.588857650756836\n",
      "Epoch 6: |          | 207/? [04:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 207, loss 4.135007381439209\n",
      "Epoch 6: |          | 208/? [04:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 208, loss 4.032170295715332\n",
      "Epoch 6: |          | 209/? [04:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 209, loss 3.873033046722412\n",
      "Epoch 6: |          | 210/? [05:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 210, loss 4.5481276512146\n",
      "Epoch 6: |          | 211/? [05:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 211, loss 3.8984451293945312\n",
      "Epoch 6: |          | 212/? [05:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 212, loss 4.133843421936035\n",
      "Epoch 6: |          | 213/? [05:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 213, loss 3.9752745628356934\n",
      "Epoch 6: |          | 214/? [05:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 214, loss 3.8482260704040527\n",
      "Epoch 6: |          | 215/? [05:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 215, loss 3.5009236335754395\n",
      "Epoch 6: |          | 216/? [05:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 216, loss 4.183382034301758\n",
      "Epoch 6: |          | 217/? [05:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 217, loss 4.084578037261963\n",
      "Epoch 6: |          | 218/? [05:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 218, loss 4.119792938232422\n",
      "Epoch 6: |          | 219/? [05:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 219, loss 4.048935890197754\n",
      "Epoch 6: |          | 220/? [05:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 220, loss 4.1164045333862305\n",
      "Epoch 6: |          | 221/? [05:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 221, loss 3.935075283050537\n",
      "Epoch 6: |          | 222/? [05:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 222, loss 3.1920018196105957\n",
      "Epoch 6: |          | 223/? [05:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 223, loss 4.1835551261901855\n",
      "Epoch 6: |          | 224/? [05:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 224, loss 4.2841105461120605\n",
      "Epoch 6: |          | 225/? [05:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 225, loss 4.010322093963623\n",
      "Epoch 6: |          | 226/? [05:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 226, loss 3.9457828998565674\n",
      "Epoch 6: |          | 227/? [05:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 227, loss 4.2959089279174805\n",
      "Epoch 6: |          | 228/? [05:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 228, loss 3.985201597213745\n",
      "Epoch 6: |          | 229/? [05:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 229, loss 4.055754661560059\n",
      "Epoch 6: |          | 230/? [05:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 230, loss 4.002671241760254\n",
      "Epoch 6: |          | 231/? [05:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 231, loss 3.9331352710723877\n",
      "Epoch 6: |          | 232/? [05:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 232, loss 3.684058666229248\n",
      "Epoch 6: |          | 233/? [05:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 233, loss 4.343410015106201\n",
      "Epoch 6: |          | 234/? [05:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 234, loss 4.3519487380981445\n",
      "Epoch 6: |          | 235/? [05:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 235, loss 4.411314487457275\n",
      "Epoch 6: |          | 236/? [05:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 236, loss 3.8642303943634033\n",
      "Epoch 6: |          | 237/? [05:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 237, loss 4.01057767868042\n",
      "Epoch 6: |          | 238/? [05:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 238, loss 4.275511264801025\n",
      "Epoch 6: |          | 239/? [05:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 239, loss 3.8483288288116455\n",
      "Epoch 6: |          | 240/? [05:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 240, loss 3.3681979179382324\n",
      "Epoch 6: |          | 241/? [05:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 241, loss 4.016351699829102\n",
      "Epoch 6: |          | 242/? [05:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 242, loss 4.365760326385498\n",
      "Epoch 6: |          | 243/? [05:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 243, loss 3.2295851707458496\n",
      "Epoch 6: |          | 244/? [05:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 244, loss 3.673820972442627\n",
      "Epoch 6: |          | 245/? [05:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 245, loss 3.991790294647217\n",
      "Epoch 6: |          | 246/? [05:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 246, loss 4.094005584716797\n",
      "Epoch 6: |          | 247/? [05:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 247, loss 4.1206464767456055\n",
      "Epoch 6: |          | 248/? [05:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 248, loss 3.6595451831817627\n",
      "Epoch 6: |          | 249/? [05:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 249, loss 3.498116970062256\n",
      "Epoch 6: |          | 250/? [05:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 250, loss 4.1006550788879395\n",
      "Epoch 6: |          | 251/? [06:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 251, loss 4.1037702560424805\n",
      "Epoch 6: |          | 252/? [06:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 252, loss 3.884519100189209\n",
      "Epoch 6: |          | 253/? [06:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 253, loss 4.685542106628418\n",
      "Epoch 6: |          | 254/? [06:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 254, loss 4.3871026039123535\n",
      "Epoch 6: |          | 255/? [06:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 255, loss 3.9764206409454346\n",
      "Epoch 6: |          | 256/? [06:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 256, loss 5.063010215759277\n",
      "Epoch 6: |          | 257/? [06:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 257, loss 3.8632988929748535\n",
      "Epoch 6: |          | 258/? [06:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 258, loss 3.9707255363464355\n",
      "Epoch 6: |          | 259/? [06:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 259, loss 3.8402016162872314\n",
      "Epoch 6: |          | 260/? [06:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 260, loss 3.7787418365478516\n",
      "Epoch 6: |          | 261/? [06:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 261, loss 3.602708101272583\n",
      "Epoch 6: |          | 262/? [06:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 262, loss 4.305537223815918\n",
      "Epoch 6: |          | 263/? [06:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 263, loss 3.9452743530273438\n",
      "Epoch 6: |          | 264/? [06:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 264, loss 4.111573696136475\n",
      "Epoch 6: |          | 265/? [06:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 265, loss 3.5107429027557373\n",
      "Epoch 6: |          | 266/? [06:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 266, loss 4.0012736320495605\n",
      "Epoch 6: |          | 267/? [06:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 267, loss 3.556490659713745\n",
      "Epoch 6: |          | 268/? [06:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 268, loss 3.933018207550049\n",
      "Epoch 6: |          | 269/? [06:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 269, loss 4.321320533752441\n",
      "Epoch 6: |          | 270/? [06:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 270, loss 3.913900852203369\n",
      "Epoch 6: |          | 271/? [06:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 271, loss 4.214309215545654\n",
      "Epoch 6: |          | 272/? [06:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 272, loss 4.493631839752197\n",
      "Epoch 6: |          | 273/? [06:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 273, loss 3.6519551277160645\n",
      "Epoch 6: |          | 274/? [06:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 274, loss 4.530191898345947\n",
      "Epoch 6: |          | 275/? [06:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 275, loss 4.126755237579346\n",
      "Epoch 6: |          | 276/? [06:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 276, loss 3.4100100994110107\n",
      "Epoch 6: |          | 277/? [06:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 277, loss 4.106249809265137\n",
      "Epoch 6: |          | 278/? [06:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 278, loss 3.1013245582580566\n",
      "Epoch 6: |          | 279/? [06:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 279, loss 3.8261475563049316\n",
      "Epoch 6: |          | 280/? [06:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 280, loss 3.528078079223633\n",
      "Epoch 6: |          | 281/? [06:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 281, loss 4.214499473571777\n",
      "Epoch 6: |          | 282/? [06:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 282, loss 3.760584592819214\n",
      "Epoch 6: |          | 283/? [06:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 283, loss 3.8821473121643066\n",
      "Epoch 6: |          | 284/? [06:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 284, loss 3.793888568878174\n",
      "Epoch 6: |          | 285/? [06:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 285, loss 3.2762203216552734\n",
      "Epoch 6: |          | 286/? [06:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 286, loss 3.8538506031036377\n",
      "Epoch 6: |          | 287/? [06:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 287, loss 3.6073944568634033\n",
      "Epoch 6: |          | 288/? [06:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 288, loss 3.5357346534729004\n",
      "Epoch 6: |          | 289/? [06:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 289, loss 3.405747175216675\n",
      "Epoch 6: |          | 290/? [06:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 290, loss 2.931807518005371\n",
      "Epoch 6: |          | 291/? [06:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 291, loss 4.05404806137085\n",
      "Epoch 6: |          | 292/? [06:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 292, loss 3.7132351398468018\n",
      "Epoch 6: |          | 293/? [07:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 293, loss 4.029651165008545\n",
      "Epoch 6: |          | 294/? [07:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 294, loss 3.8966431617736816\n",
      "Epoch 6: |          | 295/? [07:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 295, loss 4.189492225646973\n",
      "Epoch 6: |          | 296/? [07:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 296, loss 3.670351505279541\n",
      "Epoch 6: |          | 297/? [07:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 297, loss 4.36411190032959\n",
      "Epoch 6: |          | 298/? [07:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 298, loss 4.16478967666626\n",
      "Epoch 6: |          | 299/? [07:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 299, loss 4.604561805725098\n",
      "Epoch 6: |          | 300/? [07:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 300, loss 3.9541614055633545\n",
      "Epoch 6: |          | 301/? [07:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 301, loss 3.812791109085083\n",
      "Epoch 6: |          | 302/? [07:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 302, loss 4.213939666748047\n",
      "Epoch 6: |          | 303/? [07:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 303, loss 4.009275913238525\n",
      "Epoch 6: |          | 304/? [07:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 304, loss 4.184537410736084\n",
      "Epoch 6: |          | 305/? [07:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 305, loss 4.247315406799316\n",
      "Epoch 6: |          | 306/? [07:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 306, loss 3.8994712829589844\n",
      "Epoch 6: |          | 307/? [07:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 307, loss 4.128525257110596\n",
      "Epoch 6: |          | 308/? [07:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 308, loss 4.305453777313232\n",
      "Epoch 6: |          | 309/? [07:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 309, loss 3.916929244995117\n",
      "Epoch 6: |          | 310/? [07:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 310, loss 4.340753078460693\n",
      "Epoch 6: |          | 311/? [07:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 311, loss 3.938915967941284\n",
      "Epoch 6: |          | 312/? [07:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 312, loss 3.9360079765319824\n",
      "Epoch 6: |          | 313/? [07:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 313, loss 3.753474712371826\n",
      "Epoch 6: |          | 314/? [07:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 314, loss 4.100613594055176\n",
      "Epoch 6: |          | 315/? [07:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 315, loss 3.7376480102539062\n",
      "Epoch 6: |          | 316/? [07:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 316, loss 4.289337635040283\n",
      "Epoch 6: |          | 317/? [07:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 317, loss 4.103496551513672\n",
      "Epoch 6: |          | 318/? [07:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 318, loss 4.253767967224121\n",
      "Epoch 6: |          | 319/? [07:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 319, loss 3.474428653717041\n",
      "Epoch 6: |          | 320/? [07:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 320, loss 3.9623324871063232\n",
      "Epoch 6: |          | 321/? [07:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 321, loss 3.765230894088745\n",
      "Epoch 6: |          | 322/? [07:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 322, loss 4.380788326263428\n",
      "Epoch 6: |          | 323/? [07:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 323, loss 4.377395153045654\n",
      "Epoch 6: |          | 324/? [07:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 324, loss 4.066889762878418\n",
      "Epoch 6: |          | 325/? [07:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 325, loss 4.482550621032715\n",
      "Epoch 6: |          | 326/? [07:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 326, loss 4.055838584899902\n",
      "Epoch 6: |          | 327/? [07:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 327, loss 3.7659614086151123\n",
      "Epoch 6: |          | 328/? [07:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 328, loss 3.538886547088623\n",
      "Epoch 6: |          | 329/? [07:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 329, loss 4.172240734100342\n",
      "Epoch 6: |          | 330/? [07:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 330, loss 4.6561079025268555\n",
      "Epoch 6: |          | 331/? [07:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 331, loss 2.9290757179260254\n",
      "Epoch 6: |          | 332/? [08:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 332, loss 3.98970103263855\n",
      "Epoch 6: |          | 333/? [08:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 333, loss 3.869459629058838\n",
      "Epoch 6: |          | 334/? [08:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 334, loss 4.549041748046875\n",
      "Epoch 6: |          | 335/? [08:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 335, loss 4.473478317260742\n",
      "Epoch 6: |          | 336/? [08:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 336, loss 4.396584510803223\n",
      "Epoch 6: |          | 337/? [08:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 337, loss 4.999873161315918\n",
      "Epoch 6: |          | 338/? [08:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 338, loss 4.718693733215332\n",
      "Epoch 6: |          | 339/? [08:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 339, loss 3.7479140758514404\n",
      "Epoch 6: |          | 340/? [08:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 340, loss 3.548276424407959\n",
      "Epoch 6: |          | 341/? [08:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 341, loss 3.4471099376678467\n",
      "Epoch 6: |          | 342/? [08:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 342, loss 4.093578815460205\n",
      "Epoch 6: |          | 343/? [08:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 343, loss 3.789046049118042\n",
      "Epoch 6: |          | 344/? [08:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 344, loss 4.626187801361084\n",
      "Epoch 6: |          | 345/? [08:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 345, loss 3.837717056274414\n",
      "Epoch 6: |          | 346/? [08:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 346, loss 4.112707614898682\n",
      "Epoch 6: |          | 347/? [08:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 347, loss 3.8771655559539795\n",
      "Epoch 6: |          | 348/? [08:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 348, loss 3.2604095935821533\n",
      "Epoch 6: |          | 349/? [08:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 349, loss 3.112384080886841\n",
      "Epoch 6: |          | 350/? [08:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 350, loss 4.340994834899902\n",
      "Epoch 6: |          | 351/? [08:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 351, loss 4.374598503112793\n",
      "Epoch 6: |          | 352/? [08:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 352, loss 3.6604790687561035\n",
      "Epoch 6: |          | 353/? [08:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 353, loss 3.4222216606140137\n",
      "Epoch 6: |          | 354/? [08:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 354, loss 3.837134838104248\n",
      "Epoch 6: |          | 355/? [08:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 355, loss 4.1473517417907715\n",
      "Epoch 6: |          | 356/? [08:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 356, loss 4.176913261413574\n",
      "Epoch 6: |          | 357/? [08:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 357, loss 3.667552947998047\n",
      "Epoch 6: |          | 358/? [08:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 358, loss 3.625497817993164\n",
      "Epoch 6: |          | 359/? [08:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 359, loss 4.192134857177734\n",
      "Epoch 6: |          | 360/? [08:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 360, loss 3.7957119941711426\n",
      "Epoch 6: |          | 361/? [08:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 361, loss 3.9095287322998047\n",
      "Epoch 6: |          | 362/? [08:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 362, loss 3.72326922416687\n",
      "Epoch 6: |          | 363/? [08:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 363, loss 3.6290862560272217\n",
      "Epoch 6: |          | 364/? [08:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 364, loss 4.2255449295043945\n",
      "Epoch 6: |          | 365/? [08:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 365, loss 4.250837326049805\n",
      "Epoch 6: |          | 366/? [08:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 366, loss 4.049690246582031\n",
      "Epoch 6: |          | 367/? [08:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 367, loss 3.990554094314575\n",
      "Epoch 6: |          | 368/? [08:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 368, loss 3.564535617828369\n",
      "Epoch 6: |          | 369/? [08:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 369, loss 3.8826682567596436\n",
      "Epoch 6: |          | 370/? [08:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 370, loss 3.5435924530029297\n",
      "Epoch 6: |          | 371/? [08:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 371, loss 4.448010444641113\n",
      "Epoch 6: |          | 372/? [08:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 372, loss 3.809459686279297\n",
      "Epoch 6: |          | 373/? [08:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 373, loss 4.140158653259277\n",
      "Epoch 6: |          | 374/? [08:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 374, loss 3.825483798980713\n",
      "Epoch 6: |          | 375/? [09:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 375, loss 4.487561225891113\n",
      "Epoch 6: |          | 376/? [09:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 376, loss 3.879241466522217\n",
      "Epoch 6: |          | 377/? [09:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 377, loss 4.077465057373047\n",
      "Epoch 6: |          | 378/? [09:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 378, loss 4.223982334136963\n",
      "Epoch 6: |          | 379/? [09:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 379, loss 4.047356605529785\n",
      "Epoch 6: |          | 380/? [09:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 380, loss 4.09305477142334\n",
      "Epoch 6: |          | 381/? [09:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 381, loss 4.157336235046387\n",
      "Epoch 6: |          | 382/? [09:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 382, loss 3.9130427837371826\n",
      "Epoch 6: |          | 383/? [09:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 383, loss 3.9516608715057373\n",
      "Epoch 6: |          | 384/? [09:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 384, loss 4.3829779624938965\n",
      "Epoch 6: |          | 385/? [09:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 385, loss 4.025625705718994\n",
      "Epoch 6: |          | 386/? [09:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 386, loss 2.9879374504089355\n",
      "Epoch 6: |          | 387/? [09:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 387, loss 3.811007261276245\n",
      "Epoch 6: |          | 388/? [09:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 388, loss 3.6766579151153564\n",
      "Epoch 6: |          | 389/? [09:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 389, loss 4.330409526824951\n",
      "Epoch 6: |          | 390/? [09:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 390, loss 3.8047187328338623\n",
      "Epoch 6: |          | 391/? [09:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 391, loss 4.184798717498779\n",
      "Epoch 6: |          | 392/? [09:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 392, loss 4.313214302062988\n",
      "Epoch 6: |          | 393/? [09:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 393, loss 4.322090148925781\n",
      "Epoch 6: |          | 394/? [09:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 394, loss 3.9825527667999268\n",
      "Epoch 6: |          | 395/? [09:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 395, loss 4.1918487548828125\n",
      "Epoch 6: |          | 396/? [09:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 396, loss 4.141458511352539\n",
      "Epoch 6: |          | 397/? [09:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 397, loss 3.9146130084991455\n",
      "Epoch 6: |          | 398/? [09:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 398, loss 3.8358936309814453\n",
      "Epoch 6: |          | 399/? [09:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 399, loss 3.9046669006347656\n",
      "Epoch 6: |          | 400/? [09:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 400, loss 3.871462345123291\n",
      "Epoch 6: |          | 401/? [09:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 401, loss 3.824676036834717\n",
      "Epoch 6: |          | 402/? [09:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 402, loss 4.212965965270996\n",
      "Epoch 6: |          | 403/? [09:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 403, loss 4.125524997711182\n",
      "Epoch 6: |          | 404/? [09:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 404, loss 3.6410412788391113\n",
      "Epoch 6: |          | 405/? [09:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 405, loss 3.6906638145446777\n",
      "Epoch 6: |          | 406/? [09:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 406, loss 4.014498710632324\n",
      "Epoch 6: |          | 407/? [09:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 407, loss 3.939166307449341\n",
      "Epoch 6: |          | 408/? [09:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 408, loss 4.344656467437744\n",
      "Epoch 6: |          | 409/? [09:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 409, loss 4.289999961853027\n",
      "Epoch 6: |          | 410/? [09:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 410, loss 3.9592792987823486\n",
      "Epoch 6: |          | 411/? [09:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 411, loss 3.835148572921753\n",
      "Epoch 6: |          | 412/? [09:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 412, loss 3.389896869659424\n",
      "Epoch 6: |          | 413/? [09:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 413, loss 4.111447811126709\n",
      "Epoch 6: |          | 414/? [09:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 414, loss 3.7317371368408203\n",
      "Epoch 6: |          | 415/? [09:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 415, loss 4.162211894989014\n",
      "Epoch 6: |          | 416/? [09:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 416, loss 4.564129829406738\n",
      "Epoch 6: |          | 417/? [10:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 417, loss 4.578204154968262\n",
      "Epoch 6: |          | 418/? [10:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 418, loss 4.158543109893799\n",
      "Epoch 6: |          | 419/? [10:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 419, loss 3.961604356765747\n",
      "Epoch 6: |          | 420/? [10:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 420, loss 4.111209392547607\n",
      "Epoch 6: |          | 421/? [10:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 421, loss 4.587031364440918\n",
      "Epoch 6: |          | 422/? [10:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 422, loss 4.121380805969238\n",
      "Epoch 6: |          | 423/? [10:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 423, loss 3.7269482612609863\n",
      "Epoch 6: |          | 424/? [10:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 424, loss 4.297837734222412\n",
      "Epoch 6: |          | 425/? [10:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 425, loss 4.045788288116455\n",
      "Epoch 6: |          | 426/? [10:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 426, loss 3.7454886436462402\n",
      "Epoch 6: |          | 427/? [10:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 427, loss 3.8255226612091064\n",
      "Epoch 6: |          | 428/? [10:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 428, loss 4.550396919250488\n",
      "Epoch 6: |          | 429/? [10:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 429, loss 3.4881999492645264\n",
      "Epoch 6: |          | 430/? [10:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 430, loss 4.103323936462402\n",
      "Epoch 6: |          | 431/? [10:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 431, loss 3.976081132888794\n",
      "Epoch 6: |          | 432/? [10:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 432, loss 4.126204967498779\n",
      "Epoch 6: |          | 433/? [10:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 433, loss 4.1153459548950195\n",
      "Epoch 6: |          | 434/? [10:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 434, loss 3.9748528003692627\n",
      "Epoch 6: |          | 435/? [10:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 435, loss 3.6838958263397217\n",
      "Epoch 6: |          | 436/? [10:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 436, loss 4.040816307067871\n",
      "Epoch 6: |          | 437/? [10:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 437, loss 4.2418718338012695\n",
      "Epoch 6: |          | 438/? [10:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 438, loss 3.8825619220733643\n",
      "Epoch 6: |          | 439/? [10:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 439, loss 3.7725627422332764\n",
      "Epoch 6: |          | 440/? [10:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 440, loss 3.716838836669922\n",
      "Epoch 6: |          | 441/? [10:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 441, loss 4.085033893585205\n",
      "Epoch 6: |          | 442/? [10:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 442, loss 3.8565711975097656\n",
      "Epoch 6: |          | 443/? [10:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 443, loss 3.994363307952881\n",
      "Epoch 6: |          | 444/? [10:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 444, loss 4.0109758377075195\n",
      "Epoch 6: |          | 445/? [10:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 445, loss 4.9561896324157715\n",
      "Epoch 6: |          | 446/? [10:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 446, loss 4.008622169494629\n",
      "Epoch 6: |          | 447/? [10:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 447, loss 4.538783073425293\n",
      "Epoch 6: |          | 448/? [10:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 448, loss 3.5784804821014404\n",
      "Epoch 6: |          | 449/? [10:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 449, loss 4.010508060455322\n",
      "Epoch 6: |          | 450/? [10:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 450, loss 4.323117256164551\n",
      "Epoch 6: |          | 451/? [10:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 451, loss 3.9280407428741455\n",
      "Epoch 6: |          | 452/? [10:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 452, loss 3.73630952835083\n",
      "Epoch 6: |          | 453/? [10:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 453, loss 4.421326160430908\n",
      "Epoch 6: |          | 454/? [10:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 454, loss 3.814241409301758\n",
      "Epoch 6: |          | 455/? [10:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 455, loss 4.165339946746826\n",
      "Epoch 6: |          | 456/? [10:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 456, loss 3.4662132263183594\n",
      "Epoch 6: |          | 457/? [10:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 457, loss 3.919241428375244\n",
      "Epoch 6: |          | 458/? [11:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 458, loss 4.397875785827637\n",
      "Epoch 6: |          | 459/? [11:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 459, loss 4.344213485717773\n",
      "Epoch 6: |          | 460/? [11:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 460, loss 4.117438316345215\n",
      "Epoch 6: |          | 461/? [11:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 461, loss 4.023348331451416\n",
      "Epoch 6: |          | 462/? [11:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 462, loss 4.144519805908203\n",
      "Epoch 6: |          | 463/? [11:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 463, loss 3.957925796508789\n",
      "Epoch 6: |          | 464/? [11:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 464, loss 3.478996992111206\n",
      "Epoch 6: |          | 465/? [11:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 465, loss 3.7552151679992676\n",
      "Epoch 6: |          | 466/? [11:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 466, loss 4.241482734680176\n",
      "Epoch 6: |          | 467/? [11:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 467, loss 4.045156955718994\n",
      "Epoch 6: |          | 468/? [11:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 468, loss 3.964632511138916\n",
      "Epoch 6: |          | 469/? [11:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 469, loss 4.076458930969238\n",
      "Epoch 6: |          | 470/? [11:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 470, loss 3.487880229949951\n",
      "Epoch 6: |          | 471/? [11:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 471, loss 4.263043403625488\n",
      "Epoch 6: |          | 472/? [11:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 472, loss 3.774653911590576\n",
      "Epoch 6: |          | 473/? [11:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 473, loss 3.7724831104278564\n",
      "Epoch 6: |          | 474/? [11:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 474, loss 3.440009355545044\n",
      "Epoch 6: |          | 475/? [11:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 475, loss 4.704257488250732\n",
      "Epoch 6: |          | 476/? [11:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 476, loss 3.7042808532714844\n",
      "Epoch 6: |          | 477/? [11:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 477, loss 3.2813401222229004\n",
      "Epoch 6: |          | 478/? [11:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 478, loss 3.4807212352752686\n",
      "Epoch 6: |          | 479/? [11:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 479, loss 3.9411911964416504\n",
      "Epoch 6: |          | 480/? [11:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 480, loss 3.8561484813690186\n",
      "Epoch 6: |          | 481/? [11:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 481, loss 3.5079257488250732\n",
      "Epoch 6: |          | 482/? [11:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 482, loss 3.7588906288146973\n",
      "Epoch 6: |          | 483/? [11:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 483, loss 3.430838108062744\n",
      "Epoch 6: |          | 484/? [11:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 484, loss 4.363339424133301\n",
      "Epoch 6: |          | 485/? [11:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 485, loss 4.264536380767822\n",
      "Epoch 6: |          | 486/? [11:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 486, loss 3.851219892501831\n",
      "Epoch 6: |          | 487/? [11:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 487, loss 4.160451889038086\n",
      "Epoch 6: |          | 488/? [11:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 488, loss 3.8552937507629395\n",
      "Epoch 6: |          | 489/? [11:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 489, loss 3.4329028129577637\n",
      "Epoch 6: |          | 490/? [11:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 490, loss 4.0445075035095215\n",
      "Epoch 6: |          | 491/? [11:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 491, loss 3.90095591545105\n",
      "Epoch 6: |          | 492/? [11:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 492, loss 3.2367687225341797\n",
      "Epoch 6: |          | 493/? [11:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 493, loss 4.232955455780029\n",
      "Epoch 6: |          | 494/? [11:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 494, loss 4.048954963684082\n",
      "Epoch 6: |          | 495/? [11:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 495, loss 4.175893306732178\n",
      "Epoch 6: |          | 496/? [11:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 496, loss 3.724360704421997\n",
      "Epoch 6: |          | 497/? [11:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 497, loss 4.32661247253418\n",
      "Epoch 6: |          | 498/? [11:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 498, loss 3.929267168045044\n",
      "Epoch 6: |          | 499/? [12:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 499, loss 4.048737049102783\n",
      "Epoch 6: |          | 500/? [12:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 500, loss 3.8344318866729736\n",
      "Epoch 6: |          | 501/? [12:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 501, loss 3.5835330486297607\n",
      "Epoch 6: |          | 502/? [12:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 502, loss 4.039759635925293\n",
      "Epoch 6: |          | 503/? [12:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 503, loss 3.977144241333008\n",
      "Epoch 6: |          | 504/? [12:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 504, loss 3.877033233642578\n",
      "Epoch 6: |          | 505/? [12:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 505, loss 3.323652744293213\n",
      "Epoch 6: |          | 506/? [12:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 506, loss 3.9269096851348877\n",
      "Epoch 6: |          | 507/? [12:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 507, loss 3.9813690185546875\n",
      "Epoch 6: |          | 508/? [12:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 508, loss 4.338693141937256\n",
      "Epoch 6: |          | 509/? [12:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 509, loss 3.6846721172332764\n",
      "Epoch 6: |          | 510/? [12:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 510, loss 4.176557540893555\n",
      "Epoch 6: |          | 511/? [12:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 511, loss 4.0059733390808105\n",
      "Epoch 6: |          | 512/? [12:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 512, loss 3.42938494682312\n",
      "Epoch 6: |          | 513/? [12:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 513, loss 3.6635303497314453\n",
      "Epoch 6: |          | 514/? [12:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 514, loss 3.8332791328430176\n",
      "Epoch 6: |          | 515/? [12:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 515, loss 3.498166561126709\n",
      "Epoch 6: |          | 516/? [12:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 516, loss 3.7742114067077637\n",
      "Epoch 6: |          | 517/? [12:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 517, loss 4.0496087074279785\n",
      "Epoch 6: |          | 518/? [12:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 518, loss 3.580026149749756\n",
      "Epoch 6: |          | 519/? [12:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 519, loss 4.0423583984375\n",
      "Epoch 6: |          | 520/? [12:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 520, loss 3.890671968460083\n",
      "Epoch 6: |          | 521/? [12:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 521, loss 3.874246120452881\n",
      "Epoch 6: |          | 522/? [12:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 522, loss 4.425122261047363\n",
      "Epoch 6: |          | 523/? [12:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 523, loss 4.508099555969238\n",
      "Epoch 6: |          | 524/? [12:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 524, loss 4.29836368560791\n",
      "Epoch 6: |          | 525/? [12:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 525, loss 3.872804641723633\n",
      "Epoch 6: |          | 526/? [12:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 526, loss 3.6583144664764404\n",
      "Epoch 6: |          | 527/? [12:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 527, loss 4.334000110626221\n",
      "Epoch 6: |          | 528/? [12:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 528, loss 4.066571235656738\n",
      "Epoch 6: |          | 529/? [12:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 529, loss 3.718916654586792\n",
      "Epoch 6: |          | 530/? [12:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 530, loss 4.28257417678833\n",
      "Epoch 6: |          | 531/? [12:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 531, loss 3.7863526344299316\n",
      "Epoch 6: |          | 532/? [12:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 532, loss 4.086167335510254\n",
      "Epoch 6: |          | 533/? [12:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 533, loss 3.6845054626464844\n",
      "Epoch 6: |          | 534/? [12:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 534, loss 3.37420654296875\n",
      "Epoch 6: |          | 535/? [12:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 535, loss 3.7462189197540283\n",
      "Epoch 6: |          | 536/? [12:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 536, loss 4.340118408203125\n",
      "Epoch 6: |          | 537/? [12:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 537, loss 4.10707950592041\n",
      "Epoch 6: |          | 538/? [12:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 538, loss 3.725597858428955\n",
      "Epoch 6: |          | 539/? [12:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 539, loss 3.834075927734375\n",
      "Epoch 6: |          | 540/? [12:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 540, loss 4.229968070983887\n",
      "Epoch 6: |          | 541/? [13:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 541, loss 3.998267650604248\n",
      "Epoch 6: |          | 542/? [13:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 542, loss 3.7577900886535645\n",
      "Epoch 6: |          | 543/? [13:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 543, loss 4.1360063552856445\n",
      "Epoch 6: |          | 544/? [13:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 544, loss 4.0642876625061035\n",
      "Epoch 6: |          | 545/? [13:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 545, loss 3.3179402351379395\n",
      "Epoch 6: |          | 546/? [13:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 546, loss 4.066949844360352\n",
      "Epoch 6: |          | 547/? [13:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 547, loss 4.578310966491699\n",
      "Epoch 6: |          | 548/? [13:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 548, loss 4.195261478424072\n",
      "Epoch 6: |          | 549/? [13:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 549, loss 4.079601287841797\n",
      "Epoch 6: |          | 550/? [13:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 550, loss 4.4120683670043945\n",
      "Epoch 6: |          | 551/? [13:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 551, loss 4.0633368492126465\n",
      "Epoch 6: |          | 552/? [13:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 552, loss 4.048737525939941\n",
      "Epoch 6: |          | 553/? [13:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 553, loss 3.509124279022217\n",
      "Epoch 6: |          | 554/? [13:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 554, loss 4.071867942810059\n",
      "Epoch 6: |          | 555/? [13:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 555, loss 4.390310764312744\n",
      "Epoch 6: |          | 556/? [13:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 556, loss 4.136486530303955\n",
      "Epoch 6: |          | 557/? [13:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 557, loss 3.637519121170044\n",
      "Epoch 6: |          | 558/? [13:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 558, loss 3.8669300079345703\n",
      "Epoch 6: |          | 559/? [13:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 559, loss 3.8781464099884033\n",
      "Epoch 6: |          | 560/? [13:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 560, loss 3.350391387939453\n",
      "Epoch 6: |          | 561/? [13:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 561, loss 3.2277913093566895\n",
      "Epoch 6: |          | 562/? [13:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 562, loss 4.262628555297852\n",
      "Epoch 6: |          | 563/? [13:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 563, loss 3.3358254432678223\n",
      "Epoch 6: |          | 564/? [13:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 564, loss 3.804783344268799\n",
      "Epoch 6: |          | 565/? [13:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 565, loss 4.225874423980713\n",
      "Epoch 6: |          | 566/? [13:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 566, loss 4.245905876159668\n",
      "Epoch 6: |          | 567/? [13:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 567, loss 4.38209342956543\n",
      "Epoch 6: |          | 568/? [13:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 568, loss 3.4712681770324707\n",
      "Epoch 6: |          | 569/? [13:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 569, loss 4.092730522155762\n",
      "Epoch 6: |          | 570/? [13:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 570, loss 4.164085388183594\n",
      "Epoch 6: |          | 571/? [13:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 571, loss 3.7419962882995605\n",
      "Epoch 6: |          | 572/? [13:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 572, loss 4.672101974487305\n",
      "Epoch 6: |          | 573/? [13:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 573, loss 3.1073412895202637\n",
      "Epoch 6: |          | 574/? [13:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 574, loss 4.258889198303223\n",
      "Epoch 6: |          | 575/? [13:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 575, loss 3.5977797508239746\n",
      "Epoch 6: |          | 576/? [13:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 576, loss 3.824345827102661\n",
      "Epoch 6: |          | 577/? [13:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 577, loss 4.034512996673584\n",
      "Epoch 6: |          | 578/? [13:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 578, loss 4.267866611480713\n",
      "Epoch 6: |          | 579/? [13:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 579, loss 3.398092746734619\n",
      "Epoch 6: |          | 580/? [13:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 580, loss 4.0596723556518555\n",
      "Epoch 6: |          | 581/? [13:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 581, loss 4.0935187339782715\n",
      "Epoch 6: |          | 582/? [13:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 582, loss 4.172541618347168\n",
      "Epoch 6: |          | 583/? [13:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 583, loss 3.922877073287964\n",
      "Epoch 6: |          | 584/? [13:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 584, loss 4.1239013671875\n",
      "Epoch 6: |          | 585/? [14:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 585, loss 4.132290840148926\n",
      "Epoch 6: |          | 586/? [14:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 586, loss 4.164309978485107\n",
      "Epoch 6: |          | 587/? [14:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 587, loss 4.172266483306885\n",
      "Epoch 6: |          | 588/? [14:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 588, loss 4.120984077453613\n",
      "Epoch 6: |          | 589/? [14:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 589, loss 3.569096088409424\n",
      "Epoch 6: |          | 590/? [14:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 590, loss 4.17620849609375\n",
      "Epoch 6: |          | 591/? [14:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 591, loss 4.060329914093018\n",
      "Epoch 6: |          | 592/? [14:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 592, loss 3.7185795307159424\n",
      "Epoch 6: |          | 593/? [14:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 593, loss 4.006204128265381\n",
      "Epoch 6: |          | 594/? [14:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 594, loss 4.782475471496582\n",
      "Epoch 6: |          | 595/? [14:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 595, loss 3.5471858978271484\n",
      "Epoch 6: |          | 596/? [14:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 596, loss 3.6043834686279297\n",
      "Epoch 6: |          | 597/? [14:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 597, loss 3.837838649749756\n",
      "Epoch 6: |          | 598/? [14:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 598, loss 4.328232288360596\n",
      "Epoch 6: |          | 599/? [14:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 599, loss 3.9577057361602783\n",
      "Epoch 6: |          | 600/? [14:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 600, loss 3.740152359008789\n",
      "Epoch 6: |          | 601/? [14:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 601, loss 4.0092878341674805\n",
      "Epoch 6: |          | 602/? [14:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 602, loss 3.591597318649292\n",
      "Epoch 6: |          | 603/? [14:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 603, loss 3.702838897705078\n",
      "Epoch 6: |          | 604/? [14:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 604, loss 5.2214789390563965\n",
      "Epoch 6: |          | 605/? [14:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 605, loss 3.4389712810516357\n",
      "Epoch 6: |          | 606/? [14:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 606, loss 3.7356560230255127\n",
      "Epoch 6: |          | 607/? [14:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 607, loss 4.063620567321777\n",
      "Epoch 6: |          | 608/? [14:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 608, loss 3.8686416149139404\n",
      "Epoch 6: |          | 609/? [14:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 609, loss 3.7450966835021973\n",
      "Epoch 6: |          | 610/? [14:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 610, loss 3.8405044078826904\n",
      "Epoch 6: |          | 611/? [14:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 611, loss 4.0049967765808105\n",
      "Epoch 6: |          | 612/? [14:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 612, loss 3.735321044921875\n",
      "Epoch 6: |          | 613/? [14:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 613, loss 4.035654544830322\n",
      "Epoch 6: |          | 614/? [14:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 614, loss 3.876171827316284\n",
      "Epoch 6: |          | 615/? [14:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 615, loss 4.3879594802856445\n",
      "Epoch 6: |          | 616/? [14:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 616, loss 4.545779705047607\n",
      "Epoch 6: |          | 617/? [14:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 617, loss 3.1458001136779785\n",
      "Epoch 6: |          | 618/? [14:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 618, loss 4.082376956939697\n",
      "Epoch 6: |          | 619/? [14:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 619, loss 3.614391803741455\n",
      "Epoch 6: |          | 620/? [14:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 620, loss 4.190578937530518\n",
      "Epoch 6: |          | 621/? [14:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 621, loss 3.6463470458984375\n",
      "Epoch 6: |          | 622/? [14:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 622, loss 3.438453197479248\n",
      "Epoch 6: |          | 623/? [14:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 623, loss 3.194118022918701\n",
      "Epoch 6: |          | 624/? [14:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 624, loss 2.917607545852661\n",
      "Epoch 6: |          | 625/? [14:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 625, loss 4.439089775085449\n",
      "Epoch 6: |          | 626/? [14:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 626, loss 3.915445327758789\n",
      "Epoch 6: |          | 627/? [14:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 627, loss 3.8126397132873535\n",
      "Epoch 6: |          | 628/? [15:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 628, loss 3.8007845878601074\n",
      "Epoch 6: |          | 629/? [15:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 629, loss 4.198817253112793\n",
      "Epoch 6: |          | 630/? [15:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 630, loss 3.957517623901367\n",
      "Epoch 6: |          | 631/? [15:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 631, loss 4.075037479400635\n",
      "Epoch 6: |          | 632/? [15:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 632, loss 3.3840079307556152\n",
      "Epoch 6: |          | 633/? [15:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 633, loss 4.1683349609375\n",
      "Epoch 6: |          | 634/? [15:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 634, loss 3.7104086875915527\n",
      "Epoch 6: |          | 635/? [15:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 635, loss 3.5224146842956543\n",
      "Epoch 6: |          | 636/? [15:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 636, loss 3.9577853679656982\n",
      "Epoch 6: |          | 637/? [15:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 637, loss 3.7468323707580566\n",
      "Epoch 6: |          | 638/? [15:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 638, loss 3.9997639656066895\n",
      "Epoch 6: |          | 639/? [15:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 639, loss 3.7806968688964844\n",
      "Epoch 6: |          | 640/? [15:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 640, loss 4.334100723266602\n",
      "Epoch 6: |          | 641/? [15:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 641, loss 3.3431243896484375\n",
      "Epoch 6: |          | 642/? [15:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 642, loss 4.112240791320801\n",
      "Epoch 6: |          | 643/? [15:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 643, loss 4.038760185241699\n",
      "Epoch 6: |          | 644/? [15:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 644, loss 3.978801727294922\n",
      "Epoch 6: |          | 645/? [15:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 645, loss 3.708543062210083\n",
      "Epoch 6: |          | 646/? [15:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 646, loss 3.7755191326141357\n",
      "Epoch 6: |          | 647/? [15:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 647, loss 4.313218116760254\n",
      "Epoch 6: |          | 648/? [15:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 648, loss 3.7363200187683105\n",
      "Epoch 6: |          | 649/? [15:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 649, loss 3.223717451095581\n",
      "Epoch 6: |          | 650/? [15:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 650, loss 4.256617546081543\n",
      "Epoch 6: |          | 651/? [15:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 651, loss 4.379368782043457\n",
      "Epoch 6: |          | 652/? [15:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 652, loss 3.792982816696167\n",
      "Epoch 6: |          | 653/? [15:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 653, loss 3.9662811756134033\n",
      "Epoch 6: |          | 654/? [15:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 654, loss 4.048074245452881\n",
      "Epoch 6: |          | 655/? [15:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 655, loss 3.896214246749878\n",
      "Epoch 6: |          | 656/? [15:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 656, loss 3.496669054031372\n",
      "Epoch 6: |          | 657/? [15:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 657, loss 5.947134494781494\n",
      "Epoch 6: |          | 658/? [15:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 658, loss 3.4793734550476074\n",
      "Epoch 6: |          | 659/? [15:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 659, loss 3.95268177986145\n",
      "Epoch 6: |          | 660/? [15:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 660, loss 4.26556396484375\n",
      "Epoch 6: |          | 661/? [15:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 661, loss 4.194303035736084\n",
      "Epoch 6: |          | 662/? [15:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 662, loss 4.118361949920654\n",
      "Epoch 6: |          | 663/? [15:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 663, loss 3.8512496948242188\n",
      "Epoch 6: |          | 664/? [15:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 664, loss 3.7640697956085205\n",
      "Epoch 6: |          | 665/? [15:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 665, loss 4.068253040313721\n",
      "Epoch 6: |          | 666/? [15:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 666, loss 3.8760929107666016\n",
      "Epoch 6: |          | 667/? [15:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 667, loss 4.712922096252441\n",
      "Epoch 6: |          | 668/? [15:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 668, loss 3.4653878211975098\n",
      "Epoch 6: |          | 669/? [15:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 669, loss 3.6966450214385986\n",
      "Epoch 6: |          | 670/? [15:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 670, loss 4.367516994476318\n",
      "Epoch 6: |          | 671/? [15:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 671, loss 4.114731788635254\n",
      "Epoch 6: |          | 672/? [16:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 672, loss 4.1248369216918945\n",
      "Epoch 6: |          | 673/? [16:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 673, loss 3.96122407913208\n",
      "Epoch 6: |          | 674/? [16:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 674, loss 2.4296207427978516\n",
      "Epoch 6: |          | 675/? [16:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 675, loss 0.9556878805160522\n",
      "Epoch 6: |          | 676/? [16:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 676, loss 0.7963182926177979\n",
      "Epoch 6: |          | 677/? [16:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 677, loss 0.6458992958068848\n",
      "Epoch 6: |          | 678/? [16:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 678, loss 1.7847340106964111\n",
      "Epoch 6: |          | 679/? [16:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 679, loss 3.3930957317352295\n",
      "Epoch 6: |          | 680/? [16:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 680, loss 3.8628451824188232\n",
      "Epoch 6: |          | 681/? [16:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 681, loss 3.405066728591919\n",
      "Epoch 6: |          | 682/? [16:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 682, loss 3.7016196250915527\n",
      "Epoch 6: |          | 683/? [16:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 683, loss 3.4263782501220703\n",
      "Epoch 6: |          | 684/? [16:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 684, loss 4.472364902496338\n",
      "Epoch 6: |          | 685/? [16:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 685, loss 4.103017807006836\n",
      "Epoch 6: |          | 686/? [16:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 686, loss 3.690553665161133\n",
      "Epoch 6: |          | 687/? [16:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 687, loss 4.213141918182373\n",
      "Epoch 6: |          | 688/? [16:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 688, loss 3.767983913421631\n",
      "Epoch 6: |          | 689/? [16:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 689, loss 3.8098628520965576\n",
      "Epoch 6: |          | 690/? [16:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 690, loss 4.4586968421936035\n",
      "Epoch 6: |          | 691/? [16:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 691, loss 3.926295757293701\n",
      "Epoch 6: |          | 692/? [16:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 692, loss 3.9493401050567627\n",
      "Epoch 6: |          | 693/? [16:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 693, loss 4.504432678222656\n",
      "Epoch 6: |          | 694/? [16:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 694, loss 3.8249728679656982\n",
      "Epoch 6: |          | 695/? [16:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 695, loss 4.390143394470215\n",
      "Epoch 6: |          | 696/? [16:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 696, loss 3.6724910736083984\n",
      "Epoch 6: |          | 697/? [16:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 697, loss 3.8775134086608887\n",
      "Epoch 6: |          | 698/? [16:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 698, loss 3.273635149002075\n",
      "Epoch 6: |          | 699/? [16:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 699, loss 4.0375075340271\n",
      "Epoch 6: |          | 700/? [16:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 700, loss 4.112066268920898\n",
      "Epoch 6: |          | 701/? [16:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 701, loss 3.733369827270508\n",
      "Epoch 6: |          | 702/? [16:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 702, loss 4.029990196228027\n",
      "Epoch 6: |          | 703/? [16:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 703, loss 4.157469749450684\n",
      "Epoch 6: |          | 704/? [16:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 704, loss 4.010040760040283\n",
      "Epoch 6: |          | 705/? [16:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 705, loss 3.6390106678009033\n",
      "Epoch 6: |          | 706/? [16:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 706, loss 3.690903902053833\n",
      "Epoch 6: |          | 707/? [16:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 707, loss 4.183091640472412\n",
      "Epoch 6: |          | 708/? [16:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 708, loss 3.9170124530792236\n",
      "Epoch 6: |          | 709/? [16:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 709, loss 3.8418731689453125\n",
      "Epoch 6: |          | 710/? [16:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 710, loss 4.389987945556641\n",
      "Epoch 6: |          | 711/? [16:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 711, loss 4.500166893005371\n",
      "Epoch 6: |          | 712/? [16:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 712, loss 4.141929626464844\n",
      "Epoch 6: |          | 713/? [16:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 713, loss 4.236547470092773\n",
      "Epoch 6: |          | 714/? [16:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 714, loss 4.316331386566162\n",
      "Epoch 6: |          | 715/? [17:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 715, loss 3.2580063343048096\n",
      "Epoch 6: |          | 716/? [17:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 716, loss 3.954862117767334\n",
      "Epoch 6: |          | 717/? [17:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 717, loss 3.827666759490967\n",
      "Epoch 6: |          | 718/? [17:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 718, loss 3.3691844940185547\n",
      "Epoch 6: |          | 719/? [17:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 719, loss 3.9070956707000732\n",
      "Epoch 6: |          | 720/? [17:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 720, loss 3.5691275596618652\n",
      "Epoch 6: |          | 721/? [17:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 721, loss 4.286246299743652\n",
      "Epoch 6: |          | 722/? [17:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 722, loss 3.555898666381836\n",
      "Epoch 6: |          | 723/? [17:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 723, loss 4.127274990081787\n",
      "Epoch 6: |          | 724/? [17:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 724, loss 3.6871209144592285\n",
      "Epoch 6: |          | 725/? [17:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 725, loss 3.618170976638794\n",
      "Epoch 6: |          | 726/? [17:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 726, loss 3.8024985790252686\n",
      "Epoch 6: |          | 727/? [17:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 727, loss 3.591614246368408\n",
      "Epoch 6: |          | 728/? [17:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 728, loss 3.398056745529175\n",
      "Epoch 6: |          | 729/? [17:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 729, loss 3.8978335857391357\n",
      "Epoch 6: |          | 730/? [17:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 730, loss 3.834383487701416\n",
      "Epoch 6: |          | 731/? [17:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 731, loss 3.9769225120544434\n",
      "Epoch 6: |          | 732/? [17:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 732, loss 4.256433486938477\n",
      "Epoch 6: |          | 733/? [17:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 733, loss 3.961446762084961\n",
      "Epoch 6: |          | 734/? [17:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 734, loss 4.175007343292236\n",
      "Epoch 6: |          | 735/? [17:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 735, loss 4.051124572753906\n",
      "Epoch 6: |          | 736/? [17:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 736, loss 3.5799922943115234\n",
      "Epoch 6: |          | 737/? [17:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 737, loss 4.353141784667969\n",
      "Epoch 6: |          | 738/? [17:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 738, loss 3.547445297241211\n",
      "Epoch 6: |          | 739/? [17:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 739, loss 4.037555694580078\n",
      "Epoch 6: |          | 740/? [17:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 740, loss 3.7194931507110596\n",
      "Epoch 6: |          | 741/? [17:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 741, loss 3.8997302055358887\n",
      "Epoch 6: |          | 742/? [17:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 742, loss 4.286571502685547\n",
      "Epoch 6: |          | 743/? [17:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 743, loss 4.134167671203613\n",
      "Epoch 6: |          | 744/? [17:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 744, loss 4.088578224182129\n",
      "Epoch 6: |          | 745/? [17:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 745, loss 3.721951723098755\n",
      "Epoch 6: |          | 746/? [17:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 746, loss 3.99153208732605\n",
      "Epoch 6: |          | 747/? [17:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 747, loss 3.6995606422424316\n",
      "Epoch 6: |          | 748/? [17:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 748, loss 2.7694461345672607\n",
      "Epoch 6: |          | 749/? [17:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 749, loss 3.8947391510009766\n",
      "Epoch 6: |          | 750/? [17:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 750, loss 4.19351863861084\n",
      "Epoch 6: |          | 751/? [17:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 751, loss 2.4867892265319824\n",
      "Epoch 6: |          | 752/? [17:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 752, loss 4.068193435668945\n",
      "Epoch 6: |          | 753/? [17:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 753, loss 3.2579925060272217\n",
      "Epoch 6: |          | 754/? [17:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 754, loss 3.69396710395813\n",
      "Epoch 6: |          | 755/? [17:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 755, loss 3.4974617958068848\n",
      "Epoch 6: |          | 756/? [17:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 756, loss 3.9089722633361816\n",
      "Epoch 6: |          | 757/? [17:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 757, loss 3.9727752208709717\n",
      "Epoch 6: |          | 758/? [18:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 758, loss 3.698439359664917\n",
      "Epoch 6: |          | 759/? [18:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 759, loss 3.6671385765075684\n",
      "Epoch 6: |          | 760/? [18:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 760, loss 4.1811699867248535\n",
      "Epoch 6: |          | 761/? [18:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 761, loss 4.170533180236816\n",
      "Epoch 6: |          | 762/? [18:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 762, loss 3.8222384452819824\n",
      "Epoch 6: |          | 763/? [18:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 763, loss 4.017195701599121\n",
      "Epoch 6: |          | 764/? [18:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 764, loss 4.280022621154785\n",
      "Epoch 6: |          | 765/? [18:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 765, loss 4.011744976043701\n",
      "Epoch 6: |          | 766/? [18:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 766, loss 4.41512393951416\n",
      "Epoch 6: |          | 767/? [18:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 767, loss 4.462737083435059\n",
      "Epoch 6: |          | 768/? [18:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 768, loss 4.022838115692139\n",
      "Epoch 6: |          | 769/? [18:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 769, loss 3.2151713371276855\n",
      "Epoch 6: |          | 770/? [18:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 770, loss 3.8217926025390625\n",
      "Epoch 6: |          | 771/? [18:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 771, loss 4.509007930755615\n",
      "Epoch 6: |          | 772/? [18:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 772, loss 4.211726188659668\n",
      "Epoch 6: |          | 773/? [18:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 773, loss 3.9158928394317627\n",
      "Epoch 6: |          | 774/? [18:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 774, loss 4.037113189697266\n",
      "Epoch 6: |          | 775/? [18:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 775, loss 4.4757609367370605\n",
      "Epoch 6: |          | 776/? [18:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 776, loss 3.907907485961914\n",
      "Epoch 6: |          | 777/? [18:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 777, loss 3.8053932189941406\n",
      "Epoch 6: |          | 778/? [18:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 778, loss 4.169790267944336\n",
      "Epoch 6: |          | 779/? [18:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 779, loss 4.629038333892822\n",
      "Epoch 6: |          | 780/? [18:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 780, loss 3.636091709136963\n",
      "Epoch 6: |          | 781/? [18:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 781, loss 3.731942653656006\n",
      "Epoch 6: |          | 782/? [18:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 782, loss 4.1066484451293945\n",
      "Epoch 6: |          | 783/? [18:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 783, loss 4.2006001472473145\n",
      "Epoch 6: |          | 784/? [18:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 784, loss 3.755176067352295\n",
      "Epoch 6: |          | 785/? [18:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 785, loss 3.5127663612365723\n",
      "Epoch 6: |          | 786/? [18:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 786, loss 4.4058003425598145\n",
      "Epoch 6: |          | 787/? [18:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 787, loss 4.338479995727539\n",
      "Epoch 6: |          | 788/? [18:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 788, loss 2.3423850536346436\n",
      "Epoch 6: |          | 789/? [18:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 789, loss 3.8596222400665283\n",
      "Epoch 6: |          | 790/? [18:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 790, loss 4.699974536895752\n",
      "Epoch 6: |          | 791/? [18:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 791, loss 4.4456071853637695\n",
      "Epoch 6: |          | 792/? [18:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 792, loss 3.545832872390747\n",
      "Epoch 6: |          | 793/? [18:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 793, loss 4.101241111755371\n",
      "Epoch 6: |          | 794/? [18:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 794, loss 4.420241355895996\n",
      "Epoch 6: |          | 795/? [18:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 795, loss 3.891805648803711\n",
      "Epoch 6: |          | 796/? [18:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 796, loss 4.287242412567139\n",
      "Epoch 6: |          | 797/? [18:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 797, loss 3.2832095623016357\n",
      "Epoch 6: |          | 798/? [18:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 798, loss 3.389411449432373\n",
      "Epoch 6: |          | 799/? [18:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 799, loss 4.367539882659912\n",
      "Epoch 6: |          | 800/? [18:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 800, loss 4.1320624351501465\n",
      "Epoch 6: |          | 801/? [19:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 801, loss 3.753645420074463\n",
      "Epoch 6: |          | 802/? [19:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 802, loss 4.042922496795654\n",
      "Epoch 6: |          | 803/? [19:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 803, loss 3.864968776702881\n",
      "Epoch 6: |          | 804/? [19:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 804, loss 4.036669731140137\n",
      "Epoch 6: |          | 805/? [19:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 805, loss 4.198442459106445\n",
      "Epoch 6: |          | 806/? [19:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 806, loss 4.608244895935059\n",
      "Epoch 6: |          | 807/? [19:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 807, loss 3.965977907180786\n",
      "Epoch 6: |          | 808/? [19:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 808, loss 3.6131815910339355\n",
      "Epoch 6: |          | 809/? [19:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 809, loss 4.152590751647949\n",
      "Epoch 6: |          | 810/? [19:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 810, loss 3.9173145294189453\n",
      "Epoch 6: |          | 811/? [19:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 811, loss 4.187037944793701\n",
      "Epoch 6: |          | 812/? [19:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 812, loss 4.798855781555176\n",
      "Epoch 6: |          | 813/? [19:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 813, loss 4.586260795593262\n",
      "Epoch 6: |          | 814/? [19:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 814, loss 3.595090389251709\n",
      "Epoch 6: |          | 815/? [19:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 815, loss 4.311191082000732\n",
      "Epoch 6: |          | 816/? [19:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 816, loss 4.127405166625977\n",
      "Epoch 6: |          | 817/? [19:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 817, loss 3.4594273567199707\n",
      "Epoch 6: |          | 818/? [19:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 818, loss 4.4519267082214355\n",
      "Epoch 6: |          | 819/? [19:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 819, loss 4.133593559265137\n",
      "Epoch 6: |          | 820/? [19:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 820, loss 3.9902443885803223\n",
      "Epoch 6: |          | 821/? [19:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 821, loss 3.960287570953369\n",
      "Epoch 6: |          | 822/? [19:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 822, loss 3.609283924102783\n",
      "Epoch 6: |          | 823/? [19:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 823, loss 3.608431577682495\n",
      "Epoch 6: |          | 824/? [19:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 824, loss 4.11782693862915\n",
      "Epoch 6: |          | 825/? [19:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 825, loss 3.6297106742858887\n",
      "Epoch 6: |          | 826/? [19:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 826, loss 4.174102783203125\n",
      "Epoch 6: |          | 827/? [19:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 827, loss 3.7811119556427\n",
      "Epoch 6: |          | 828/? [19:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 828, loss 4.318530082702637\n",
      "Epoch 6: |          | 829/? [19:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 829, loss 3.985764741897583\n",
      "Epoch 6: |          | 830/? [19:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 830, loss 4.522642135620117\n",
      "Epoch 6: |          | 831/? [19:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 831, loss 2.4356722831726074\n",
      "Epoch 6: |          | 832/? [19:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 832, loss 3.9560999870300293\n",
      "Epoch 6: |          | 833/? [19:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 833, loss 3.8253390789031982\n",
      "Epoch 6: |          | 834/? [19:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 834, loss 4.575484275817871\n",
      "Epoch 6: |          | 835/? [19:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 835, loss 3.898162841796875\n",
      "Epoch 6: |          | 836/? [19:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 836, loss 4.5920281410217285\n",
      "Epoch 6: |          | 837/? [19:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 837, loss 4.034050941467285\n",
      "Epoch 6: |          | 838/? [19:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 838, loss 3.3404955863952637\n",
      "Epoch 6: |          | 839/? [19:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 839, loss 3.724915027618408\n",
      "Epoch 6: |          | 840/? [20:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 840, loss 4.266106605529785\n",
      "Epoch 6: |          | 841/? [20:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 841, loss 4.297207832336426\n",
      "Epoch 6: |          | 842/? [20:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 842, loss 3.9535903930664062\n",
      "Epoch 6: |          | 843/? [20:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 843, loss 4.279308319091797\n",
      "Epoch 6: |          | 844/? [20:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 844, loss 3.670541286468506\n",
      "Epoch 6: |          | 845/? [20:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 845, loss 4.073829174041748\n",
      "Epoch 6: |          | 846/? [20:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 846, loss 4.510012149810791\n",
      "Epoch 6: |          | 847/? [20:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 847, loss 4.076827049255371\n",
      "Epoch 6: |          | 848/? [20:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 848, loss 3.6443684101104736\n",
      "Epoch 6: |          | 849/? [20:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 849, loss 3.728787899017334\n",
      "Epoch 6: |          | 850/? [20:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 850, loss 3.8454031944274902\n",
      "Epoch 6: |          | 851/? [20:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 851, loss 4.141066074371338\n",
      "Epoch 6: |          | 852/? [20:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 852, loss 4.271228790283203\n",
      "Epoch 6: |          | 853/? [20:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 853, loss 4.096083641052246\n",
      "Epoch 6: |          | 854/? [20:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 854, loss 3.4071242809295654\n",
      "Epoch 6: |          | 855/? [20:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 855, loss 3.706634521484375\n",
      "Epoch 6: |          | 856/? [20:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 856, loss 3.6459128856658936\n",
      "Epoch 6: |          | 857/? [20:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 857, loss 4.1946821212768555\n",
      "Epoch 6: |          | 858/? [20:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 858, loss 4.080772399902344\n",
      "Epoch 6: |          | 859/? [20:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 859, loss 4.065242290496826\n",
      "Epoch 6: |          | 860/? [20:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 860, loss 4.469023704528809\n",
      "Epoch 6: |          | 861/? [20:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 861, loss 3.763043165206909\n",
      "Epoch 6: |          | 862/? [20:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 862, loss 4.114439010620117\n",
      "Epoch 6: |          | 863/? [20:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 863, loss 3.4810917377471924\n",
      "Epoch 6: |          | 864/? [20:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 864, loss 4.037772178649902\n",
      "Epoch 6: |          | 865/? [20:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 865, loss 3.9912173748016357\n",
      "Epoch 6: |          | 866/? [20:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 866, loss 3.066401243209839\n",
      "Epoch 6: |          | 867/? [20:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 867, loss 3.241669178009033\n",
      "Epoch 6: |          | 868/? [20:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 868, loss 4.11544132232666\n",
      "Epoch 6: |          | 869/? [20:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 869, loss 4.169666290283203\n",
      "Epoch 6: |          | 870/? [20:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 870, loss 3.7453975677490234\n",
      "Epoch 6: |          | 871/? [20:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 871, loss 4.092267990112305\n",
      "Epoch 6: |          | 872/? [20:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 872, loss 3.925335645675659\n",
      "Epoch 6: |          | 873/? [20:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 873, loss 3.919887065887451\n",
      "Epoch 6: |          | 874/? [20:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 874, loss 3.4026546478271484\n",
      "Epoch 6: |          | 875/? [20:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 875, loss 4.168244361877441\n",
      "Epoch 6: |          | 876/? [20:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 876, loss 3.7069332599639893\n",
      "Epoch 6: |          | 877/? [20:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 877, loss 4.07045316696167\n",
      "Epoch 6: |          | 878/? [20:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 878, loss 3.5075595378875732\n",
      "Epoch 6: |          | 879/? [20:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 879, loss 3.5812160968780518\n",
      "Epoch 6: |          | 880/? [20:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 880, loss 4.682540416717529\n",
      "Epoch 6: |          | 881/? [20:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 881, loss 4.074554920196533\n",
      "Epoch 6: |          | 882/? [20:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 882, loss 3.852949619293213\n",
      "Epoch 6: |          | 883/? [21:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 883, loss 3.9868602752685547\n",
      "Epoch 6: |          | 884/? [21:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 884, loss 4.0535197257995605\n",
      "Epoch 6: |          | 885/? [21:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 885, loss 3.8129591941833496\n",
      "Epoch 6: |          | 886/? [21:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 886, loss 4.463368892669678\n",
      "Epoch 6: |          | 887/? [21:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 887, loss 4.4868483543396\n",
      "Epoch 6: |          | 888/? [21:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 888, loss 4.191684722900391\n",
      "Epoch 6: |          | 889/? [21:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 889, loss 3.7273261547088623\n",
      "Epoch 6: |          | 890/? [21:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 890, loss 3.9996962547302246\n",
      "Epoch 6: |          | 891/? [21:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 891, loss 3.7767739295959473\n",
      "Epoch 6: |          | 892/? [21:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 892, loss 4.4092302322387695\n",
      "Epoch 6: |          | 893/? [21:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 893, loss 3.832855701446533\n",
      "Epoch 6: |          | 894/? [21:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 894, loss 3.380402088165283\n",
      "Epoch 6: |          | 895/? [21:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 895, loss 4.506220817565918\n",
      "Epoch 6: |          | 896/? [21:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 896, loss 4.09818172454834\n",
      "Epoch 6: |          | 897/? [21:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 897, loss 4.088569164276123\n",
      "Epoch 6: |          | 898/? [21:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 898, loss 4.086297512054443\n",
      "Epoch 6: |          | 899/? [21:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 899, loss 3.841351270675659\n",
      "Epoch 6: |          | 900/? [21:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 900, loss 3.7549967765808105\n",
      "Epoch 6: |          | 901/? [21:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 901, loss 4.191091537475586\n",
      "Epoch 6: |          | 902/? [21:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 902, loss 4.2950215339660645\n",
      "Epoch 6: |          | 903/? [21:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 903, loss 3.598201274871826\n",
      "Epoch 6: |          | 904/? [21:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 904, loss 4.067482948303223\n",
      "Epoch 6: |          | 905/? [21:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 905, loss 4.263884544372559\n",
      "Epoch 6: |          | 906/? [21:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 906, loss 4.016211032867432\n",
      "Epoch 6: |          | 907/? [21:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 907, loss 4.063752174377441\n",
      "Epoch 6: |          | 908/? [21:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 908, loss 4.113626003265381\n",
      "Epoch 6: |          | 909/? [21:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 909, loss 4.122805595397949\n",
      "Epoch 6: |          | 910/? [21:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 910, loss 3.8762459754943848\n",
      "Epoch 6: |          | 911/? [21:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 911, loss 3.961385726928711\n",
      "Epoch 6: |          | 912/? [21:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 912, loss 3.8987879753112793\n",
      "Epoch 6: |          | 913/? [21:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 913, loss 3.9296772480010986\n",
      "Epoch 6: |          | 914/? [21:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 914, loss 4.202093601226807\n",
      "Epoch 6: |          | 915/? [21:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 915, loss 4.037795066833496\n",
      "Epoch 6: |          | 916/? [21:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 916, loss 3.9274845123291016\n",
      "Epoch 6: |          | 917/? [21:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 917, loss 3.8796634674072266\n",
      "Epoch 6: |          | 918/? [21:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 918, loss 3.829943895339966\n",
      "Epoch 6: |          | 919/? [21:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 919, loss 3.8269622325897217\n",
      "Epoch 6: |          | 920/? [21:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 920, loss 3.9928882122039795\n",
      "Epoch 6: |          | 921/? [21:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 921, loss 3.8171164989471436\n",
      "Epoch 6: |          | 922/? [21:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 922, loss 3.9732136726379395\n",
      "Epoch 6: |          | 923/? [21:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 923, loss 3.828761339187622\n",
      "Epoch 6: |          | 924/? [21:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 924, loss 3.797931671142578\n",
      "Epoch 6: |          | 925/? [21:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 925, loss 4.122903347015381\n",
      "Epoch 6: |          | 926/? [22:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 926, loss 3.916721820831299\n",
      "Epoch 6: |          | 927/? [22:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 927, loss 4.1879472732543945\n",
      "Epoch 6: |          | 928/? [22:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 928, loss 3.6624927520751953\n",
      "Epoch 6: |          | 929/? [22:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 929, loss 3.7882137298583984\n",
      "Epoch 6: |          | 930/? [22:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 930, loss 3.677633285522461\n",
      "Epoch 6: |          | 931/? [22:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 931, loss 3.4104480743408203\n",
      "Epoch 6: |          | 932/? [22:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 932, loss 4.039383411407471\n",
      "Epoch 6: |          | 933/? [22:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 933, loss 3.768749713897705\n",
      "Epoch 6: |          | 934/? [22:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 934, loss 4.367408752441406\n",
      "Epoch 6: |          | 935/? [22:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 935, loss 4.707097053527832\n",
      "Epoch 6: |          | 936/? [22:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 936, loss 3.926607608795166\n",
      "Epoch 6: |          | 937/? [22:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 937, loss 3.9072234630584717\n",
      "Epoch 6: |          | 938/? [22:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 938, loss 3.8451759815216064\n",
      "Epoch 6: |          | 939/? [22:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 939, loss 4.080564975738525\n",
      "Epoch 6: |          | 940/? [22:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 940, loss 4.283920764923096\n",
      "Epoch 6: |          | 941/? [22:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 941, loss 3.8195621967315674\n",
      "Epoch 6: |          | 942/? [22:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 942, loss 3.310349225997925\n",
      "Epoch 6: |          | 943/? [22:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 943, loss 4.205583572387695\n",
      "Epoch 6: |          | 944/? [22:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 944, loss 3.228410243988037\n",
      "Epoch 6: |          | 945/? [22:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 945, loss 3.973278045654297\n",
      "Epoch 6: |          | 946/? [22:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 946, loss 3.8783233165740967\n",
      "Epoch 6: |          | 947/? [22:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 947, loss 3.8148319721221924\n",
      "Epoch 6: |          | 948/? [22:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 948, loss 4.064317226409912\n",
      "Epoch 6: |          | 949/? [22:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 949, loss 3.9070663452148438\n",
      "Epoch 6: |          | 950/? [22:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 950, loss 3.693711042404175\n",
      "Epoch 6: |          | 951/? [22:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 951, loss 4.326189994812012\n",
      "Epoch 6: |          | 952/? [22:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 952, loss 4.302020072937012\n",
      "Epoch 6: |          | 953/? [22:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 953, loss 4.836291790008545\n",
      "Epoch 6: |          | 954/? [22:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 954, loss 3.8150432109832764\n",
      "Epoch 6: |          | 955/? [22:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 955, loss 4.4560956954956055\n",
      "Epoch 6: |          | 956/? [22:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 956, loss 3.8847179412841797\n",
      "Epoch 6: |          | 957/? [22:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 957, loss 4.08076286315918\n",
      "Epoch 6: |          | 958/? [22:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 958, loss 4.158352851867676\n",
      "Epoch 6: |          | 959/? [22:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 959, loss 3.7233142852783203\n",
      "Epoch 6: |          | 960/? [22:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 960, loss 4.21035099029541\n",
      "Epoch 6: |          | 961/? [22:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 961, loss 4.466092109680176\n",
      "Epoch 6: |          | 962/? [22:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 962, loss 3.9755587577819824\n",
      "Epoch 6: |          | 963/? [22:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 963, loss 3.761219024658203\n",
      "Epoch 6: |          | 964/? [22:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 964, loss 4.233347415924072\n",
      "Epoch 6: |          | 965/? [22:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 965, loss 3.6913247108459473\n",
      "Epoch 6: |          | 966/? [22:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 966, loss 3.561534881591797\n",
      "Epoch 6: |          | 967/? [22:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 967, loss 3.8075804710388184\n",
      "Epoch 6: |          | 968/? [22:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 968, loss 3.756718158721924\n",
      "Epoch 6: |          | 969/? [22:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 969, loss 3.6165060997009277\n",
      "Epoch 6: |          | 970/? [23:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 970, loss 4.113177299499512\n",
      "Epoch 6: |          | 971/? [23:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 971, loss 4.330474853515625\n",
      "Epoch 6: |          | 972/? [23:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 972, loss 3.7690491676330566\n",
      "Epoch 6: |          | 973/? [23:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 973, loss 3.9214301109313965\n",
      "Epoch 6: |          | 974/? [23:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 974, loss 3.9426865577697754\n",
      "Epoch 6: |          | 975/? [23:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 975, loss 3.987898588180542\n",
      "Epoch 6: |          | 976/? [23:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 976, loss 4.0519843101501465\n",
      "Epoch 6: |          | 977/? [23:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 977, loss 4.656042575836182\n",
      "Epoch 6: |          | 978/? [23:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 978, loss 4.081768035888672\n",
      "Epoch 6: |          | 979/? [23:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 979, loss 4.416804790496826\n",
      "Epoch 6: |          | 980/? [23:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 980, loss 3.522765636444092\n",
      "Epoch 6: |          | 981/? [23:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 981, loss 3.346881866455078\n",
      "Epoch 6: |          | 982/? [23:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 982, loss 4.0074052810668945\n",
      "Epoch 6: |          | 983/? [23:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 983, loss 4.438352108001709\n",
      "Epoch 6: |          | 984/? [23:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 984, loss 3.5040481090545654\n",
      "Epoch 6: |          | 985/? [23:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 985, loss 3.7441391944885254\n",
      "Epoch 6: |          | 986/? [23:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 986, loss 3.7884230613708496\n",
      "Epoch 6: |          | 987/? [23:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 987, loss 3.299011707305908\n",
      "Epoch 6: |          | 988/? [23:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 988, loss 4.327253341674805\n",
      "Epoch 6: |          | 989/? [23:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 989, loss 3.958775281906128\n",
      "Epoch 6: |          | 990/? [23:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 990, loss 3.2953503131866455\n",
      "Epoch 6: |          | 991/? [23:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 991, loss 4.038257598876953\n",
      "Epoch 6: |          | 992/? [23:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 992, loss 4.72258996963501\n",
      "Epoch 6: |          | 993/? [23:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 993, loss 3.8241488933563232\n",
      "Epoch 6: |          | 994/? [23:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 994, loss 3.837998151779175\n",
      "Epoch 6: |          | 995/? [23:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 995, loss 4.217339515686035\n",
      "Epoch 6: |          | 996/? [23:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 996, loss 4.233534336090088\n",
      "Epoch 6: |          | 997/? [23:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 997, loss 3.8474414348602295\n",
      "Epoch 6: |          | 998/? [23:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 998, loss 4.09774923324585\n",
      "Epoch 6: |          | 999/? [23:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 999, loss 4.083395957946777\n",
      "Epoch 6: |          | 1000/? [23:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1000, loss 3.571474075317383\n",
      "Epoch 6: |          | 1001/? [23:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1001, loss 4.257216453552246\n",
      "Epoch 6: |          | 1002/? [23:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1002, loss 4.173453330993652\n",
      "Epoch 6: |          | 1003/? [23:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1003, loss 4.373744010925293\n",
      "Epoch 6: |          | 1004/? [23:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1004, loss 3.4354186058044434\n",
      "Epoch 6: |          | 1005/? [23:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1005, loss 3.9432950019836426\n",
      "Epoch 6: |          | 1006/? [23:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1006, loss 4.269792079925537\n",
      "Epoch 6: |          | 1007/? [23:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1007, loss 3.8258023262023926\n",
      "Epoch 6: |          | 1008/? [23:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1008, loss 3.9893202781677246\n",
      "Epoch 6: |          | 1009/? [23:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1009, loss 4.265578269958496\n",
      "Epoch 6: |          | 1010/? [23:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1010, loss 3.391599178314209\n",
      "Epoch 6: |          | 1011/? [23:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1011, loss 3.976426601409912\n",
      "Epoch 6: |          | 1012/? [23:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1012, loss 3.77319598197937\n",
      "Epoch 6: |          | 1013/? [24:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1013, loss 3.9560916423797607\n",
      "Epoch 6: |          | 1014/? [24:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1014, loss 4.385380744934082\n",
      "Epoch 6: |          | 1015/? [24:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1015, loss 4.065549850463867\n",
      "Epoch 6: |          | 1016/? [24:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1016, loss 3.802602767944336\n",
      "Epoch 6: |          | 1017/? [24:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1017, loss 3.2353644371032715\n",
      "Epoch 6: |          | 1018/? [24:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1018, loss 3.888035535812378\n",
      "Epoch 6: |          | 1019/? [24:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1019, loss 3.943453550338745\n",
      "Epoch 6: |          | 1020/? [24:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1020, loss 3.5649638175964355\n",
      "Epoch 6: |          | 1021/? [24:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1021, loss 3.7961363792419434\n",
      "Epoch 6: |          | 1022/? [24:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1022, loss 3.5431926250457764\n",
      "Epoch 6: |          | 1023/? [24:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1023, loss 3.3166470527648926\n",
      "Epoch 6: |          | 1024/? [24:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1024, loss 3.7871081829071045\n",
      "Epoch 6: |          | 1025/? [24:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1025, loss 3.6829609870910645\n",
      "Epoch 6: |          | 1026/? [24:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1026, loss 2.8158209323883057\n",
      "Epoch 6: |          | 1027/? [24:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1027, loss 4.048303127288818\n",
      "Epoch 6: |          | 1028/? [24:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1028, loss 3.87135648727417\n",
      "Epoch 6: |          | 1029/? [24:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1029, loss 3.730409622192383\n",
      "Epoch 6: |          | 1030/? [24:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1030, loss 3.6049602031707764\n",
      "Epoch 6: |          | 1031/? [24:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1031, loss 3.6582865715026855\n",
      "Epoch 6: |          | 1032/? [24:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1032, loss 4.085532188415527\n",
      "Epoch 6: |          | 1033/? [24:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1033, loss 4.3214192390441895\n",
      "Epoch 6: |          | 1034/? [24:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1034, loss 3.687391757965088\n",
      "Epoch 6: |          | 1035/? [24:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1035, loss 3.7075564861297607\n",
      "Epoch 6: |          | 1036/? [24:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1036, loss 3.6373825073242188\n",
      "Epoch 6: |          | 1037/? [24:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1037, loss 4.237988471984863\n",
      "Epoch 6: |          | 1038/? [24:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1038, loss 4.416033744812012\n",
      "Epoch 6: |          | 1039/? [24:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1039, loss 4.7363810539245605\n",
      "Epoch 6: |          | 1040/? [24:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1040, loss 4.0133819580078125\n",
      "Epoch 6: |          | 1041/? [24:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1041, loss 4.325948238372803\n",
      "Epoch 6: |          | 1042/? [24:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1042, loss 3.9279918670654297\n",
      "Epoch 6: |          | 1043/? [24:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1043, loss 4.308229923248291\n",
      "Epoch 6: |          | 1044/? [24:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1044, loss 3.8902363777160645\n",
      "Epoch 6: |          | 1045/? [24:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1045, loss 3.4135384559631348\n",
      "Epoch 6: |          | 1046/? [24:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1046, loss 3.251859188079834\n",
      "Epoch 6: |          | 1047/? [24:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1047, loss 4.461073875427246\n",
      "Epoch 6: |          | 1048/? [24:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1048, loss 3.877427339553833\n",
      "Epoch 6: |          | 1049/? [24:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1049, loss 4.088685512542725\n",
      "Epoch 6: |          | 1050/? [24:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1050, loss 3.6570677757263184\n",
      "Epoch 6: |          | 1051/? [24:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1051, loss 3.6010048389434814\n",
      "Epoch 6: |          | 1052/? [24:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1052, loss 4.200571060180664\n",
      "Epoch 6: |          | 1053/? [24:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1053, loss 4.410467624664307\n",
      "Epoch 6: |          | 1054/? [24:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1054, loss 3.78171968460083\n",
      "Epoch 6: |          | 1055/? [24:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1055, loss 3.4803779125213623\n",
      "Epoch 6: |          | 1056/? [25:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1056, loss 3.494588851928711\n",
      "Epoch 6: |          | 1057/? [25:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1057, loss 4.134366035461426\n",
      "Epoch 6: |          | 1058/? [25:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1058, loss 3.674778699874878\n",
      "Epoch 6: |          | 1059/? [25:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1059, loss 4.28427791595459\n",
      "Epoch 6: |          | 1060/? [25:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1060, loss 4.168435096740723\n",
      "Epoch 6: |          | 1061/? [25:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1061, loss 2.866785764694214\n",
      "Epoch 6: |          | 1062/? [25:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1062, loss 3.830993175506592\n",
      "Epoch 6: |          | 1063/? [25:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1063, loss 3.9034836292266846\n",
      "Epoch 6: |          | 1064/? [25:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1064, loss 4.0792741775512695\n",
      "Epoch 6: |          | 1065/? [25:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1065, loss 2.711784839630127\n",
      "Epoch 6: |          | 1066/? [25:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1066, loss 4.001837253570557\n",
      "Epoch 6: |          | 1067/? [25:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1067, loss 3.5342202186584473\n",
      "Epoch 6: |          | 1068/? [25:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1068, loss 3.6782708168029785\n",
      "Epoch 6: |          | 1069/? [25:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1069, loss 4.0859198570251465\n",
      "Epoch 6: |          | 1070/? [25:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1070, loss 3.8102078437805176\n",
      "Epoch 6: |          | 1071/? [25:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1071, loss 4.236642360687256\n",
      "Epoch 6: |          | 1072/? [25:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1072, loss 4.250619888305664\n",
      "Epoch 6: |          | 1073/? [25:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1073, loss 4.428830146789551\n",
      "Epoch 6: |          | 1074/? [25:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1074, loss 3.7502541542053223\n",
      "Epoch 6: |          | 1075/? [25:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1075, loss 3.547297716140747\n",
      "Epoch 6: |          | 1076/? [25:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1076, loss 4.121417999267578\n",
      "Epoch 6: |          | 1077/? [25:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1077, loss 3.6492786407470703\n",
      "Epoch 6: |          | 1078/? [25:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1078, loss 3.9591457843780518\n",
      "Epoch 6: |          | 1079/? [25:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1079, loss 4.367459774017334\n",
      "Epoch 6: |          | 1080/? [25:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1080, loss 3.8806986808776855\n",
      "Epoch 6: |          | 1081/? [25:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1081, loss 4.175367832183838\n",
      "Epoch 6: |          | 1082/? [25:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1082, loss 3.755993366241455\n",
      "Epoch 6: |          | 1083/? [25:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1083, loss 3.336777925491333\n",
      "Epoch 6: |          | 1084/? [25:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1084, loss 3.1347570419311523\n",
      "Epoch 6: |          | 1085/? [25:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1085, loss 3.8129754066467285\n",
      "Epoch 6: |          | 1086/? [25:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1086, loss 4.098802089691162\n",
      "Epoch 6: |          | 1087/? [25:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1087, loss 4.5981245040893555\n",
      "Epoch 6: |          | 1088/? [25:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1088, loss 4.160737991333008\n",
      "Epoch 6: |          | 1089/? [25:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1089, loss 4.099112510681152\n",
      "Epoch 6: |          | 1090/? [25:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1090, loss 3.9384829998016357\n",
      "Epoch 6: |          | 1091/? [25:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1091, loss 3.704798936843872\n",
      "Epoch 6: |          | 1092/? [25:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1092, loss 4.00009822845459\n",
      "Epoch 6: |          | 1093/? [25:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1093, loss 3.4687561988830566\n",
      "Epoch 6: |          | 1094/? [25:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1094, loss 3.968104124069214\n",
      "Epoch 6: |          | 1095/? [25:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1095, loss 4.051307201385498\n",
      "Epoch 6: |          | 1096/? [25:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1096, loss 4.285185813903809\n",
      "Epoch 6: |          | 1097/? [25:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1097, loss 3.84472393989563\n",
      "Epoch 6: |          | 1098/? [25:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1098, loss 3.1336770057678223\n",
      "Epoch 6: |          | 1099/? [26:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1099, loss 3.82275652885437\n",
      "Epoch 6: |          | 1100/? [26:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1100, loss 4.011540412902832\n",
      "Epoch 6: |          | 1101/? [26:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1101, loss 3.6553025245666504\n",
      "Epoch 6: |          | 1102/? [26:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1102, loss 4.392780303955078\n",
      "Epoch 6: |          | 1103/? [26:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1103, loss 4.842522621154785\n",
      "Epoch 6: |          | 1104/? [26:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1104, loss 4.153234004974365\n",
      "Epoch 6: |          | 1105/? [26:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1105, loss 4.324027061462402\n",
      "Epoch 6: |          | 1106/? [26:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1106, loss 3.829843044281006\n",
      "Epoch 6: |          | 1107/? [26:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1107, loss 3.9444222450256348\n",
      "Epoch 6: |          | 1108/? [26:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1108, loss 3.9273269176483154\n",
      "Epoch 6: |          | 1109/? [26:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1109, loss 3.5417683124542236\n",
      "Epoch 6: |          | 1110/? [26:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1110, loss 4.466204643249512\n",
      "Epoch 6: |          | 1111/? [26:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1111, loss 4.148558139801025\n",
      "Epoch 6: |          | 1112/? [26:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1112, loss 4.013027667999268\n",
      "Epoch 6: |          | 1113/? [26:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1113, loss 3.838719606399536\n",
      "Epoch 6: |          | 1114/? [26:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1114, loss 3.2640011310577393\n",
      "Epoch 6: |          | 1115/? [26:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1115, loss 3.0516746044158936\n",
      "Epoch 6: |          | 1116/? [26:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1116, loss 3.455413341522217\n",
      "Epoch 6: |          | 1117/? [26:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1117, loss 3.601712465286255\n",
      "Epoch 6: |          | 1118/? [26:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1118, loss 3.7680869102478027\n",
      "Epoch 6: |          | 1119/? [26:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1119, loss 4.386258602142334\n",
      "Epoch 6: |          | 1120/? [26:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1120, loss 3.9122097492218018\n",
      "Epoch 6: |          | 1121/? [26:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1121, loss 4.184922218322754\n",
      "Epoch 6: |          | 1122/? [26:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1122, loss 3.706554889678955\n",
      "Epoch 6: |          | 1123/? [26:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1123, loss 3.9816787242889404\n",
      "Epoch 6: |          | 1124/? [26:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1124, loss 4.331028938293457\n",
      "Epoch 6: |          | 1125/? [26:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1125, loss 3.6102302074432373\n",
      "Epoch 6: |          | 1126/? [26:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1126, loss 3.5091309547424316\n",
      "Epoch 6: |          | 1127/? [26:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1127, loss 3.8129684925079346\n",
      "Epoch 6: |          | 1128/? [26:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1128, loss 3.9129390716552734\n",
      "Epoch 6: |          | 1129/? [26:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1129, loss 4.023514270782471\n",
      "Epoch 6: |          | 1130/? [26:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1130, loss 4.194835662841797\n",
      "Epoch 6: |          | 1131/? [26:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1131, loss 4.289763450622559\n",
      "Epoch 6: |          | 1132/? [26:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1132, loss 3.0481812953948975\n",
      "Epoch 6: |          | 1133/? [26:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1133, loss 3.8736565113067627\n",
      "Epoch 6: |          | 1134/? [26:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1134, loss 3.6831696033477783\n",
      "Epoch 6: |          | 1135/? [26:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1135, loss 4.3140130043029785\n",
      "Epoch 6: |          | 1136/? [26:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1136, loss 3.95293927192688\n",
      "Epoch 6: |          | 1137/? [26:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1137, loss 3.9526610374450684\n",
      "Epoch 6: |          | 1138/? [26:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1138, loss 4.481247901916504\n",
      "Epoch 6: |          | 1139/? [26:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1139, loss 4.140740871429443\n",
      "Epoch 6: |          | 1140/? [26:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1140, loss 3.7580349445343018\n",
      "Epoch 6: |          | 1141/? [26:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1141, loss 4.253387928009033\n",
      "Epoch 6: |          | 1142/? [27:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1142, loss 4.363499641418457\n",
      "Epoch 6: |          | 1143/? [27:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1143, loss 4.3963212966918945\n",
      "Epoch 6: |          | 1144/? [27:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1144, loss 3.834367275238037\n",
      "Epoch 6: |          | 1145/? [27:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1145, loss 3.9321188926696777\n",
      "Epoch 6: |          | 1146/? [27:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1146, loss 3.580127000808716\n",
      "Epoch 6: |          | 1147/? [27:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1147, loss 3.5112099647521973\n",
      "Epoch 6: |          | 1148/? [27:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1148, loss 3.696434497833252\n",
      "Epoch 6: |          | 1149/? [27:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1149, loss 4.9042253494262695\n",
      "Epoch 6: |          | 1150/? [27:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1150, loss 4.124001979827881\n",
      "Epoch 6: |          | 1151/? [27:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1151, loss 4.42171049118042\n",
      "Epoch 6: |          | 1152/? [27:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1152, loss 3.622882843017578\n",
      "Epoch 6: |          | 1153/? [27:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1153, loss 4.039738655090332\n",
      "Epoch 6: |          | 1154/? [27:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1154, loss 3.715789794921875\n",
      "Epoch 6: |          | 1155/? [27:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1155, loss 3.913207530975342\n",
      "Epoch 6: |          | 1156/? [27:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1156, loss 3.9164421558380127\n",
      "Epoch 6: |          | 1157/? [27:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1157, loss 4.168944835662842\n",
      "Epoch 6: |          | 1158/? [27:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1158, loss 4.3760175704956055\n",
      "Epoch 6: |          | 1159/? [27:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1159, loss 3.167710781097412\n",
      "Epoch 6: |          | 1160/? [27:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1160, loss 4.348970413208008\n",
      "Epoch 6: |          | 1161/? [27:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1161, loss 4.197040557861328\n",
      "Epoch 6: |          | 1162/? [27:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1162, loss 4.116530895233154\n",
      "Epoch 6: |          | 1163/? [27:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1163, loss 4.686248302459717\n",
      "Epoch 6: |          | 1164/? [27:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1164, loss 4.480278015136719\n",
      "Epoch 6: |          | 1165/? [27:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1165, loss 3.629913806915283\n",
      "Epoch 6: |          | 1166/? [27:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1166, loss 4.148673057556152\n",
      "Epoch 6: |          | 1167/? [27:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1167, loss 4.176141262054443\n",
      "Epoch 6: |          | 1168/? [27:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1168, loss 4.631452560424805\n",
      "Epoch 6: |          | 1169/? [27:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1169, loss 3.686371326446533\n",
      "Epoch 6: |          | 1170/? [27:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1170, loss 4.265244007110596\n",
      "Epoch 6: |          | 1171/? [27:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1171, loss 3.654568910598755\n",
      "Epoch 6: |          | 1172/? [27:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1172, loss 3.56193208694458\n",
      "Epoch 6: |          | 1173/? [27:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1173, loss 4.117292881011963\n",
      "Epoch 6: |          | 1174/? [27:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1174, loss 3.5716774463653564\n",
      "Epoch 6: |          | 1175/? [27:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1175, loss 4.210986137390137\n",
      "Epoch 6: |          | 1176/? [27:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1176, loss 4.23203182220459\n",
      "Epoch 6: |          | 1177/? [27:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1177, loss 4.433093547821045\n",
      "Epoch 6: |          | 1178/? [27:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1178, loss 3.8336997032165527\n",
      "Epoch 6: |          | 1179/? [27:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1179, loss 4.3492655754089355\n",
      "Epoch 6: |          | 1180/? [27:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1180, loss 4.159660339355469\n",
      "Epoch 6: |          | 1181/? [27:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1181, loss 4.108691215515137\n",
      "Epoch 6: |          | 1182/? [27:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1182, loss 3.9188907146453857\n",
      "Epoch 6: |          | 1183/? [27:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1183, loss 3.6250178813934326\n",
      "Epoch 6: |          | 1184/? [27:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1184, loss 4.0230584144592285\n",
      "Epoch 6: |          | 1185/? [28:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1185, loss 3.8127875328063965\n",
      "Epoch 6: |          | 1186/? [28:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1186, loss 4.017780780792236\n",
      "Epoch 6: |          | 1187/? [28:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1187, loss 3.8993289470672607\n",
      "Epoch 6: |          | 1188/? [28:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1188, loss 4.277012825012207\n",
      "Epoch 6: |          | 1189/? [28:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1189, loss 4.368006706237793\n",
      "Epoch 6: |          | 1190/? [28:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1190, loss 3.8801429271698\n",
      "Epoch 6: |          | 1191/? [28:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1191, loss 3.950270175933838\n",
      "Epoch 6: |          | 1192/? [28:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1192, loss 4.270977973937988\n",
      "Epoch 6: |          | 1193/? [28:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1193, loss 3.7607853412628174\n",
      "Epoch 6: |          | 1194/? [28:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1194, loss 3.435941696166992\n",
      "Epoch 6: |          | 1195/? [28:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1195, loss 3.9946448802948\n",
      "Epoch 6: |          | 1196/? [28:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1196, loss 4.186217308044434\n",
      "Epoch 6: |          | 1197/? [28:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1197, loss 3.9634532928466797\n",
      "Epoch 6: |          | 1198/? [28:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1198, loss 4.070677280426025\n",
      "Epoch 6: |          | 1199/? [28:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1199, loss 4.330935955047607\n",
      "Epoch 6: |          | 1200/? [28:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1200, loss 3.552569627761841\n",
      "Epoch 6: |          | 1201/? [28:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1201, loss 4.143799781799316\n",
      "Epoch 6: |          | 1202/? [28:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1202, loss 3.8416504859924316\n",
      "Epoch 6: |          | 1203/? [28:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1203, loss 3.816277265548706\n",
      "Epoch 6: |          | 1204/? [28:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1204, loss 3.3421263694763184\n",
      "Epoch 6: |          | 1205/? [28:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1205, loss 3.9041664600372314\n",
      "Epoch 6: |          | 1206/? [28:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1206, loss 3.927147626876831\n",
      "Epoch 6: |          | 1207/? [28:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1207, loss 4.248660564422607\n",
      "Epoch 6: |          | 1208/? [28:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1208, loss 4.442906856536865\n",
      "Epoch 6: |          | 1209/? [28:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1209, loss 4.019988536834717\n",
      "Epoch 6: |          | 1210/? [28:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1210, loss 4.280940532684326\n",
      "Epoch 6: |          | 1211/? [28:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1211, loss 4.262365818023682\n",
      "Epoch 6: |          | 1212/? [28:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1212, loss 4.065370082855225\n",
      "Epoch 6: |          | 1213/? [28:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1213, loss 3.771789073944092\n",
      "Epoch 6: |          | 1214/? [28:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1214, loss 4.372943878173828\n",
      "Epoch 6: |          | 1215/? [28:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1215, loss 3.8200480937957764\n",
      "Epoch 6: |          | 1216/? [28:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1216, loss 3.9668402671813965\n",
      "Epoch 6: |          | 1217/? [28:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1217, loss 4.105680465698242\n",
      "Epoch 6: |          | 1218/? [28:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1218, loss 4.122982978820801\n",
      "Epoch 6: |          | 1219/? [28:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1219, loss 3.8049628734588623\n",
      "Epoch 6: |          | 1220/? [28:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1220, loss 4.482378959655762\n",
      "Epoch 6: |          | 1221/? [28:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1221, loss 4.040303707122803\n",
      "Epoch 6: |          | 1222/? [28:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1222, loss 3.1713709831237793\n",
      "Epoch 6: |          | 1223/? [28:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1223, loss 3.348810911178589\n",
      "Epoch 6: |          | 1224/? [28:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1224, loss 3.704720973968506\n",
      "Epoch 6: |          | 1225/? [28:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1225, loss 4.352173328399658\n",
      "Epoch 6: |          | 1226/? [28:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1226, loss 4.33396053314209\n",
      "Epoch 6: |          | 1227/? [29:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1227, loss 3.980426788330078\n",
      "Epoch 6: |          | 1228/? [29:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1228, loss 3.8568520545959473\n",
      "Epoch 6: |          | 1229/? [29:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1229, loss 3.450932264328003\n",
      "Epoch 6: |          | 1230/? [29:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1230, loss 4.141654968261719\n",
      "Epoch 6: |          | 1231/? [29:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1231, loss 4.175960540771484\n",
      "Epoch 6: |          | 1232/? [29:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1232, loss 4.327229976654053\n",
      "Epoch 6: |          | 1233/? [29:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1233, loss 4.1117024421691895\n",
      "Epoch 6: |          | 1234/? [29:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1234, loss 3.153688430786133\n",
      "Epoch 6: |          | 1235/? [29:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1235, loss 4.235759735107422\n",
      "Epoch 6: |          | 1236/? [29:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1236, loss 3.5979933738708496\n",
      "Epoch 6: |          | 1237/? [29:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1237, loss 3.8872134685516357\n",
      "Epoch 6: |          | 1238/? [29:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1238, loss 3.9335885047912598\n",
      "Epoch 6: |          | 1239/? [29:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1239, loss 3.7923645973205566\n",
      "Epoch 6: |          | 1240/? [29:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1240, loss 4.438090801239014\n",
      "Epoch 6: |          | 1241/? [29:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1241, loss 3.9803481101989746\n",
      "Epoch 6: |          | 1242/? [29:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1242, loss 3.8078696727752686\n",
      "Epoch 6: |          | 1243/? [29:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1243, loss 3.695523738861084\n",
      "Epoch 6: |          | 1244/? [29:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1244, loss 3.8577370643615723\n",
      "Epoch 6: |          | 1245/? [29:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1245, loss 3.4451346397399902\n",
      "Epoch 6: |          | 1246/? [29:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1246, loss 4.134021282196045\n",
      "Epoch 6: |          | 1247/? [29:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1247, loss 4.13242244720459\n",
      "Epoch 6: |          | 1248/? [29:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1248, loss 3.692474365234375\n",
      "Epoch 6: |          | 1249/? [29:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1249, loss 3.856052875518799\n",
      "Epoch 6: |          | 1250/? [29:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1250, loss 4.0144734382629395\n",
      "Epoch 6: |          | 1251/? [29:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1251, loss 3.7301154136657715\n",
      "Epoch 6: |          | 1252/? [29:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1252, loss 4.515171051025391\n",
      "Epoch 6: |          | 1253/? [29:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1253, loss 3.897932767868042\n",
      "Epoch 6: |          | 1254/? [29:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1254, loss 3.2159037590026855\n",
      "Epoch 6: |          | 1255/? [29:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1255, loss 4.595574378967285\n",
      "Epoch 6: |          | 1256/? [29:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1256, loss 3.5795860290527344\n",
      "Epoch 6: |          | 1257/? [29:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1257, loss 3.630462646484375\n",
      "Epoch 6: |          | 1258/? [29:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1258, loss 4.3264923095703125\n",
      "Epoch 6: |          | 1259/? [29:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1259, loss 3.9802284240722656\n",
      "Epoch 6: |          | 1260/? [29:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1260, loss 4.465329647064209\n",
      "Epoch 6: |          | 1261/? [29:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1261, loss 3.7872378826141357\n",
      "Epoch 6: |          | 1262/? [29:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1262, loss 3.7679061889648438\n",
      "Epoch 6: |          | 1263/? [29:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1263, loss 4.117008686065674\n",
      "Epoch 6: |          | 1264/? [29:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1264, loss 4.38637638092041\n",
      "Epoch 6: |          | 1265/? [29:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1265, loss 4.238054275512695\n",
      "Epoch 6: |          | 1266/? [29:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1266, loss 3.8987579345703125\n",
      "Epoch 6: |          | 1267/? [29:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1267, loss 3.99412202835083\n",
      "Epoch 6: |          | 1268/? [29:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1268, loss 3.865816831588745\n",
      "Epoch 6: |          | 1269/? [30:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1269, loss 3.4385859966278076\n",
      "Epoch 6: |          | 1270/? [30:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1270, loss 3.7528820037841797\n",
      "Epoch 6: |          | 1271/? [30:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1271, loss 3.937941074371338\n",
      "Epoch 6: |          | 1272/? [30:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1272, loss 3.53039813041687\n",
      "Epoch 6: |          | 1273/? [30:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1273, loss 4.2053937911987305\n",
      "Epoch 6: |          | 1274/? [30:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1274, loss 3.1091229915618896\n",
      "Epoch 6: |          | 1275/? [30:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1275, loss 3.6475296020507812\n",
      "Epoch 6: |          | 1276/? [30:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1276, loss 3.965951919555664\n",
      "Epoch 6: |          | 1277/? [30:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1277, loss 3.6683192253112793\n",
      "Epoch 6: |          | 1278/? [30:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1278, loss 3.3521809577941895\n",
      "Epoch 6: |          | 1279/? [30:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1279, loss 4.072506904602051\n",
      "Epoch 6: |          | 1280/? [30:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1280, loss 3.2537102699279785\n",
      "Epoch 6: |          | 1281/? [30:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1281, loss 3.7852940559387207\n",
      "Epoch 6: |          | 1282/? [30:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1282, loss 3.4576447010040283\n",
      "Epoch 6: |          | 1283/? [30:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1283, loss 4.2378644943237305\n",
      "Epoch 6: |          | 1284/? [30:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1284, loss 3.3231189250946045\n",
      "Epoch 6: |          | 1285/? [30:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1285, loss 4.410798072814941\n",
      "Epoch 6: |          | 1286/? [30:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1286, loss 2.86190128326416\n",
      "Epoch 6: |          | 1287/? [30:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1287, loss 4.305788993835449\n",
      "Epoch 6: |          | 1288/? [30:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1288, loss 4.1259846687316895\n",
      "Epoch 6: |          | 1289/? [30:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1289, loss 3.218552827835083\n",
      "Epoch 6: |          | 1290/? [30:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1290, loss 4.011960029602051\n",
      "Epoch 6: |          | 1291/? [30:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1291, loss 4.867131233215332\n",
      "Epoch 6: |          | 1292/? [30:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1292, loss 4.191323280334473\n",
      "Epoch 6: |          | 1293/? [30:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1293, loss 3.711315155029297\n",
      "Epoch 6: |          | 1294/? [30:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1294, loss 3.939174175262451\n",
      "Epoch 6: |          | 1295/? [30:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1295, loss 4.012172698974609\n",
      "Epoch 6: |          | 1296/? [30:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1296, loss 3.2972755432128906\n",
      "Epoch 6: |          | 1297/? [30:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1297, loss 4.206315040588379\n",
      "Epoch 6: |          | 1298/? [30:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1298, loss 3.928522825241089\n",
      "Epoch 6: |          | 1299/? [30:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1299, loss 3.0719966888427734\n",
      "Epoch 6: |          | 1300/? [30:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1300, loss 3.9301199913024902\n",
      "Epoch 6: |          | 1301/? [30:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1301, loss 3.6641452312469482\n",
      "Epoch 6: |          | 1302/? [30:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1302, loss 3.801138401031494\n",
      "Epoch 6: |          | 1303/? [30:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1303, loss 3.7515950202941895\n",
      "Epoch 6: |          | 1304/? [30:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1304, loss 4.485456943511963\n",
      "Epoch 6: |          | 1305/? [30:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1305, loss 3.300750255584717\n",
      "Epoch 6: |          | 1306/? [30:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1306, loss 3.9129891395568848\n",
      "Epoch 6: |          | 1307/? [30:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1307, loss 3.5224609375\n",
      "Epoch 6: |          | 1308/? [30:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1308, loss 3.468339204788208\n",
      "Epoch 6: |          | 1309/? [30:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1309, loss 3.5421135425567627\n",
      "Epoch 6: |          | 1310/? [30:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1310, loss 4.087224006652832\n",
      "Epoch 6: |          | 1311/? [30:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1311, loss 3.535177230834961\n",
      "Epoch 6: |          | 1312/? [31:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1312, loss 3.29628324508667\n",
      "Epoch 6: |          | 1313/? [31:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1313, loss 4.434005260467529\n",
      "Epoch 6: |          | 1314/? [31:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1314, loss 3.639615297317505\n",
      "Epoch 6: |          | 1315/? [31:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1315, loss 4.3922834396362305\n",
      "Epoch 6: |          | 1316/? [31:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1316, loss 4.167662620544434\n",
      "Epoch 6: |          | 1317/? [31:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1317, loss 3.7943484783172607\n",
      "Epoch 6: |          | 1318/? [31:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1318, loss 3.9978880882263184\n",
      "Epoch 6: |          | 1319/? [31:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1319, loss 4.1685943603515625\n",
      "Epoch 6: |          | 1320/? [31:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1320, loss 3.7482192516326904\n",
      "Epoch 6: |          | 1321/? [31:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1321, loss 4.200072765350342\n",
      "Epoch 6: |          | 1322/? [31:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1322, loss 4.051613807678223\n",
      "Epoch 6: |          | 1323/? [31:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1323, loss 3.5298378467559814\n",
      "Epoch 6: |          | 1324/? [31:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1324, loss 4.485660076141357\n",
      "Epoch 6: |          | 1325/? [31:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1325, loss 4.639959335327148\n",
      "Epoch 6: |          | 1326/? [31:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1326, loss 4.054339408874512\n",
      "Epoch 6: |          | 1327/? [31:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1327, loss 3.914729356765747\n",
      "Epoch 6: |          | 1328/? [31:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1328, loss 3.6106536388397217\n",
      "Epoch 6: |          | 1329/? [31:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1329, loss 4.200003147125244\n",
      "Epoch 6: |          | 1330/? [31:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1330, loss 3.9401843547821045\n",
      "Epoch 6: |          | 1331/? [31:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1331, loss 4.047196388244629\n",
      "Epoch 6: |          | 1332/? [31:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1332, loss 3.7616875171661377\n",
      "Epoch 6: |          | 1333/? [31:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1333, loss 3.746283769607544\n",
      "Epoch 6: |          | 1334/? [31:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1334, loss 3.872446060180664\n",
      "Epoch 6: |          | 1335/? [31:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1335, loss 3.81536865234375\n",
      "Epoch 6: |          | 1336/? [31:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1336, loss 3.4056549072265625\n",
      "Epoch 6: |          | 1337/? [31:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1337, loss 4.044222831726074\n",
      "Epoch 6: |          | 1338/? [31:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1338, loss 3.273778200149536\n",
      "Epoch 6: |          | 1339/? [31:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1339, loss 3.9578311443328857\n",
      "Epoch 6: |          | 1340/? [31:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1340, loss 3.339263916015625\n",
      "Epoch 6: |          | 1341/? [31:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1341, loss 4.070616722106934\n",
      "Epoch 6: |          | 1342/? [31:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1342, loss 4.325723171234131\n",
      "Epoch 6: |          | 1343/? [31:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1343, loss 3.825568675994873\n",
      "Epoch 6: |          | 1344/? [31:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1344, loss 3.920700788497925\n",
      "Epoch 6: |          | 1345/? [31:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1345, loss 4.060284614562988\n",
      "Epoch 6: |          | 1346/? [31:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1346, loss 5.2050089836120605\n",
      "Epoch 6: |          | 1347/? [31:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1347, loss 4.115422248840332\n",
      "Epoch 6: |          | 1348/? [31:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1348, loss 4.2613701820373535\n",
      "Epoch 6: |          | 1349/? [31:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1349, loss 4.113239765167236\n",
      "Epoch 6: |          | 1350/? [31:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1350, loss 4.251248359680176\n",
      "Epoch 6: |          | 1351/? [31:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1351, loss 4.154394626617432\n",
      "Epoch 6: |          | 1352/? [32:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1352, loss 3.379213333129883\n",
      "Epoch 6: |          | 1353/? [32:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1353, loss 3.724729061126709\n",
      "Epoch 6: |          | 1354/? [32:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1354, loss 4.134779930114746\n",
      "Epoch 6: |          | 1355/? [32:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1355, loss 4.289111137390137\n",
      "Epoch 6: |          | 1356/? [32:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1356, loss 4.004846096038818\n",
      "Epoch 6: |          | 1357/? [32:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1357, loss 3.7686080932617188\n",
      "Epoch 6: |          | 1358/? [32:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1358, loss 3.9528427124023438\n",
      "Epoch 6: |          | 1359/? [32:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1359, loss 3.8284268379211426\n",
      "Epoch 6: |          | 1360/? [32:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1360, loss 4.001046657562256\n",
      "Epoch 6: |          | 1361/? [32:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1361, loss 3.9832472801208496\n",
      "Epoch 6: |          | 1362/? [32:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1362, loss 3.812776565551758\n",
      "Epoch 6: |          | 1363/? [32:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1363, loss 3.2545623779296875\n",
      "Epoch 6: |          | 1364/? [32:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1364, loss 3.7426066398620605\n",
      "Epoch 6: |          | 1365/? [32:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1365, loss 3.443814516067505\n",
      "Epoch 6: |          | 1366/? [32:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1366, loss 4.176661968231201\n",
      "Epoch 6: |          | 1367/? [32:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1367, loss 3.4719364643096924\n",
      "Epoch 6: |          | 1368/? [32:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1368, loss 3.278796434402466\n",
      "Epoch 6: |          | 1369/? [32:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1369, loss 3.9621453285217285\n",
      "Epoch 6: |          | 1370/? [32:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1370, loss 3.4924869537353516\n",
      "Epoch 6: |          | 1371/? [32:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1371, loss 4.427711486816406\n",
      "Epoch 6: |          | 1372/? [32:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1372, loss 3.835855007171631\n",
      "Epoch 6: |          | 1373/? [32:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1373, loss 4.272982120513916\n",
      "Epoch 6: |          | 1374/? [32:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1374, loss 3.364074230194092\n",
      "Epoch 6: |          | 1375/? [32:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1375, loss 4.007353782653809\n",
      "Epoch 6: |          | 1376/? [32:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1376, loss 3.8638992309570312\n",
      "Epoch 6: |          | 1377/? [32:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1377, loss 3.8694164752960205\n",
      "Epoch 6: |          | 1378/? [32:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1378, loss 3.8570849895477295\n",
      "Epoch 6: |          | 1379/? [32:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1379, loss 3.883911609649658\n",
      "Epoch 6: |          | 1380/? [32:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1380, loss 3.987792491912842\n",
      "Epoch 6: |          | 1381/? [32:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1381, loss 4.1023945808410645\n",
      "Epoch 6: |          | 1382/? [32:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1382, loss 3.6832423210144043\n",
      "Epoch 6: |          | 1383/? [32:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1383, loss 3.8834404945373535\n",
      "Epoch 6: |          | 1384/? [32:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1384, loss 3.909313201904297\n",
      "Epoch 6: |          | 1385/? [32:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1385, loss 3.8648910522460938\n",
      "Epoch 6: |          | 1386/? [32:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1386, loss 3.9299826622009277\n",
      "Epoch 6: |          | 1387/? [32:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1387, loss 3.8690834045410156\n",
      "Epoch 6: |          | 1388/? [32:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1388, loss 3.38523530960083\n",
      "Epoch 6: |          | 1389/? [32:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1389, loss 4.092976093292236\n",
      "Epoch 6: |          | 1390/? [32:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1390, loss 4.442620754241943\n",
      "Epoch 6: |          | 1391/? [32:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1391, loss 4.040445327758789\n",
      "Epoch 6: |          | 1392/? [32:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1392, loss 3.528968334197998\n",
      "Epoch 6: |          | 1393/? [32:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1393, loss 3.6846117973327637\n",
      "Epoch 6: |          | 1394/? [32:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1394, loss 3.3432796001434326\n",
      "Epoch 6: |          | 1395/? [33:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1395, loss 4.021351337432861\n",
      "Epoch 6: |          | 1396/? [33:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1396, loss 3.9320366382598877\n",
      "Epoch 6: |          | 1397/? [33:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1397, loss 3.14113712310791\n",
      "Epoch 6: |          | 1398/? [33:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1398, loss 4.362477779388428\n",
      "Epoch 6: |          | 1399/? [33:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1399, loss 4.391454696655273\n",
      "Epoch 6: |          | 1400/? [33:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1400, loss 3.4783599376678467\n",
      "Epoch 6: |          | 1401/? [33:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1401, loss 4.283339500427246\n",
      "Epoch 6: |          | 1402/? [33:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1402, loss 3.871030330657959\n",
      "Epoch 6: |          | 1403/? [33:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1403, loss 4.0724663734436035\n",
      "Epoch 6: |          | 1404/? [33:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1404, loss 4.051209449768066\n",
      "Epoch 6: |          | 1405/? [33:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1405, loss 4.37888240814209\n",
      "Epoch 6: |          | 1406/? [33:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1406, loss 4.287156105041504\n",
      "Epoch 6: |          | 1407/? [33:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1407, loss 4.371419429779053\n",
      "Epoch 6: |          | 1408/? [33:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1408, loss 3.5710277557373047\n",
      "Epoch 6: |          | 1409/? [33:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1409, loss 3.6803035736083984\n",
      "Epoch 6: |          | 1410/? [33:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1410, loss 3.689135789871216\n",
      "Epoch 6: |          | 1411/? [33:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1411, loss 4.0138444900512695\n",
      "Epoch 6: |          | 1412/? [33:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1412, loss 3.6661391258239746\n",
      "Epoch 6: |          | 1413/? [33:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1413, loss 3.5653672218322754\n",
      "Epoch 6: |          | 1414/? [33:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1414, loss 3.6919150352478027\n",
      "Epoch 6: |          | 1415/? [33:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1415, loss 3.960078716278076\n",
      "Epoch 6: |          | 1416/? [33:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1416, loss 4.3418989181518555\n",
      "Epoch 6: |          | 1417/? [33:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1417, loss 3.9855639934539795\n",
      "Epoch 6: |          | 1418/? [33:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1418, loss 4.133513450622559\n",
      "Epoch 6: |          | 1419/? [33:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1419, loss 3.907346248626709\n",
      "Epoch 6: |          | 1420/? [33:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1420, loss 3.737462282180786\n",
      "Epoch 6: |          | 1421/? [33:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1421, loss 3.500145673751831\n",
      "Epoch 6: |          | 1422/? [33:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1422, loss 4.161679267883301\n",
      "Epoch 6: |          | 1423/? [33:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1423, loss 4.204285621643066\n",
      "Epoch 6: |          | 1424/? [33:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1424, loss 3.8887577056884766\n",
      "Epoch 6: |          | 1425/? [33:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1425, loss 4.097264766693115\n",
      "Epoch 6: |          | 1426/? [33:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1426, loss 3.625286102294922\n",
      "Epoch 6: |          | 1427/? [33:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1427, loss 4.247934341430664\n",
      "Epoch 6: |          | 1428/? [33:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1428, loss 4.217728614807129\n",
      "Epoch 6: |          | 1429/? [33:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1429, loss 4.114771366119385\n",
      "Epoch 6: |          | 1430/? [33:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1430, loss 4.1803669929504395\n",
      "Epoch 6: |          | 1431/? [33:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1431, loss 3.924623489379883\n",
      "Epoch 6: |          | 1432/? [33:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1432, loss 3.9803707599639893\n",
      "Epoch 6: |          | 1433/? [33:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1433, loss 3.870745897293091\n",
      "Epoch 6: |          | 1434/? [33:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1434, loss 4.052209377288818\n",
      "Epoch 6: |          | 1435/? [33:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1435, loss 3.6348507404327393\n",
      "Epoch 6: |          | 1436/? [33:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1436, loss 3.990760087966919\n",
      "Epoch 6: |          | 1437/? [33:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1437, loss 3.2317557334899902\n",
      "Epoch 6: |          | 1438/? [34:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1438, loss 4.893857955932617\n",
      "Epoch 6: |          | 1439/? [34:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1439, loss 4.021236419677734\n",
      "Epoch 6: |          | 1440/? [34:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1440, loss 4.158322334289551\n",
      "Epoch 6: |          | 1441/? [34:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1441, loss 4.487654685974121\n",
      "Epoch 6: |          | 1442/? [34:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1442, loss 4.500162124633789\n",
      "Epoch 6: |          | 1443/? [34:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1443, loss 3.638432025909424\n",
      "Epoch 6: |          | 1444/? [34:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1444, loss 3.668302059173584\n",
      "Epoch 6: |          | 1445/? [34:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1445, loss 4.174325466156006\n",
      "Epoch 6: |          | 1446/? [34:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1446, loss 3.7527523040771484\n",
      "Epoch 6: |          | 1447/? [34:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1447, loss 3.8933188915252686\n",
      "Epoch 6: |          | 1448/? [34:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1448, loss 3.715259075164795\n",
      "Epoch 6: |          | 1449/? [34:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1449, loss 3.922370195388794\n",
      "Epoch 6: |          | 1450/? [34:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1450, loss 3.992950439453125\n",
      "Epoch 6: |          | 1451/? [34:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1451, loss 4.352758884429932\n",
      "Epoch 6: |          | 1452/? [34:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1452, loss 4.007316589355469\n",
      "Epoch 6: |          | 1453/? [34:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1453, loss 3.348475217819214\n",
      "Epoch 6: |          | 1454/? [34:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1454, loss 3.945697069168091\n",
      "Epoch 6: |          | 1455/? [34:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1455, loss 4.020120620727539\n",
      "Epoch 6: |          | 1456/? [34:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1456, loss 3.610325336456299\n",
      "Epoch 6: |          | 1457/? [34:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1457, loss 3.7852840423583984\n",
      "Epoch 6: |          | 1458/? [34:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1458, loss 3.9878878593444824\n",
      "Epoch 6: |          | 1459/? [34:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1459, loss 4.126734733581543\n",
      "Epoch 6: |          | 1460/? [34:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1460, loss 4.055098056793213\n",
      "Epoch 6: |          | 1461/? [34:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1461, loss 3.9857685565948486\n",
      "Epoch 6: |          | 1462/? [34:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1462, loss 4.313565254211426\n",
      "Epoch 6: |          | 1463/? [34:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1463, loss 4.226248264312744\n",
      "Epoch 6: |          | 1464/? [34:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1464, loss 3.5065479278564453\n",
      "Epoch 6: |          | 1465/? [34:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1465, loss 3.8902175426483154\n",
      "Epoch 6: |          | 1466/? [34:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1466, loss 3.572775363922119\n",
      "Epoch 6: |          | 1467/? [34:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1467, loss 4.221524238586426\n",
      "Epoch 6: |          | 1468/? [34:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1468, loss 3.801828384399414\n",
      "Epoch 6: |          | 1469/? [34:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1469, loss 3.5197384357452393\n",
      "Epoch 6: |          | 1470/? [34:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1470, loss 4.082848072052002\n",
      "Epoch 6: |          | 1471/? [34:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1471, loss 4.343951225280762\n",
      "Epoch 6: |          | 1472/? [34:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1472, loss 4.063042640686035\n",
      "Epoch 6: |          | 1473/? [34:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1473, loss 3.8754844665527344\n",
      "Epoch 6: |          | 1474/? [34:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1474, loss 3.7804694175720215\n",
      "Epoch 6: |          | 1475/? [34:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1475, loss 3.377686023712158\n",
      "Epoch 6: |          | 1476/? [34:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1476, loss 3.9772326946258545\n",
      "Epoch 6: |          | 1477/? [34:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1477, loss 3.975384473800659\n",
      "Epoch 6: |          | 1478/? [34:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1478, loss 3.8574390411376953\n",
      "Epoch 6: |          | 1479/? [34:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1479, loss 4.388755798339844\n",
      "Epoch 6: |          | 1480/? [34:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1480, loss 4.102782249450684\n",
      "Epoch 6: |          | 1481/? [35:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1481, loss 3.8618311882019043\n",
      "Epoch 6: |          | 1482/? [35:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1482, loss 4.002799987792969\n",
      "Epoch 6: |          | 1483/? [35:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1483, loss 3.670680284500122\n",
      "Epoch 6: |          | 1484/? [35:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1484, loss 3.853487730026245\n",
      "Epoch 6: |          | 1485/? [35:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1485, loss 3.9972305297851562\n",
      "Epoch 6: |          | 1486/? [35:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1486, loss 3.9864745140075684\n",
      "Epoch 6: |          | 1487/? [35:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1487, loss 3.4408841133117676\n",
      "Epoch 6: |          | 1488/? [35:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1488, loss 4.09616756439209\n",
      "Epoch 6: |          | 1489/? [35:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1489, loss 4.061346054077148\n",
      "Epoch 6: |          | 1490/? [35:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1490, loss 3.9469192028045654\n",
      "Epoch 6: |          | 1491/? [35:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1491, loss 2.9302289485931396\n",
      "Epoch 6: |          | 1492/? [35:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1492, loss 3.4885330200195312\n",
      "Epoch 6: |          | 1493/? [35:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1493, loss 3.0849578380584717\n",
      "Epoch 6: |          | 1494/? [35:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1494, loss 3.863784074783325\n",
      "Epoch 6: |          | 1495/? [35:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1495, loss 3.7401604652404785\n",
      "Epoch 6: |          | 1496/? [35:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1496, loss 4.086400032043457\n",
      "Epoch 6: |          | 1497/? [35:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1497, loss 3.344472885131836\n",
      "Epoch 6: |          | 1498/? [35:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1498, loss 3.621387481689453\n",
      "Epoch 6: |          | 1499/? [35:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1499, loss 4.1904191970825195\n",
      "Epoch 6: |          | 1500/? [35:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1500, loss 4.1321892738342285\n",
      "Epoch 6: |          | 1501/? [35:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1501, loss 3.95076060295105\n",
      "Epoch 6: |          | 1502/? [35:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1502, loss 4.0316853523254395\n",
      "Epoch 6: |          | 1503/? [35:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1503, loss 3.787640333175659\n",
      "Epoch 6: |          | 1504/? [35:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1504, loss 4.43351411819458\n",
      "Epoch 6: |          | 1505/? [35:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1505, loss 4.242116451263428\n",
      "Epoch 6: |          | 1506/? [35:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1506, loss 3.8715577125549316\n",
      "Epoch 6: |          | 1507/? [35:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1507, loss 3.7203001976013184\n",
      "Epoch 6: |          | 1508/? [35:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1508, loss 3.859572649002075\n",
      "Epoch 6: |          | 1509/? [35:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1509, loss 3.877836227416992\n",
      "Epoch 6: |          | 1510/? [35:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1510, loss 4.1629438400268555\n",
      "Epoch 6: |          | 1511/? [35:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1511, loss 3.741156816482544\n",
      "Epoch 6: |          | 1512/? [35:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1512, loss 4.333395004272461\n",
      "Epoch 6: |          | 1513/? [35:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1513, loss 4.5539937019348145\n",
      "Epoch 6: |          | 1514/? [35:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1514, loss 3.619861125946045\n",
      "Epoch 6: |          | 1515/? [35:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1515, loss 4.498335361480713\n",
      "Epoch 6: |          | 1516/? [35:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1516, loss 4.3644585609436035\n",
      "Epoch 6: |          | 1517/? [35:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1517, loss 3.8301901817321777\n",
      "Epoch 6: |          | 1518/? [35:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1518, loss 3.604480028152466\n",
      "Epoch 6: |          | 1519/? [35:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1519, loss 4.173417091369629\n",
      "Epoch 6: |          | 1520/? [35:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1520, loss 4.216280460357666\n",
      "Epoch 6: |          | 1521/? [35:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1521, loss 3.9513111114501953\n",
      "Epoch 6: |          | 1522/? [35:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1522, loss 3.704659938812256\n",
      "Epoch 6: |          | 1523/? [35:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1523, loss 4.036445140838623\n",
      "Epoch 6: |          | 1524/? [36:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1524, loss 3.909418821334839\n",
      "Epoch 6: |          | 1525/? [36:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1525, loss 3.606299638748169\n",
      "Epoch 6: |          | 1526/? [36:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1526, loss 4.16488790512085\n",
      "Epoch 6: |          | 1527/? [36:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1527, loss 4.173754692077637\n",
      "Epoch 6: |          | 1528/? [36:06<00:00,  0.71it/s, v_num=31]ERROR: Input has inproper shape\n",
      "Epoch 6: |          | 1529/? [36:06<00:00,  0.71it/s, v_num=31]   VALIDATION: Batch 0, loss 4.591297149658203\n",
      "   VALIDATION: Batch 1, loss 3.593816041946411\n",
      "   VALIDATION: Batch 2, loss 4.775237083435059\n",
      "   VALIDATION: Batch 3, loss 4.448824882507324\n",
      "   VALIDATION: Batch 4, loss 4.0267815589904785\n",
      "   VALIDATION: Batch 5, loss 3.7026753425598145\n",
      "   VALIDATION: Batch 6, loss 3.9906742572784424\n",
      "   VALIDATION: Batch 7, loss 4.652305603027344\n",
      "   VALIDATION: Batch 8, loss 4.5007123947143555\n",
      "   VALIDATION: Batch 9, loss 4.651139259338379\n",
      "   VALIDATION: Batch 10, loss 4.302142143249512\n",
      "   VALIDATION: Batch 11, loss 3.992605686187744\n",
      "   VALIDATION: Batch 12, loss 4.16832160949707\n",
      "   VALIDATION: Batch 13, loss 4.718579292297363\n",
      "   VALIDATION: Batch 14, loss 3.9708428382873535\n",
      "   VALIDATION: Batch 15, loss 3.9482407569885254\n",
      "   VALIDATION: Batch 16, loss 4.604873180389404\n",
      "   VALIDATION: Batch 17, loss 4.301634788513184\n",
      "   VALIDATION: Batch 18, loss 3.534532070159912\n",
      "   VALIDATION: Batch 19, loss 4.426854133605957\n",
      "   VALIDATION: Batch 20, loss 4.722663879394531\n",
      "   VALIDATION: Batch 21, loss 4.948931694030762\n",
      "   VALIDATION: Batch 22, loss 4.6038737297058105\n",
      "   VALIDATION: Batch 23, loss 4.143002986907959\n",
      "   VALIDATION: Batch 24, loss 3.953852415084839\n",
      "   VALIDATION: Batch 25, loss 4.369523525238037\n",
      "   VALIDATION: Batch 26, loss 4.59616756439209\n",
      "   VALIDATION: Batch 27, loss 4.463738441467285\n",
      "   VALIDATION: Batch 28, loss 4.248434066772461\n",
      "   VALIDATION: Batch 29, loss 4.433367729187012\n",
      "   VALIDATION: Batch 30, loss 4.0743207931518555\n",
      "   VALIDATION: Batch 31, loss 4.376221656799316\n",
      "   VALIDATION: Batch 32, loss 4.916571617126465\n",
      "   VALIDATION: Batch 33, loss 3.144993305206299\n",
      "   VALIDATION: Batch 34, loss 4.321618556976318\n",
      "   VALIDATION: Batch 35, loss 4.554417610168457\n",
      "   VALIDATION: Batch 36, loss 3.843954086303711\n",
      "   VALIDATION: Batch 37, loss 3.86674165725708\n",
      "   VALIDATION: Batch 38, loss 3.950047254562378\n",
      "   VALIDATION: Batch 39, loss 4.33643913269043\n",
      "   VALIDATION: Batch 40, loss 4.343746185302734\n",
      "   VALIDATION: Batch 41, loss 3.200239658355713\n",
      "   VALIDATION: Batch 42, loss 4.43340539932251\n",
      "   VALIDATION: Batch 43, loss 4.5627641677856445\n",
      "   VALIDATION: Batch 44, loss 4.157742023468018\n",
      "   VALIDATION: Batch 45, loss 4.55730676651001\n",
      "   VALIDATION: Batch 46, loss 3.7065768241882324\n",
      "   VALIDATION: Batch 47, loss 4.688767433166504\n",
      "   VALIDATION: Batch 48, loss 4.798553466796875\n",
      "   VALIDATION: Batch 49, loss 4.406919956207275\n",
      "   VALIDATION: Batch 50, loss 4.41998291015625\n",
      "   VALIDATION: Batch 51, loss 4.86057186126709\n",
      "   VALIDATION: Batch 52, loss 4.051693439483643\n",
      "   VALIDATION: Batch 53, loss 3.9058520793914795\n",
      "   VALIDATION: Batch 54, loss 4.026216983795166\n",
      "   VALIDATION: Batch 55, loss 4.700127601623535\n",
      "   VALIDATION: Batch 56, loss 4.138091087341309\n",
      "   VALIDATION: Batch 57, loss 5.6073126792907715\n",
      "   VALIDATION: Batch 58, loss 4.251077175140381\n",
      "   VALIDATION: Batch 59, loss 3.8910908699035645\n",
      "   VALIDATION: Batch 60, loss 3.4319233894348145\n",
      "   VALIDATION: Batch 61, loss 4.293456077575684\n",
      "   VALIDATION: Batch 62, loss 4.28213357925415\n",
      "   VALIDATION: Batch 63, loss 4.801648139953613\n",
      "   VALIDATION: Batch 64, loss 4.588122844696045\n",
      "   VALIDATION: Batch 65, loss 3.7569098472595215\n",
      "   VALIDATION: Batch 66, loss 4.685815811157227\n",
      "   VALIDATION: Batch 67, loss 4.098198890686035\n",
      "   VALIDATION: Batch 68, loss 4.256436824798584\n",
      "   VALIDATION: Batch 69, loss 4.500874042510986\n",
      "   VALIDATION: Batch 70, loss 4.659885883331299\n",
      "   VALIDATION: Batch 71, loss 4.1675896644592285\n",
      "   VALIDATION: Batch 72, loss 5.0485029220581055\n",
      "   VALIDATION: Batch 73, loss 3.8717422485351562\n",
      "   VALIDATION: Batch 74, loss 4.473866939544678\n",
      "   VALIDATION: Batch 75, loss 4.432955265045166\n",
      "   VALIDATION: Batch 76, loss 4.306977272033691\n",
      "   VALIDATION: Batch 77, loss 4.599543571472168\n",
      "   VALIDATION: Batch 78, loss 4.383379936218262\n",
      "   VALIDATION: Batch 79, loss 4.381461143493652\n",
      "   VALIDATION: Batch 80, loss 4.45269250869751\n",
      "   VALIDATION: Batch 81, loss 4.169285297393799\n",
      "   VALIDATION: Batch 82, loss 4.542725086212158\n",
      "   VALIDATION: Batch 83, loss 3.835885524749756\n",
      "   VALIDATION: Batch 84, loss 4.504703521728516\n",
      "   VALIDATION: Batch 85, loss 4.167950630187988\n",
      "   VALIDATION: Batch 86, loss 4.244688510894775\n",
      "   VALIDATION: Batch 87, loss 4.107430934906006\n",
      "   VALIDATION: Batch 88, loss 3.7144126892089844\n",
      "   VALIDATION: Batch 89, loss 4.039434909820557\n",
      "   VALIDATION: Batch 90, loss 4.266965866088867\n",
      "   VALIDATION: Batch 91, loss 4.289911270141602\n",
      "   VALIDATION: Batch 92, loss 3.9873929023742676\n",
      "   VALIDATION: Batch 93, loss 4.760732173919678\n",
      "   VALIDATION: Batch 94, loss 4.218628883361816\n",
      "   VALIDATION: Batch 95, loss 3.7396349906921387\n",
      "   VALIDATION: Batch 96, loss 4.179441452026367\n",
      "   VALIDATION: Batch 97, loss 3.911883592605591\n",
      "   VALIDATION: Batch 98, loss 4.50727653503418\n",
      "   VALIDATION: Batch 99, loss 4.638784408569336\n",
      "   VALIDATION: Batch 100, loss 4.960565090179443\n",
      "   VALIDATION: Batch 101, loss 3.567682981491089\n",
      "   VALIDATION: Batch 102, loss 4.974789619445801\n",
      "   VALIDATION: Batch 103, loss 4.9116387367248535\n",
      "   VALIDATION: Batch 104, loss 3.8617968559265137\n",
      "   VALIDATION: Batch 105, loss 4.327925205230713\n",
      "   VALIDATION: Batch 106, loss 4.18062686920166\n",
      "   VALIDATION: Batch 107, loss 4.322585582733154\n",
      "   VALIDATION: Batch 108, loss 4.022593021392822\n",
      "   VALIDATION: Batch 109, loss 4.631402492523193\n",
      "   VALIDATION: Batch 110, loss 4.304811477661133\n",
      "   VALIDATION: Batch 111, loss 4.64011287689209\n",
      "   VALIDATION: Batch 112, loss 5.488614559173584\n",
      "   VALIDATION: Batch 113, loss 4.796724319458008\n",
      "   VALIDATION: Batch 114, loss 4.599217891693115\n",
      "   VALIDATION: Batch 115, loss 4.062340259552002\n",
      "   VALIDATION: Batch 116, loss 3.896925687789917\n",
      "   VALIDATION: Batch 117, loss 4.546975612640381\n",
      "   VALIDATION: Batch 118, loss 4.738773345947266\n",
      "   VALIDATION: Batch 119, loss 3.924363374710083\n",
      "   VALIDATION: Batch 120, loss 3.5569908618927\n",
      "   VALIDATION: Batch 121, loss 3.7895145416259766\n",
      "   VALIDATION: Batch 122, loss 4.1599273681640625\n",
      "   VALIDATION: Batch 123, loss 4.2116618156433105\n",
      "   VALIDATION: Batch 124, loss 3.6180319786071777\n",
      "   VALIDATION: Batch 125, loss 4.232382297515869\n",
      "   VALIDATION: Batch 126, loss 4.45566463470459\n",
      "   VALIDATION: Batch 127, loss 4.19128942489624\n",
      "   VALIDATION: Batch 128, loss 4.372208595275879\n",
      "   VALIDATION: Batch 129, loss 3.987967014312744\n",
      "   VALIDATION: Batch 130, loss 3.660524845123291\n",
      "   VALIDATION: Batch 131, loss 3.670794725418091\n",
      "   VALIDATION: Batch 132, loss 4.303574085235596\n",
      "   VALIDATION: Batch 133, loss 4.475937366485596\n",
      "   VALIDATION: Batch 134, loss 4.342021465301514\n",
      "   VALIDATION: Batch 135, loss 4.604720115661621\n",
      "   VALIDATION: Batch 136, loss 4.720465183258057\n",
      "   VALIDATION: Batch 137, loss 4.540882587432861\n",
      "   VALIDATION: Batch 138, loss 4.260316848754883\n",
      "   VALIDATION: Batch 139, loss 4.649535655975342\n",
      "   VALIDATION: Batch 140, loss 3.7616240978240967\n",
      "   VALIDATION: Batch 141, loss 4.707387447357178\n",
      "   VALIDATION: Batch 142, loss 3.444596529006958\n",
      "   VALIDATION: Batch 143, loss 4.23408842086792\n",
      "   VALIDATION: Batch 144, loss 4.498685359954834\n",
      "   VALIDATION: Batch 145, loss 4.268107891082764\n",
      "   VALIDATION: Batch 146, loss 4.105408668518066\n",
      "   VALIDATION: Batch 147, loss 4.441803932189941\n",
      "   VALIDATION: Batch 148, loss 4.520084381103516\n",
      "   VALIDATION: Batch 149, loss 5.0243330001831055\n",
      "   VALIDATION: Batch 150, loss 4.581236362457275\n",
      "   VALIDATION: Batch 151, loss 4.901580333709717\n",
      "   VALIDATION: Batch 152, loss 4.243298053741455\n",
      "   VALIDATION: Batch 153, loss 4.500733852386475\n",
      "   VALIDATION: Batch 154, loss 4.328598976135254\n",
      "   VALIDATION: Batch 155, loss 4.093958854675293\n",
      "   VALIDATION: Batch 156, loss 4.7495012283325195\n",
      "   VALIDATION: Batch 157, loss 4.4207353591918945\n",
      "   VALIDATION: Batch 158, loss 3.8621814250946045\n",
      "   VALIDATION: Batch 159, loss 4.299952507019043\n",
      "   VALIDATION: Batch 160, loss 4.649785041809082\n",
      "   VALIDATION: Batch 161, loss 4.807068347930908\n",
      "   VALIDATION: Batch 162, loss 4.260094165802002\n",
      "   VALIDATION: Batch 163, loss 3.731785535812378\n",
      "   VALIDATION: Batch 164, loss 4.255640983581543\n",
      "   VALIDATION: Batch 165, loss 4.681123733520508\n",
      "   VALIDATION: Batch 166, loss 4.10687255859375\n",
      "   VALIDATION: Batch 167, loss 4.572033405303955\n",
      "   VALIDATION: Batch 168, loss 3.434587001800537\n",
      "   VALIDATION: Batch 169, loss 4.112034320831299\n",
      "   VALIDATION: Batch 170, loss 4.286412239074707\n",
      "   VALIDATION: Batch 171, loss 4.440302848815918\n",
      "   VALIDATION: Batch 172, loss 4.2895331382751465\n",
      "   VALIDATION: Batch 173, loss 4.2096266746521\n",
      "   VALIDATION: Batch 174, loss 4.614204406738281\n",
      "   VALIDATION: Batch 175, loss 4.391722679138184\n",
      "   VALIDATION: Batch 176, loss 4.169779300689697\n",
      "   VALIDATION: Batch 177, loss 4.1253662109375\n",
      "   VALIDATION: Batch 178, loss 5.194674015045166\n",
      "   VALIDATION: Batch 179, loss 4.468942165374756\n",
      "   VALIDATION: Batch 180, loss 3.983355760574341\n",
      "   VALIDATION: Batch 181, loss 4.100650310516357\n",
      "   VALIDATION: Batch 182, loss 4.419611930847168\n",
      "   VALIDATION: Batch 183, loss 3.5017364025115967\n",
      "   VALIDATION: Batch 184, loss 3.2615318298339844\n",
      "   VALIDATION: Batch 185, loss 4.0294623374938965\n",
      "   VALIDATION: Batch 186, loss 3.8945915699005127\n",
      "   VALIDATION: Batch 187, loss 4.117995738983154\n",
      "   VALIDATION: Batch 188, loss 4.522862911224365\n",
      "   VALIDATION: Batch 189, loss 3.8599724769592285\n",
      "   VALIDATION: Batch 190, loss 3.959324359893799\n",
      "   VALIDATION: Batch 191, loss 4.44165563583374\n",
      "   VALIDATION: Batch 192, loss 4.732667446136475\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 7: |          | 0/? [00:00<?, ?it/s, v_num=31]              TRRAINING: Batch 0, loss 4.172449111938477\n",
      "Epoch 7: |          | 1/? [00:01<00:00,  0.57it/s, v_num=31]   TRRAINING: Batch 1, loss 3.724064350128174\n",
      "Epoch 7: |          | 2/? [00:03<00:00,  0.63it/s, v_num=31]   TRRAINING: Batch 2, loss 3.815814256668091\n",
      "Epoch 7: |          | 3/? [00:04<00:00,  0.65it/s, v_num=31]   TRRAINING: Batch 3, loss 3.4927666187286377\n",
      "Epoch 7: |          | 4/? [00:06<00:00,  0.66it/s, v_num=31]   TRRAINING: Batch 4, loss 3.904113292694092\n",
      "Epoch 7: |          | 5/? [00:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 5, loss 4.689789772033691\n",
      "Epoch 7: |          | 6/? [00:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 6, loss 4.167903900146484\n",
      "Epoch 7: |          | 7/? [00:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 7, loss 3.5002148151397705\n",
      "Epoch 7: |          | 8/? [00:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 8, loss 3.6025421619415283\n",
      "Epoch 7: |          | 9/? [00:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 9, loss 3.920067548751831\n",
      "Epoch 7: |          | 10/? [00:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 10, loss 4.136927604675293\n",
      "Epoch 7: |          | 11/? [00:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 11, loss 4.058829307556152\n",
      "Epoch 7: |          | 12/? [00:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 12, loss 4.592543601989746\n",
      "Epoch 7: |          | 13/? [00:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 13, loss 4.014400482177734\n",
      "Epoch 7: |          | 14/? [00:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 14, loss 4.22641134262085\n",
      "Epoch 7: |          | 15/? [00:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 15, loss 3.5049960613250732\n",
      "Epoch 7: |          | 16/? [00:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 16, loss 3.3097434043884277\n",
      "Epoch 7: |          | 17/? [00:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 17, loss 4.462217807769775\n",
      "Epoch 7: |          | 18/? [00:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 18, loss 3.9738306999206543\n",
      "Epoch 7: |          | 19/? [00:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 19, loss 3.7568092346191406\n",
      "Epoch 7: |          | 20/? [00:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 20, loss 4.025067329406738\n",
      "Epoch 7: |          | 21/? [00:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 21, loss 4.169825553894043\n",
      "Epoch 7: |          | 22/? [00:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 22, loss 4.049638748168945\n",
      "Epoch 7: |          | 23/? [00:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 23, loss 3.4401049613952637\n",
      "Epoch 7: |          | 24/? [00:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 24, loss 4.071658134460449\n",
      "Epoch 7: |          | 25/? [00:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 25, loss 3.8731510639190674\n",
      "Epoch 7: |          | 26/? [00:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 26, loss 3.6444649696350098\n",
      "Epoch 7: |          | 27/? [00:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 27, loss 3.6115634441375732\n",
      "Epoch 7: |          | 28/? [00:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 28, loss 4.497499942779541\n",
      "Epoch 7: |          | 29/? [00:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 29, loss 4.028210639953613\n",
      "Epoch 7: |          | 30/? [00:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 30, loss 3.887223720550537\n",
      "Epoch 7: |          | 31/? [00:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 31, loss 4.473202705383301\n",
      "Epoch 7: |          | 32/? [00:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 32, loss 4.003121376037598\n",
      "Epoch 7: |          | 33/? [00:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 33, loss 3.8541691303253174\n",
      "Epoch 7: |          | 34/? [00:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 34, loss 3.8252010345458984\n",
      "Epoch 7: |          | 35/? [00:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 35, loss 3.2586681842803955\n",
      "Epoch 7: |          | 36/? [00:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 36, loss 4.054351806640625\n",
      "Epoch 7: |          | 37/? [00:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 37, loss 4.218138217926025\n",
      "Epoch 7: |          | 38/? [00:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 38, loss 4.530181407928467\n",
      "Epoch 7: |          | 39/? [00:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 39, loss 4.401607990264893\n",
      "Epoch 7: |          | 40/? [00:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 40, loss 3.81426739692688\n",
      "Epoch 7: |          | 41/? [00:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 41, loss 3.9016921520233154\n",
      "Epoch 7: |          | 42/? [00:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 42, loss 3.6690917015075684\n",
      "Epoch 7: |          | 43/? [01:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 43, loss 3.8557868003845215\n",
      "Epoch 7: |          | 44/? [01:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 44, loss 3.010075092315674\n",
      "Epoch 7: |          | 45/? [01:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 45, loss 2.588369846343994\n",
      "Epoch 7: |          | 46/? [01:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 46, loss 4.4268927574157715\n",
      "Epoch 7: |          | 47/? [01:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 47, loss 3.6352267265319824\n",
      "Epoch 7: |          | 48/? [01:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 48, loss 3.4942779541015625\n",
      "Epoch 7: |          | 49/? [01:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 49, loss 3.9770495891571045\n",
      "Epoch 7: |          | 50/? [01:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 50, loss 3.816279172897339\n",
      "Epoch 7: |          | 51/? [01:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 51, loss 3.7907066345214844\n",
      "Epoch 7: |          | 52/? [01:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 52, loss 4.4798383712768555\n",
      "Epoch 7: |          | 53/? [01:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 53, loss 4.087157249450684\n",
      "Epoch 7: |          | 54/? [01:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 54, loss 3.893040418624878\n",
      "Epoch 7: |          | 55/? [01:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 55, loss 3.958099365234375\n",
      "Epoch 7: |          | 56/? [01:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 56, loss 4.077801704406738\n",
      "Epoch 7: |          | 57/? [01:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 57, loss 3.9785354137420654\n",
      "Epoch 7: |          | 58/? [01:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 58, loss 5.177317142486572\n",
      "Epoch 7: |          | 59/? [01:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 59, loss 4.128085613250732\n",
      "Epoch 7: |          | 60/? [01:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 60, loss 4.276395797729492\n",
      "Epoch 7: |          | 61/? [01:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 61, loss 4.235579013824463\n",
      "Epoch 7: |          | 62/? [01:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 62, loss 3.853787899017334\n",
      "Epoch 7: |          | 63/? [01:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 63, loss 4.056791305541992\n",
      "Epoch 7: |          | 64/? [01:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 64, loss 3.8912200927734375\n",
      "Epoch 7: |          | 65/? [01:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 65, loss 3.9097118377685547\n",
      "Epoch 7: |          | 66/? [01:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 66, loss 3.3681061267852783\n",
      "Epoch 7: |          | 67/? [01:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 67, loss 4.046566963195801\n",
      "Epoch 7: |          | 68/? [01:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 68, loss 4.064310550689697\n",
      "Epoch 7: |          | 69/? [01:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 69, loss 3.9917197227478027\n",
      "Epoch 7: |          | 70/? [01:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 70, loss 3.649712324142456\n",
      "Epoch 7: |          | 71/? [01:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 71, loss 3.5987448692321777\n",
      "Epoch 7: |          | 72/? [01:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 72, loss 3.8205904960632324\n",
      "Epoch 7: |          | 73/? [01:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 73, loss 4.019903659820557\n",
      "Epoch 7: |          | 74/? [01:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 74, loss 3.7662463188171387\n",
      "Epoch 7: |          | 75/? [01:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 75, loss 3.885578155517578\n",
      "Epoch 7: |          | 76/? [01:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 76, loss 3.8972721099853516\n",
      "Epoch 7: |          | 77/? [01:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 77, loss 3.8991940021514893\n",
      "Epoch 7: |          | 78/? [01:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 78, loss 3.7509639263153076\n",
      "Epoch 7: |          | 79/? [01:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 79, loss 3.9273715019226074\n",
      "Epoch 7: |          | 80/? [01:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 80, loss 3.8451313972473145\n",
      "Epoch 7: |          | 81/? [01:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 81, loss 3.326061248779297\n",
      "Epoch 7: |          | 82/? [01:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 82, loss 4.121757984161377\n",
      "Epoch 7: |          | 83/? [01:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 83, loss 3.5855553150177\n",
      "Epoch 7: |          | 84/? [01:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 84, loss 3.44225811958313\n",
      "Epoch 7: |          | 85/? [01:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 85, loss 3.2894654273986816\n",
      "Epoch 7: |          | 86/? [02:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 86, loss 3.4590086936950684\n",
      "Epoch 7: |          | 87/? [02:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 87, loss 3.6326942443847656\n",
      "Epoch 7: |          | 88/? [02:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 88, loss 4.285870552062988\n",
      "Epoch 7: |          | 89/? [02:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 89, loss 4.26071310043335\n",
      "Epoch 7: |          | 90/? [02:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 90, loss 4.012232780456543\n",
      "Epoch 7: |          | 91/? [02:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 91, loss 3.7227425575256348\n",
      "Epoch 7: |          | 92/? [02:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 92, loss 4.194663047790527\n",
      "Epoch 7: |          | 93/? [02:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 93, loss 4.330084800720215\n",
      "Epoch 7: |          | 94/? [02:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 94, loss 4.170987129211426\n",
      "Epoch 7: |          | 95/? [02:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 95, loss 4.131629467010498\n",
      "Epoch 7: |          | 96/? [02:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 96, loss 3.7423205375671387\n",
      "Epoch 7: |          | 97/? [02:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 97, loss 3.5296833515167236\n",
      "Epoch 7: |          | 98/? [02:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 98, loss 4.065126895904541\n",
      "Epoch 7: |          | 99/? [02:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 99, loss 4.248410224914551\n",
      "Epoch 7: |          | 100/? [02:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 100, loss 4.192837715148926\n",
      "Epoch 7: |          | 101/? [02:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 101, loss 3.8505606651306152\n",
      "Epoch 7: |          | 102/? [02:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 102, loss 3.8477325439453125\n",
      "Epoch 7: |          | 103/? [02:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 103, loss 3.643306255340576\n",
      "Epoch 7: |          | 104/? [02:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 104, loss 3.9824821949005127\n",
      "Epoch 7: |          | 105/? [02:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 105, loss 3.9186744689941406\n",
      "Epoch 7: |          | 106/? [02:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 106, loss 4.0799665451049805\n",
      "Epoch 7: |          | 107/? [02:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 107, loss 4.0836639404296875\n",
      "Epoch 7: |          | 108/? [02:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 108, loss 4.035734176635742\n",
      "Epoch 7: |          | 109/? [02:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 109, loss 3.773513078689575\n",
      "Epoch 7: |          | 110/? [02:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 110, loss 3.96380352973938\n",
      "Epoch 7: |          | 111/? [02:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 111, loss 4.586386203765869\n",
      "Epoch 7: |          | 112/? [02:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 112, loss 3.2753453254699707\n",
      "Epoch 7: |          | 113/? [02:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 113, loss 2.8702468872070312\n",
      "Epoch 7: |          | 114/? [02:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 114, loss 4.169865608215332\n",
      "Epoch 7: |          | 115/? [02:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 115, loss 4.317698001861572\n",
      "Epoch 7: |          | 116/? [02:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 116, loss 3.4440531730651855\n",
      "Epoch 7: |          | 117/? [02:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 117, loss 3.4467875957489014\n",
      "Epoch 7: |          | 118/? [02:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 118, loss 4.235745429992676\n",
      "Epoch 7: |          | 119/? [02:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 119, loss 4.485063552856445\n",
      "Epoch 7: |          | 120/? [02:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 120, loss 4.247180461883545\n",
      "Epoch 7: |          | 121/? [02:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 121, loss 3.958160877227783\n",
      "Epoch 7: |          | 122/? [02:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 122, loss 3.425030469894409\n",
      "Epoch 7: |          | 123/? [02:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 123, loss 3.920637607574463\n",
      "Epoch 7: |          | 124/? [02:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 124, loss 4.079622268676758\n",
      "Epoch 7: |          | 125/? [02:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 125, loss 3.713278293609619\n",
      "Epoch 7: |          | 126/? [02:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 126, loss 4.173003196716309\n",
      "Epoch 7: |          | 127/? [02:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 127, loss 4.306210041046143\n",
      "Epoch 7: |          | 128/? [02:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 128, loss 3.4246952533721924\n",
      "Epoch 7: |          | 129/? [03:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 129, loss 4.032314777374268\n",
      "Epoch 7: |          | 130/? [03:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 130, loss 3.148123264312744\n",
      "Epoch 7: |          | 131/? [03:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 131, loss 3.9714927673339844\n",
      "Epoch 7: |          | 132/? [03:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 132, loss 3.9973888397216797\n",
      "Epoch 7: |          | 133/? [03:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 133, loss 3.8934619426727295\n",
      "Epoch 7: |          | 134/? [03:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 134, loss 4.0905351638793945\n",
      "Epoch 7: |          | 135/? [03:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 135, loss 4.159675121307373\n",
      "Epoch 7: |          | 136/? [03:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 136, loss 4.221879005432129\n",
      "Epoch 7: |          | 137/? [03:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 137, loss 3.2131752967834473\n",
      "Epoch 7: |          | 138/? [03:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 138, loss 3.7820656299591064\n",
      "Epoch 7: |          | 139/? [03:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 139, loss 4.250940322875977\n",
      "Epoch 7: |          | 140/? [03:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 140, loss 3.496739625930786\n",
      "Epoch 7: |          | 141/? [03:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 141, loss 3.5840296745300293\n",
      "Epoch 7: |          | 142/? [03:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 142, loss 5.050847053527832\n",
      "Epoch 7: |          | 143/? [03:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 143, loss 4.71467924118042\n",
      "Epoch 7: |          | 144/? [03:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 144, loss 3.9371705055236816\n",
      "Epoch 7: |          | 145/? [03:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 145, loss 3.4314446449279785\n",
      "Epoch 7: |          | 146/? [03:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 146, loss 3.5977509021759033\n",
      "Epoch 7: |          | 147/? [03:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 147, loss 3.910619020462036\n",
      "Epoch 7: |          | 148/? [03:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 148, loss 3.679600477218628\n",
      "Epoch 7: |          | 149/? [03:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 149, loss 3.2126212120056152\n",
      "Epoch 7: |          | 150/? [03:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 150, loss 4.085112571716309\n",
      "Epoch 7: |          | 151/? [03:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 151, loss 4.189205646514893\n",
      "Epoch 7: |          | 152/? [03:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 152, loss 4.113799571990967\n",
      "Epoch 7: |          | 153/? [03:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 153, loss 3.239234209060669\n",
      "Epoch 7: |          | 154/? [03:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 154, loss 4.487547874450684\n",
      "Epoch 7: |          | 155/? [03:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 155, loss 4.010367393493652\n",
      "Epoch 7: |          | 156/? [03:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 156, loss 3.390773057937622\n",
      "Epoch 7: |          | 157/? [03:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 157, loss 4.069913864135742\n",
      "Epoch 7: |          | 158/? [03:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 158, loss 4.16634464263916\n",
      "Epoch 7: |          | 159/? [03:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 159, loss 3.8836162090301514\n",
      "Epoch 7: |          | 160/? [03:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 160, loss 3.6549880504608154\n",
      "Epoch 7: |          | 161/? [03:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 161, loss 4.032179355621338\n",
      "Epoch 7: |          | 162/? [03:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 162, loss 4.043869972229004\n",
      "Epoch 7: |          | 163/? [03:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 163, loss 3.185930013656616\n",
      "Epoch 7: |          | 164/? [03:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 164, loss 3.6255042552948\n",
      "Epoch 7: |          | 165/? [03:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 165, loss 4.424511909484863\n",
      "Epoch 7: |          | 166/? [03:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 166, loss 4.089512825012207\n",
      "Epoch 7: |          | 167/? [03:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 167, loss 4.190469741821289\n",
      "Epoch 7: |          | 168/? [03:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 168, loss 3.788717269897461\n",
      "Epoch 7: |          | 169/? [03:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 169, loss 3.4448082447052\n",
      "Epoch 7: |          | 170/? [03:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 170, loss 3.6422924995422363\n",
      "Epoch 7: |          | 171/? [03:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 171, loss 4.001815319061279\n",
      "Epoch 7: |          | 172/? [03:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 172, loss 3.828477144241333\n",
      "Epoch 7: |          | 173/? [04:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 173, loss 4.440972328186035\n",
      "Epoch 7: |          | 174/? [04:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 174, loss 4.054351806640625\n",
      "Epoch 7: |          | 175/? [04:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 175, loss 4.53352165222168\n",
      "Epoch 7: |          | 176/? [04:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 176, loss 3.743140697479248\n",
      "Epoch 7: |          | 177/? [04:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 177, loss 3.8122856616973877\n",
      "Epoch 7: |          | 178/? [04:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 178, loss 3.617927074432373\n",
      "Epoch 7: |          | 179/? [04:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 179, loss 4.325131416320801\n",
      "Epoch 7: |          | 180/? [04:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 180, loss 3.847684144973755\n",
      "Epoch 7: |          | 181/? [04:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 181, loss 3.6440811157226562\n",
      "Epoch 7: |          | 182/? [04:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 182, loss 4.050607204437256\n",
      "Epoch 7: |          | 183/? [04:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 183, loss 3.536001205444336\n",
      "Epoch 7: |          | 184/? [04:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 184, loss 3.7449004650115967\n",
      "Epoch 7: |          | 185/? [04:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 185, loss 4.305497646331787\n",
      "Epoch 7: |          | 186/? [04:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 186, loss 3.7744083404541016\n",
      "Epoch 7: |          | 187/? [04:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 187, loss 4.28325891494751\n",
      "Epoch 7: |          | 188/? [04:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 188, loss 3.6760411262512207\n",
      "Epoch 7: |          | 189/? [04:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 189, loss 4.316224098205566\n",
      "Epoch 7: |          | 190/? [04:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 190, loss 3.848996639251709\n",
      "Epoch 7: |          | 191/? [04:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 191, loss 4.614176273345947\n",
      "Epoch 7: |          | 192/? [04:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 192, loss 4.348467826843262\n",
      "Epoch 7: |          | 193/? [04:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 193, loss 3.711500883102417\n",
      "Epoch 7: |          | 194/? [04:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 194, loss 3.5930778980255127\n",
      "Epoch 7: |          | 195/? [04:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 195, loss 4.273393630981445\n",
      "Epoch 7: |          | 196/? [04:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 196, loss 4.151638984680176\n",
      "Epoch 7: |          | 197/? [04:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 197, loss 4.0215229988098145\n",
      "Epoch 7: |          | 198/? [04:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 198, loss 3.339168071746826\n",
      "Epoch 7: |          | 199/? [04:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 199, loss 4.258955478668213\n",
      "Epoch 7: |          | 200/? [04:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 200, loss 3.7182869911193848\n",
      "Epoch 7: |          | 201/? [04:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 201, loss 4.0039777755737305\n",
      "Epoch 7: |          | 202/? [04:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 202, loss 4.122906684875488\n",
      "Epoch 7: |          | 203/? [04:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 203, loss 3.7820091247558594\n",
      "Epoch 7: |          | 204/? [04:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 204, loss 3.8339715003967285\n",
      "Epoch 7: |          | 205/? [04:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 205, loss 3.711905002593994\n",
      "Epoch 7: |          | 206/? [04:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 206, loss 3.5265679359436035\n",
      "Epoch 7: |          | 207/? [04:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 207, loss 4.079433917999268\n",
      "Epoch 7: |          | 208/? [04:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 208, loss 3.957005739212036\n",
      "Epoch 7: |          | 209/? [04:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 209, loss 3.7924370765686035\n",
      "Epoch 7: |          | 210/? [04:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 210, loss 4.511017322540283\n",
      "Epoch 7: |          | 211/? [04:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 211, loss 3.8176071643829346\n",
      "Epoch 7: |          | 212/? [04:55<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 212, loss 4.070598125457764\n",
      "Epoch 7: |          | 213/? [04:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 213, loss 3.9172637462615967\n",
      "Epoch 7: |          | 214/? [04:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 214, loss 3.787179470062256\n",
      "Epoch 7: |          | 215/? [04:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 215, loss 3.4485695362091064\n",
      "Epoch 7: |          | 216/? [05:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 216, loss 4.104707717895508\n",
      "Epoch 7: |          | 217/? [05:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 217, loss 4.015999794006348\n",
      "Epoch 7: |          | 218/? [05:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 218, loss 4.0371623039245605\n",
      "Epoch 7: |          | 219/? [05:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 219, loss 3.9877407550811768\n",
      "Epoch 7: |          | 220/? [05:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 220, loss 4.059573173522949\n",
      "Epoch 7: |          | 221/? [05:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 221, loss 3.8667407035827637\n",
      "Epoch 7: |          | 222/? [05:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 222, loss 3.1399459838867188\n",
      "Epoch 7: |          | 223/? [05:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 223, loss 4.1254730224609375\n",
      "Epoch 7: |          | 224/? [05:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 224, loss 4.224256992340088\n",
      "Epoch 7: |          | 225/? [05:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 225, loss 3.9489669799804688\n",
      "Epoch 7: |          | 226/? [05:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 226, loss 3.889530658721924\n",
      "Epoch 7: |          | 227/? [05:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 227, loss 4.22511100769043\n",
      "Epoch 7: |          | 228/? [05:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 228, loss 3.923786163330078\n",
      "Epoch 7: |          | 229/? [05:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 229, loss 3.9839701652526855\n",
      "Epoch 7: |          | 230/? [05:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 230, loss 3.929692506790161\n",
      "Epoch 7: |          | 231/? [05:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 231, loss 3.8738455772399902\n",
      "Epoch 7: |          | 232/? [05:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 232, loss 3.6055843830108643\n",
      "Epoch 7: |          | 233/? [05:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 233, loss 4.261749744415283\n",
      "Epoch 7: |          | 234/? [05:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 234, loss 4.277087211608887\n",
      "Epoch 7: |          | 235/? [05:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 235, loss 4.35455322265625\n",
      "Epoch 7: |          | 236/? [05:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 236, loss 3.7967724800109863\n",
      "Epoch 7: |          | 237/? [05:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 237, loss 3.9442925453186035\n",
      "Epoch 7: |          | 238/? [05:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 238, loss 4.202663898468018\n",
      "Epoch 7: |          | 239/? [05:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 239, loss 3.7731070518493652\n",
      "Epoch 7: |          | 240/? [05:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 240, loss 3.315415859222412\n",
      "Epoch 7: |          | 241/? [05:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 241, loss 3.9526679515838623\n",
      "Epoch 7: |          | 242/? [05:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 242, loss 4.292111873626709\n",
      "Epoch 7: |          | 243/? [05:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 243, loss 3.163557767868042\n",
      "Epoch 7: |          | 244/? [05:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 244, loss 3.6016082763671875\n",
      "Epoch 7: |          | 245/? [05:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 245, loss 3.927219867706299\n",
      "Epoch 7: |          | 246/? [05:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 246, loss 4.027594566345215\n",
      "Epoch 7: |          | 247/? [05:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 247, loss 4.05226469039917\n",
      "Epoch 7: |          | 248/? [05:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 248, loss 3.5919837951660156\n",
      "Epoch 7: |          | 249/? [05:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 249, loss 3.4336695671081543\n",
      "Epoch 7: |          | 250/? [05:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 250, loss 4.035694599151611\n",
      "Epoch 7: |          | 251/? [05:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 251, loss 4.029651165008545\n",
      "Epoch 7: |          | 252/? [05:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 252, loss 3.822938919067383\n",
      "Epoch 7: |          | 253/? [05:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 253, loss 4.621848106384277\n",
      "Epoch 7: |          | 254/? [05:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 254, loss 4.316077709197998\n",
      "Epoch 7: |          | 255/? [05:55<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 255, loss 3.919206142425537\n",
      "Epoch 7: |          | 256/? [05:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 256, loss 4.922584056854248\n",
      "Epoch 7: |          | 257/? [05:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 257, loss 3.797903537750244\n",
      "Epoch 7: |          | 258/? [05:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 258, loss 3.8947911262512207\n",
      "Epoch 7: |          | 259/? [06:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 259, loss 3.7785861492156982\n",
      "Epoch 7: |          | 260/? [06:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 260, loss 3.6961708068847656\n",
      "Epoch 7: |          | 261/? [06:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 261, loss 3.5112271308898926\n",
      "Epoch 7: |          | 262/? [06:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 262, loss 4.211264610290527\n",
      "Epoch 7: |          | 263/? [06:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 263, loss 3.8810107707977295\n",
      "Epoch 7: |          | 264/? [06:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 264, loss 4.0431623458862305\n",
      "Epoch 7: |          | 265/? [06:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 265, loss 3.444803237915039\n",
      "Epoch 7: |          | 266/? [06:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 266, loss 3.9331917762756348\n",
      "Epoch 7: |          | 267/? [06:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 267, loss 3.489234209060669\n",
      "Epoch 7: |          | 268/? [06:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 268, loss 3.8773021697998047\n",
      "Epoch 7: |          | 269/? [06:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 269, loss 4.253674030303955\n",
      "Epoch 7: |          | 270/? [06:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 270, loss 3.840183973312378\n",
      "Epoch 7: |          | 271/? [06:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 271, loss 4.129552841186523\n",
      "Epoch 7: |          | 272/? [06:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 272, loss 4.423804759979248\n",
      "Epoch 7: |          | 273/? [06:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 273, loss 3.5851216316223145\n",
      "Epoch 7: |          | 274/? [06:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 274, loss 4.453936576843262\n",
      "Epoch 7: |          | 275/? [06:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 275, loss 4.07368278503418\n",
      "Epoch 7: |          | 276/? [06:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 276, loss 3.3544230461120605\n",
      "Epoch 7: |          | 277/? [06:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 277, loss 4.043547630310059\n",
      "Epoch 7: |          | 278/? [06:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 278, loss 3.0637264251708984\n",
      "Epoch 7: |          | 279/? [06:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 279, loss 3.7747561931610107\n",
      "Epoch 7: |          | 280/? [06:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 280, loss 3.4801952838897705\n",
      "Epoch 7: |          | 281/? [06:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 281, loss 4.1501359939575195\n",
      "Epoch 7: |          | 282/? [06:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 282, loss 3.688716173171997\n",
      "Epoch 7: |          | 283/? [06:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 283, loss 3.819324493408203\n",
      "Epoch 7: |          | 284/? [06:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 284, loss 3.712930202484131\n",
      "Epoch 7: |          | 285/? [06:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 285, loss 3.2233664989471436\n",
      "Epoch 7: |          | 286/? [06:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 286, loss 3.792909622192383\n",
      "Epoch 7: |          | 287/? [06:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 287, loss 3.5544517040252686\n",
      "Epoch 7: |          | 288/? [06:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 288, loss 3.477449417114258\n",
      "Epoch 7: |          | 289/? [06:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 289, loss 3.341759443283081\n",
      "Epoch 7: |          | 290/? [06:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 290, loss 2.9077413082122803\n",
      "Epoch 7: |          | 291/? [06:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 291, loss 3.997471332550049\n",
      "Epoch 7: |          | 292/? [06:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 292, loss 3.656381130218506\n",
      "Epoch 7: |          | 293/? [06:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 293, loss 3.9527175426483154\n",
      "Epoch 7: |          | 294/? [06:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 294, loss 3.8314201831817627\n",
      "Epoch 7: |          | 295/? [06:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 295, loss 4.133324146270752\n",
      "Epoch 7: |          | 296/? [06:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 296, loss 3.6022510528564453\n",
      "Epoch 7: |          | 297/? [06:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 297, loss 4.30070161819458\n",
      "Epoch 7: |          | 298/? [06:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 298, loss 4.106096267700195\n",
      "Epoch 7: |          | 299/? [07:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 299, loss 4.5084357261657715\n",
      "Epoch 7: |          | 300/? [07:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 300, loss 3.8690667152404785\n",
      "Epoch 7: |          | 301/? [07:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 301, loss 3.745699405670166\n",
      "Epoch 7: |          | 302/? [07:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 302, loss 4.131307601928711\n",
      "Epoch 7: |          | 303/? [07:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 303, loss 3.9585609436035156\n",
      "Epoch 7: |          | 304/? [07:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 304, loss 4.120485305786133\n",
      "Epoch 7: |          | 305/? [07:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 305, loss 4.17680025100708\n",
      "Epoch 7: |          | 306/? [07:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 306, loss 3.836331605911255\n",
      "Epoch 7: |          | 307/? [07:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 307, loss 4.065543174743652\n",
      "Epoch 7: |          | 308/? [07:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 308, loss 4.234581470489502\n",
      "Epoch 7: |          | 309/? [07:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 309, loss 3.84755277633667\n",
      "Epoch 7: |          | 310/? [07:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 310, loss 4.2483391761779785\n",
      "Epoch 7: |          | 311/? [07:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 311, loss 3.864227294921875\n",
      "Epoch 7: |          | 312/? [07:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 312, loss 3.8478119373321533\n",
      "Epoch 7: |          | 313/? [07:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 313, loss 3.68890643119812\n",
      "Epoch 7: |          | 314/? [07:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 314, loss 4.036833763122559\n",
      "Epoch 7: |          | 315/? [07:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 315, loss 3.6681580543518066\n",
      "Epoch 7: |          | 316/? [07:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 316, loss 4.207037925720215\n",
      "Epoch 7: |          | 317/? [07:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 317, loss 4.024327278137207\n",
      "Epoch 7: |          | 318/? [07:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 318, loss 4.174844264984131\n",
      "Epoch 7: |          | 319/? [07:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 319, loss 3.4011807441711426\n",
      "Epoch 7: |          | 320/? [07:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 320, loss 3.900334119796753\n",
      "Epoch 7: |          | 321/? [07:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 321, loss 3.7108078002929688\n",
      "Epoch 7: |          | 322/? [07:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 322, loss 4.316677570343018\n",
      "Epoch 7: |          | 323/? [07:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 323, loss 4.2863264083862305\n",
      "Epoch 7: |          | 324/? [07:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 324, loss 3.99895977973938\n",
      "Epoch 7: |          | 325/? [07:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 325, loss 4.386280536651611\n",
      "Epoch 7: |          | 326/? [07:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 326, loss 3.983095645904541\n",
      "Epoch 7: |          | 327/? [07:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 327, loss 3.6901755332946777\n",
      "Epoch 7: |          | 328/? [07:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 328, loss 3.486290454864502\n",
      "Epoch 7: |          | 329/? [07:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 329, loss 4.086823463439941\n",
      "Epoch 7: |          | 330/? [07:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 330, loss 4.586257457733154\n",
      "Epoch 7: |          | 331/? [07:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 331, loss 2.8721776008605957\n",
      "Epoch 7: |          | 332/? [07:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 332, loss 3.926082134246826\n",
      "Epoch 7: |          | 333/? [07:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 333, loss 3.8165767192840576\n",
      "Epoch 7: |          | 334/? [07:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 334, loss 4.445899486541748\n",
      "Epoch 7: |          | 335/? [07:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 335, loss 4.368805408477783\n",
      "Epoch 7: |          | 336/? [07:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 336, loss 4.328113555908203\n",
      "Epoch 7: |          | 337/? [07:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 337, loss 4.892091751098633\n",
      "Epoch 7: |          | 338/? [07:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 338, loss 4.61741304397583\n",
      "Epoch 7: |          | 339/? [07:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 339, loss 3.6812796592712402\n",
      "Epoch 7: |          | 340/? [07:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 340, loss 3.476475477218628\n",
      "Epoch 7: |          | 341/? [07:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 341, loss 3.397082567214966\n",
      "Epoch 7: |          | 342/? [08:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 342, loss 4.02012825012207\n",
      "Epoch 7: |          | 343/? [08:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 343, loss 3.7137274742126465\n",
      "Epoch 7: |          | 344/? [08:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 344, loss 4.561944484710693\n",
      "Epoch 7: |          | 345/? [08:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 345, loss 3.762118101119995\n",
      "Epoch 7: |          | 346/? [08:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 346, loss 4.04396915435791\n",
      "Epoch 7: |          | 347/? [08:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 347, loss 3.8147475719451904\n",
      "Epoch 7: |          | 348/? [08:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 348, loss 3.2097480297088623\n",
      "Epoch 7: |          | 349/? [08:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 349, loss 3.063809394836426\n",
      "Epoch 7: |          | 350/? [08:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 350, loss 4.277165412902832\n",
      "Epoch 7: |          | 351/? [08:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 351, loss 4.291872501373291\n",
      "Epoch 7: |          | 352/? [08:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 352, loss 3.5932323932647705\n",
      "Epoch 7: |          | 353/? [08:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 353, loss 3.3608810901641846\n",
      "Epoch 7: |          | 354/? [08:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 354, loss 3.773738145828247\n",
      "Epoch 7: |          | 355/? [08:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 355, loss 4.083335876464844\n",
      "Epoch 7: |          | 356/? [08:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 356, loss 4.106723308563232\n",
      "Epoch 7: |          | 357/? [08:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 357, loss 3.5865261554718018\n",
      "Epoch 7: |          | 358/? [08:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 358, loss 3.5579617023468018\n",
      "Epoch 7: |          | 359/? [08:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 359, loss 4.116885185241699\n",
      "Epoch 7: |          | 360/? [08:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 360, loss 3.7367522716522217\n",
      "Epoch 7: |          | 361/? [08:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 361, loss 3.851836681365967\n",
      "Epoch 7: |          | 362/? [08:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 362, loss 3.6475510597229004\n",
      "Epoch 7: |          | 363/? [08:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 363, loss 3.5502371788024902\n",
      "Epoch 7: |          | 364/? [08:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 364, loss 4.143421173095703\n",
      "Epoch 7: |          | 365/? [08:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 365, loss 4.170186519622803\n",
      "Epoch 7: |          | 366/? [08:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 366, loss 3.979339122772217\n",
      "Epoch 7: |          | 367/? [08:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 367, loss 3.921778917312622\n",
      "Epoch 7: |          | 368/? [08:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 368, loss 3.510985851287842\n",
      "Epoch 7: |          | 369/? [08:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 369, loss 3.79620623588562\n",
      "Epoch 7: |          | 370/? [08:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 370, loss 3.472043514251709\n",
      "Epoch 7: |          | 371/? [08:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 371, loss 4.389596462249756\n",
      "Epoch 7: |          | 372/? [08:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 372, loss 3.741443157196045\n",
      "Epoch 7: |          | 373/? [08:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 373, loss 4.082186698913574\n",
      "Epoch 7: |          | 374/? [08:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 374, loss 3.7568199634552\n",
      "Epoch 7: |          | 375/? [08:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 375, loss 4.4116740226745605\n",
      "Epoch 7: |          | 376/? [08:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 376, loss 3.8137779235839844\n",
      "Epoch 7: |          | 377/? [08:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 377, loss 3.994633197784424\n",
      "Epoch 7: |          | 378/? [08:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 378, loss 4.156915664672852\n",
      "Epoch 7: |          | 379/? [08:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 379, loss 3.9695754051208496\n",
      "Epoch 7: |          | 380/? [08:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 380, loss 4.012012004852295\n",
      "Epoch 7: |          | 381/? [08:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 381, loss 4.086586952209473\n",
      "Epoch 7: |          | 382/? [08:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 382, loss 3.8121707439422607\n",
      "Epoch 7: |          | 383/? [08:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 383, loss 3.8904731273651123\n",
      "Epoch 7: |          | 384/? [08:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 384, loss 4.306702136993408\n",
      "Epoch 7: |          | 385/? [09:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 385, loss 3.934222459793091\n",
      "Epoch 7: |          | 386/? [09:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 386, loss 2.9355616569519043\n",
      "Epoch 7: |          | 387/? [09:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 387, loss 3.748018741607666\n",
      "Epoch 7: |          | 388/? [09:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 388, loss 3.5679073333740234\n",
      "Epoch 7: |          | 389/? [09:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 389, loss 4.26027774810791\n",
      "Epoch 7: |          | 390/? [09:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 390, loss 3.7432990074157715\n",
      "Epoch 7: |          | 391/? [09:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 391, loss 4.097347736358643\n",
      "Epoch 7: |          | 392/? [09:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 392, loss 4.232725620269775\n",
      "Epoch 7: |          | 393/? [09:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 393, loss 4.253726005554199\n",
      "Epoch 7: |          | 394/? [09:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 394, loss 3.9090399742126465\n",
      "Epoch 7: |          | 395/? [09:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 395, loss 4.114008903503418\n",
      "Epoch 7: |          | 396/? [09:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 396, loss 4.057817459106445\n",
      "Epoch 7: |          | 397/? [09:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 397, loss 3.8339977264404297\n",
      "Epoch 7: |          | 398/? [09:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 398, loss 3.7604827880859375\n",
      "Epoch 7: |          | 399/? [09:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 399, loss 3.8298847675323486\n",
      "Epoch 7: |          | 400/? [09:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 400, loss 3.7902140617370605\n",
      "Epoch 7: |          | 401/? [09:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 401, loss 3.7736706733703613\n",
      "Epoch 7: |          | 402/? [09:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 402, loss 4.118316650390625\n",
      "Epoch 7: |          | 403/? [09:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 403, loss 4.05510950088501\n",
      "Epoch 7: |          | 404/? [09:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 404, loss 3.5677618980407715\n",
      "Epoch 7: |          | 405/? [09:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 405, loss 3.6319282054901123\n",
      "Epoch 7: |          | 406/? [09:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 406, loss 3.9472079277038574\n",
      "Epoch 7: |          | 407/? [09:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 407, loss 3.8509838581085205\n",
      "Epoch 7: |          | 408/? [09:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 408, loss 4.280169486999512\n",
      "Epoch 7: |          | 409/? [09:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 409, loss 4.208683013916016\n",
      "Epoch 7: |          | 410/? [09:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 410, loss 3.8846435546875\n",
      "Epoch 7: |          | 411/? [09:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 411, loss 3.7802135944366455\n",
      "Epoch 7: |          | 412/? [09:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 412, loss 3.3338279724121094\n",
      "Epoch 7: |          | 413/? [09:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 413, loss 4.043981075286865\n",
      "Epoch 7: |          | 414/? [09:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 414, loss 3.6647891998291016\n",
      "Epoch 7: |          | 415/? [09:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 415, loss 4.0896830558776855\n",
      "Epoch 7: |          | 416/? [09:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 416, loss 4.490513801574707\n",
      "Epoch 7: |          | 417/? [09:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 417, loss 4.497000694274902\n",
      "Epoch 7: |          | 418/? [09:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 418, loss 4.081780910491943\n",
      "Epoch 7: |          | 419/? [09:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 419, loss 3.8729584217071533\n",
      "Epoch 7: |          | 420/? [09:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 420, loss 4.029503345489502\n",
      "Epoch 7: |          | 421/? [09:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 421, loss 4.515735626220703\n",
      "Epoch 7: |          | 422/? [09:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 422, loss 4.058887958526611\n",
      "Epoch 7: |          | 423/? [09:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 423, loss 3.6732559204101562\n",
      "Epoch 7: |          | 424/? [09:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 424, loss 4.22236442565918\n",
      "Epoch 7: |          | 425/? [09:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 425, loss 3.9316437244415283\n",
      "Epoch 7: |          | 426/? [09:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 426, loss 3.661365032196045\n",
      "Epoch 7: |          | 427/? [09:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 427, loss 3.7434887886047363\n",
      "Epoch 7: |          | 428/? [09:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 428, loss 4.466107368469238\n",
      "Epoch 7: |          | 429/? [10:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 429, loss 3.374603748321533\n",
      "Epoch 7: |          | 430/? [10:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 430, loss 4.019454002380371\n",
      "Epoch 7: |          | 431/? [10:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 431, loss 3.9056029319763184\n",
      "Epoch 7: |          | 432/? [10:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 432, loss 4.041210174560547\n",
      "Epoch 7: |          | 433/? [10:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 433, loss 4.029933929443359\n",
      "Epoch 7: |          | 434/? [10:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 434, loss 3.897902727127075\n",
      "Epoch 7: |          | 435/? [10:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 435, loss 3.6103858947753906\n",
      "Epoch 7: |          | 436/? [10:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 436, loss 3.9698290824890137\n",
      "Epoch 7: |          | 437/? [10:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 437, loss 4.167779922485352\n",
      "Epoch 7: |          | 438/? [10:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 438, loss 3.806269407272339\n",
      "Epoch 7: |          | 439/? [10:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 439, loss 3.688025951385498\n",
      "Epoch 7: |          | 440/? [10:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 440, loss 3.624790668487549\n",
      "Epoch 7: |          | 441/? [10:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 441, loss 3.995537519454956\n",
      "Epoch 7: |          | 442/? [10:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 442, loss 3.7862181663513184\n",
      "Epoch 7: |          | 443/? [10:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 443, loss 3.912311553955078\n",
      "Epoch 7: |          | 444/? [10:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 444, loss 3.9535605907440186\n",
      "Epoch 7: |          | 445/? [10:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 445, loss 4.885188579559326\n",
      "Epoch 7: |          | 446/? [10:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 446, loss 3.914924144744873\n",
      "Epoch 7: |          | 447/? [10:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 447, loss 4.433619976043701\n",
      "Epoch 7: |          | 448/? [10:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 448, loss 3.491922378540039\n",
      "Epoch 7: |          | 449/? [10:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 449, loss 3.932788848876953\n",
      "Epoch 7: |          | 450/? [10:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 450, loss 4.257275581359863\n",
      "Epoch 7: |          | 451/? [10:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 451, loss 3.861268997192383\n",
      "Epoch 7: |          | 452/? [10:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 452, loss 3.6572041511535645\n",
      "Epoch 7: |          | 453/? [10:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 453, loss 4.342001914978027\n",
      "Epoch 7: |          | 454/? [10:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 454, loss 3.732196092605591\n",
      "Epoch 7: |          | 455/? [10:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 455, loss 4.082700252532959\n",
      "Epoch 7: |          | 456/? [10:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 456, loss 3.4033894538879395\n",
      "Epoch 7: |          | 457/? [10:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 457, loss 3.8543362617492676\n",
      "Epoch 7: |          | 458/? [10:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 458, loss 4.326587677001953\n",
      "Epoch 7: |          | 459/? [10:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 459, loss 4.275412082672119\n",
      "Epoch 7: |          | 460/? [10:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 460, loss 4.046157360076904\n",
      "Epoch 7: |          | 461/? [10:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 461, loss 3.9510021209716797\n",
      "Epoch 7: |          | 462/? [10:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 462, loss 4.067068099975586\n",
      "Epoch 7: |          | 463/? [10:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 463, loss 3.892699718475342\n",
      "Epoch 7: |          | 464/? [10:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 464, loss 3.4169068336486816\n",
      "Epoch 7: |          | 465/? [10:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 465, loss 3.688382387161255\n",
      "Epoch 7: |          | 466/? [10:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 466, loss 4.163311958312988\n",
      "Epoch 7: |          | 467/? [10:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 467, loss 3.9666786193847656\n",
      "Epoch 7: |          | 468/? [10:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 468, loss 3.8989062309265137\n",
      "Epoch 7: |          | 469/? [10:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 469, loss 4.006997108459473\n",
      "Epoch 7: |          | 470/? [10:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 470, loss 3.4284541606903076\n",
      "Epoch 7: |          | 471/? [10:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 471, loss 4.207633018493652\n",
      "Epoch 7: |          | 472/? [11:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 472, loss 3.7029502391815186\n",
      "Epoch 7: |          | 473/? [11:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 473, loss 3.7003250122070312\n",
      "Epoch 7: |          | 474/? [11:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 474, loss 3.3787646293640137\n",
      "Epoch 7: |          | 475/? [11:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 475, loss 4.613848686218262\n",
      "Epoch 7: |          | 476/? [11:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 476, loss 3.651221752166748\n",
      "Epoch 7: |          | 477/? [11:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 477, loss 3.198716640472412\n",
      "Epoch 7: |          | 478/? [11:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 478, loss 3.412928819656372\n",
      "Epoch 7: |          | 479/? [11:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 479, loss 3.8405921459198\n",
      "Epoch 7: |          | 480/? [11:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 480, loss 3.750645160675049\n",
      "Epoch 7: |          | 481/? [11:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 481, loss 3.411226987838745\n",
      "Epoch 7: |          | 482/? [11:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 482, loss 3.6828033924102783\n",
      "Epoch 7: |          | 483/? [11:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 483, loss 3.360661745071411\n",
      "Epoch 7: |          | 484/? [11:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 484, loss 4.279263973236084\n",
      "Epoch 7: |          | 485/? [11:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 485, loss 4.18478536605835\n",
      "Epoch 7: |          | 486/? [11:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 486, loss 3.7524123191833496\n",
      "Epoch 7: |          | 487/? [11:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 487, loss 4.069878578186035\n",
      "Epoch 7: |          | 488/? [11:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 488, loss 3.784982681274414\n",
      "Epoch 7: |          | 489/? [11:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 489, loss 3.376741886138916\n",
      "Epoch 7: |          | 490/? [11:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 490, loss 3.974543809890747\n",
      "Epoch 7: |          | 491/? [11:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 491, loss 3.8318228721618652\n",
      "Epoch 7: |          | 492/? [11:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 492, loss 3.1627402305603027\n",
      "Epoch 7: |          | 493/? [11:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 493, loss 4.1735944747924805\n",
      "Epoch 7: |          | 494/? [11:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 494, loss 3.961632490158081\n",
      "Epoch 7: |          | 495/? [11:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 495, loss 4.092533111572266\n",
      "Epoch 7: |          | 496/? [11:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 496, loss 3.666217088699341\n",
      "Epoch 7: |          | 497/? [11:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 497, loss 4.252203941345215\n",
      "Epoch 7: |          | 498/? [11:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 498, loss 3.8644256591796875\n",
      "Epoch 7: |          | 499/? [11:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 499, loss 3.9776711463928223\n",
      "Epoch 7: |          | 500/? [11:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 500, loss 3.7681052684783936\n",
      "Epoch 7: |          | 501/? [11:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 501, loss 3.5217792987823486\n",
      "Epoch 7: |          | 502/? [11:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 502, loss 3.9667344093322754\n",
      "Epoch 7: |          | 503/? [11:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 503, loss 3.9073753356933594\n",
      "Epoch 7: |          | 504/? [11:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 504, loss 3.7996978759765625\n",
      "Epoch 7: |          | 505/? [11:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 505, loss 3.287005662918091\n",
      "Epoch 7: |          | 506/? [11:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 506, loss 3.881227970123291\n",
      "Epoch 7: |          | 507/? [11:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 507, loss 3.896310806274414\n",
      "Epoch 7: |          | 508/? [11:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 508, loss 4.270120620727539\n",
      "Epoch 7: |          | 509/? [11:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 509, loss 3.630031108856201\n",
      "Epoch 7: |          | 510/? [11:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 510, loss 4.102598667144775\n",
      "Epoch 7: |          | 511/? [11:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 511, loss 3.9398179054260254\n",
      "Epoch 7: |          | 512/? [11:55<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 512, loss 3.35400390625\n",
      "Epoch 7: |          | 513/? [11:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 513, loss 3.6106059551239014\n",
      "Epoch 7: |          | 514/? [11:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 514, loss 3.7908852100372314\n",
      "Epoch 7: |          | 515/? [11:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 515, loss 3.4442038536071777\n",
      "Epoch 7: |          | 516/? [12:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 516, loss 3.7118465900421143\n",
      "Epoch 7: |          | 517/? [12:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 517, loss 3.975964069366455\n",
      "Epoch 7: |          | 518/? [12:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 518, loss 3.511897325515747\n",
      "Epoch 7: |          | 519/? [12:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 519, loss 3.9741408824920654\n",
      "Epoch 7: |          | 520/? [12:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 520, loss 3.8283698558807373\n",
      "Epoch 7: |          | 521/? [12:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 521, loss 3.8059420585632324\n",
      "Epoch 7: |          | 522/? [12:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 522, loss 4.36270809173584\n",
      "Epoch 7: |          | 523/? [12:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 523, loss 4.444314479827881\n",
      "Epoch 7: |          | 524/? [12:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 524, loss 4.217013359069824\n",
      "Epoch 7: |          | 525/? [12:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 525, loss 3.8138022422790527\n",
      "Epoch 7: |          | 526/? [12:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 526, loss 3.5997841358184814\n",
      "Epoch 7: |          | 527/? [12:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 527, loss 4.274507999420166\n",
      "Epoch 7: |          | 528/? [12:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 528, loss 3.998121976852417\n",
      "Epoch 7: |          | 529/? [12:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 529, loss 3.653529405593872\n",
      "Epoch 7: |          | 530/? [12:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 530, loss 4.202757835388184\n",
      "Epoch 7: |          | 531/? [12:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 531, loss 3.7122726440429688\n",
      "Epoch 7: |          | 532/? [12:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 532, loss 4.012806415557861\n",
      "Epoch 7: |          | 533/? [12:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 533, loss 3.6308600902557373\n",
      "Epoch 7: |          | 534/? [12:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 534, loss 3.305577516555786\n",
      "Epoch 7: |          | 535/? [12:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 535, loss 3.604090452194214\n",
      "Epoch 7: |          | 536/? [12:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 536, loss 4.256336688995361\n",
      "Epoch 7: |          | 537/? [12:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 537, loss 4.036931037902832\n",
      "Epoch 7: |          | 538/? [12:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 538, loss 3.6819217205047607\n",
      "Epoch 7: |          | 539/? [12:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 539, loss 3.7650017738342285\n",
      "Epoch 7: |          | 540/? [12:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 540, loss 4.146805286407471\n",
      "Epoch 7: |          | 541/? [12:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 541, loss 3.9233767986297607\n",
      "Epoch 7: |          | 542/? [12:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 542, loss 3.698514461517334\n",
      "Epoch 7: |          | 543/? [12:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 543, loss 4.060433387756348\n",
      "Epoch 7: |          | 544/? [12:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 544, loss 4.000170707702637\n",
      "Epoch 7: |          | 545/? [12:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 545, loss 3.2637362480163574\n",
      "Epoch 7: |          | 546/? [12:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 546, loss 4.005825042724609\n",
      "Epoch 7: |          | 547/? [12:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 547, loss 4.505354881286621\n",
      "Epoch 7: |          | 548/? [12:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 548, loss 4.124687194824219\n",
      "Epoch 7: |          | 549/? [12:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 549, loss 4.0041184425354\n",
      "Epoch 7: |          | 550/? [12:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 550, loss 4.342272758483887\n",
      "Epoch 7: |          | 551/? [12:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 551, loss 3.9997315406799316\n",
      "Epoch 7: |          | 552/? [12:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 552, loss 3.96657133102417\n",
      "Epoch 7: |          | 553/? [12:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 553, loss 3.4584903717041016\n",
      "Epoch 7: |          | 554/? [12:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 554, loss 4.000686168670654\n",
      "Epoch 7: |          | 555/? [12:55<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 555, loss 4.324092864990234\n",
      "Epoch 7: |          | 556/? [12:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 556, loss 4.089667320251465\n",
      "Epoch 7: |          | 557/? [12:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 557, loss 3.572721481323242\n",
      "Epoch 7: |          | 558/? [13:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 558, loss 3.8167030811309814\n",
      "Epoch 7: |          | 559/? [13:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 559, loss 3.8049933910369873\n",
      "Epoch 7: |          | 560/? [13:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 560, loss 3.289583921432495\n",
      "Epoch 7: |          | 561/? [13:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 561, loss 3.0668411254882812\n",
      "Epoch 7: |          | 562/? [13:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 562, loss 4.203933238983154\n",
      "Epoch 7: |          | 563/? [13:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 563, loss 3.2837677001953125\n",
      "Epoch 7: |          | 564/? [13:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 564, loss 3.7322113513946533\n",
      "Epoch 7: |          | 565/? [13:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 565, loss 4.133481025695801\n",
      "Epoch 7: |          | 566/? [13:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 566, loss 4.179506778717041\n",
      "Epoch 7: |          | 567/? [13:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 567, loss 4.294056415557861\n",
      "Epoch 7: |          | 568/? [13:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 568, loss 3.3866653442382812\n",
      "Epoch 7: |          | 569/? [13:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 569, loss 4.02054500579834\n",
      "Epoch 7: |          | 570/? [13:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 570, loss 4.10430383682251\n",
      "Epoch 7: |          | 571/? [13:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 571, loss 3.68497896194458\n",
      "Epoch 7: |          | 572/? [13:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 572, loss 4.602407932281494\n",
      "Epoch 7: |          | 573/? [13:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 573, loss 3.047278642654419\n",
      "Epoch 7: |          | 574/? [13:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 574, loss 4.1776604652404785\n",
      "Epoch 7: |          | 575/? [13:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 575, loss 3.5393900871276855\n",
      "Epoch 7: |          | 576/? [13:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 576, loss 3.7654857635498047\n",
      "Epoch 7: |          | 577/? [13:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 577, loss 3.9769012928009033\n",
      "Epoch 7: |          | 578/? [13:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 578, loss 4.196684837341309\n",
      "Epoch 7: |          | 579/? [13:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 579, loss 3.3432674407958984\n",
      "Epoch 7: |          | 580/? [13:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 580, loss 3.996520519256592\n",
      "Epoch 7: |          | 581/? [13:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 581, loss 4.012771129608154\n",
      "Epoch 7: |          | 582/? [13:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 582, loss 4.100800514221191\n",
      "Epoch 7: |          | 583/? [13:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 583, loss 3.8548636436462402\n",
      "Epoch 7: |          | 584/? [13:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 584, loss 4.066532611846924\n",
      "Epoch 7: |          | 585/? [13:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 585, loss 4.046388626098633\n",
      "Epoch 7: |          | 586/? [13:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 586, loss 4.086606025695801\n",
      "Epoch 7: |          | 587/? [13:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 587, loss 4.122078895568848\n",
      "Epoch 7: |          | 588/? [13:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 588, loss 4.047017574310303\n",
      "Epoch 7: |          | 589/? [13:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 589, loss 3.5082175731658936\n",
      "Epoch 7: |          | 590/? [13:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 590, loss 4.088291645050049\n",
      "Epoch 7: |          | 591/? [13:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 591, loss 3.9807987213134766\n",
      "Epoch 7: |          | 592/? [13:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 592, loss 3.656653881072998\n",
      "Epoch 7: |          | 593/? [13:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 593, loss 3.9360263347625732\n",
      "Epoch 7: |          | 594/? [13:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 594, loss 4.722381114959717\n",
      "Epoch 7: |          | 595/? [13:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 595, loss 3.4855895042419434\n",
      "Epoch 7: |          | 596/? [13:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 596, loss 3.539930820465088\n",
      "Epoch 7: |          | 597/? [13:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 597, loss 3.7625019550323486\n",
      "Epoch 7: |          | 598/? [13:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 598, loss 4.264863014221191\n",
      "Epoch 7: |          | 599/? [13:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 599, loss 3.8899199962615967\n",
      "Epoch 7: |          | 600/? [13:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 600, loss 3.686413526535034\n",
      "Epoch 7: |          | 601/? [13:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 601, loss 3.946519374847412\n",
      "Epoch 7: |          | 602/? [14:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 602, loss 3.5280399322509766\n",
      "Epoch 7: |          | 603/? [14:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 603, loss 3.614823818206787\n",
      "Epoch 7: |          | 604/? [14:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 604, loss 5.008162498474121\n",
      "Epoch 7: |          | 605/? [14:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 605, loss 3.3547940254211426\n",
      "Epoch 7: |          | 606/? [14:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 606, loss 3.675508499145508\n",
      "Epoch 7: |          | 607/? [14:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 607, loss 3.9997801780700684\n",
      "Epoch 7: |          | 608/? [14:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 608, loss 3.7874398231506348\n",
      "Epoch 7: |          | 609/? [14:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 609, loss 3.6861732006073\n",
      "Epoch 7: |          | 610/? [14:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 610, loss 3.771930694580078\n",
      "Epoch 7: |          | 611/? [14:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 611, loss 3.9309401512145996\n",
      "Epoch 7: |          | 612/? [14:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 612, loss 3.669254779815674\n",
      "Epoch 7: |          | 613/? [14:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 613, loss 3.980539321899414\n",
      "Epoch 7: |          | 614/? [14:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 614, loss 3.80210542678833\n",
      "Epoch 7: |          | 615/? [14:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 615, loss 4.318611145019531\n",
      "Epoch 7: |          | 616/? [14:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 616, loss 4.4703264236450195\n",
      "Epoch 7: |          | 617/? [14:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 617, loss 3.0074245929718018\n",
      "Epoch 7: |          | 618/? [14:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 618, loss 4.015472412109375\n",
      "Epoch 7: |          | 619/? [14:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 619, loss 3.522221326828003\n",
      "Epoch 7: |          | 620/? [14:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 620, loss 4.122501850128174\n",
      "Epoch 7: |          | 621/? [14:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 621, loss 3.58827543258667\n",
      "Epoch 7: |          | 622/? [14:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 622, loss 3.363576889038086\n",
      "Epoch 7: |          | 623/? [14:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 623, loss 3.1386866569519043\n",
      "Epoch 7: |          | 624/? [14:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 624, loss 2.858898639678955\n",
      "Epoch 7: |          | 625/? [14:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 625, loss 4.366074085235596\n",
      "Epoch 7: |          | 626/? [14:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 626, loss 3.839357376098633\n",
      "Epoch 7: |          | 627/? [14:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 627, loss 3.7435805797576904\n",
      "Epoch 7: |          | 628/? [14:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 628, loss 3.726968765258789\n",
      "Epoch 7: |          | 629/? [14:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 629, loss 4.113195896148682\n",
      "Epoch 7: |          | 630/? [14:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 630, loss 3.8838257789611816\n",
      "Epoch 7: |          | 631/? [14:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 631, loss 4.005239009857178\n",
      "Epoch 7: |          | 632/? [14:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 632, loss 3.338080883026123\n",
      "Epoch 7: |          | 633/? [14:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 633, loss 4.09732723236084\n",
      "Epoch 7: |          | 634/? [14:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 634, loss 3.675309658050537\n",
      "Epoch 7: |          | 635/? [14:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 635, loss 3.4691295623779297\n",
      "Epoch 7: |          | 636/? [14:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 636, loss 3.892111301422119\n",
      "Epoch 7: |          | 637/? [14:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 637, loss 3.6688995361328125\n",
      "Epoch 7: |          | 638/? [14:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 638, loss 3.92448091506958\n",
      "Epoch 7: |          | 639/? [14:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 639, loss 3.7019190788269043\n",
      "Epoch 7: |          | 640/? [14:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 640, loss 4.267892360687256\n",
      "Epoch 7: |          | 641/? [14:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 641, loss 3.2767345905303955\n",
      "Epoch 7: |          | 642/? [14:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 642, loss 4.044050693511963\n",
      "Epoch 7: |          | 643/? [14:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 643, loss 3.9577980041503906\n",
      "Epoch 7: |          | 644/? [14:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 644, loss 3.908273696899414\n",
      "Epoch 7: |          | 645/? [15:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 645, loss 3.64235258102417\n",
      "Epoch 7: |          | 646/? [15:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 646, loss 3.684929609298706\n",
      "Epoch 7: |          | 647/? [15:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 647, loss 4.22910213470459\n",
      "Epoch 7: |          | 648/? [15:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 648, loss 3.641414165496826\n",
      "Epoch 7: |          | 649/? [15:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 649, loss 3.124541997909546\n",
      "Epoch 7: |          | 650/? [15:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 650, loss 4.1842122077941895\n",
      "Epoch 7: |          | 651/? [15:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 651, loss 4.296535015106201\n",
      "Epoch 7: |          | 652/? [15:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 652, loss 3.74225115776062\n",
      "Epoch 7: |          | 653/? [15:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 653, loss 3.888495683670044\n",
      "Epoch 7: |          | 654/? [15:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 654, loss 3.9673209190368652\n",
      "Epoch 7: |          | 655/? [15:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 655, loss 3.827580690383911\n",
      "Epoch 7: |          | 656/? [15:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 656, loss 3.4381237030029297\n",
      "Epoch 7: |          | 657/? [15:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 657, loss 5.873912334442139\n",
      "Epoch 7: |          | 658/? [15:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 658, loss 3.3813068866729736\n",
      "Epoch 7: |          | 659/? [15:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 659, loss 3.894455671310425\n",
      "Epoch 7: |          | 660/? [15:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 660, loss 4.218039512634277\n",
      "Epoch 7: |          | 661/? [15:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 661, loss 4.128489017486572\n",
      "Epoch 7: |          | 662/? [15:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 662, loss 4.032154083251953\n",
      "Epoch 7: |          | 663/? [15:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 663, loss 3.7862167358398438\n",
      "Epoch 7: |          | 664/? [15:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 664, loss 3.7173972129821777\n",
      "Epoch 7: |          | 665/? [15:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 665, loss 4.013345718383789\n",
      "Epoch 7: |          | 666/? [15:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 666, loss 3.8264102935791016\n",
      "Epoch 7: |          | 667/? [15:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 667, loss 4.635616779327393\n",
      "Epoch 7: |          | 668/? [15:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 668, loss 3.405649185180664\n",
      "Epoch 7: |          | 669/? [15:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 669, loss 3.6368002891540527\n",
      "Epoch 7: |          | 670/? [15:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 670, loss 4.295216083526611\n",
      "Epoch 7: |          | 671/? [15:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 671, loss 4.031072616577148\n",
      "Epoch 7: |          | 672/? [15:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 672, loss 4.043917179107666\n",
      "Epoch 7: |          | 673/? [15:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 673, loss 3.89147686958313\n",
      "Epoch 7: |          | 674/? [15:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 674, loss 2.3273065090179443\n",
      "Epoch 7: |          | 675/? [15:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 675, loss 0.8832910656929016\n",
      "Epoch 7: |          | 676/? [15:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 676, loss 0.8232029676437378\n",
      "Epoch 7: |          | 677/? [15:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 677, loss 0.6405182480812073\n",
      "Epoch 7: |          | 678/? [15:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 678, loss 1.7695293426513672\n",
      "Epoch 7: |          | 679/? [15:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 679, loss 3.316627025604248\n",
      "Epoch 7: |          | 680/? [15:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 680, loss 3.7857890129089355\n",
      "Epoch 7: |          | 681/? [15:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 681, loss 3.3567264080047607\n",
      "Epoch 7: |          | 682/? [15:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 682, loss 3.63641357421875\n",
      "Epoch 7: |          | 683/? [15:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 683, loss 3.3383097648620605\n",
      "Epoch 7: |          | 684/? [15:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 684, loss 4.396665573120117\n",
      "Epoch 7: |          | 685/? [15:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 685, loss 4.01761531829834\n",
      "Epoch 7: |          | 686/? [15:55<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 686, loss 3.625633716583252\n",
      "Epoch 7: |          | 687/? [15:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 687, loss 4.113282680511475\n",
      "Epoch 7: |          | 688/? [15:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 688, loss 3.66990327835083\n",
      "Epoch 7: |          | 689/? [16:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 689, loss 3.738816738128662\n",
      "Epoch 7: |          | 690/? [16:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 690, loss 4.384964466094971\n",
      "Epoch 7: |          | 691/? [16:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 691, loss 3.861530303955078\n",
      "Epoch 7: |          | 692/? [16:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 692, loss 3.8738341331481934\n",
      "Epoch 7: |          | 693/? [16:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 693, loss 4.4285478591918945\n",
      "Epoch 7: |          | 694/? [16:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 694, loss 3.7593445777893066\n",
      "Epoch 7: |          | 695/? [16:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 695, loss 4.305760383605957\n",
      "Epoch 7: |          | 696/? [16:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 696, loss 3.5861191749572754\n",
      "Epoch 7: |          | 697/? [16:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 697, loss 3.8047709465026855\n",
      "Epoch 7: |          | 698/? [16:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 698, loss 3.23248553276062\n",
      "Epoch 7: |          | 699/? [16:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 699, loss 3.9785447120666504\n",
      "Epoch 7: |          | 700/? [16:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 700, loss 4.048155784606934\n",
      "Epoch 7: |          | 701/? [16:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 701, loss 3.6747660636901855\n",
      "Epoch 7: |          | 702/? [16:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 702, loss 3.934385299682617\n",
      "Epoch 7: |          | 703/? [16:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 703, loss 4.090991020202637\n",
      "Epoch 7: |          | 704/? [16:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 704, loss 3.9479241371154785\n",
      "Epoch 7: |          | 705/? [16:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 705, loss 3.5794670581817627\n",
      "Epoch 7: |          | 706/? [16:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 706, loss 3.6331405639648438\n",
      "Epoch 7: |          | 707/? [16:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 707, loss 4.12153959274292\n",
      "Epoch 7: |          | 708/? [16:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 708, loss 3.839550018310547\n",
      "Epoch 7: |          | 709/? [16:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 709, loss 3.7610230445861816\n",
      "Epoch 7: |          | 710/? [16:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 710, loss 4.320805549621582\n",
      "Epoch 7: |          | 711/? [16:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 711, loss 4.417454719543457\n",
      "Epoch 7: |          | 712/? [16:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 712, loss 4.075359344482422\n",
      "Epoch 7: |          | 713/? [16:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 713, loss 4.156418800354004\n",
      "Epoch 7: |          | 714/? [16:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 714, loss 4.237833499908447\n",
      "Epoch 7: |          | 715/? [16:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 715, loss 3.2007644176483154\n",
      "Epoch 7: |          | 716/? [16:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 716, loss 3.8914902210235596\n",
      "Epoch 7: |          | 717/? [16:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 717, loss 3.7610859870910645\n",
      "Epoch 7: |          | 718/? [16:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 718, loss 3.2990870475769043\n",
      "Epoch 7: |          | 719/? [16:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 719, loss 3.8502776622772217\n",
      "Epoch 7: |          | 720/? [16:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 720, loss 3.4972853660583496\n",
      "Epoch 7: |          | 721/? [16:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 721, loss 4.177663803100586\n",
      "Epoch 7: |          | 722/? [16:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 722, loss 3.4940075874328613\n",
      "Epoch 7: |          | 723/? [16:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 723, loss 4.0683207511901855\n",
      "Epoch 7: |          | 724/? [16:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 724, loss 3.618722438812256\n",
      "Epoch 7: |          | 725/? [16:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 725, loss 3.545975923538208\n",
      "Epoch 7: |          | 726/? [16:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 726, loss 3.739417314529419\n",
      "Epoch 7: |          | 727/? [16:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 727, loss 3.533933639526367\n",
      "Epoch 7: |          | 728/? [16:55<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 728, loss 3.3352649211883545\n",
      "Epoch 7: |          | 729/? [16:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 729, loss 3.8331298828125\n",
      "Epoch 7: |          | 730/? [16:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 730, loss 3.7883803844451904\n",
      "Epoch 7: |          | 731/? [16:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 731, loss 3.9102015495300293\n",
      "Epoch 7: |          | 732/? [17:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 732, loss 4.178138256072998\n",
      "Epoch 7: |          | 733/? [17:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 733, loss 3.8924148082733154\n",
      "Epoch 7: |          | 734/? [17:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 734, loss 4.088838577270508\n",
      "Epoch 7: |          | 735/? [17:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 735, loss 3.9828362464904785\n",
      "Epoch 7: |          | 736/? [17:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 736, loss 3.5148422718048096\n",
      "Epoch 7: |          | 737/? [17:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 737, loss 4.299454689025879\n",
      "Epoch 7: |          | 738/? [17:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 738, loss 3.481032133102417\n",
      "Epoch 7: |          | 739/? [17:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 739, loss 3.958242416381836\n",
      "Epoch 7: |          | 740/? [17:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 740, loss 3.663764238357544\n",
      "Epoch 7: |          | 741/? [17:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 741, loss 3.828451633453369\n",
      "Epoch 7: |          | 742/? [17:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 742, loss 4.219303607940674\n",
      "Epoch 7: |          | 743/? [17:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 743, loss 4.065121650695801\n",
      "Epoch 7: |          | 744/? [17:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 744, loss 4.0214738845825195\n",
      "Epoch 7: |          | 745/? [17:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 745, loss 3.6599602699279785\n",
      "Epoch 7: |          | 746/? [17:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 746, loss 3.914194107055664\n",
      "Epoch 7: |          | 747/? [17:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 747, loss 3.6375739574432373\n",
      "Epoch 7: |          | 748/? [17:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 748, loss 2.6781580448150635\n",
      "Epoch 7: |          | 749/? [17:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 749, loss 3.822064161300659\n",
      "Epoch 7: |          | 750/? [17:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 750, loss 4.094432353973389\n",
      "Epoch 7: |          | 751/? [17:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 751, loss 2.4054770469665527\n",
      "Epoch 7: |          | 752/? [17:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 752, loss 3.9882216453552246\n",
      "Epoch 7: |          | 753/? [17:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 753, loss 3.1718990802764893\n",
      "Epoch 7: |          | 754/? [17:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 754, loss 3.6121952533721924\n",
      "Epoch 7: |          | 755/? [17:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 755, loss 3.443650722503662\n",
      "Epoch 7: |          | 756/? [17:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 756, loss 3.824665069580078\n",
      "Epoch 7: |          | 757/? [17:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 757, loss 3.9047951698303223\n",
      "Epoch 7: |          | 758/? [17:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 758, loss 3.643690824508667\n",
      "Epoch 7: |          | 759/? [17:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 759, loss 3.5900001525878906\n",
      "Epoch 7: |          | 760/? [17:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 760, loss 4.098586082458496\n",
      "Epoch 7: |          | 761/? [17:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 761, loss 4.0964765548706055\n",
      "Epoch 7: |          | 762/? [17:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 762, loss 3.7606005668640137\n",
      "Epoch 7: |          | 763/? [17:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 763, loss 3.9146487712860107\n",
      "Epoch 7: |          | 764/? [17:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 764, loss 4.1829752922058105\n",
      "Epoch 7: |          | 765/? [17:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 765, loss 3.9407927989959717\n",
      "Epoch 7: |          | 766/? [17:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 766, loss 4.345770835876465\n",
      "Epoch 7: |          | 767/? [17:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 767, loss 4.37048864364624\n",
      "Epoch 7: |          | 768/? [17:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 768, loss 3.951719284057617\n",
      "Epoch 7: |          | 769/? [17:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 769, loss 3.1495728492736816\n",
      "Epoch 7: |          | 770/? [17:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 770, loss 3.7287590503692627\n",
      "Epoch 7: |          | 771/? [17:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 771, loss 4.4365739822387695\n",
      "Epoch 7: |          | 772/? [17:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 772, loss 4.1259236335754395\n",
      "Epoch 7: |          | 773/? [17:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 773, loss 3.840167284011841\n",
      "Epoch 7: |          | 774/? [17:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 774, loss 3.963829755783081\n",
      "Epoch 7: |          | 775/? [18:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 775, loss 4.419295310974121\n",
      "Epoch 7: |          | 776/? [18:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 776, loss 3.8311667442321777\n",
      "Epoch 7: |          | 777/? [18:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 777, loss 3.7246081829071045\n",
      "Epoch 7: |          | 778/? [18:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 778, loss 4.104527950286865\n",
      "Epoch 7: |          | 779/? [18:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 779, loss 4.556524753570557\n",
      "Epoch 7: |          | 780/? [18:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 780, loss 3.5643153190612793\n",
      "Epoch 7: |          | 781/? [18:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 781, loss 3.6654422283172607\n",
      "Epoch 7: |          | 782/? [18:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 782, loss 4.037775993347168\n",
      "Epoch 7: |          | 783/? [18:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 783, loss 4.12164306640625\n",
      "Epoch 7: |          | 784/? [18:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 784, loss 3.6896586418151855\n",
      "Epoch 7: |          | 785/? [18:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 785, loss 3.4539763927459717\n",
      "Epoch 7: |          | 786/? [18:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 786, loss 4.318630218505859\n",
      "Epoch 7: |          | 787/? [18:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 787, loss 4.266934394836426\n",
      "Epoch 7: |          | 788/? [18:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 788, loss 2.2074172496795654\n",
      "Epoch 7: |          | 789/? [18:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 789, loss 3.792924165725708\n",
      "Epoch 7: |          | 790/? [18:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 790, loss 4.608733177185059\n",
      "Epoch 7: |          | 791/? [18:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 791, loss 4.341001033782959\n",
      "Epoch 7: |          | 792/? [18:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 792, loss 3.4810359477996826\n",
      "Epoch 7: |          | 793/? [18:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 793, loss 4.040244102478027\n",
      "Epoch 7: |          | 794/? [18:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 794, loss 4.346031188964844\n",
      "Epoch 7: |          | 795/? [18:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 795, loss 3.820432662963867\n",
      "Epoch 7: |          | 796/? [18:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 796, loss 4.214014053344727\n",
      "Epoch 7: |          | 797/? [18:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 797, loss 3.2233214378356934\n",
      "Epoch 7: |          | 798/? [18:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 798, loss 3.3190903663635254\n",
      "Epoch 7: |          | 799/? [18:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 799, loss 4.269334316253662\n",
      "Epoch 7: |          | 800/? [18:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 800, loss 4.053467273712158\n",
      "Epoch 7: |          | 801/? [18:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 801, loss 3.6767914295196533\n",
      "Epoch 7: |          | 802/? [18:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 802, loss 3.953601837158203\n",
      "Epoch 7: |          | 803/? [18:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 803, loss 3.790689468383789\n",
      "Epoch 7: |          | 804/? [18:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 804, loss 3.965252637863159\n",
      "Epoch 7: |          | 805/? [18:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 805, loss 4.1212849617004395\n",
      "Epoch 7: |          | 806/? [18:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 806, loss 4.535848617553711\n",
      "Epoch 7: |          | 807/? [18:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 807, loss 3.890894651412964\n",
      "Epoch 7: |          | 808/? [18:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 808, loss 3.5462565422058105\n",
      "Epoch 7: |          | 809/? [18:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 809, loss 4.087605953216553\n",
      "Epoch 7: |          | 810/? [18:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 810, loss 3.8379197120666504\n",
      "Epoch 7: |          | 811/? [18:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 811, loss 4.1240129470825195\n",
      "Epoch 7: |          | 812/? [18:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 812, loss 4.726336479187012\n",
      "Epoch 7: |          | 813/? [18:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 813, loss 4.497007369995117\n",
      "Epoch 7: |          | 814/? [18:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 814, loss 3.5361990928649902\n",
      "Epoch 7: |          | 815/? [19:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 815, loss 4.232320785522461\n",
      "Epoch 7: |          | 816/? [19:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 816, loss 4.051766872406006\n",
      "Epoch 7: |          | 817/? [19:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 817, loss 3.3964619636535645\n",
      "Epoch 7: |          | 818/? [19:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 818, loss 4.368661403656006\n",
      "Epoch 7: |          | 819/? [19:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 819, loss 4.054073333740234\n",
      "Epoch 7: |          | 820/? [19:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 820, loss 3.919989824295044\n",
      "Epoch 7: |          | 821/? [19:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 821, loss 3.877377986907959\n",
      "Epoch 7: |          | 822/? [19:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 822, loss 3.5470592975616455\n",
      "Epoch 7: |          | 823/? [19:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 823, loss 3.5633487701416016\n",
      "Epoch 7: |          | 824/? [19:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 824, loss 4.044264793395996\n",
      "Epoch 7: |          | 825/? [19:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 825, loss 3.5700602531433105\n",
      "Epoch 7: |          | 826/? [19:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 826, loss 4.109279155731201\n",
      "Epoch 7: |          | 827/? [19:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 827, loss 3.7103958129882812\n",
      "Epoch 7: |          | 828/? [19:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 828, loss 4.247129917144775\n",
      "Epoch 7: |          | 829/? [19:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 829, loss 3.9130465984344482\n",
      "Epoch 7: |          | 830/? [19:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 830, loss 4.447338104248047\n",
      "Epoch 7: |          | 831/? [19:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 831, loss 2.383195400238037\n",
      "Epoch 7: |          | 832/? [19:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 832, loss 3.8762447834014893\n",
      "Epoch 7: |          | 833/? [19:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 833, loss 3.756282329559326\n",
      "Epoch 7: |          | 834/? [19:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 834, loss 4.507406234741211\n",
      "Epoch 7: |          | 835/? [19:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 835, loss 3.8429667949676514\n",
      "Epoch 7: |          | 836/? [19:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 836, loss 4.533721923828125\n",
      "Epoch 7: |          | 837/? [19:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 837, loss 3.964430332183838\n",
      "Epoch 7: |          | 838/? [19:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 838, loss 3.318816661834717\n",
      "Epoch 7: |          | 839/? [19:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 839, loss 3.6562888622283936\n",
      "Epoch 7: |          | 840/? [19:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 840, loss 4.197174072265625\n",
      "Epoch 7: |          | 841/? [19:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 841, loss 4.229265213012695\n",
      "Epoch 7: |          | 842/? [19:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 842, loss 3.894918918609619\n",
      "Epoch 7: |          | 843/? [19:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 843, loss 4.205508708953857\n",
      "Epoch 7: |          | 844/? [19:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 844, loss 3.58772611618042\n",
      "Epoch 7: |          | 845/? [19:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 845, loss 4.007161617279053\n",
      "Epoch 7: |          | 846/? [19:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 846, loss 4.422611236572266\n",
      "Epoch 7: |          | 847/? [19:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 847, loss 4.008090496063232\n",
      "Epoch 7: |          | 848/? [19:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 848, loss 3.5659217834472656\n",
      "Epoch 7: |          | 849/? [19:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 849, loss 3.655656099319458\n",
      "Epoch 7: |          | 850/? [19:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 850, loss 3.7684245109558105\n",
      "Epoch 7: |          | 851/? [19:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 851, loss 4.0600972175598145\n",
      "Epoch 7: |          | 852/? [19:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 852, loss 4.203989505767822\n",
      "Epoch 7: |          | 853/? [19:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 853, loss 4.022265911102295\n",
      "Epoch 7: |          | 854/? [19:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 854, loss 3.3530116081237793\n",
      "Epoch 7: |          | 855/? [19:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 855, loss 3.63651967048645\n",
      "Epoch 7: |          | 856/? [19:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 856, loss 3.5721466541290283\n",
      "Epoch 7: |          | 857/? [19:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 857, loss 4.10087776184082\n",
      "Epoch 7: |          | 858/? [20:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 858, loss 4.009936809539795\n",
      "Epoch 7: |          | 859/? [20:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 859, loss 3.9989140033721924\n",
      "Epoch 7: |          | 860/? [20:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 860, loss 4.393293857574463\n",
      "Epoch 7: |          | 861/? [20:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 861, loss 3.6941535472869873\n",
      "Epoch 7: |          | 862/? [20:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 862, loss 4.026268005371094\n",
      "Epoch 7: |          | 863/? [20:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 863, loss 3.4255733489990234\n",
      "Epoch 7: |          | 864/? [20:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 864, loss 3.9657349586486816\n",
      "Epoch 7: |          | 865/? [20:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 865, loss 3.919987440109253\n",
      "Epoch 7: |          | 866/? [20:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 866, loss 2.9863481521606445\n",
      "Epoch 7: |          | 867/? [20:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 867, loss 3.1653285026550293\n",
      "Epoch 7: |          | 868/? [20:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 868, loss 4.04044771194458\n",
      "Epoch 7: |          | 869/? [20:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 869, loss 4.100341796875\n",
      "Epoch 7: |          | 870/? [20:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 870, loss 3.6840603351593018\n",
      "Epoch 7: |          | 871/? [20:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 871, loss 4.0089945793151855\n",
      "Epoch 7: |          | 872/? [20:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 872, loss 3.8506031036376953\n",
      "Epoch 7: |          | 873/? [20:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 873, loss 3.8648083209991455\n",
      "Epoch 7: |          | 874/? [20:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 874, loss 3.3422763347625732\n",
      "Epoch 7: |          | 875/? [20:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 875, loss 4.106176376342773\n",
      "Epoch 7: |          | 876/? [20:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 876, loss 3.6265788078308105\n",
      "Epoch 7: |          | 877/? [20:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 877, loss 4.02027702331543\n",
      "Epoch 7: |          | 878/? [20:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 878, loss 3.442596435546875\n",
      "Epoch 7: |          | 879/? [20:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 879, loss 3.516695022583008\n",
      "Epoch 7: |          | 880/? [20:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 880, loss 4.586870193481445\n",
      "Epoch 7: |          | 881/? [20:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 881, loss 4.009840965270996\n",
      "Epoch 7: |          | 882/? [20:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 882, loss 3.8107006549835205\n",
      "Epoch 7: |          | 883/? [20:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 883, loss 3.919321060180664\n",
      "Epoch 7: |          | 884/? [20:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 884, loss 3.9686484336853027\n",
      "Epoch 7: |          | 885/? [20:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 885, loss 3.7507922649383545\n",
      "Epoch 7: |          | 886/? [20:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 886, loss 4.399206161499023\n",
      "Epoch 7: |          | 887/? [20:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 887, loss 4.415610313415527\n",
      "Epoch 7: |          | 888/? [20:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 888, loss 4.125370502471924\n",
      "Epoch 7: |          | 889/? [20:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 889, loss 3.6510138511657715\n",
      "Epoch 7: |          | 890/? [20:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 890, loss 3.9286599159240723\n",
      "Epoch 7: |          | 891/? [20:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 891, loss 3.7270607948303223\n",
      "Epoch 7: |          | 892/? [20:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 892, loss 4.322196006774902\n",
      "Epoch 7: |          | 893/? [20:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 893, loss 3.7851669788360596\n",
      "Epoch 7: |          | 894/? [20:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 894, loss 3.3024258613586426\n",
      "Epoch 7: |          | 895/? [20:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 895, loss 4.430901527404785\n",
      "Epoch 7: |          | 896/? [20:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 896, loss 4.034136772155762\n",
      "Epoch 7: |          | 897/? [20:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 897, loss 4.0152177810668945\n",
      "Epoch 7: |          | 898/? [20:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 898, loss 3.9958081245422363\n",
      "Epoch 7: |          | 899/? [20:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 899, loss 3.7789688110351562\n",
      "Epoch 7: |          | 900/? [20:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 900, loss 3.7023258209228516\n",
      "Epoch 7: |          | 901/? [21:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 901, loss 4.114675998687744\n",
      "Epoch 7: |          | 902/? [21:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 902, loss 4.224456787109375\n",
      "Epoch 7: |          | 903/? [21:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 903, loss 3.5208137035369873\n",
      "Epoch 7: |          | 904/? [21:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 904, loss 3.9973480701446533\n",
      "Epoch 7: |          | 905/? [21:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 905, loss 4.1934099197387695\n",
      "Epoch 7: |          | 906/? [21:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 906, loss 3.9531772136688232\n",
      "Epoch 7: |          | 907/? [21:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 907, loss 4.000232219696045\n",
      "Epoch 7: |          | 908/? [21:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 908, loss 4.050134181976318\n",
      "Epoch 7: |          | 909/? [21:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 909, loss 4.052249431610107\n",
      "Epoch 7: |          | 910/? [21:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 910, loss 3.832338809967041\n",
      "Epoch 7: |          | 911/? [21:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 911, loss 3.8976707458496094\n",
      "Epoch 7: |          | 912/? [21:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 912, loss 3.8413143157958984\n",
      "Epoch 7: |          | 913/? [21:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 913, loss 3.8542733192443848\n",
      "Epoch 7: |          | 914/? [21:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 914, loss 4.135783672332764\n",
      "Epoch 7: |          | 915/? [21:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 915, loss 3.9396958351135254\n",
      "Epoch 7: |          | 916/? [21:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 916, loss 3.883070707321167\n",
      "Epoch 7: |          | 917/? [21:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 917, loss 3.812201738357544\n",
      "Epoch 7: |          | 918/? [21:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 918, loss 3.7641079425811768\n",
      "Epoch 7: |          | 919/? [21:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 919, loss 3.7577157020568848\n",
      "Epoch 7: |          | 920/? [21:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 920, loss 3.921647548675537\n",
      "Epoch 7: |          | 921/? [21:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 921, loss 3.746340274810791\n",
      "Epoch 7: |          | 922/? [21:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 922, loss 3.9070446491241455\n",
      "Epoch 7: |          | 923/? [21:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 923, loss 3.77058744430542\n",
      "Epoch 7: |          | 924/? [21:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 924, loss 3.7273898124694824\n",
      "Epoch 7: |          | 925/? [21:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 925, loss 4.063780307769775\n",
      "Epoch 7: |          | 926/? [21:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 926, loss 3.839085102081299\n",
      "Epoch 7: |          | 927/? [21:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 927, loss 4.123300075531006\n",
      "Epoch 7: |          | 928/? [21:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 928, loss 3.6101279258728027\n",
      "Epoch 7: |          | 929/? [21:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 929, loss 3.732386350631714\n",
      "Epoch 7: |          | 930/? [21:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 930, loss 3.6168789863586426\n",
      "Epoch 7: |          | 931/? [21:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 931, loss 3.3579769134521484\n",
      "Epoch 7: |          | 932/? [21:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 932, loss 3.971869945526123\n",
      "Epoch 7: |          | 933/? [21:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 933, loss 3.7012202739715576\n",
      "Epoch 7: |          | 934/? [21:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 934, loss 4.299962997436523\n",
      "Epoch 7: |          | 935/? [21:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 935, loss 4.648068428039551\n",
      "Epoch 7: |          | 936/? [21:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 936, loss 3.8423290252685547\n",
      "Epoch 7: |          | 937/? [21:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 937, loss 3.798285722732544\n",
      "Epoch 7: |          | 938/? [21:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 938, loss 3.787895917892456\n",
      "Epoch 7: |          | 939/? [21:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 939, loss 4.010420799255371\n",
      "Epoch 7: |          | 940/? [21:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 940, loss 4.212531089782715\n",
      "Epoch 7: |          | 941/? [21:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 941, loss 3.7778244018554688\n",
      "Epoch 7: |          | 942/? [21:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 942, loss 3.2543864250183105\n",
      "Epoch 7: |          | 943/? [21:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 943, loss 4.129297733306885\n",
      "Epoch 7: |          | 944/? [22:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 944, loss 3.181777238845825\n",
      "Epoch 7: |          | 945/? [22:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 945, loss 3.90508770942688\n",
      "Epoch 7: |          | 946/? [22:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 946, loss 3.8022987842559814\n",
      "Epoch 7: |          | 947/? [22:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 947, loss 3.7432315349578857\n",
      "Epoch 7: |          | 948/? [22:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 948, loss 4.001059532165527\n",
      "Epoch 7: |          | 949/? [22:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 949, loss 3.8309547901153564\n",
      "Epoch 7: |          | 950/? [22:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 950, loss 3.622685194015503\n",
      "Epoch 7: |          | 951/? [22:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 951, loss 4.272784233093262\n",
      "Epoch 7: |          | 952/? [22:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 952, loss 4.221427917480469\n",
      "Epoch 7: |          | 953/? [22:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 953, loss 4.761044025421143\n",
      "Epoch 7: |          | 954/? [22:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 954, loss 3.7553863525390625\n",
      "Epoch 7: |          | 955/? [22:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 955, loss 4.359689235687256\n",
      "Epoch 7: |          | 956/? [22:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 956, loss 3.8277652263641357\n",
      "Epoch 7: |          | 957/? [22:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 957, loss 4.030245304107666\n",
      "Epoch 7: |          | 958/? [22:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 958, loss 4.064850807189941\n",
      "Epoch 7: |          | 959/? [22:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 959, loss 3.604938507080078\n",
      "Epoch 7: |          | 960/? [22:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 960, loss 4.137406826019287\n",
      "Epoch 7: |          | 961/? [22:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 961, loss 4.397453308105469\n",
      "Epoch 7: |          | 962/? [22:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 962, loss 3.9087345600128174\n",
      "Epoch 7: |          | 963/? [22:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 963, loss 3.7014076709747314\n",
      "Epoch 7: |          | 964/? [22:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 964, loss 4.15873384475708\n",
      "Epoch 7: |          | 965/? [22:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 965, loss 3.6388888359069824\n",
      "Epoch 7: |          | 966/? [22:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 966, loss 3.4960334300994873\n",
      "Epoch 7: |          | 967/? [22:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 967, loss 3.761608839035034\n",
      "Epoch 7: |          | 968/? [22:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 968, loss 3.681278944015503\n",
      "Epoch 7: |          | 969/? [22:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 969, loss 3.5757980346679688\n",
      "Epoch 7: |          | 970/? [22:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 970, loss 4.042638301849365\n",
      "Epoch 7: |          | 971/? [22:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 971, loss 4.238368034362793\n",
      "Epoch 7: |          | 972/? [22:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 972, loss 3.704540967941284\n",
      "Epoch 7: |          | 973/? [22:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 973, loss 3.8424172401428223\n",
      "Epoch 7: |          | 974/? [22:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 974, loss 3.8779289722442627\n",
      "Epoch 7: |          | 975/? [22:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 975, loss 3.919187545776367\n",
      "Epoch 7: |          | 976/? [22:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 976, loss 3.98173451423645\n",
      "Epoch 7: |          | 977/? [22:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 977, loss 4.5830078125\n",
      "Epoch 7: |          | 978/? [22:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 978, loss 4.001449108123779\n",
      "Epoch 7: |          | 979/? [22:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 979, loss 4.3394880294799805\n",
      "Epoch 7: |          | 980/? [22:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 980, loss 3.4558310508728027\n",
      "Epoch 7: |          | 981/? [22:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 981, loss 3.2938671112060547\n",
      "Epoch 7: |          | 982/? [22:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 982, loss 3.941302537918091\n",
      "Epoch 7: |          | 983/? [22:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 983, loss 4.364680290222168\n",
      "Epoch 7: |          | 984/? [22:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 984, loss 3.454153537750244\n",
      "Epoch 7: |          | 985/? [22:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 985, loss 3.671388626098633\n",
      "Epoch 7: |          | 986/? [22:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 986, loss 3.729084014892578\n",
      "Epoch 7: |          | 987/? [23:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 987, loss 3.25239896774292\n",
      "Epoch 7: |          | 988/? [23:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 988, loss 4.25783634185791\n",
      "Epoch 7: |          | 989/? [23:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 989, loss 3.8875675201416016\n",
      "Epoch 7: |          | 990/? [23:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 990, loss 3.239900588989258\n",
      "Epoch 7: |          | 991/? [23:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 991, loss 3.9756476879119873\n",
      "Epoch 7: |          | 992/? [23:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 992, loss 4.657268047332764\n",
      "Epoch 7: |          | 993/? [23:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 993, loss 3.7667269706726074\n",
      "Epoch 7: |          | 994/? [23:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 994, loss 3.775153398513794\n",
      "Epoch 7: |          | 995/? [23:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 995, loss 4.139847278594971\n",
      "Epoch 7: |          | 996/? [23:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 996, loss 4.1614251136779785\n",
      "Epoch 7: |          | 997/? [23:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 997, loss 3.7843902111053467\n",
      "Epoch 7: |          | 998/? [23:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 998, loss 4.032989025115967\n",
      "Epoch 7: |          | 999/? [23:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 999, loss 3.9829680919647217\n",
      "Epoch 7: |          | 1000/? [23:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1000, loss 3.5023703575134277\n",
      "Epoch 7: |          | 1001/? [23:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1001, loss 4.180573463439941\n",
      "Epoch 7: |          | 1002/? [23:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1002, loss 4.096905708312988\n",
      "Epoch 7: |          | 1003/? [23:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1003, loss 4.308983325958252\n",
      "Epoch 7: |          | 1004/? [23:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1004, loss 3.328758716583252\n",
      "Epoch 7: |          | 1005/? [23:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1005, loss 3.876277208328247\n",
      "Epoch 7: |          | 1006/? [23:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1006, loss 4.182154178619385\n",
      "Epoch 7: |          | 1007/? [23:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1007, loss 3.779461622238159\n",
      "Epoch 7: |          | 1008/? [23:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1008, loss 3.9279487133026123\n",
      "Epoch 7: |          | 1009/? [23:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1009, loss 4.194900989532471\n",
      "Epoch 7: |          | 1010/? [23:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1010, loss 3.332183361053467\n",
      "Epoch 7: |          | 1011/? [23:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1011, loss 3.908334732055664\n",
      "Epoch 7: |          | 1012/? [23:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1012, loss 3.7169506549835205\n",
      "Epoch 7: |          | 1013/? [23:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1013, loss 3.980717182159424\n",
      "Epoch 7: |          | 1014/? [23:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1014, loss 4.321686744689941\n",
      "Epoch 7: |          | 1015/? [23:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1015, loss 3.992711305618286\n",
      "Epoch 7: |          | 1016/? [23:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1016, loss 3.7217915058135986\n",
      "Epoch 7: |          | 1017/? [23:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1017, loss 3.1019058227539062\n",
      "Epoch 7: |          | 1018/? [23:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1018, loss 3.8217787742614746\n",
      "Epoch 7: |          | 1019/? [23:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1019, loss 3.8656184673309326\n",
      "Epoch 7: |          | 1020/? [23:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1020, loss 3.5030391216278076\n",
      "Epoch 7: |          | 1021/? [23:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1021, loss 3.738105297088623\n",
      "Epoch 7: |          | 1022/? [23:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1022, loss 3.4919791221618652\n",
      "Epoch 7: |          | 1023/? [23:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1023, loss 3.266235828399658\n",
      "Epoch 7: |          | 1024/? [23:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1024, loss 3.7209575176239014\n",
      "Epoch 7: |          | 1025/? [23:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1025, loss 3.6133079528808594\n",
      "Epoch 7: |          | 1026/? [23:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1026, loss 2.767585515975952\n",
      "Epoch 7: |          | 1027/? [23:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1027, loss 3.9904847145080566\n",
      "Epoch 7: |          | 1028/? [23:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1028, loss 3.8176321983337402\n",
      "Epoch 7: |          | 1029/? [23:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1029, loss 3.671447277069092\n",
      "Epoch 7: |          | 1030/? [24:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1030, loss 3.5493416786193848\n",
      "Epoch 7: |          | 1031/? [24:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1031, loss 3.6110801696777344\n",
      "Epoch 7: |          | 1032/? [24:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1032, loss 4.003971099853516\n",
      "Epoch 7: |          | 1033/? [24:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1033, loss 4.247590065002441\n",
      "Epoch 7: |          | 1034/? [24:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1034, loss 3.6277003288269043\n",
      "Epoch 7: |          | 1035/? [24:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1035, loss 3.6432361602783203\n",
      "Epoch 7: |          | 1036/? [24:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1036, loss 3.587085008621216\n",
      "Epoch 7: |          | 1037/? [24:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1037, loss 4.174976348876953\n",
      "Epoch 7: |          | 1038/? [24:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1038, loss 4.352429389953613\n",
      "Epoch 7: |          | 1039/? [24:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1039, loss 4.68938684463501\n",
      "Epoch 7: |          | 1040/? [24:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1040, loss 3.9451427459716797\n",
      "Epoch 7: |          | 1041/? [24:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1041, loss 4.245221138000488\n",
      "Epoch 7: |          | 1042/? [24:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1042, loss 3.8623435497283936\n",
      "Epoch 7: |          | 1043/? [24:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1043, loss 4.240693092346191\n",
      "Epoch 7: |          | 1044/? [24:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1044, loss 3.8207297325134277\n",
      "Epoch 7: |          | 1045/? [24:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1045, loss 3.3408265113830566\n",
      "Epoch 7: |          | 1046/? [24:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1046, loss 3.1982665061950684\n",
      "Epoch 7: |          | 1047/? [24:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1047, loss 4.3733320236206055\n",
      "Epoch 7: |          | 1048/? [24:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1048, loss 3.816232204437256\n",
      "Epoch 7: |          | 1049/? [24:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1049, loss 4.030119895935059\n",
      "Epoch 7: |          | 1050/? [24:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1050, loss 3.60783052444458\n",
      "Epoch 7: |          | 1051/? [24:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1051, loss 3.5217254161834717\n",
      "Epoch 7: |          | 1052/? [24:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1052, loss 4.140865802764893\n",
      "Epoch 7: |          | 1053/? [24:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1053, loss 4.354037284851074\n",
      "Epoch 7: |          | 1054/? [24:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1054, loss 3.720510482788086\n",
      "Epoch 7: |          | 1055/? [24:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1055, loss 3.420447587966919\n",
      "Epoch 7: |          | 1056/? [24:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1056, loss 3.4508278369903564\n",
      "Epoch 7: |          | 1057/? [24:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1057, loss 4.054160118103027\n",
      "Epoch 7: |          | 1058/? [24:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1058, loss 3.626743793487549\n",
      "Epoch 7: |          | 1059/? [24:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1059, loss 4.204644203186035\n",
      "Epoch 7: |          | 1060/? [24:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1060, loss 4.095675945281982\n",
      "Epoch 7: |          | 1061/? [24:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1061, loss 2.8120627403259277\n",
      "Epoch 7: |          | 1062/? [24:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1062, loss 3.79353666305542\n",
      "Epoch 7: |          | 1063/? [24:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1063, loss 3.836885929107666\n",
      "Epoch 7: |          | 1064/? [24:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1064, loss 4.016741752624512\n",
      "Epoch 7: |          | 1065/? [24:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1065, loss 2.647749423980713\n",
      "Epoch 7: |          | 1066/? [24:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1066, loss 3.9292538166046143\n",
      "Epoch 7: |          | 1067/? [24:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1067, loss 3.4630866050720215\n",
      "Epoch 7: |          | 1068/? [24:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1068, loss 3.63580322265625\n",
      "Epoch 7: |          | 1069/? [24:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1069, loss 3.9844284057617188\n",
      "Epoch 7: |          | 1070/? [24:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1070, loss 3.761219024658203\n",
      "Epoch 7: |          | 1071/? [24:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1071, loss 4.180280685424805\n",
      "Epoch 7: |          | 1072/? [24:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1072, loss 4.2024641036987305\n",
      "Epoch 7: |          | 1073/? [25:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1073, loss 4.372751235961914\n",
      "Epoch 7: |          | 1074/? [25:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1074, loss 3.7000572681427\n",
      "Epoch 7: |          | 1075/? [25:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1075, loss 3.513908863067627\n",
      "Epoch 7: |          | 1076/? [25:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1076, loss 4.055146217346191\n",
      "Epoch 7: |          | 1077/? [25:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1077, loss 3.593088150024414\n",
      "Epoch 7: |          | 1078/? [25:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1078, loss 3.902832508087158\n",
      "Epoch 7: |          | 1079/? [25:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1079, loss 4.322961330413818\n",
      "Epoch 7: |          | 1080/? [25:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1080, loss 3.8079686164855957\n",
      "Epoch 7: |          | 1081/? [25:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1081, loss 4.1194963455200195\n",
      "Epoch 7: |          | 1082/? [25:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1082, loss 3.701463222503662\n",
      "Epoch 7: |          | 1083/? [25:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1083, loss 3.28031849861145\n",
      "Epoch 7: |          | 1084/? [25:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1084, loss 3.0793404579162598\n",
      "Epoch 7: |          | 1085/? [25:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1085, loss 3.7529430389404297\n",
      "Epoch 7: |          | 1086/? [25:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1086, loss 4.031155586242676\n",
      "Epoch 7: |          | 1087/? [25:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1087, loss 4.539463996887207\n",
      "Epoch 7: |          | 1088/? [25:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1088, loss 4.102768898010254\n",
      "Epoch 7: |          | 1089/? [25:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1089, loss 4.009889125823975\n",
      "Epoch 7: |          | 1090/? [25:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1090, loss 3.880117416381836\n",
      "Epoch 7: |          | 1091/? [25:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1091, loss 3.640148162841797\n",
      "Epoch 7: |          | 1092/? [25:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1092, loss 3.9347140789031982\n",
      "Epoch 7: |          | 1093/? [25:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1093, loss 3.40510892868042\n",
      "Epoch 7: |          | 1094/? [25:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1094, loss 3.9042587280273438\n",
      "Epoch 7: |          | 1095/? [25:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1095, loss 3.9915366172790527\n",
      "Epoch 7: |          | 1096/? [25:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1096, loss 4.237547874450684\n",
      "Epoch 7: |          | 1097/? [25:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1097, loss 3.796752452850342\n",
      "Epoch 7: |          | 1098/? [25:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1098, loss 3.0846855640411377\n",
      "Epoch 7: |          | 1099/? [25:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1099, loss 3.7546844482421875\n",
      "Epoch 7: |          | 1100/? [25:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1100, loss 3.958848476409912\n",
      "Epoch 7: |          | 1101/? [25:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1101, loss 3.6018004417419434\n",
      "Epoch 7: |          | 1102/? [25:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1102, loss 4.326483726501465\n",
      "Epoch 7: |          | 1103/? [25:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1103, loss 4.752683639526367\n",
      "Epoch 7: |          | 1104/? [25:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1104, loss 4.0851054191589355\n",
      "Epoch 7: |          | 1105/? [25:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1105, loss 4.264814376831055\n",
      "Epoch 7: |          | 1106/? [25:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1106, loss 3.7687525749206543\n",
      "Epoch 7: |          | 1107/? [25:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1107, loss 3.8670620918273926\n",
      "Epoch 7: |          | 1108/? [25:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1108, loss 3.87455677986145\n",
      "Epoch 7: |          | 1109/? [25:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1109, loss 3.482290744781494\n",
      "Epoch 7: |          | 1110/? [25:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1110, loss 4.389025688171387\n",
      "Epoch 7: |          | 1111/? [25:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1111, loss 4.067974090576172\n",
      "Epoch 7: |          | 1112/? [25:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1112, loss 3.943438768386841\n",
      "Epoch 7: |          | 1113/? [25:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1113, loss 3.75500750541687\n",
      "Epoch 7: |          | 1114/? [25:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1114, loss 3.2211241722106934\n",
      "Epoch 7: |          | 1115/? [25:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1115, loss 3.0091607570648193\n",
      "Epoch 7: |          | 1116/? [26:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1116, loss 3.3829479217529297\n",
      "Epoch 7: |          | 1117/? [26:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1117, loss 3.5166518688201904\n",
      "Epoch 7: |          | 1118/? [26:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1118, loss 3.7109246253967285\n",
      "Epoch 7: |          | 1119/? [26:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1119, loss 4.318532943725586\n",
      "Epoch 7: |          | 1120/? [26:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1120, loss 3.856674909591675\n",
      "Epoch 7: |          | 1121/? [26:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1121, loss 4.107418060302734\n",
      "Epoch 7: |          | 1122/? [26:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1122, loss 3.616483688354492\n",
      "Epoch 7: |          | 1123/? [26:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1123, loss 3.9121365547180176\n",
      "Epoch 7: |          | 1124/? [26:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1124, loss 4.2465739250183105\n",
      "Epoch 7: |          | 1125/? [26:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1125, loss 3.548168182373047\n",
      "Epoch 7: |          | 1126/? [26:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1126, loss 3.4615912437438965\n",
      "Epoch 7: |          | 1127/? [26:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1127, loss 3.7799172401428223\n",
      "Epoch 7: |          | 1128/? [26:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1128, loss 3.8390612602233887\n",
      "Epoch 7: |          | 1129/? [26:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1129, loss 3.9530863761901855\n",
      "Epoch 7: |          | 1130/? [26:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1130, loss 4.131974697113037\n",
      "Epoch 7: |          | 1131/? [26:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1131, loss 4.231184482574463\n",
      "Epoch 7: |          | 1132/? [26:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1132, loss 3.003871440887451\n",
      "Epoch 7: |          | 1133/? [26:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1133, loss 3.8084914684295654\n",
      "Epoch 7: |          | 1134/? [26:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1134, loss 3.6388611793518066\n",
      "Epoch 7: |          | 1135/? [26:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1135, loss 4.24899959564209\n",
      "Epoch 7: |          | 1136/? [26:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1136, loss 3.8858890533447266\n",
      "Epoch 7: |          | 1137/? [26:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1137, loss 3.8840224742889404\n",
      "Epoch 7: |          | 1138/? [26:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1138, loss 4.397037982940674\n",
      "Epoch 7: |          | 1139/? [26:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1139, loss 4.332289695739746\n",
      "Epoch 7: |          | 1140/? [26:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1140, loss 3.7289252281188965\n",
      "Epoch 7: |          | 1141/? [26:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1141, loss 4.223666191101074\n",
      "Epoch 7: |          | 1142/? [26:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1142, loss 4.295674800872803\n",
      "Epoch 7: |          | 1143/? [26:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1143, loss 4.323628902435303\n",
      "Epoch 7: |          | 1144/? [26:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1144, loss 3.754962205886841\n",
      "Epoch 7: |          | 1145/? [26:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1145, loss 3.85896635055542\n",
      "Epoch 7: |          | 1146/? [26:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1146, loss 3.486804246902466\n",
      "Epoch 7: |          | 1147/? [26:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1147, loss 3.463418960571289\n",
      "Epoch 7: |          | 1148/? [26:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1148, loss 3.6302459239959717\n",
      "Epoch 7: |          | 1149/? [26:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1149, loss 4.677378177642822\n",
      "Epoch 7: |          | 1150/? [26:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1150, loss 4.054518222808838\n",
      "Epoch 7: |          | 1151/? [26:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1151, loss 4.333728790283203\n",
      "Epoch 7: |          | 1152/? [26:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1152, loss 3.558351993560791\n",
      "Epoch 7: |          | 1153/? [26:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1153, loss 3.9689178466796875\n",
      "Epoch 7: |          | 1154/? [26:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1154, loss 3.6546273231506348\n",
      "Epoch 7: |          | 1155/? [26:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1155, loss 3.8536040782928467\n",
      "Epoch 7: |          | 1156/? [26:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1156, loss 3.8318896293640137\n",
      "Epoch 7: |          | 1157/? [26:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1157, loss 4.078247547149658\n",
      "Epoch 7: |          | 1158/? [26:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1158, loss 4.276583671569824\n",
      "Epoch 7: |          | 1159/? [27:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1159, loss 3.1079254150390625\n",
      "Epoch 7: |          | 1160/? [27:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1160, loss 4.272388458251953\n",
      "Epoch 7: |          | 1161/? [27:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1161, loss 4.114170551300049\n",
      "Epoch 7: |          | 1162/? [27:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1162, loss 4.043883800506592\n",
      "Epoch 7: |          | 1163/? [27:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1163, loss 4.630217552185059\n",
      "Epoch 7: |          | 1164/? [27:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1164, loss 4.416008949279785\n",
      "Epoch 7: |          | 1165/? [27:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1165, loss 3.5754382610321045\n",
      "Epoch 7: |          | 1166/? [27:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1166, loss 4.07951545715332\n",
      "Epoch 7: |          | 1167/? [27:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1167, loss 4.100711822509766\n",
      "Epoch 7: |          | 1168/? [27:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1168, loss 4.5443949699401855\n",
      "Epoch 7: |          | 1169/? [27:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1169, loss 3.6365935802459717\n",
      "Epoch 7: |          | 1170/? [27:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1170, loss 4.180718421936035\n",
      "Epoch 7: |          | 1171/? [27:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1171, loss 3.570615768432617\n",
      "Epoch 7: |          | 1172/? [27:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1172, loss 3.4891109466552734\n",
      "Epoch 7: |          | 1173/? [27:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1173, loss 4.047417640686035\n",
      "Epoch 7: |          | 1174/? [27:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1174, loss 3.5095622539520264\n",
      "Epoch 7: |          | 1175/? [27:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1175, loss 4.141263008117676\n",
      "Epoch 7: |          | 1176/? [27:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1176, loss 4.173618316650391\n",
      "Epoch 7: |          | 1177/? [27:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1177, loss 4.341765403747559\n",
      "Epoch 7: |          | 1178/? [27:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1178, loss 3.7672762870788574\n",
      "Epoch 7: |          | 1179/? [27:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1179, loss 4.288313388824463\n",
      "Epoch 7: |          | 1180/? [27:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1180, loss 4.099027633666992\n",
      "Epoch 7: |          | 1181/? [27:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1181, loss 4.033516883850098\n",
      "Epoch 7: |          | 1182/? [27:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1182, loss 3.841841220855713\n",
      "Epoch 7: |          | 1183/? [27:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1183, loss 3.558588743209839\n",
      "Epoch 7: |          | 1184/? [27:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1184, loss 3.9612057209014893\n",
      "Epoch 7: |          | 1185/? [27:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 1185, loss 3.7427051067352295\n",
      "Epoch 7: |          | 1186/? [27:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1186, loss 3.9419422149658203\n",
      "Epoch 7: |          | 1187/? [27:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1187, loss 3.8310821056365967\n",
      "Epoch 7: |          | 1188/? [27:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1188, loss 4.197497367858887\n",
      "Epoch 7: |          | 1189/? [27:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1189, loss 4.312897682189941\n",
      "Epoch 7: |          | 1190/? [27:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1190, loss 3.8195064067840576\n",
      "Epoch 7: |          | 1191/? [27:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1191, loss 3.8775696754455566\n",
      "Epoch 7: |          | 1192/? [27:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1192, loss 4.205439567565918\n",
      "Epoch 7: |          | 1193/? [27:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1193, loss 3.6832242012023926\n",
      "Epoch 7: |          | 1194/? [27:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1194, loss 3.4052138328552246\n",
      "Epoch 7: |          | 1195/? [27:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1195, loss 3.923428773880005\n",
      "Epoch 7: |          | 1196/? [27:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1196, loss 4.1205644607543945\n",
      "Epoch 7: |          | 1197/? [27:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1197, loss 3.914921522140503\n",
      "Epoch 7: |          | 1198/? [27:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1198, loss 4.005375862121582\n",
      "Epoch 7: |          | 1199/? [27:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1199, loss 4.26994514465332\n",
      "Epoch 7: |          | 1200/? [27:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1200, loss 3.4931957721710205\n",
      "Epoch 7: |          | 1201/? [28:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1201, loss 4.070904731750488\n",
      "Epoch 7: |          | 1202/? [28:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1202, loss 3.7816853523254395\n",
      "Epoch 7: |          | 1203/? [28:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1203, loss 3.746779680252075\n",
      "Epoch 7: |          | 1204/? [28:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1204, loss 3.290776014328003\n",
      "Epoch 7: |          | 1205/? [28:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1205, loss 3.86072039604187\n",
      "Epoch 7: |          | 1206/? [28:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1206, loss 3.8732800483703613\n",
      "Epoch 7: |          | 1207/? [28:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1207, loss 4.205790042877197\n",
      "Epoch 7: |          | 1208/? [28:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1208, loss 4.369795322418213\n",
      "Epoch 7: |          | 1209/? [28:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1209, loss 3.9421935081481934\n",
      "Epoch 7: |          | 1210/? [28:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1210, loss 4.217483043670654\n",
      "Epoch 7: |          | 1211/? [28:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1211, loss 4.182507514953613\n",
      "Epoch 7: |          | 1212/? [28:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1212, loss 4.010462760925293\n",
      "Epoch 7: |          | 1213/? [28:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1213, loss 3.712317943572998\n",
      "Epoch 7: |          | 1214/? [28:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1214, loss 4.309559345245361\n",
      "Epoch 7: |          | 1215/? [28:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1215, loss 3.76141619682312\n",
      "Epoch 7: |          | 1216/? [28:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1216, loss 3.876854419708252\n",
      "Epoch 7: |          | 1217/? [28:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1217, loss 4.015410423278809\n",
      "Epoch 7: |          | 1218/? [28:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1218, loss 4.050100326538086\n",
      "Epoch 7: |          | 1219/? [28:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1219, loss 3.73563814163208\n",
      "Epoch 7: |          | 1220/? [28:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1220, loss 4.421657562255859\n",
      "Epoch 7: |          | 1221/? [28:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1221, loss 4.000790596008301\n",
      "Epoch 7: |          | 1222/? [28:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1222, loss 3.1132848262786865\n",
      "Epoch 7: |          | 1223/? [28:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1223, loss 3.3082404136657715\n",
      "Epoch 7: |          | 1224/? [28:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1224, loss 3.6512291431427\n",
      "Epoch 7: |          | 1225/? [28:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1225, loss 4.284841060638428\n",
      "Epoch 7: |          | 1226/? [28:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1226, loss 4.268435001373291\n",
      "Epoch 7: |          | 1227/? [28:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1227, loss 3.915320634841919\n",
      "Epoch 7: |          | 1228/? [28:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1228, loss 3.7848262786865234\n",
      "Epoch 7: |          | 1229/? [28:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1229, loss 3.3899829387664795\n",
      "Epoch 7: |          | 1230/? [28:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1230, loss 4.08152961730957\n",
      "Epoch 7: |          | 1231/? [28:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1231, loss 4.119806289672852\n",
      "Epoch 7: |          | 1232/? [28:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1232, loss 4.262207508087158\n",
      "Epoch 7: |          | 1233/? [28:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1233, loss 4.051415920257568\n",
      "Epoch 7: |          | 1234/? [28:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1234, loss 3.0947585105895996\n",
      "Epoch 7: |          | 1235/? [28:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1235, loss 4.196505069732666\n",
      "Epoch 7: |          | 1236/? [28:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1236, loss 3.5531418323516846\n",
      "Epoch 7: |          | 1237/? [28:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1237, loss 3.821280002593994\n",
      "Epoch 7: |          | 1238/? [28:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1238, loss 3.8806259632110596\n",
      "Epoch 7: |          | 1239/? [28:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1239, loss 3.7390503883361816\n",
      "Epoch 7: |          | 1240/? [28:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1240, loss 4.3719000816345215\n",
      "Epoch 7: |          | 1241/? [28:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1241, loss 3.9061858654022217\n",
      "Epoch 7: |          | 1242/? [28:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1242, loss 3.735802412033081\n",
      "Epoch 7: |          | 1243/? [28:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1243, loss 3.6382617950439453\n",
      "Epoch 7: |          | 1244/? [29:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1244, loss 3.78790283203125\n",
      "Epoch 7: |          | 1245/? [29:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1245, loss 3.39660382270813\n",
      "Epoch 7: |          | 1246/? [29:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1246, loss 4.081033706665039\n",
      "Epoch 7: |          | 1247/? [29:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1247, loss 4.064257621765137\n",
      "Epoch 7: |          | 1248/? [29:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1248, loss 3.6443519592285156\n",
      "Epoch 7: |          | 1249/? [29:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1249, loss 3.789844512939453\n",
      "Epoch 7: |          | 1250/? [29:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1250, loss 3.951784133911133\n",
      "Epoch 7: |          | 1251/? [29:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1251, loss 3.6643478870391846\n",
      "Epoch 7: |          | 1252/? [29:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1252, loss 4.453944206237793\n",
      "Epoch 7: |          | 1253/? [29:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1253, loss 3.8220036029815674\n",
      "Epoch 7: |          | 1254/? [29:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1254, loss 3.161597728729248\n",
      "Epoch 7: |          | 1255/? [29:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1255, loss 4.541295051574707\n",
      "Epoch 7: |          | 1256/? [29:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1256, loss 3.515928268432617\n",
      "Epoch 7: |          | 1257/? [29:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1257, loss 3.563951015472412\n",
      "Epoch 7: |          | 1258/? [29:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1258, loss 4.259912014007568\n",
      "Epoch 7: |          | 1259/? [29:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1259, loss 3.9213263988494873\n",
      "Epoch 7: |          | 1260/? [29:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1260, loss 4.385543346405029\n",
      "Epoch 7: |          | 1261/? [29:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1261, loss 3.7152915000915527\n",
      "Epoch 7: |          | 1262/? [29:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1262, loss 3.7087769508361816\n",
      "Epoch 7: |          | 1263/? [29:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1263, loss 4.040686130523682\n",
      "Epoch 7: |          | 1264/? [29:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1264, loss 4.32807731628418\n",
      "Epoch 7: |          | 1265/? [29:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1265, loss 4.176014423370361\n",
      "Epoch 7: |          | 1266/? [29:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1266, loss 3.8376975059509277\n",
      "Epoch 7: |          | 1267/? [29:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1267, loss 3.935075283050537\n",
      "Epoch 7: |          | 1268/? [29:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1268, loss 3.810096025466919\n",
      "Epoch 7: |          | 1269/? [29:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1269, loss 3.3853485584259033\n",
      "Epoch 7: |          | 1270/? [29:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1270, loss 3.7023215293884277\n",
      "Epoch 7: |          | 1271/? [29:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1271, loss 3.8702919483184814\n",
      "Epoch 7: |          | 1272/? [29:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1272, loss 3.479041576385498\n",
      "Epoch 7: |          | 1273/? [29:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1273, loss 4.1514058113098145\n",
      "Epoch 7: |          | 1274/? [29:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1274, loss 3.0473568439483643\n",
      "Epoch 7: |          | 1275/? [29:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1275, loss 3.5681354999542236\n",
      "Epoch 7: |          | 1276/? [29:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1276, loss 3.895648956298828\n",
      "Epoch 7: |          | 1277/? [29:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1277, loss 3.6159961223602295\n",
      "Epoch 7: |          | 1278/? [29:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1278, loss 3.3159689903259277\n",
      "Epoch 7: |          | 1279/? [29:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1279, loss 3.9983315467834473\n",
      "Epoch 7: |          | 1280/? [29:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1280, loss 3.1914799213409424\n",
      "Epoch 7: |          | 1281/? [29:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1281, loss 3.7204384803771973\n",
      "Epoch 7: |          | 1282/? [29:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1282, loss 3.3948211669921875\n",
      "Epoch 7: |          | 1283/? [29:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1283, loss 4.166266441345215\n",
      "Epoch 7: |          | 1284/? [29:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1284, loss 3.275611162185669\n",
      "Epoch 7: |          | 1285/? [29:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1285, loss 4.341189384460449\n",
      "Epoch 7: |          | 1286/? [30:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1286, loss 2.809323787689209\n",
      "Epoch 7: |          | 1287/? [30:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1287, loss 4.245593070983887\n",
      "Epoch 7: |          | 1288/? [30:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1288, loss 4.069018363952637\n",
      "Epoch 7: |          | 1289/? [30:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1289, loss 3.181429862976074\n",
      "Epoch 7: |          | 1290/? [30:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1290, loss 3.9414069652557373\n",
      "Epoch 7: |          | 1291/? [30:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1291, loss 4.799822807312012\n",
      "Epoch 7: |          | 1292/? [30:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1292, loss 4.142529487609863\n",
      "Epoch 7: |          | 1293/? [30:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1293, loss 3.6526248455047607\n",
      "Epoch 7: |          | 1294/? [30:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1294, loss 3.875095844268799\n",
      "Epoch 7: |          | 1295/? [30:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1295, loss 3.9456963539123535\n",
      "Epoch 7: |          | 1296/? [30:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1296, loss 3.2575793266296387\n",
      "Epoch 7: |          | 1297/? [30:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1297, loss 4.127340316772461\n",
      "Epoch 7: |          | 1298/? [30:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1298, loss 3.8595612049102783\n",
      "Epoch 7: |          | 1299/? [30:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1299, loss 3.022235155105591\n",
      "Epoch 7: |          | 1300/? [30:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1300, loss 3.894587278366089\n",
      "Epoch 7: |          | 1301/? [30:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1301, loss 3.601280689239502\n",
      "Epoch 7: |          | 1302/? [30:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1302, loss 3.7298779487609863\n",
      "Epoch 7: |          | 1303/? [30:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1303, loss 3.690741777420044\n",
      "Epoch 7: |          | 1304/? [30:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1304, loss 4.421151161193848\n",
      "Epoch 7: |          | 1305/? [30:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1305, loss 3.2523276805877686\n",
      "Epoch 7: |          | 1306/? [30:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1306, loss 3.8595283031463623\n",
      "Epoch 7: |          | 1307/? [30:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1307, loss 3.4685568809509277\n",
      "Epoch 7: |          | 1308/? [30:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1308, loss 3.41918683052063\n",
      "Epoch 7: |          | 1309/? [30:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1309, loss 3.490001678466797\n",
      "Epoch 7: |          | 1310/? [30:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1310, loss 4.016866207122803\n",
      "Epoch 7: |          | 1311/? [30:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1311, loss 3.4808945655822754\n",
      "Epoch 7: |          | 1312/? [30:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1312, loss 3.237539768218994\n",
      "Epoch 7: |          | 1313/? [30:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1313, loss 4.371971130371094\n",
      "Epoch 7: |          | 1314/? [30:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1314, loss 3.5921738147735596\n",
      "Epoch 7: |          | 1315/? [30:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1315, loss 4.344246864318848\n",
      "Epoch 7: |          | 1316/? [30:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1316, loss 4.097269535064697\n",
      "Epoch 7: |          | 1317/? [30:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1317, loss 3.7478702068328857\n",
      "Epoch 7: |          | 1318/? [30:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1318, loss 3.960447311401367\n",
      "Epoch 7: |          | 1319/? [30:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1319, loss 4.10526704788208\n",
      "Epoch 7: |          | 1320/? [30:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1320, loss 3.692492961883545\n",
      "Epoch 7: |          | 1321/? [30:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1321, loss 4.17172908782959\n",
      "Epoch 7: |          | 1322/? [30:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1322, loss 3.9778666496276855\n",
      "Epoch 7: |          | 1323/? [30:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1323, loss 3.4738268852233887\n",
      "Epoch 7: |          | 1324/? [30:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1324, loss 4.426693439483643\n",
      "Epoch 7: |          | 1325/? [30:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1325, loss 4.567137718200684\n",
      "Epoch 7: |          | 1326/? [31:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1326, loss 3.9813055992126465\n",
      "Epoch 7: |          | 1327/? [31:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1327, loss 3.865133285522461\n",
      "Epoch 7: |          | 1328/? [31:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1328, loss 3.5470376014709473\n",
      "Epoch 7: |          | 1329/? [31:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1329, loss 4.13442325592041\n",
      "Epoch 7: |          | 1330/? [31:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1330, loss 3.8740439414978027\n",
      "Epoch 7: |          | 1331/? [31:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1331, loss 3.977820873260498\n",
      "Epoch 7: |          | 1332/? [31:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1332, loss 3.7013497352600098\n",
      "Epoch 7: |          | 1333/? [31:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1333, loss 3.683361768722534\n",
      "Epoch 7: |          | 1334/? [31:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1334, loss 3.8173928260803223\n",
      "Epoch 7: |          | 1335/? [31:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1335, loss 3.73378324508667\n",
      "Epoch 7: |          | 1336/? [31:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1336, loss 3.346292495727539\n",
      "Epoch 7: |          | 1337/? [31:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1337, loss 3.986294984817505\n",
      "Epoch 7: |          | 1338/? [31:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1338, loss 3.2208988666534424\n",
      "Epoch 7: |          | 1339/? [31:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1339, loss 3.8918354511260986\n",
      "Epoch 7: |          | 1340/? [31:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1340, loss 3.2851929664611816\n",
      "Epoch 7: |          | 1341/? [31:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1341, loss 4.010939598083496\n",
      "Epoch 7: |          | 1342/? [31:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1342, loss 4.27608585357666\n",
      "Epoch 7: |          | 1343/? [31:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1343, loss 3.7569801807403564\n",
      "Epoch 7: |          | 1344/? [31:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1344, loss 3.860527515411377\n",
      "Epoch 7: |          | 1345/? [31:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1345, loss 3.989488124847412\n",
      "Epoch 7: |          | 1346/? [31:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1346, loss 5.132772922515869\n",
      "Epoch 7: |          | 1347/? [31:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1347, loss 4.03676700592041\n",
      "Epoch 7: |          | 1348/? [31:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1348, loss 4.200577735900879\n",
      "Epoch 7: |          | 1349/? [31:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1349, loss 4.046647071838379\n",
      "Epoch 7: |          | 1350/? [31:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1350, loss 4.145890235900879\n",
      "Epoch 7: |          | 1351/? [31:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1351, loss 4.083085536956787\n",
      "Epoch 7: |          | 1352/? [31:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1352, loss 3.309682846069336\n",
      "Epoch 7: |          | 1353/? [31:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1353, loss 3.678666353225708\n",
      "Epoch 7: |          | 1354/? [31:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1354, loss 4.056973934173584\n",
      "Epoch 7: |          | 1355/? [31:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1355, loss 4.212639331817627\n",
      "Epoch 7: |          | 1356/? [31:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1356, loss 3.9366512298583984\n",
      "Epoch 7: |          | 1357/? [31:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1357, loss 3.7270164489746094\n",
      "Epoch 7: |          | 1358/? [31:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1358, loss 3.887402296066284\n",
      "Epoch 7: |          | 1359/? [31:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1359, loss 3.7651848793029785\n",
      "Epoch 7: |          | 1360/? [31:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1360, loss 3.936555862426758\n",
      "Epoch 7: |          | 1361/? [31:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1361, loss 3.912421464920044\n",
      "Epoch 7: |          | 1362/? [31:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1362, loss 3.7422938346862793\n",
      "Epoch 7: |          | 1363/? [31:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1363, loss 3.193390369415283\n",
      "Epoch 7: |          | 1364/? [31:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1364, loss 3.6722500324249268\n",
      "Epoch 7: |          | 1365/? [31:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1365, loss 3.3888869285583496\n",
      "Epoch 7: |          | 1366/? [31:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1366, loss 4.114165306091309\n",
      "Epoch 7: |          | 1367/? [31:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1367, loss 3.406780242919922\n",
      "Epoch 7: |          | 1368/? [31:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1368, loss 3.219683885574341\n",
      "Epoch 7: |          | 1369/? [32:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1369, loss 3.8959343433380127\n",
      "Epoch 7: |          | 1370/? [32:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1370, loss 3.435157060623169\n",
      "Epoch 7: |          | 1371/? [32:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1371, loss 4.365899085998535\n",
      "Epoch 7: |          | 1372/? [32:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1372, loss 3.7981362342834473\n",
      "Epoch 7: |          | 1373/? [32:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1373, loss 4.203639507293701\n",
      "Epoch 7: |          | 1374/? [32:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1374, loss 3.2976322174072266\n",
      "Epoch 7: |          | 1375/? [32:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1375, loss 3.932645320892334\n",
      "Epoch 7: |          | 1376/? [32:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1376, loss 3.7944748401641846\n",
      "Epoch 7: |          | 1377/? [32:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1377, loss 3.8019535541534424\n",
      "Epoch 7: |          | 1378/? [32:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1378, loss 3.7495949268341064\n",
      "Epoch 7: |          | 1379/? [32:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1379, loss 3.8128554821014404\n",
      "Epoch 7: |          | 1380/? [32:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1380, loss 3.9283134937286377\n",
      "Epoch 7: |          | 1381/? [32:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1381, loss 4.039673805236816\n",
      "Epoch 7: |          | 1382/? [32:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1382, loss 3.610935926437378\n",
      "Epoch 7: |          | 1383/? [32:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1383, loss 3.8073813915252686\n",
      "Epoch 7: |          | 1384/? [32:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1384, loss 3.8453750610351562\n",
      "Epoch 7: |          | 1385/? [32:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1385, loss 3.7995452880859375\n",
      "Epoch 7: |          | 1386/? [32:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1386, loss 3.8766465187072754\n",
      "Epoch 7: |          | 1387/? [32:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1387, loss 3.8078880310058594\n",
      "Epoch 7: |          | 1388/? [32:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1388, loss 3.3245480060577393\n",
      "Epoch 7: |          | 1389/? [32:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1389, loss 4.036837577819824\n",
      "Epoch 7: |          | 1390/? [32:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1390, loss 4.369935035705566\n",
      "Epoch 7: |          | 1391/? [32:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1391, loss 3.974013090133667\n",
      "Epoch 7: |          | 1392/? [32:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1392, loss 3.468592882156372\n",
      "Epoch 7: |          | 1393/? [32:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1393, loss 3.6239376068115234\n",
      "Epoch 7: |          | 1394/? [32:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1394, loss 3.2749180793762207\n",
      "Epoch 7: |          | 1395/? [32:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1395, loss 3.9580249786376953\n",
      "Epoch 7: |          | 1396/? [32:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1396, loss 3.8599255084991455\n",
      "Epoch 7: |          | 1397/? [32:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1397, loss 3.0842785835266113\n",
      "Epoch 7: |          | 1398/? [32:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1398, loss 4.295857906341553\n",
      "Epoch 7: |          | 1399/? [32:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1399, loss 4.3122100830078125\n",
      "Epoch 7: |          | 1400/? [32:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1400, loss 3.4145591259002686\n",
      "Epoch 7: |          | 1401/? [32:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1401, loss 4.21169376373291\n",
      "Epoch 7: |          | 1402/? [32:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1402, loss 3.803851366043091\n",
      "Epoch 7: |          | 1403/? [32:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1403, loss 4.016519546508789\n",
      "Epoch 7: |          | 1404/? [32:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1404, loss 3.9871718883514404\n",
      "Epoch 7: |          | 1405/? [32:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1405, loss 4.315197944641113\n",
      "Epoch 7: |          | 1406/? [32:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1406, loss 4.235320568084717\n",
      "Epoch 7: |          | 1407/? [32:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1407, loss 4.315077781677246\n",
      "Epoch 7: |          | 1408/? [32:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1408, loss 3.5037693977355957\n",
      "Epoch 7: |          | 1409/? [32:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1409, loss 3.608830690383911\n",
      "Epoch 7: |          | 1410/? [32:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1410, loss 3.6376824378967285\n",
      "Epoch 7: |          | 1411/? [32:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1411, loss 3.9506518840789795\n",
      "Epoch 7: |          | 1412/? [33:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1412, loss 3.5972015857696533\n",
      "Epoch 7: |          | 1413/? [33:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1413, loss 3.499768018722534\n",
      "Epoch 7: |          | 1414/? [33:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1414, loss 3.6264984607696533\n",
      "Epoch 7: |          | 1415/? [33:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1415, loss 3.8890113830566406\n",
      "Epoch 7: |          | 1416/? [33:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1416, loss 4.25861930847168\n",
      "Epoch 7: |          | 1417/? [33:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1417, loss 3.930229663848877\n",
      "Epoch 7: |          | 1418/? [33:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1418, loss 4.063966751098633\n",
      "Epoch 7: |          | 1419/? [33:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1419, loss 3.844647169113159\n",
      "Epoch 7: |          | 1420/? [33:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1420, loss 3.6813671588897705\n",
      "Epoch 7: |          | 1421/? [33:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1421, loss 3.446509838104248\n",
      "Epoch 7: |          | 1422/? [33:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1422, loss 4.096663475036621\n",
      "Epoch 7: |          | 1423/? [33:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1423, loss 4.136137962341309\n",
      "Epoch 7: |          | 1424/? [33:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1424, loss 3.8282999992370605\n",
      "Epoch 7: |          | 1425/? [33:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1425, loss 4.028092384338379\n",
      "Epoch 7: |          | 1426/? [33:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1426, loss 3.55877685546875\n",
      "Epoch 7: |          | 1427/? [33:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1427, loss 4.174832344055176\n",
      "Epoch 7: |          | 1428/? [33:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1428, loss 4.15526819229126\n",
      "Epoch 7: |          | 1429/? [33:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1429, loss 4.053347110748291\n",
      "Epoch 7: |          | 1430/? [33:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1430, loss 4.1079020500183105\n",
      "Epoch 7: |          | 1431/? [33:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1431, loss 3.864159345626831\n",
      "Epoch 7: |          | 1432/? [33:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1432, loss 3.9223721027374268\n",
      "Epoch 7: |          | 1433/? [33:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1433, loss 3.817559003829956\n",
      "Epoch 7: |          | 1434/? [33:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1434, loss 3.9710421562194824\n",
      "Epoch 7: |          | 1435/? [33:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1435, loss 3.570129871368408\n",
      "Epoch 7: |          | 1436/? [33:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1436, loss 3.936047077178955\n",
      "Epoch 7: |          | 1437/? [33:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1437, loss 3.171501636505127\n",
      "Epoch 7: |          | 1438/? [33:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1438, loss 4.8268022537231445\n",
      "Epoch 7: |          | 1439/? [33:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1439, loss 3.9375247955322266\n",
      "Epoch 7: |          | 1440/? [33:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1440, loss 4.099307060241699\n",
      "Epoch 7: |          | 1441/? [33:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1441, loss 4.408102035522461\n",
      "Epoch 7: |          | 1442/? [33:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1442, loss 4.445371627807617\n",
      "Epoch 7: |          | 1443/? [33:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1443, loss 3.579235553741455\n",
      "Epoch 7: |          | 1444/? [33:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1444, loss 3.6065337657928467\n",
      "Epoch 7: |          | 1445/? [33:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1445, loss 4.098081111907959\n",
      "Epoch 7: |          | 1446/? [33:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1446, loss 3.680966854095459\n",
      "Epoch 7: |          | 1447/? [33:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1447, loss 3.8276798725128174\n",
      "Epoch 7: |          | 1448/? [33:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1448, loss 3.6599559783935547\n",
      "Epoch 7: |          | 1449/? [33:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1449, loss 3.8630523681640625\n",
      "Epoch 7: |          | 1450/? [33:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1450, loss 3.922699451446533\n",
      "Epoch 7: |          | 1451/? [33:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1451, loss 4.300044536590576\n",
      "Epoch 7: |          | 1452/? [33:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1452, loss 3.9367759227752686\n",
      "Epoch 7: |          | 1453/? [33:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1453, loss 3.288670301437378\n",
      "Epoch 7: |          | 1454/? [33:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1454, loss 3.8866703510284424\n",
      "Epoch 7: |          | 1455/? [34:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1455, loss 3.9605445861816406\n",
      "Epoch 7: |          | 1456/? [34:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1456, loss 3.5360286235809326\n",
      "Epoch 7: |          | 1457/? [34:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1457, loss 3.717880964279175\n",
      "Epoch 7: |          | 1458/? [34:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1458, loss 3.9375557899475098\n",
      "Epoch 7: |          | 1459/? [34:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1459, loss 4.063190937042236\n",
      "Epoch 7: |          | 1460/? [34:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1460, loss 4.000202655792236\n",
      "Epoch 7: |          | 1461/? [34:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1461, loss 3.9228854179382324\n",
      "Epoch 7: |          | 1462/? [34:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1462, loss 4.2161641120910645\n",
      "Epoch 7: |          | 1463/? [34:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1463, loss 4.146658897399902\n",
      "Epoch 7: |          | 1464/? [34:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1464, loss 3.4519639015197754\n",
      "Epoch 7: |          | 1465/? [34:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1465, loss 3.8308143615722656\n",
      "Epoch 7: |          | 1466/? [34:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1466, loss 3.5111842155456543\n",
      "Epoch 7: |          | 1467/? [34:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1467, loss 4.154021263122559\n",
      "Epoch 7: |          | 1468/? [34:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1468, loss 3.7329792976379395\n",
      "Epoch 7: |          | 1469/? [34:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1469, loss 3.4655563831329346\n",
      "Epoch 7: |          | 1470/? [34:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1470, loss 3.993927001953125\n",
      "Epoch 7: |          | 1471/? [34:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1471, loss 4.278813362121582\n",
      "Epoch 7: |          | 1472/? [34:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1472, loss 3.99139404296875\n",
      "Epoch 7: |          | 1473/? [34:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1473, loss 3.812443971633911\n",
      "Epoch 7: |          | 1474/? [34:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1474, loss 3.729795455932617\n",
      "Epoch 7: |          | 1475/? [34:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1475, loss 3.3231842517852783\n",
      "Epoch 7: |          | 1476/? [34:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1476, loss 3.900477647781372\n",
      "Epoch 7: |          | 1477/? [34:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1477, loss 3.9154746532440186\n",
      "Epoch 7: |          | 1478/? [34:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1478, loss 3.79119873046875\n",
      "Epoch 7: |          | 1479/? [34:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1479, loss 4.312630653381348\n",
      "Epoch 7: |          | 1480/? [34:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1480, loss 4.052813529968262\n",
      "Epoch 7: |          | 1481/? [34:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1481, loss 3.8027942180633545\n",
      "Epoch 7: |          | 1482/? [34:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1482, loss 3.9414398670196533\n",
      "Epoch 7: |          | 1483/? [34:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1483, loss 3.604846477508545\n",
      "Epoch 7: |          | 1484/? [34:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1484, loss 3.799776077270508\n",
      "Epoch 7: |          | 1485/? [34:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1485, loss 3.9219985008239746\n",
      "Epoch 7: |          | 1486/? [34:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1486, loss 3.923470973968506\n",
      "Epoch 7: |          | 1487/? [34:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1487, loss 3.377148389816284\n",
      "Epoch 7: |          | 1488/? [34:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1488, loss 4.030758857727051\n",
      "Epoch 7: |          | 1489/? [34:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1489, loss 3.9923813343048096\n",
      "Epoch 7: |          | 1490/? [34:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1490, loss 3.86961030960083\n",
      "Epoch 7: |          | 1491/? [34:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1491, loss 2.8810861110687256\n",
      "Epoch 7: |          | 1492/? [34:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1492, loss 3.439990282058716\n",
      "Epoch 7: |          | 1493/? [34:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1493, loss 2.999417543411255\n",
      "Epoch 7: |          | 1494/? [34:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1494, loss 3.796588897705078\n",
      "Epoch 7: |          | 1495/? [34:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1495, loss 3.678389072418213\n",
      "Epoch 7: |          | 1496/? [34:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1496, loss 4.0296549797058105\n",
      "Epoch 7: |          | 1497/? [34:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1497, loss 3.2744624614715576\n",
      "Epoch 7: |          | 1498/? [35:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1498, loss 3.5512185096740723\n",
      "Epoch 7: |          | 1499/? [35:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1499, loss 4.120028018951416\n",
      "Epoch 7: |          | 1500/? [35:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1500, loss 4.052412509918213\n",
      "Epoch 7: |          | 1501/? [35:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1501, loss 3.887838363647461\n",
      "Epoch 7: |          | 1502/? [35:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1502, loss 3.9680118560791016\n",
      "Epoch 7: |          | 1503/? [35:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1503, loss 3.719827175140381\n",
      "Epoch 7: |          | 1504/? [35:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1504, loss 4.363764762878418\n",
      "Epoch 7: |          | 1505/? [35:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1505, loss 4.178813457489014\n",
      "Epoch 7: |          | 1506/? [35:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1506, loss 3.79160737991333\n",
      "Epoch 7: |          | 1507/? [35:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1507, loss 3.6673691272735596\n",
      "Epoch 7: |          | 1508/? [35:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1508, loss 3.8023972511291504\n",
      "Epoch 7: |          | 1509/? [35:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1509, loss 3.8156027793884277\n",
      "Epoch 7: |          | 1510/? [35:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1510, loss 4.10598611831665\n",
      "Epoch 7: |          | 1511/? [35:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1511, loss 3.6825103759765625\n",
      "Epoch 7: |          | 1512/? [35:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1512, loss 4.255398750305176\n",
      "Epoch 7: |          | 1513/? [35:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1513, loss 4.484025955200195\n",
      "Epoch 7: |          | 1514/? [35:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1514, loss 3.563802719116211\n",
      "Epoch 7: |          | 1515/? [35:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1515, loss 4.431159019470215\n",
      "Epoch 7: |          | 1516/? [35:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1516, loss 4.318160533905029\n",
      "Epoch 7: |          | 1517/? [35:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1517, loss 3.7707772254943848\n",
      "Epoch 7: |          | 1518/? [35:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1518, loss 3.5419373512268066\n",
      "Epoch 7: |          | 1519/? [35:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1519, loss 4.121092796325684\n",
      "Epoch 7: |          | 1520/? [35:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1520, loss 4.146029472351074\n",
      "Epoch 7: |          | 1521/? [35:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1521, loss 3.8940253257751465\n",
      "Epoch 7: |          | 1522/? [35:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1522, loss 3.6478867530822754\n",
      "Epoch 7: |          | 1523/? [35:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1523, loss 3.9824252128601074\n",
      "Epoch 7: |          | 1524/? [35:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1524, loss 3.8567988872528076\n",
      "Epoch 7: |          | 1525/? [35:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1525, loss 3.529489517211914\n",
      "Epoch 7: |          | 1526/? [35:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1526, loss 4.104691505432129\n",
      "Epoch 7: |          | 1527/? [35:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1527, loss 4.116856575012207\n",
      "Epoch 7: |          | 1528/? [35:42<00:00,  0.71it/s, v_num=31]ERROR: Input has inproper shape\n",
      "Epoch 7: |          | 1529/? [35:42<00:00,  0.71it/s, v_num=31]   VALIDATION: Batch 0, loss 4.5752363204956055\n",
      "   VALIDATION: Batch 1, loss 3.57771635055542\n",
      "   VALIDATION: Batch 2, loss 4.758090972900391\n",
      "   VALIDATION: Batch 3, loss 4.443634510040283\n",
      "   VALIDATION: Batch 4, loss 3.9985969066619873\n",
      "   VALIDATION: Batch 5, loss 3.6937999725341797\n",
      "   VALIDATION: Batch 6, loss 3.9758312702178955\n",
      "   VALIDATION: Batch 7, loss 4.647188663482666\n",
      "   VALIDATION: Batch 8, loss 4.478030204772949\n",
      "   VALIDATION: Batch 9, loss 4.644463539123535\n",
      "   VALIDATION: Batch 10, loss 4.288959980010986\n",
      "   VALIDATION: Batch 11, loss 3.975806474685669\n",
      "   VALIDATION: Batch 12, loss 4.156617164611816\n",
      "   VALIDATION: Batch 13, loss 4.7093377113342285\n",
      "   VALIDATION: Batch 14, loss 3.9589545726776123\n",
      "   VALIDATION: Batch 15, loss 3.9213058948516846\n",
      "   VALIDATION: Batch 16, loss 4.630062103271484\n",
      "   VALIDATION: Batch 17, loss 4.312906265258789\n",
      "   VALIDATION: Batch 18, loss 3.526996612548828\n",
      "   VALIDATION: Batch 19, loss 4.411316871643066\n",
      "   VALIDATION: Batch 20, loss 4.708569526672363\n",
      "   VALIDATION: Batch 21, loss 4.938037872314453\n",
      "   VALIDATION: Batch 22, loss 4.589915752410889\n",
      "   VALIDATION: Batch 23, loss 4.12191104888916\n",
      "   VALIDATION: Batch 24, loss 3.949676036834717\n",
      "   VALIDATION: Batch 25, loss 4.338551044464111\n",
      "   VALIDATION: Batch 26, loss 4.584232807159424\n",
      "   VALIDATION: Batch 27, loss 4.443450450897217\n",
      "   VALIDATION: Batch 28, loss 4.242748737335205\n",
      "   VALIDATION: Batch 29, loss 4.410383224487305\n",
      "   VALIDATION: Batch 30, loss 4.065980434417725\n",
      "   VALIDATION: Batch 31, loss 4.366860389709473\n",
      "   VALIDATION: Batch 32, loss 4.921601295471191\n",
      "   VALIDATION: Batch 33, loss 3.1348235607147217\n",
      "   VALIDATION: Batch 34, loss 4.322955131530762\n",
      "   VALIDATION: Batch 35, loss 4.541269779205322\n",
      "   VALIDATION: Batch 36, loss 3.8238532543182373\n",
      "   VALIDATION: Batch 37, loss 3.8451218605041504\n",
      "   VALIDATION: Batch 38, loss 3.927272081375122\n",
      "   VALIDATION: Batch 39, loss 4.324121952056885\n",
      "   VALIDATION: Batch 40, loss 4.356338024139404\n",
      "   VALIDATION: Batch 41, loss 3.1729776859283447\n",
      "   VALIDATION: Batch 42, loss 4.347247123718262\n",
      "   VALIDATION: Batch 43, loss 4.5416460037231445\n",
      "   VALIDATION: Batch 44, loss 4.143538951873779\n",
      "   VALIDATION: Batch 45, loss 4.539146423339844\n",
      "   VALIDATION: Batch 46, loss 3.686126708984375\n",
      "   VALIDATION: Batch 47, loss 4.670390605926514\n",
      "   VALIDATION: Batch 48, loss 4.7905378341674805\n",
      "   VALIDATION: Batch 49, loss 4.399488925933838\n",
      "   VALIDATION: Batch 50, loss 4.390138149261475\n",
      "   VALIDATION: Batch 51, loss 4.847415447235107\n",
      "   VALIDATION: Batch 52, loss 4.036408424377441\n",
      "   VALIDATION: Batch 53, loss 3.8935863971710205\n",
      "   VALIDATION: Batch 54, loss 4.010011196136475\n",
      "   VALIDATION: Batch 55, loss 4.6884965896606445\n",
      "   VALIDATION: Batch 56, loss 4.128425121307373\n",
      "   VALIDATION: Batch 57, loss 5.730278968811035\n",
      "   VALIDATION: Batch 58, loss 4.243246555328369\n",
      "   VALIDATION: Batch 59, loss 3.900782823562622\n",
      "   VALIDATION: Batch 60, loss 3.4167797565460205\n",
      "   VALIDATION: Batch 61, loss 4.275978088378906\n",
      "   VALIDATION: Batch 62, loss 4.2614006996154785\n",
      "   VALIDATION: Batch 63, loss 4.7956342697143555\n",
      "   VALIDATION: Batch 64, loss 4.574881076812744\n",
      "   VALIDATION: Batch 65, loss 3.748485565185547\n",
      "   VALIDATION: Batch 66, loss 4.676049709320068\n",
      "   VALIDATION: Batch 67, loss 4.091494560241699\n",
      "   VALIDATION: Batch 68, loss 4.243168830871582\n",
      "   VALIDATION: Batch 69, loss 4.471950054168701\n",
      "   VALIDATION: Batch 70, loss 4.64206600189209\n",
      "   VALIDATION: Batch 71, loss 4.162519931793213\n",
      "   VALIDATION: Batch 72, loss 5.044882774353027\n",
      "   VALIDATION: Batch 73, loss 3.855794906616211\n",
      "   VALIDATION: Batch 74, loss 4.451546669006348\n",
      "   VALIDATION: Batch 75, loss 4.417136192321777\n",
      "   VALIDATION: Batch 76, loss 4.271608352661133\n",
      "   VALIDATION: Batch 77, loss 4.577569484710693\n",
      "   VALIDATION: Batch 78, loss 4.388851165771484\n",
      "   VALIDATION: Batch 79, loss 4.3626203536987305\n",
      "   VALIDATION: Batch 80, loss 4.4446940422058105\n",
      "   VALIDATION: Batch 81, loss 4.1448845863342285\n",
      "   VALIDATION: Batch 82, loss 4.539035320281982\n",
      "   VALIDATION: Batch 83, loss 3.830235719680786\n",
      "   VALIDATION: Batch 84, loss 4.507737159729004\n",
      "   VALIDATION: Batch 85, loss 4.149263858795166\n",
      "   VALIDATION: Batch 86, loss 4.23984432220459\n",
      "   VALIDATION: Batch 87, loss 4.104072570800781\n",
      "   VALIDATION: Batch 88, loss 3.6922898292541504\n",
      "   VALIDATION: Batch 89, loss 4.0206756591796875\n",
      "   VALIDATION: Batch 90, loss 4.267219543457031\n",
      "   VALIDATION: Batch 91, loss 4.237762928009033\n",
      "   VALIDATION: Batch 92, loss 3.912315845489502\n",
      "   VALIDATION: Batch 93, loss 4.748201847076416\n",
      "   VALIDATION: Batch 94, loss 4.206360816955566\n",
      "   VALIDATION: Batch 95, loss 3.7294795513153076\n",
      "   VALIDATION: Batch 96, loss 4.165058135986328\n",
      "   VALIDATION: Batch 97, loss 3.895866870880127\n",
      "   VALIDATION: Batch 98, loss 4.516039848327637\n",
      "   VALIDATION: Batch 99, loss 4.630715370178223\n",
      "   VALIDATION: Batch 100, loss 4.960040092468262\n",
      "   VALIDATION: Batch 101, loss 3.5622036457061768\n",
      "   VALIDATION: Batch 102, loss 4.970038414001465\n",
      "   VALIDATION: Batch 103, loss 4.904456615447998\n",
      "   VALIDATION: Batch 104, loss 3.8465983867645264\n",
      "   VALIDATION: Batch 105, loss 4.303088665008545\n",
      "   VALIDATION: Batch 106, loss 4.189290523529053\n",
      "   VALIDATION: Batch 107, loss 4.317069053649902\n",
      "   VALIDATION: Batch 108, loss 4.005002498626709\n",
      "   VALIDATION: Batch 109, loss 4.617064476013184\n",
      "   VALIDATION: Batch 110, loss 4.286617755889893\n",
      "   VALIDATION: Batch 111, loss 4.628257751464844\n",
      "   VALIDATION: Batch 112, loss 5.484407901763916\n",
      "   VALIDATION: Batch 113, loss 4.781973361968994\n",
      "   VALIDATION: Batch 114, loss 4.58658504486084\n",
      "   VALIDATION: Batch 115, loss 4.033506870269775\n",
      "   VALIDATION: Batch 116, loss 3.8786778450012207\n",
      "   VALIDATION: Batch 117, loss 4.532591819763184\n",
      "   VALIDATION: Batch 118, loss 4.729538917541504\n",
      "   VALIDATION: Batch 119, loss 3.9284427165985107\n",
      "   VALIDATION: Batch 120, loss 3.5514678955078125\n",
      "   VALIDATION: Batch 121, loss 3.7763938903808594\n",
      "   VALIDATION: Batch 122, loss 4.163230895996094\n",
      "   VALIDATION: Batch 123, loss 4.18724250793457\n",
      "   VALIDATION: Batch 124, loss 3.6077651977539062\n",
      "   VALIDATION: Batch 125, loss 4.223738670349121\n",
      "   VALIDATION: Batch 126, loss 4.440804958343506\n",
      "   VALIDATION: Batch 127, loss 4.184232711791992\n",
      "   VALIDATION: Batch 128, loss 4.363018989562988\n",
      "   VALIDATION: Batch 129, loss 3.9800026416778564\n",
      "   VALIDATION: Batch 130, loss 3.6458988189697266\n",
      "   VALIDATION: Batch 131, loss 3.670412063598633\n",
      "   VALIDATION: Batch 132, loss 4.297814846038818\n",
      "   VALIDATION: Batch 133, loss 4.46217679977417\n",
      "   VALIDATION: Batch 134, loss 4.336638450622559\n",
      "   VALIDATION: Batch 135, loss 4.616082191467285\n",
      "   VALIDATION: Batch 136, loss 4.704085350036621\n",
      "   VALIDATION: Batch 137, loss 4.528332710266113\n",
      "   VALIDATION: Batch 138, loss 4.257522106170654\n",
      "   VALIDATION: Batch 139, loss 4.634268283843994\n",
      "   VALIDATION: Batch 140, loss 3.7578463554382324\n",
      "   VALIDATION: Batch 141, loss 4.6948747634887695\n",
      "   VALIDATION: Batch 142, loss 3.4268405437469482\n",
      "   VALIDATION: Batch 143, loss 4.2243452072143555\n",
      "   VALIDATION: Batch 144, loss 4.483920097351074\n",
      "   VALIDATION: Batch 145, loss 4.2627129554748535\n",
      "   VALIDATION: Batch 146, loss 4.0858259201049805\n",
      "   VALIDATION: Batch 147, loss 4.426275730133057\n",
      "   VALIDATION: Batch 148, loss 4.496199607849121\n",
      "   VALIDATION: Batch 149, loss 5.033811092376709\n",
      "   VALIDATION: Batch 150, loss 4.5765862464904785\n",
      "   VALIDATION: Batch 151, loss 4.88906192779541\n",
      "   VALIDATION: Batch 152, loss 4.240051746368408\n",
      "   VALIDATION: Batch 153, loss 4.486178874969482\n",
      "   VALIDATION: Batch 154, loss 4.3114118576049805\n",
      "   VALIDATION: Batch 155, loss 4.0924882888793945\n",
      "   VALIDATION: Batch 156, loss 4.7340288162231445\n",
      "   VALIDATION: Batch 157, loss 4.3902587890625\n",
      "   VALIDATION: Batch 158, loss 3.8565402030944824\n",
      "   VALIDATION: Batch 159, loss 4.291043758392334\n",
      "   VALIDATION: Batch 160, loss 4.648979663848877\n",
      "   VALIDATION: Batch 161, loss 4.7833147048950195\n",
      "   VALIDATION: Batch 162, loss 4.250622749328613\n",
      "   VALIDATION: Batch 163, loss 3.7157740592956543\n",
      "   VALIDATION: Batch 164, loss 4.257460117340088\n",
      "   VALIDATION: Batch 165, loss 4.6744232177734375\n",
      "   VALIDATION: Batch 166, loss 4.102517127990723\n",
      "   VALIDATION: Batch 167, loss 4.5470170974731445\n",
      "   VALIDATION: Batch 168, loss 3.417797088623047\n",
      "   VALIDATION: Batch 169, loss 4.112584114074707\n",
      "   VALIDATION: Batch 170, loss 4.279608249664307\n",
      "   VALIDATION: Batch 171, loss 4.438875198364258\n",
      "   VALIDATION: Batch 172, loss 4.280544757843018\n",
      "   VALIDATION: Batch 173, loss 4.193917274475098\n",
      "   VALIDATION: Batch 174, loss 4.597430229187012\n",
      "   VALIDATION: Batch 175, loss 4.3824076652526855\n",
      "   VALIDATION: Batch 176, loss 4.1553730964660645\n",
      "   VALIDATION: Batch 177, loss 4.041718482971191\n",
      "   VALIDATION: Batch 178, loss 5.1724467277526855\n",
      "   VALIDATION: Batch 179, loss 4.460787773132324\n",
      "   VALIDATION: Batch 180, loss 3.9615817070007324\n",
      "   VALIDATION: Batch 181, loss 4.119258880615234\n",
      "   VALIDATION: Batch 182, loss 4.4171857833862305\n",
      "   VALIDATION: Batch 183, loss 3.5080981254577637\n",
      "   VALIDATION: Batch 184, loss 3.273207426071167\n",
      "   VALIDATION: Batch 185, loss 4.024758338928223\n",
      "   VALIDATION: Batch 186, loss 3.887808322906494\n",
      "   VALIDATION: Batch 187, loss 4.094281196594238\n",
      "   VALIDATION: Batch 188, loss 4.501952171325684\n",
      "   VALIDATION: Batch 189, loss 3.8507049083709717\n",
      "   VALIDATION: Batch 190, loss 3.9636542797088623\n",
      "   VALIDATION: Batch 191, loss 4.432236671447754\n",
      "   VALIDATION: Batch 192, loss 4.715427398681641\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 8: |          | 0/? [00:00<?, ?it/s, v_num=31]              TRRAINING: Batch 0, loss 4.0962300300598145\n",
      "Epoch 8: |          | 1/? [00:01<00:00,  0.58it/s, v_num=31]   TRRAINING: Batch 1, loss 3.681248188018799\n",
      "Epoch 8: |          | 2/? [00:03<00:00,  0.64it/s, v_num=31]   TRRAINING: Batch 2, loss 3.758934497833252\n",
      "Epoch 8: |          | 3/? [00:04<00:00,  0.65it/s, v_num=31]   TRRAINING: Batch 3, loss 3.4367058277130127\n",
      "Epoch 8: |          | 4/? [00:05<00:00,  0.67it/s, v_num=31]   TRRAINING: Batch 4, loss 3.846097230911255\n",
      "Epoch 8: |          | 5/? [00:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 5, loss 4.615406036376953\n",
      "Epoch 8: |          | 6/? [00:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 6, loss 4.1004958152771\n",
      "Epoch 8: |          | 7/? [00:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 7, loss 3.435591459274292\n",
      "Epoch 8: |          | 8/? [00:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 8, loss 3.530336380004883\n",
      "Epoch 8: |          | 9/? [00:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 9, loss 3.8402953147888184\n",
      "Epoch 8: |          | 10/? [00:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 10, loss 4.0749688148498535\n",
      "Epoch 8: |          | 11/? [00:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 11, loss 3.988476514816284\n",
      "Epoch 8: |          | 12/? [00:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 12, loss 4.485007286071777\n",
      "Epoch 8: |          | 13/? [00:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 13, loss 3.9652531147003174\n",
      "Epoch 8: |          | 14/? [00:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 14, loss 4.1683878898620605\n",
      "Epoch 8: |          | 15/? [00:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 15, loss 3.4402496814727783\n",
      "Epoch 8: |          | 16/? [00:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 16, loss 3.2585582733154297\n",
      "Epoch 8: |          | 17/? [00:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 17, loss 4.396824836730957\n",
      "Epoch 8: |          | 18/? [00:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 18, loss 3.914328098297119\n",
      "Epoch 8: |          | 19/? [00:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 19, loss 3.687434434890747\n",
      "Epoch 8: |          | 20/? [00:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 20, loss 3.9568302631378174\n",
      "Epoch 8: |          | 21/? [00:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 21, loss 4.100204944610596\n",
      "Epoch 8: |          | 22/? [00:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 22, loss 3.994860887527466\n",
      "Epoch 8: |          | 23/? [00:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 23, loss 3.3840713500976562\n",
      "Epoch 8: |          | 24/? [00:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 24, loss 4.003078937530518\n",
      "Epoch 8: |          | 25/? [00:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 25, loss 3.8197810649871826\n",
      "Epoch 8: |          | 26/? [00:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 26, loss 3.588296890258789\n",
      "Epoch 8: |          | 27/? [00:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 27, loss 3.551501750946045\n",
      "Epoch 8: |          | 28/? [00:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 28, loss 4.3956804275512695\n",
      "Epoch 8: |          | 29/? [00:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 29, loss 3.9550044536590576\n",
      "Epoch 8: |          | 30/? [00:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 30, loss 3.8283653259277344\n",
      "Epoch 8: |          | 31/? [00:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 31, loss 4.402838230133057\n",
      "Epoch 8: |          | 32/? [00:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 32, loss 3.9405624866485596\n",
      "Epoch 8: |          | 33/? [00:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 33, loss 3.794991970062256\n",
      "Epoch 8: |          | 34/? [00:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 34, loss 3.774169921875\n",
      "Epoch 8: |          | 35/? [00:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 35, loss 3.211350917816162\n",
      "Epoch 8: |          | 36/? [00:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 36, loss 3.9901554584503174\n",
      "Epoch 8: |          | 37/? [00:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 37, loss 4.152533531188965\n",
      "Epoch 8: |          | 38/? [00:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 38, loss 4.466992378234863\n",
      "Epoch 8: |          | 39/? [00:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 39, loss 4.324728965759277\n",
      "Epoch 8: |          | 40/? [00:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 40, loss 3.743342161178589\n",
      "Epoch 8: |          | 41/? [00:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 41, loss 3.8326754570007324\n",
      "Epoch 8: |          | 42/? [00:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 42, loss 3.597665786743164\n",
      "Epoch 8: |          | 43/? [01:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 43, loss 3.791179656982422\n",
      "Epoch 8: |          | 44/? [01:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 44, loss 2.95698881149292\n",
      "Epoch 8: |          | 45/? [01:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 45, loss 2.5076797008514404\n",
      "Epoch 8: |          | 46/? [01:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 46, loss 4.3485541343688965\n",
      "Epoch 8: |          | 47/? [01:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 47, loss 3.5777626037597656\n",
      "Epoch 8: |          | 48/? [01:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 48, loss 3.449934720993042\n",
      "Epoch 8: |          | 49/? [01:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 49, loss 3.9206302165985107\n",
      "Epoch 8: |          | 50/? [01:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 50, loss 3.7465224266052246\n",
      "Epoch 8: |          | 51/? [01:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 51, loss 3.7089900970458984\n",
      "Epoch 8: |          | 52/? [01:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 52, loss 4.415589332580566\n",
      "Epoch 8: |          | 53/? [01:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 53, loss 4.02779483795166\n",
      "Epoch 8: |          | 54/? [01:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 54, loss 3.829921245574951\n",
      "Epoch 8: |          | 55/? [01:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 55, loss 3.8870925903320312\n",
      "Epoch 8: |          | 56/? [01:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 56, loss 4.029522895812988\n",
      "Epoch 8: |          | 57/? [01:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 57, loss 3.9210076332092285\n",
      "Epoch 8: |          | 58/? [01:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 58, loss 5.09333610534668\n",
      "Epoch 8: |          | 59/? [01:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 59, loss 4.077207565307617\n",
      "Epoch 8: |          | 60/? [01:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 60, loss 4.211960792541504\n",
      "Epoch 8: |          | 61/? [01:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 61, loss 4.169749736785889\n",
      "Epoch 8: |          | 62/? [01:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 62, loss 3.803081512451172\n",
      "Epoch 8: |          | 63/? [01:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 63, loss 3.991680860519409\n",
      "Epoch 8: |          | 64/? [01:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 64, loss 3.8258910179138184\n",
      "Epoch 8: |          | 65/? [01:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 65, loss 3.8599281311035156\n",
      "Epoch 8: |          | 66/? [01:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 66, loss 3.3240790367126465\n",
      "Epoch 8: |          | 67/? [01:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 67, loss 3.9823899269104004\n",
      "Epoch 8: |          | 68/? [01:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 68, loss 3.9954769611358643\n",
      "Epoch 8: |          | 69/? [01:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 69, loss 3.944131374359131\n",
      "Epoch 8: |          | 70/? [01:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 70, loss 3.587526321411133\n",
      "Epoch 8: |          | 71/? [01:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 71, loss 3.539076328277588\n",
      "Epoch 8: |          | 72/? [01:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 72, loss 3.7506179809570312\n",
      "Epoch 8: |          | 73/? [01:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 73, loss 3.9378254413604736\n",
      "Epoch 8: |          | 74/? [01:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 74, loss 3.73481822013855\n",
      "Epoch 8: |          | 75/? [01:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 75, loss 3.8287765979766846\n",
      "Epoch 8: |          | 76/? [01:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 76, loss 3.8441061973571777\n",
      "Epoch 8: |          | 77/? [01:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 77, loss 3.850487470626831\n",
      "Epoch 8: |          | 78/? [01:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 78, loss 3.701760768890381\n",
      "Epoch 8: |          | 79/? [01:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 79, loss 3.874272584915161\n",
      "Epoch 8: |          | 80/? [01:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 80, loss 3.7960104942321777\n",
      "Epoch 8: |          | 81/? [01:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 81, loss 3.271388530731201\n",
      "Epoch 8: |          | 82/? [01:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 82, loss 4.054994583129883\n",
      "Epoch 8: |          | 83/? [01:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 83, loss 3.552330732345581\n",
      "Epoch 8: |          | 84/? [01:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 84, loss 3.3939709663391113\n",
      "Epoch 8: |          | 85/? [01:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 85, loss 3.2351536750793457\n",
      "Epoch 8: |          | 86/? [02:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 86, loss 3.4115958213806152\n",
      "Epoch 8: |          | 87/? [02:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 87, loss 3.582688808441162\n",
      "Epoch 8: |          | 88/? [02:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 88, loss 4.248480319976807\n",
      "Epoch 8: |          | 89/? [02:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 89, loss 4.199386119842529\n",
      "Epoch 8: |          | 90/? [02:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 90, loss 3.9422523975372314\n",
      "Epoch 8: |          | 91/? [02:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 91, loss 3.651390790939331\n",
      "Epoch 8: |          | 92/? [02:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 92, loss 4.121240615844727\n",
      "Epoch 8: |          | 93/? [02:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 93, loss 4.279180526733398\n",
      "Epoch 8: |          | 94/? [02:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 94, loss 4.1183881759643555\n",
      "Epoch 8: |          | 95/? [02:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 95, loss 4.14146089553833\n",
      "Epoch 8: |          | 96/? [02:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 96, loss 3.6921768188476562\n",
      "Epoch 8: |          | 97/? [02:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 97, loss 3.482755184173584\n",
      "Epoch 8: |          | 98/? [02:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 98, loss 4.011843681335449\n",
      "Epoch 8: |          | 99/? [02:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 99, loss 4.205129623413086\n",
      "Epoch 8: |          | 100/? [02:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 100, loss 4.130197525024414\n",
      "Epoch 8: |          | 101/? [02:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 101, loss 3.801537275314331\n",
      "Epoch 8: |          | 102/? [02:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 102, loss 3.792536973953247\n",
      "Epoch 8: |          | 103/? [02:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 103, loss 3.600165843963623\n",
      "Epoch 8: |          | 104/? [02:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 104, loss 3.9279205799102783\n",
      "Epoch 8: |          | 105/? [02:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 105, loss 3.87287974357605\n",
      "Epoch 8: |          | 106/? [02:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 106, loss 4.0228424072265625\n",
      "Epoch 8: |          | 107/? [02:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 107, loss 4.025791645050049\n",
      "Epoch 8: |          | 108/? [02:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 108, loss 4.00724458694458\n",
      "Epoch 8: |          | 109/? [02:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 109, loss 3.733537197113037\n",
      "Epoch 8: |          | 110/? [02:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 110, loss 3.900489330291748\n",
      "Epoch 8: |          | 111/? [02:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 111, loss 4.522193908691406\n",
      "Epoch 8: |          | 112/? [02:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 112, loss 3.2350234985351562\n",
      "Epoch 8: |          | 113/? [02:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 113, loss 2.828535795211792\n",
      "Epoch 8: |          | 114/? [02:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 114, loss 4.108176231384277\n",
      "Epoch 8: |          | 115/? [02:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 115, loss 4.252824783325195\n",
      "Epoch 8: |          | 116/? [02:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 116, loss 3.432543992996216\n",
      "Epoch 8: |          | 117/? [02:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 117, loss 3.3638691902160645\n",
      "Epoch 8: |          | 118/? [02:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 118, loss 4.178956031799316\n",
      "Epoch 8: |          | 119/? [02:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 119, loss 4.4198150634765625\n",
      "Epoch 8: |          | 120/? [02:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 120, loss 4.18842077255249\n",
      "Epoch 8: |          | 121/? [02:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 121, loss 3.897907257080078\n",
      "Epoch 8: |          | 122/? [02:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 122, loss 3.3646304607391357\n",
      "Epoch 8: |          | 123/? [02:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 123, loss 3.8512015342712402\n",
      "Epoch 8: |          | 124/? [02:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 124, loss 4.010415077209473\n",
      "Epoch 8: |          | 125/? [02:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 125, loss 3.633178234100342\n",
      "Epoch 8: |          | 126/? [02:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 126, loss 4.0952677726745605\n",
      "Epoch 8: |          | 127/? [02:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 127, loss 4.232052326202393\n",
      "Epoch 8: |          | 128/? [02:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 128, loss 3.378107786178589\n",
      "Epoch 8: |          | 129/? [03:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 129, loss 3.968876361846924\n",
      "Epoch 8: |          | 130/? [03:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 130, loss 3.089895725250244\n",
      "Epoch 8: |          | 131/? [03:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 131, loss 3.897609233856201\n",
      "Epoch 8: |          | 132/? [03:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 132, loss 3.9476616382598877\n",
      "Epoch 8: |          | 133/? [03:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 133, loss 3.8213424682617188\n",
      "Epoch 8: |          | 134/? [03:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 134, loss 4.007760047912598\n",
      "Epoch 8: |          | 135/? [03:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 135, loss 4.090469837188721\n",
      "Epoch 8: |          | 136/? [03:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 136, loss 4.14256477355957\n",
      "Epoch 8: |          | 137/? [03:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 137, loss 3.155818223953247\n",
      "Epoch 8: |          | 138/? [03:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 138, loss 3.723194122314453\n",
      "Epoch 8: |          | 139/? [03:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 139, loss 4.171000957489014\n",
      "Epoch 8: |          | 140/? [03:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 140, loss 3.442099094390869\n",
      "Epoch 8: |          | 141/? [03:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 141, loss 3.523913860321045\n",
      "Epoch 8: |          | 142/? [03:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 142, loss 4.969357490539551\n",
      "Epoch 8: |          | 143/? [03:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 143, loss 4.639752388000488\n",
      "Epoch 8: |          | 144/? [03:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 144, loss 3.8837578296661377\n",
      "Epoch 8: |          | 145/? [03:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 145, loss 3.3690428733825684\n",
      "Epoch 8: |          | 146/? [03:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 146, loss 3.5360007286071777\n",
      "Epoch 8: |          | 147/? [03:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 147, loss 3.8467488288879395\n",
      "Epoch 8: |          | 148/? [03:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 148, loss 3.602773666381836\n",
      "Epoch 8: |          | 149/? [03:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 149, loss 3.154162645339966\n",
      "Epoch 8: |          | 150/? [03:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 150, loss 4.016003131866455\n",
      "Epoch 8: |          | 151/? [03:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 151, loss 4.133241653442383\n",
      "Epoch 8: |          | 152/? [03:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 152, loss 4.061922073364258\n",
      "Epoch 8: |          | 153/? [03:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 153, loss 3.176598072052002\n",
      "Epoch 8: |          | 154/? [03:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 154, loss 4.437854766845703\n",
      "Epoch 8: |          | 155/? [03:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 155, loss 3.9504313468933105\n",
      "Epoch 8: |          | 156/? [03:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 156, loss 3.3267104625701904\n",
      "Epoch 8: |          | 157/? [03:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 157, loss 4.000031471252441\n",
      "Epoch 8: |          | 158/? [03:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 158, loss 4.098358631134033\n",
      "Epoch 8: |          | 159/? [03:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 159, loss 3.818493366241455\n",
      "Epoch 8: |          | 160/? [03:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 160, loss 3.5873165130615234\n",
      "Epoch 8: |          | 161/? [03:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 161, loss 3.9522922039031982\n",
      "Epoch 8: |          | 162/? [03:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 162, loss 3.9842910766601562\n",
      "Epoch 8: |          | 163/? [03:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 163, loss 3.1469764709472656\n",
      "Epoch 8: |          | 164/? [03:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 164, loss 3.5782952308654785\n",
      "Epoch 8: |          | 165/? [03:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 165, loss 4.357496738433838\n",
      "Epoch 8: |          | 166/? [03:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 166, loss 4.024125576019287\n",
      "Epoch 8: |          | 167/? [03:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 167, loss 4.122780799865723\n",
      "Epoch 8: |          | 168/? [03:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 168, loss 3.726506471633911\n",
      "Epoch 8: |          | 169/? [03:55<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 169, loss 3.3857617378234863\n",
      "Epoch 8: |          | 170/? [03:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 170, loss 3.5830154418945312\n",
      "Epoch 8: |          | 171/? [03:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 171, loss 3.9511210918426514\n",
      "Epoch 8: |          | 172/? [03:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 172, loss 3.7660605907440186\n",
      "Epoch 8: |          | 173/? [04:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 173, loss 4.371641159057617\n",
      "Epoch 8: |          | 174/? [04:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 174, loss 3.9634857177734375\n",
      "Epoch 8: |          | 175/? [04:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 175, loss 4.466228485107422\n",
      "Epoch 8: |          | 176/? [04:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 176, loss 3.685576915740967\n",
      "Epoch 8: |          | 177/? [04:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 177, loss 3.7271952629089355\n",
      "Epoch 8: |          | 178/? [04:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 178, loss 3.573617935180664\n",
      "Epoch 8: |          | 179/? [04:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 179, loss 4.244910717010498\n",
      "Epoch 8: |          | 180/? [04:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 180, loss 3.798633575439453\n",
      "Epoch 8: |          | 181/? [04:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 181, loss 3.5892651081085205\n",
      "Epoch 8: |          | 182/? [04:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 182, loss 3.9853153228759766\n",
      "Epoch 8: |          | 183/? [04:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 183, loss 3.481044054031372\n",
      "Epoch 8: |          | 184/? [04:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 184, loss 3.706815719604492\n",
      "Epoch 8: |          | 185/? [04:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 185, loss 4.245855808258057\n",
      "Epoch 8: |          | 186/? [04:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 186, loss 3.7235233783721924\n",
      "Epoch 8: |          | 187/? [04:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 187, loss 4.227628231048584\n",
      "Epoch 8: |          | 188/? [04:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 188, loss 3.619898557662964\n",
      "Epoch 8: |          | 189/? [04:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 189, loss 4.257525444030762\n",
      "Epoch 8: |          | 190/? [04:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 190, loss 3.780097484588623\n",
      "Epoch 8: |          | 191/? [04:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 191, loss 4.541189193725586\n",
      "Epoch 8: |          | 192/? [04:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 192, loss 4.2807817459106445\n",
      "Epoch 8: |          | 193/? [04:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 193, loss 3.6470673084259033\n",
      "Epoch 8: |          | 194/? [04:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 194, loss 3.5407187938690186\n",
      "Epoch 8: |          | 195/? [04:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 195, loss 4.240752220153809\n",
      "Epoch 8: |          | 196/? [04:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 196, loss 4.07960319519043\n",
      "Epoch 8: |          | 197/? [04:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 197, loss 3.9705004692077637\n",
      "Epoch 8: |          | 198/? [04:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 198, loss 3.287221908569336\n",
      "Epoch 8: |          | 199/? [04:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 199, loss 4.209662914276123\n",
      "Epoch 8: |          | 200/? [04:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 200, loss 3.667259931564331\n",
      "Epoch 8: |          | 201/? [04:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 201, loss 3.9144721031188965\n",
      "Epoch 8: |          | 202/? [04:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 202, loss 4.0796284675598145\n",
      "Epoch 8: |          | 203/? [04:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 203, loss 3.7124202251434326\n",
      "Epoch 8: |          | 204/? [04:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 204, loss 3.7507598400115967\n",
      "Epoch 8: |          | 205/? [04:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 205, loss 3.6310718059539795\n",
      "Epoch 8: |          | 206/? [04:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 206, loss 3.47356915473938\n",
      "Epoch 8: |          | 207/? [04:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 207, loss 4.017746925354004\n",
      "Epoch 8: |          | 208/? [04:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 208, loss 3.8958351612091064\n",
      "Epoch 8: |          | 209/? [04:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 209, loss 3.729114055633545\n",
      "Epoch 8: |          | 210/? [04:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 210, loss 4.426689147949219\n",
      "Epoch 8: |          | 211/? [04:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 211, loss 3.760939359664917\n",
      "Epoch 8: |          | 212/? [04:55<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 212, loss 4.02103328704834\n",
      "Epoch 8: |          | 213/? [04:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 213, loss 3.856201171875\n",
      "Epoch 8: |          | 214/? [04:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 214, loss 3.725329875946045\n",
      "Epoch 8: |          | 215/? [05:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 215, loss 3.3880324363708496\n",
      "Epoch 8: |          | 216/? [05:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 216, loss 4.035090446472168\n",
      "Epoch 8: |          | 217/? [05:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 217, loss 3.9684691429138184\n",
      "Epoch 8: |          | 218/? [05:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 218, loss 3.9848732948303223\n",
      "Epoch 8: |          | 219/? [05:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 219, loss 3.9357452392578125\n",
      "Epoch 8: |          | 220/? [05:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 220, loss 3.989022731781006\n",
      "Epoch 8: |          | 221/? [05:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 221, loss 3.804445266723633\n",
      "Epoch 8: |          | 222/? [05:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 222, loss 3.0955004692077637\n",
      "Epoch 8: |          | 223/? [05:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 223, loss 4.046725749969482\n",
      "Epoch 8: |          | 224/? [05:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 224, loss 4.153642177581787\n",
      "Epoch 8: |          | 225/? [05:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 225, loss 3.8785643577575684\n",
      "Epoch 8: |          | 226/? [05:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 226, loss 3.8351387977600098\n",
      "Epoch 8: |          | 227/? [05:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 227, loss 4.154127597808838\n",
      "Epoch 8: |          | 228/? [05:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 228, loss 3.8774895668029785\n",
      "Epoch 8: |          | 229/? [05:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 229, loss 3.9171993732452393\n",
      "Epoch 8: |          | 230/? [05:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 230, loss 3.8697731494903564\n",
      "Epoch 8: |          | 231/? [05:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 231, loss 3.825753688812256\n",
      "Epoch 8: |          | 232/? [05:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 232, loss 3.534257411956787\n",
      "Epoch 8: |          | 233/? [05:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 233, loss 4.190520763397217\n",
      "Epoch 8: |          | 234/? [05:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 234, loss 4.212824821472168\n",
      "Epoch 8: |          | 235/? [05:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 235, loss 4.300324440002441\n",
      "Epoch 8: |          | 236/? [05:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 236, loss 3.732285737991333\n",
      "Epoch 8: |          | 237/? [05:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 237, loss 3.8722946643829346\n",
      "Epoch 8: |          | 238/? [05:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 238, loss 4.134232997894287\n",
      "Epoch 8: |          | 239/? [05:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 239, loss 3.7195842266082764\n",
      "Epoch 8: |          | 240/? [05:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 240, loss 3.2594542503356934\n",
      "Epoch 8: |          | 241/? [05:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 241, loss 3.8799819946289062\n",
      "Epoch 8: |          | 242/? [05:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 242, loss 4.222878932952881\n",
      "Epoch 8: |          | 243/? [05:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 243, loss 3.129507303237915\n",
      "Epoch 8: |          | 244/? [05:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 244, loss 3.5430748462677\n",
      "Epoch 8: |          | 245/? [05:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 245, loss 3.8674404621124268\n",
      "Epoch 8: |          | 246/? [05:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 246, loss 3.9671435356140137\n",
      "Epoch 8: |          | 247/? [05:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 247, loss 3.98754620552063\n",
      "Epoch 8: |          | 248/? [05:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 248, loss 3.551553249359131\n",
      "Epoch 8: |          | 249/? [05:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 249, loss 3.3913474082946777\n",
      "Epoch 8: |          | 250/? [05:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 250, loss 3.995114803314209\n",
      "Epoch 8: |          | 251/? [05:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 251, loss 3.9769489765167236\n",
      "Epoch 8: |          | 252/? [05:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 252, loss 3.7676422595977783\n",
      "Epoch 8: |          | 253/? [05:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 253, loss 4.543430805206299\n",
      "Epoch 8: |          | 254/? [05:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 254, loss 4.244351387023926\n",
      "Epoch 8: |          | 255/? [05:55<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 255, loss 3.857590436935425\n",
      "Epoch 8: |          | 256/? [05:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 256, loss 4.81217098236084\n",
      "Epoch 8: |          | 257/? [05:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 257, loss 3.749605655670166\n",
      "Epoch 8: |          | 258/? [05:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 258, loss 3.868687868118286\n",
      "Epoch 8: |          | 259/? [06:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 259, loss 3.742936372756958\n",
      "Epoch 8: |          | 260/? [06:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 260, loss 3.663198947906494\n",
      "Epoch 8: |          | 261/? [06:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 261, loss 3.4741692543029785\n",
      "Epoch 8: |          | 262/? [06:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 262, loss 4.173825263977051\n",
      "Epoch 8: |          | 263/? [06:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 263, loss 3.821519136428833\n",
      "Epoch 8: |          | 264/? [06:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 264, loss 3.982595443725586\n",
      "Epoch 8: |          | 265/? [06:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 265, loss 3.4003021717071533\n",
      "Epoch 8: |          | 266/? [06:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 266, loss 3.8867099285125732\n",
      "Epoch 8: |          | 267/? [06:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 267, loss 3.4416186809539795\n",
      "Epoch 8: |          | 268/? [06:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 268, loss 3.8182740211486816\n",
      "Epoch 8: |          | 269/? [06:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 269, loss 4.210623741149902\n",
      "Epoch 8: |          | 270/? [06:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 270, loss 3.7909324169158936\n",
      "Epoch 8: |          | 271/? [06:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 271, loss 4.060854911804199\n",
      "Epoch 8: |          | 272/? [06:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 272, loss 4.35988712310791\n",
      "Epoch 8: |          | 273/? [06:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 273, loss 3.5251858234405518\n",
      "Epoch 8: |          | 274/? [06:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 274, loss 4.387221813201904\n",
      "Epoch 8: |          | 275/? [06:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 275, loss 4.033908843994141\n",
      "Epoch 8: |          | 276/? [06:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 276, loss 3.2883517742156982\n",
      "Epoch 8: |          | 277/? [06:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 277, loss 3.9958128929138184\n",
      "Epoch 8: |          | 278/? [06:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 278, loss 3.0159170627593994\n",
      "Epoch 8: |          | 279/? [06:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 279, loss 3.7133820056915283\n",
      "Epoch 8: |          | 280/? [06:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 280, loss 3.430039644241333\n",
      "Epoch 8: |          | 281/? [06:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 281, loss 4.089056968688965\n",
      "Epoch 8: |          | 282/? [06:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 282, loss 3.6293857097625732\n",
      "Epoch 8: |          | 283/? [06:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 283, loss 3.761442184448242\n",
      "Epoch 8: |          | 284/? [06:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 284, loss 3.674870252609253\n",
      "Epoch 8: |          | 285/? [06:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 285, loss 3.1670796871185303\n",
      "Epoch 8: |          | 286/? [06:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 286, loss 3.745542049407959\n",
      "Epoch 8: |          | 287/? [06:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 287, loss 3.4820308685302734\n",
      "Epoch 8: |          | 288/? [06:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 288, loss 3.4159011840820312\n",
      "Epoch 8: |          | 289/? [06:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 289, loss 3.285222291946411\n",
      "Epoch 8: |          | 290/? [06:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 290, loss 2.851701259613037\n",
      "Epoch 8: |          | 291/? [06:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 291, loss 3.946446180343628\n",
      "Epoch 8: |          | 292/? [06:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 292, loss 3.608924150466919\n",
      "Epoch 8: |          | 293/? [06:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 293, loss 3.9103615283966064\n",
      "Epoch 8: |          | 294/? [06:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 294, loss 3.778515577316284\n",
      "Epoch 8: |          | 295/? [06:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 295, loss 4.0664849281311035\n",
      "Epoch 8: |          | 296/? [06:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 296, loss 3.557844877243042\n",
      "Epoch 8: |          | 297/? [06:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 297, loss 4.239697456359863\n",
      "Epoch 8: |          | 298/? [06:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 298, loss 4.033545970916748\n",
      "Epoch 8: |          | 299/? [07:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 299, loss 4.444635391235352\n",
      "Epoch 8: |          | 300/? [07:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 300, loss 3.8243651390075684\n",
      "Epoch 8: |          | 301/? [07:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 301, loss 3.6910483837127686\n",
      "Epoch 8: |          | 302/? [07:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 302, loss 4.082501411437988\n",
      "Epoch 8: |          | 303/? [07:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 303, loss 3.885463237762451\n",
      "Epoch 8: |          | 304/? [07:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 304, loss 4.061871528625488\n",
      "Epoch 8: |          | 305/? [07:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 305, loss 4.106986045837402\n",
      "Epoch 8: |          | 306/? [07:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 306, loss 3.782374858856201\n",
      "Epoch 8: |          | 307/? [07:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 307, loss 4.015683650970459\n",
      "Epoch 8: |          | 308/? [07:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 308, loss 4.186648368835449\n",
      "Epoch 8: |          | 309/? [07:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 309, loss 3.7950439453125\n",
      "Epoch 8: |          | 310/? [07:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 310, loss 4.1803436279296875\n",
      "Epoch 8: |          | 311/? [07:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 311, loss 3.815211772918701\n",
      "Epoch 8: |          | 312/? [07:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 312, loss 3.7915172576904297\n",
      "Epoch 8: |          | 313/? [07:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 313, loss 3.6509463787078857\n",
      "Epoch 8: |          | 314/? [07:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 314, loss 3.9778504371643066\n",
      "Epoch 8: |          | 315/? [07:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 315, loss 3.6214969158172607\n",
      "Epoch 8: |          | 316/? [07:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 316, loss 4.143615245819092\n",
      "Epoch 8: |          | 317/? [07:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 317, loss 3.9696762561798096\n",
      "Epoch 8: |          | 318/? [07:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 318, loss 4.1254472732543945\n",
      "Epoch 8: |          | 319/? [07:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 319, loss 3.3627593517303467\n",
      "Epoch 8: |          | 320/? [07:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 320, loss 3.8534233570098877\n",
      "Epoch 8: |          | 321/? [07:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 321, loss 3.656369686126709\n",
      "Epoch 8: |          | 322/? [07:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 322, loss 4.259625434875488\n",
      "Epoch 8: |          | 323/? [07:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 323, loss 4.2145867347717285\n",
      "Epoch 8: |          | 324/? [07:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 324, loss 3.9490790367126465\n",
      "Epoch 8: |          | 325/? [07:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 325, loss 4.326047897338867\n",
      "Epoch 8: |          | 326/? [07:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 326, loss 3.9641177654266357\n",
      "Epoch 8: |          | 327/? [07:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 327, loss 3.6200549602508545\n",
      "Epoch 8: |          | 328/? [07:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 328, loss 3.4189581871032715\n",
      "Epoch 8: |          | 329/? [07:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 329, loss 4.0301947593688965\n",
      "Epoch 8: |          | 330/? [07:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 330, loss 4.521488666534424\n",
      "Epoch 8: |          | 331/? [07:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 331, loss 2.8262085914611816\n",
      "Epoch 8: |          | 332/? [07:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 332, loss 3.8611931800842285\n",
      "Epoch 8: |          | 333/? [07:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 333, loss 3.7675704956054688\n",
      "Epoch 8: |          | 334/? [07:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 334, loss 4.394652366638184\n",
      "Epoch 8: |          | 335/? [07:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 335, loss 4.313430309295654\n",
      "Epoch 8: |          | 336/? [07:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 336, loss 4.255730628967285\n",
      "Epoch 8: |          | 337/? [07:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 337, loss 4.807622909545898\n",
      "Epoch 8: |          | 338/? [07:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 338, loss 4.542634010314941\n",
      "Epoch 8: |          | 339/? [07:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 339, loss 3.6223552227020264\n",
      "Epoch 8: |          | 340/? [07:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 340, loss 3.4229159355163574\n",
      "Epoch 8: |          | 341/? [07:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 341, loss 3.3508963584899902\n",
      "Epoch 8: |          | 342/? [08:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 342, loss 3.960308790206909\n",
      "Epoch 8: |          | 343/? [08:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 343, loss 3.6530025005340576\n",
      "Epoch 8: |          | 344/? [08:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 344, loss 4.502355575561523\n",
      "Epoch 8: |          | 345/? [08:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 345, loss 3.688924789428711\n",
      "Epoch 8: |          | 346/? [08:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 346, loss 3.9929280281066895\n",
      "Epoch 8: |          | 347/? [08:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 347, loss 3.7693817615509033\n",
      "Epoch 8: |          | 348/? [08:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 348, loss 3.164398670196533\n",
      "Epoch 8: |          | 349/? [08:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 349, loss 3.024329662322998\n",
      "Epoch 8: |          | 350/? [08:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 350, loss 4.208963394165039\n",
      "Epoch 8: |          | 351/? [08:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 351, loss 4.244354724884033\n",
      "Epoch 8: |          | 352/? [08:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 352, loss 3.528258800506592\n",
      "Epoch 8: |          | 353/? [08:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 353, loss 3.31901478767395\n",
      "Epoch 8: |          | 354/? [08:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 354, loss 3.7244536876678467\n",
      "Epoch 8: |          | 355/? [08:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 355, loss 4.032154560089111\n",
      "Epoch 8: |          | 356/? [08:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 356, loss 4.05483865737915\n",
      "Epoch 8: |          | 357/? [08:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 357, loss 3.5410068035125732\n",
      "Epoch 8: |          | 358/? [08:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 358, loss 3.501880645751953\n",
      "Epoch 8: |          | 359/? [08:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 359, loss 4.057326793670654\n",
      "Epoch 8: |          | 360/? [08:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 360, loss 3.6740200519561768\n",
      "Epoch 8: |          | 361/? [08:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 361, loss 3.801198959350586\n",
      "Epoch 8: |          | 362/? [08:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 362, loss 3.596511125564575\n",
      "Epoch 8: |          | 363/? [08:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 363, loss 3.5054283142089844\n",
      "Epoch 8: |          | 364/? [08:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 364, loss 4.081887245178223\n",
      "Epoch 8: |          | 365/? [08:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 365, loss 4.110577583312988\n",
      "Epoch 8: |          | 366/? [08:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 366, loss 3.9203193187713623\n",
      "Epoch 8: |          | 367/? [08:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 367, loss 3.8604493141174316\n",
      "Epoch 8: |          | 368/? [08:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 368, loss 3.461366653442383\n",
      "Epoch 8: |          | 369/? [08:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 369, loss 3.7407708168029785\n",
      "Epoch 8: |          | 370/? [08:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 370, loss 3.4273815155029297\n",
      "Epoch 8: |          | 371/? [08:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 371, loss 4.340020179748535\n",
      "Epoch 8: |          | 372/? [08:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 372, loss 3.681114912033081\n",
      "Epoch 8: |          | 373/? [08:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 373, loss 4.027297019958496\n",
      "Epoch 8: |          | 374/? [08:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 374, loss 3.7139294147491455\n",
      "Epoch 8: |          | 375/? [08:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 375, loss 4.354183197021484\n",
      "Epoch 8: |          | 376/? [08:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 376, loss 3.7728800773620605\n",
      "Epoch 8: |          | 377/? [08:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 377, loss 3.9406898021698\n",
      "Epoch 8: |          | 378/? [08:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 378, loss 4.099036693572998\n",
      "Epoch 8: |          | 379/? [08:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 379, loss 3.895231246948242\n",
      "Epoch 8: |          | 380/? [08:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 380, loss 3.9605841636657715\n",
      "Epoch 8: |          | 381/? [08:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 381, loss 4.031323432922363\n",
      "Epoch 8: |          | 382/? [08:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 382, loss 3.752729892730713\n",
      "Epoch 8: |          | 383/? [08:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 383, loss 3.8407540321350098\n",
      "Epoch 8: |          | 384/? [09:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 384, loss 4.247524738311768\n",
      "Epoch 8: |          | 385/? [09:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 385, loss 3.876687526702881\n",
      "Epoch 8: |          | 386/? [09:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 386, loss 2.8952934741973877\n",
      "Epoch 8: |          | 387/? [09:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 387, loss 3.6832001209259033\n",
      "Epoch 8: |          | 388/? [09:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 388, loss 3.474238157272339\n",
      "Epoch 8: |          | 389/? [09:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 389, loss 4.201260566711426\n",
      "Epoch 8: |          | 390/? [09:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 390, loss 3.671464204788208\n",
      "Epoch 8: |          | 391/? [09:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 391, loss 4.0462870597839355\n",
      "Epoch 8: |          | 392/? [09:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 392, loss 4.1823649406433105\n",
      "Epoch 8: |          | 393/? [09:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 393, loss 4.198445796966553\n",
      "Epoch 8: |          | 394/? [09:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 394, loss 3.8547656536102295\n",
      "Epoch 8: |          | 395/? [09:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 395, loss 4.057284355163574\n",
      "Epoch 8: |          | 396/? [09:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 396, loss 4.0027241706848145\n",
      "Epoch 8: |          | 397/? [09:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 397, loss 3.763441801071167\n",
      "Epoch 8: |          | 398/? [09:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 398, loss 3.7014641761779785\n",
      "Epoch 8: |          | 399/? [09:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 399, loss 3.7781643867492676\n",
      "Epoch 8: |          | 400/? [09:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 400, loss 3.74287486076355\n",
      "Epoch 8: |          | 401/? [09:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 401, loss 3.7055065631866455\n",
      "Epoch 8: |          | 402/? [09:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 402, loss 4.069118022918701\n",
      "Epoch 8: |          | 403/? [09:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 403, loss 3.9892685413360596\n",
      "Epoch 8: |          | 404/? [09:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 404, loss 3.524430751800537\n",
      "Epoch 8: |          | 405/? [09:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 405, loss 3.586449384689331\n",
      "Epoch 8: |          | 406/? [09:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 406, loss 3.897333860397339\n",
      "Epoch 8: |          | 407/? [09:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 407, loss 3.7941203117370605\n",
      "Epoch 8: |          | 408/? [09:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 408, loss 4.22543478012085\n",
      "Epoch 8: |          | 409/? [09:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 409, loss 4.144454479217529\n",
      "Epoch 8: |          | 410/? [09:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 410, loss 3.79760479927063\n",
      "Epoch 8: |          | 411/? [09:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 411, loss 3.718121290206909\n",
      "Epoch 8: |          | 412/? [09:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 412, loss 3.2932288646698\n",
      "Epoch 8: |          | 413/? [09:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 413, loss 3.9844977855682373\n",
      "Epoch 8: |          | 414/? [09:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 414, loss 3.6359024047851562\n",
      "Epoch 8: |          | 415/? [09:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 415, loss 4.029088020324707\n",
      "Epoch 8: |          | 416/? [09:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 416, loss 4.428135395050049\n",
      "Epoch 8: |          | 417/? [09:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 417, loss 4.420145034790039\n",
      "Epoch 8: |          | 418/? [09:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 418, loss 4.028487205505371\n",
      "Epoch 8: |          | 419/? [09:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 419, loss 3.7998547554016113\n",
      "Epoch 8: |          | 420/? [09:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 420, loss 3.9801185131073\n",
      "Epoch 8: |          | 421/? [09:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 421, loss 4.451249122619629\n",
      "Epoch 8: |          | 422/? [09:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 422, loss 4.001479625701904\n",
      "Epoch 8: |          | 423/? [09:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 423, loss 3.615539073944092\n",
      "Epoch 8: |          | 424/? [09:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 424, loss 4.144076824188232\n",
      "Epoch 8: |          | 425/? [09:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 425, loss 3.8549728393554688\n",
      "Epoch 8: |          | 426/? [09:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 426, loss 3.5979163646698\n",
      "Epoch 8: |          | 427/? [09:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 427, loss 3.694613218307495\n",
      "Epoch 8: |          | 428/? [10:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 428, loss 4.405002117156982\n",
      "Epoch 8: |          | 429/? [10:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 429, loss 3.3443474769592285\n",
      "Epoch 8: |          | 430/? [10:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 430, loss 3.966207504272461\n",
      "Epoch 8: |          | 431/? [10:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 431, loss 3.843964099884033\n",
      "Epoch 8: |          | 432/? [10:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 432, loss 3.9893264770507812\n",
      "Epoch 8: |          | 433/? [10:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 433, loss 3.9895553588867188\n",
      "Epoch 8: |          | 434/? [10:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 434, loss 3.8400397300720215\n",
      "Epoch 8: |          | 435/? [10:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 435, loss 3.5567824840545654\n",
      "Epoch 8: |          | 436/? [10:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 436, loss 3.9207656383514404\n",
      "Epoch 8: |          | 437/? [10:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 437, loss 4.122114658355713\n",
      "Epoch 8: |          | 438/? [10:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 438, loss 3.7531375885009766\n",
      "Epoch 8: |          | 439/? [10:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 439, loss 3.6215996742248535\n",
      "Epoch 8: |          | 440/? [10:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 440, loss 3.5539417266845703\n",
      "Epoch 8: |          | 441/? [10:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 441, loss 3.938267230987549\n",
      "Epoch 8: |          | 442/? [10:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 442, loss 3.74238657951355\n",
      "Epoch 8: |          | 443/? [10:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 443, loss 3.877295732498169\n",
      "Epoch 8: |          | 444/? [10:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 444, loss 3.88472056388855\n",
      "Epoch 8: |          | 445/? [10:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 445, loss 4.82781982421875\n",
      "Epoch 8: |          | 446/? [10:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 446, loss 3.862595796585083\n",
      "Epoch 8: |          | 447/? [10:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 447, loss 4.391748905181885\n",
      "Epoch 8: |          | 448/? [10:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 448, loss 3.43957257270813\n",
      "Epoch 8: |          | 449/? [10:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 449, loss 3.879868268966675\n",
      "Epoch 8: |          | 450/? [10:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 450, loss 4.199090003967285\n",
      "Epoch 8: |          | 451/? [10:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 451, loss 3.799907684326172\n",
      "Epoch 8: |          | 452/? [10:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 452, loss 3.5925590991973877\n",
      "Epoch 8: |          | 453/? [10:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 453, loss 4.298798561096191\n",
      "Epoch 8: |          | 454/? [10:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 454, loss 3.692356824874878\n",
      "Epoch 8: |          | 455/? [10:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 455, loss 4.033522129058838\n",
      "Epoch 8: |          | 456/? [10:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 456, loss 3.3619792461395264\n",
      "Epoch 8: |          | 457/? [10:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 457, loss 3.80427622795105\n",
      "Epoch 8: |          | 458/? [10:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 458, loss 4.259326934814453\n",
      "Epoch 8: |          | 459/? [10:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 459, loss 4.227476596832275\n",
      "Epoch 8: |          | 460/? [10:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 460, loss 3.9827945232391357\n",
      "Epoch 8: |          | 461/? [10:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 461, loss 3.9048354625701904\n",
      "Epoch 8: |          | 462/? [10:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 462, loss 3.9981460571289062\n",
      "Epoch 8: |          | 463/? [10:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 463, loss 3.8348701000213623\n",
      "Epoch 8: |          | 464/? [10:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 464, loss 3.3544375896453857\n",
      "Epoch 8: |          | 465/? [10:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 465, loss 3.6390414237976074\n",
      "Epoch 8: |          | 466/? [10:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 466, loss 4.116494178771973\n",
      "Epoch 8: |          | 467/? [10:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 467, loss 3.9184563159942627\n",
      "Epoch 8: |          | 468/? [10:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 468, loss 3.841845989227295\n",
      "Epoch 8: |          | 469/? [10:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 469, loss 3.9428577423095703\n",
      "Epoch 8: |          | 470/? [10:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 470, loss 3.379610776901245\n",
      "Epoch 8: |          | 471/? [10:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 471, loss 4.171535968780518\n",
      "Epoch 8: |          | 472/? [11:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 472, loss 3.649709701538086\n",
      "Epoch 8: |          | 473/? [11:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 473, loss 3.6536762714385986\n",
      "Epoch 8: |          | 474/? [11:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 474, loss 3.3426480293273926\n",
      "Epoch 8: |          | 475/? [11:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 475, loss 4.5611371994018555\n",
      "Epoch 8: |          | 476/? [11:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 476, loss 3.600299835205078\n",
      "Epoch 8: |          | 477/? [11:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 477, loss 3.1553807258605957\n",
      "Epoch 8: |          | 478/? [11:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 478, loss 3.3737761974334717\n",
      "Epoch 8: |          | 479/? [11:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 479, loss 3.806215286254883\n",
      "Epoch 8: |          | 480/? [11:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 480, loss 3.6792845726013184\n",
      "Epoch 8: |          | 481/? [11:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 481, loss 3.3242785930633545\n",
      "Epoch 8: |          | 482/? [11:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 482, loss 3.635758876800537\n",
      "Epoch 8: |          | 483/? [11:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 483, loss 3.296257495880127\n",
      "Epoch 8: |          | 484/? [11:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 484, loss 4.225842475891113\n",
      "Epoch 8: |          | 485/? [11:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 485, loss 4.140761375427246\n",
      "Epoch 8: |          | 486/? [11:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 486, loss 3.6981091499328613\n",
      "Epoch 8: |          | 487/? [11:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 487, loss 4.0159101486206055\n",
      "Epoch 8: |          | 488/? [11:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 488, loss 3.7427992820739746\n",
      "Epoch 8: |          | 489/? [11:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 489, loss 3.33442759513855\n",
      "Epoch 8: |          | 490/? [11:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 490, loss 3.915699005126953\n",
      "Epoch 8: |          | 491/? [11:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 491, loss 3.7901344299316406\n",
      "Epoch 8: |          | 492/? [11:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 492, loss 3.1408817768096924\n",
      "Epoch 8: |          | 493/? [11:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 493, loss 4.103801250457764\n",
      "Epoch 8: |          | 494/? [11:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 494, loss 3.887376070022583\n",
      "Epoch 8: |          | 495/? [11:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 495, loss 4.023935317993164\n",
      "Epoch 8: |          | 496/? [11:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 496, loss 3.6154136657714844\n",
      "Epoch 8: |          | 497/? [11:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 497, loss 4.197343349456787\n",
      "Epoch 8: |          | 498/? [11:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 498, loss 3.7996582984924316\n",
      "Epoch 8: |          | 499/? [11:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 499, loss 3.927835464477539\n",
      "Epoch 8: |          | 500/? [11:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 500, loss 3.701439619064331\n",
      "Epoch 8: |          | 501/? [11:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 501, loss 3.4818663597106934\n",
      "Epoch 8: |          | 502/? [11:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 502, loss 3.904503583908081\n",
      "Epoch 8: |          | 503/? [11:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 503, loss 3.8450839519500732\n",
      "Epoch 8: |          | 504/? [11:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 504, loss 3.7523396015167236\n",
      "Epoch 8: |          | 505/? [11:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 505, loss 3.2349929809570312\n",
      "Epoch 8: |          | 506/? [11:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 506, loss 3.8112435340881348\n",
      "Epoch 8: |          | 507/? [11:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 507, loss 3.8346378803253174\n",
      "Epoch 8: |          | 508/? [11:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 508, loss 4.220488548278809\n",
      "Epoch 8: |          | 509/? [11:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 509, loss 3.574122667312622\n",
      "Epoch 8: |          | 510/? [11:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 510, loss 4.0508503913879395\n",
      "Epoch 8: |          | 511/? [11:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 511, loss 3.8876919746398926\n",
      "Epoch 8: |          | 512/? [11:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 512, loss 3.304255723953247\n",
      "Epoch 8: |          | 513/? [11:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 513, loss 3.547473430633545\n",
      "Epoch 8: |          | 514/? [11:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 514, loss 3.733849048614502\n",
      "Epoch 8: |          | 515/? [12:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 515, loss 3.3756275177001953\n",
      "Epoch 8: |          | 516/? [12:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 516, loss 3.6483631134033203\n",
      "Epoch 8: |          | 517/? [12:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 517, loss 3.8993124961853027\n",
      "Epoch 8: |          | 518/? [12:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 518, loss 3.463061809539795\n",
      "Epoch 8: |          | 519/? [12:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 519, loss 3.9170143604278564\n",
      "Epoch 8: |          | 520/? [12:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 520, loss 3.7662672996520996\n",
      "Epoch 8: |          | 521/? [12:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 521, loss 3.7547831535339355\n",
      "Epoch 8: |          | 522/? [12:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 522, loss 4.284590244293213\n",
      "Epoch 8: |          | 523/? [12:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 523, loss 4.373055458068848\n",
      "Epoch 8: |          | 524/? [12:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 524, loss 4.144356727600098\n",
      "Epoch 8: |          | 525/? [12:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 525, loss 3.7451446056365967\n",
      "Epoch 8: |          | 526/? [12:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 526, loss 3.548226833343506\n",
      "Epoch 8: |          | 527/? [12:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 527, loss 4.216716766357422\n",
      "Epoch 8: |          | 528/? [12:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 528, loss 3.9431865215301514\n",
      "Epoch 8: |          | 529/? [12:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 529, loss 3.591423511505127\n",
      "Epoch 8: |          | 530/? [12:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 530, loss 4.12645959854126\n",
      "Epoch 8: |          | 531/? [12:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 531, loss 3.6478984355926514\n",
      "Epoch 8: |          | 532/? [12:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 532, loss 3.954097032546997\n",
      "Epoch 8: |          | 533/? [12:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 533, loss 3.5474114418029785\n",
      "Epoch 8: |          | 534/? [12:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 534, loss 3.260667324066162\n",
      "Epoch 8: |          | 535/? [12:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 535, loss 3.53692364692688\n",
      "Epoch 8: |          | 536/? [12:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 536, loss 4.193495750427246\n",
      "Epoch 8: |          | 537/? [12:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 537, loss 3.973423719406128\n",
      "Epoch 8: |          | 538/? [12:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 538, loss 3.6053333282470703\n",
      "Epoch 8: |          | 539/? [12:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 539, loss 3.7108397483825684\n",
      "Epoch 8: |          | 540/? [12:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 540, loss 4.0912652015686035\n",
      "Epoch 8: |          | 541/? [12:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 541, loss 3.8677520751953125\n",
      "Epoch 8: |          | 542/? [12:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 542, loss 3.632810115814209\n",
      "Epoch 8: |          | 543/? [12:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 543, loss 3.9971957206726074\n",
      "Epoch 8: |          | 544/? [12:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 544, loss 3.9420413970947266\n",
      "Epoch 8: |          | 545/? [12:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 545, loss 3.2147178649902344\n",
      "Epoch 8: |          | 546/? [12:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 546, loss 3.959163188934326\n",
      "Epoch 8: |          | 547/? [12:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 547, loss 4.450577735900879\n",
      "Epoch 8: |          | 548/? [12:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 548, loss 4.0590667724609375\n",
      "Epoch 8: |          | 549/? [12:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 549, loss 3.958305835723877\n",
      "Epoch 8: |          | 550/? [12:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 550, loss 4.280171871185303\n",
      "Epoch 8: |          | 551/? [12:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 551, loss 3.9468390941619873\n",
      "Epoch 8: |          | 552/? [12:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 552, loss 3.9010562896728516\n",
      "Epoch 8: |          | 553/? [12:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 553, loss 3.407909393310547\n",
      "Epoch 8: |          | 554/? [12:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 554, loss 3.9451651573181152\n",
      "Epoch 8: |          | 555/? [12:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 555, loss 4.258699417114258\n",
      "Epoch 8: |          | 556/? [12:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 556, loss 4.016480445861816\n",
      "Epoch 8: |          | 557/? [12:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 557, loss 3.5019965171813965\n",
      "Epoch 8: |          | 558/? [13:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 558, loss 3.7645931243896484\n",
      "Epoch 8: |          | 559/? [13:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 559, loss 3.7489490509033203\n",
      "Epoch 8: |          | 560/? [13:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 560, loss 3.235590696334839\n",
      "Epoch 8: |          | 561/? [13:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 561, loss 3.0254342555999756\n",
      "Epoch 8: |          | 562/? [13:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 562, loss 4.1518449783325195\n",
      "Epoch 8: |          | 563/? [13:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 563, loss 3.2329533100128174\n",
      "Epoch 8: |          | 564/? [13:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 564, loss 3.6664836406707764\n",
      "Epoch 8: |          | 565/? [13:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 565, loss 4.080099105834961\n",
      "Epoch 8: |          | 566/? [13:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 566, loss 4.1279802322387695\n",
      "Epoch 8: |          | 567/? [13:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 567, loss 4.234846115112305\n",
      "Epoch 8: |          | 568/? [13:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 568, loss 3.3275139331817627\n",
      "Epoch 8: |          | 569/? [13:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 569, loss 3.9513297080993652\n",
      "Epoch 8: |          | 570/? [13:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 570, loss 4.04688835144043\n",
      "Epoch 8: |          | 571/? [13:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 571, loss 3.6193184852600098\n",
      "Epoch 8: |          | 572/? [13:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 572, loss 4.547462463378906\n",
      "Epoch 8: |          | 573/? [13:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 573, loss 3.010956287384033\n",
      "Epoch 8: |          | 574/? [13:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 574, loss 4.126374244689941\n",
      "Epoch 8: |          | 575/? [13:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 575, loss 3.4768898487091064\n",
      "Epoch 8: |          | 576/? [13:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 576, loss 3.710131883621216\n",
      "Epoch 8: |          | 577/? [13:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 577, loss 3.913963794708252\n",
      "Epoch 8: |          | 578/? [13:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 578, loss 4.136237621307373\n",
      "Epoch 8: |          | 579/? [13:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 579, loss 3.295625686645508\n",
      "Epoch 8: |          | 580/? [13:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 580, loss 3.9346299171447754\n",
      "Epoch 8: |          | 581/? [13:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 581, loss 3.9649288654327393\n",
      "Epoch 8: |          | 582/? [13:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 582, loss 4.046723365783691\n",
      "Epoch 8: |          | 583/? [13:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 583, loss 3.7920737266540527\n",
      "Epoch 8: |          | 584/? [13:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 584, loss 4.017760276794434\n",
      "Epoch 8: |          | 585/? [13:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 585, loss 3.9847652912139893\n",
      "Epoch 8: |          | 586/? [13:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 586, loss 4.029601097106934\n",
      "Epoch 8: |          | 587/? [13:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 587, loss 4.064014434814453\n",
      "Epoch 8: |          | 588/? [13:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 588, loss 4.004873752593994\n",
      "Epoch 8: |          | 589/? [13:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 589, loss 3.4577553272247314\n",
      "Epoch 8: |          | 590/? [13:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 590, loss 4.019887447357178\n",
      "Epoch 8: |          | 591/? [13:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 591, loss 3.9342904090881348\n",
      "Epoch 8: |          | 592/? [13:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 592, loss 3.5979347229003906\n",
      "Epoch 8: |          | 593/? [13:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 593, loss 3.889263153076172\n",
      "Epoch 8: |          | 594/? [13:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 594, loss 4.659733295440674\n",
      "Epoch 8: |          | 595/? [13:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 595, loss 3.4438014030456543\n",
      "Epoch 8: |          | 596/? [13:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 596, loss 3.5053184032440186\n",
      "Epoch 8: |          | 597/? [13:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 597, loss 3.7081775665283203\n",
      "Epoch 8: |          | 598/? [13:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 598, loss 4.2117533683776855\n",
      "Epoch 8: |          | 599/? [13:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 599, loss 3.8231968879699707\n",
      "Epoch 8: |          | 600/? [13:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 600, loss 3.636979341506958\n",
      "Epoch 8: |          | 601/? [14:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 601, loss 3.9012627601623535\n",
      "Epoch 8: |          | 602/? [14:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 602, loss 3.4822731018066406\n",
      "Epoch 8: |          | 603/? [14:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 603, loss 3.583291530609131\n",
      "Epoch 8: |          | 604/? [14:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 604, loss 4.825695991516113\n",
      "Epoch 8: |          | 605/? [14:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 605, loss 3.3117783069610596\n",
      "Epoch 8: |          | 606/? [14:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 606, loss 3.621356964111328\n",
      "Epoch 8: |          | 607/? [14:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 607, loss 3.942779064178467\n",
      "Epoch 8: |          | 608/? [14:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 608, loss 3.730926990509033\n",
      "Epoch 8: |          | 609/? [14:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 609, loss 3.6337828636169434\n",
      "Epoch 8: |          | 610/? [14:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 610, loss 3.7179222106933594\n",
      "Epoch 8: |          | 611/? [14:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 611, loss 3.8709349632263184\n",
      "Epoch 8: |          | 612/? [14:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 612, loss 3.6168601512908936\n",
      "Epoch 8: |          | 613/? [14:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 613, loss 3.9119937419891357\n",
      "Epoch 8: |          | 614/? [14:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 614, loss 3.7509853839874268\n",
      "Epoch 8: |          | 615/? [14:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 615, loss 4.250429630279541\n",
      "Epoch 8: |          | 616/? [14:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 616, loss 4.384903907775879\n",
      "Epoch 8: |          | 617/? [14:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 617, loss 2.9599437713623047\n",
      "Epoch 8: |          | 618/? [14:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 618, loss 3.941087245941162\n",
      "Epoch 8: |          | 619/? [14:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 619, loss 3.476513385772705\n",
      "Epoch 8: |          | 620/? [14:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 620, loss 4.067191123962402\n",
      "Epoch 8: |          | 621/? [14:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 621, loss 3.522622585296631\n",
      "Epoch 8: |          | 622/? [14:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 622, loss 3.3203861713409424\n",
      "Epoch 8: |          | 623/? [14:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 623, loss 3.1062026023864746\n",
      "Epoch 8: |          | 624/? [14:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 624, loss 2.836454153060913\n",
      "Epoch 8: |          | 625/? [14:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 625, loss 4.307928562164307\n",
      "Epoch 8: |          | 626/? [14:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 626, loss 3.7829811573028564\n",
      "Epoch 8: |          | 627/? [14:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 627, loss 3.6808300018310547\n",
      "Epoch 8: |          | 628/? [14:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 628, loss 3.693242311477661\n",
      "Epoch 8: |          | 629/? [14:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 629, loss 4.061309814453125\n",
      "Epoch 8: |          | 630/? [14:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 630, loss 3.832474946975708\n",
      "Epoch 8: |          | 631/? [14:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 631, loss 3.9439117908477783\n",
      "Epoch 8: |          | 632/? [14:43<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 632, loss 3.3003439903259277\n",
      "Epoch 8: |          | 633/? [14:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 633, loss 4.0428338050842285\n",
      "Epoch 8: |          | 634/? [14:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 634, loss 3.605253219604492\n",
      "Epoch 8: |          | 635/? [14:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 635, loss 3.428420305252075\n",
      "Epoch 8: |          | 636/? [14:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 636, loss 3.8484675884246826\n",
      "Epoch 8: |          | 637/? [14:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 637, loss 3.6245548725128174\n",
      "Epoch 8: |          | 638/? [14:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 638, loss 3.8680999279022217\n",
      "Epoch 8: |          | 639/? [14:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 639, loss 3.640164852142334\n",
      "Epoch 8: |          | 640/? [14:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 640, loss 4.202887535095215\n",
      "Epoch 8: |          | 641/? [14:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 641, loss 3.224766492843628\n",
      "Epoch 8: |          | 642/? [14:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 642, loss 3.9821372032165527\n",
      "Epoch 8: |          | 643/? [14:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 643, loss 3.8865952491760254\n",
      "Epoch 8: |          | 644/? [15:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 644, loss 3.874695301055908\n",
      "Epoch 8: |          | 645/? [15:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 645, loss 3.5892558097839355\n",
      "Epoch 8: |          | 646/? [15:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 646, loss 3.6468989849090576\n",
      "Epoch 8: |          | 647/? [15:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 647, loss 4.1587419509887695\n",
      "Epoch 8: |          | 648/? [15:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 648, loss 3.5895800590515137\n",
      "Epoch 8: |          | 649/? [15:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 649, loss 3.021662712097168\n",
      "Epoch 8: |          | 650/? [15:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 650, loss 4.124777793884277\n",
      "Epoch 8: |          | 651/? [15:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 651, loss 4.222705841064453\n",
      "Epoch 8: |          | 652/? [15:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 652, loss 3.690187931060791\n",
      "Epoch 8: |          | 653/? [15:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 653, loss 3.84053111076355\n",
      "Epoch 8: |          | 654/? [15:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 654, loss 3.896801710128784\n",
      "Epoch 8: |          | 655/? [15:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 655, loss 3.7753093242645264\n",
      "Epoch 8: |          | 656/? [15:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 656, loss 3.404963970184326\n",
      "Epoch 8: |          | 657/? [15:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 657, loss 5.715201377868652\n",
      "Epoch 8: |          | 658/? [15:19<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 658, loss 3.313704252243042\n",
      "Epoch 8: |          | 659/? [15:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 659, loss 3.832195997238159\n",
      "Epoch 8: |          | 660/? [15:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 660, loss 4.140110015869141\n",
      "Epoch 8: |          | 661/? [15:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 661, loss 4.059934139251709\n",
      "Epoch 8: |          | 662/? [15:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 662, loss 3.958110809326172\n",
      "Epoch 8: |          | 663/? [15:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 663, loss 3.7275824546813965\n",
      "Epoch 8: |          | 664/? [15:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 664, loss 3.6610026359558105\n",
      "Epoch 8: |          | 665/? [15:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 665, loss 3.948726177215576\n",
      "Epoch 8: |          | 666/? [15:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 666, loss 3.7437806129455566\n",
      "Epoch 8: |          | 667/? [15:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 667, loss 4.577842712402344\n",
      "Epoch 8: |          | 668/? [15:33<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 668, loss 3.3602962493896484\n",
      "Epoch 8: |          | 669/? [15:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 669, loss 3.5825295448303223\n",
      "Epoch 8: |          | 670/? [15:36<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 670, loss 4.2429399490356445\n",
      "Epoch 8: |          | 671/? [15:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 671, loss 3.9796319007873535\n",
      "Epoch 8: |          | 672/? [15:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 672, loss 3.9893577098846436\n",
      "Epoch 8: |          | 673/? [15:40<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 673, loss 3.8420021533966064\n",
      "Epoch 8: |          | 674/? [15:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 674, loss 2.276794910430908\n",
      "Epoch 8: |          | 675/? [15:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 675, loss 0.8489076495170593\n",
      "Epoch 8: |          | 676/? [15:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 676, loss 0.7097657322883606\n",
      "Epoch 8: |          | 677/? [15:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 677, loss 0.610066294670105\n",
      "Epoch 8: |          | 678/? [15:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 678, loss 1.7233827114105225\n",
      "Epoch 8: |          | 679/? [15:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 679, loss 3.2698845863342285\n",
      "Epoch 8: |          | 680/? [15:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 680, loss 3.7362632751464844\n",
      "Epoch 8: |          | 681/? [15:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 681, loss 3.31646990776062\n",
      "Epoch 8: |          | 682/? [15:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 682, loss 3.5861854553222656\n",
      "Epoch 8: |          | 683/? [15:53<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 683, loss 3.317056179046631\n",
      "Epoch 8: |          | 684/? [15:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 684, loss 4.330843925476074\n",
      "Epoch 8: |          | 685/? [15:56<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 685, loss 3.962817430496216\n",
      "Epoch 8: |          | 686/? [15:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 686, loss 3.5818111896514893\n",
      "Epoch 8: |          | 687/? [15:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 687, loss 4.041662216186523\n",
      "Epoch 8: |          | 688/? [16:00<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 688, loss 3.598381757736206\n",
      "Epoch 8: |          | 689/? [16:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 689, loss 3.657395124435425\n",
      "Epoch 8: |          | 690/? [16:03<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 690, loss 4.332063674926758\n",
      "Epoch 8: |          | 691/? [16:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 691, loss 3.804511308670044\n",
      "Epoch 8: |          | 692/? [16:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 692, loss 3.817223072052002\n",
      "Epoch 8: |          | 693/? [16:07<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 693, loss 4.362402439117432\n",
      "Epoch 8: |          | 694/? [16:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 694, loss 3.7084929943084717\n",
      "Epoch 8: |          | 695/? [16:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 695, loss 4.242011070251465\n",
      "Epoch 8: |          | 696/? [16:11<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 696, loss 3.549896240234375\n",
      "Epoch 8: |          | 697/? [16:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 697, loss 3.7437663078308105\n",
      "Epoch 8: |          | 698/? [16:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 698, loss 3.1826419830322266\n",
      "Epoch 8: |          | 699/? [16:15<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 699, loss 3.921729326248169\n",
      "Epoch 8: |          | 700/? [16:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 700, loss 3.990225315093994\n",
      "Epoch 8: |          | 701/? [16:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 701, loss 3.6271471977233887\n",
      "Epoch 8: |          | 702/? [16:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 702, loss 3.895555019378662\n",
      "Epoch 8: |          | 703/? [16:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 703, loss 4.048986434936523\n",
      "Epoch 8: |          | 704/? [16:22<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 704, loss 3.8917808532714844\n",
      "Epoch 8: |          | 705/? [16:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 705, loss 3.527101993560791\n",
      "Epoch 8: |          | 706/? [16:25<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 706, loss 3.5739383697509766\n",
      "Epoch 8: |          | 707/? [16:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 707, loss 4.05556583404541\n",
      "Epoch 8: |          | 708/? [16:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 708, loss 3.796541690826416\n",
      "Epoch 8: |          | 709/? [16:29<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 709, loss 3.7214064598083496\n",
      "Epoch 8: |          | 710/? [16:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 710, loss 4.263500213623047\n",
      "Epoch 8: |          | 711/? [16:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 711, loss 4.3396525382995605\n",
      "Epoch 8: |          | 712/? [16:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 712, loss 4.014003276824951\n",
      "Epoch 8: |          | 713/? [16:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 713, loss 4.111472129821777\n",
      "Epoch 8: |          | 714/? [16:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 714, loss 4.167733192443848\n",
      "Epoch 8: |          | 715/? [16:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 715, loss 3.1544716358184814\n",
      "Epoch 8: |          | 716/? [16:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 716, loss 3.8474090099334717\n",
      "Epoch 8: |          | 717/? [16:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 717, loss 3.7202811241149902\n",
      "Epoch 8: |          | 718/? [16:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 718, loss 3.2589869499206543\n",
      "Epoch 8: |          | 719/? [16:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 719, loss 3.775390625\n",
      "Epoch 8: |          | 720/? [16:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 720, loss 3.4367241859436035\n",
      "Epoch 8: |          | 721/? [16:47<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 721, loss 4.123197555541992\n",
      "Epoch 8: |          | 722/? [16:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 722, loss 3.4560208320617676\n",
      "Epoch 8: |          | 723/? [16:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 723, loss 4.003362655639648\n",
      "Epoch 8: |          | 724/? [16:51<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 724, loss 3.5493462085723877\n",
      "Epoch 8: |          | 725/? [16:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 725, loss 3.510303020477295\n",
      "Epoch 8: |          | 726/? [16:54<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 726, loss 3.6697654724121094\n",
      "Epoch 8: |          | 727/? [16:55<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 727, loss 3.4783318042755127\n",
      "Epoch 8: |          | 728/? [16:57<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 728, loss 3.2888431549072266\n",
      "Epoch 8: |          | 729/? [16:58<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 729, loss 3.7914786338806152\n",
      "Epoch 8: |          | 730/? [16:59<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 730, loss 3.731660842895508\n",
      "Epoch 8: |          | 731/? [17:01<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 731, loss 3.846277952194214\n",
      "Epoch 8: |          | 732/? [17:02<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 732, loss 4.117064952850342\n",
      "Epoch 8: |          | 733/? [17:04<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 733, loss 3.8420066833496094\n",
      "Epoch 8: |          | 734/? [17:05<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 734, loss 4.014860153198242\n",
      "Epoch 8: |          | 735/? [17:06<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 735, loss 3.92335844039917\n",
      "Epoch 8: |          | 736/? [17:08<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 736, loss 3.4591708183288574\n",
      "Epoch 8: |          | 737/? [17:09<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 737, loss 4.234210968017578\n",
      "Epoch 8: |          | 738/? [17:10<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 738, loss 3.4234282970428467\n",
      "Epoch 8: |          | 739/? [17:12<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 739, loss 3.908939838409424\n",
      "Epoch 8: |          | 740/? [17:13<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 740, loss 3.6161327362060547\n",
      "Epoch 8: |          | 741/? [17:14<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 741, loss 3.7556991577148438\n",
      "Epoch 8: |          | 742/? [17:16<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 742, loss 4.163253307342529\n",
      "Epoch 8: |          | 743/? [17:17<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 743, loss 3.9981014728546143\n",
      "Epoch 8: |          | 744/? [17:18<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 744, loss 3.960233211517334\n",
      "Epoch 8: |          | 745/? [17:20<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 745, loss 3.6007087230682373\n",
      "Epoch 8: |          | 746/? [17:21<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 746, loss 3.855208158493042\n",
      "Epoch 8: |          | 747/? [17:23<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 747, loss 3.5625202655792236\n",
      "Epoch 8: |          | 748/? [17:24<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 748, loss 2.5991690158843994\n",
      "Epoch 8: |          | 749/? [17:26<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 749, loss 3.7637741565704346\n",
      "Epoch 8: |          | 750/? [17:27<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 750, loss 4.012063026428223\n",
      "Epoch 8: |          | 751/? [17:28<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 751, loss 2.322417736053467\n",
      "Epoch 8: |          | 752/? [17:30<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 752, loss 3.9224495887756348\n",
      "Epoch 8: |          | 753/? [17:31<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 753, loss 3.118957042694092\n",
      "Epoch 8: |          | 754/? [17:32<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 754, loss 3.5589499473571777\n",
      "Epoch 8: |          | 755/? [17:34<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 755, loss 3.3778910636901855\n",
      "Epoch 8: |          | 756/? [17:35<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 756, loss 3.7877070903778076\n",
      "Epoch 8: |          | 757/? [17:37<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 757, loss 3.8387608528137207\n",
      "Epoch 8: |          | 758/? [17:38<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 758, loss 3.5835201740264893\n",
      "Epoch 8: |          | 759/? [17:39<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 759, loss 3.5317680835723877\n",
      "Epoch 8: |          | 760/? [17:41<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 760, loss 4.041800498962402\n",
      "Epoch 8: |          | 761/? [17:42<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 761, loss 4.049807548522949\n",
      "Epoch 8: |          | 762/? [17:44<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 762, loss 3.7174782752990723\n",
      "Epoch 8: |          | 763/? [17:45<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 763, loss 3.8513526916503906\n",
      "Epoch 8: |          | 764/? [17:46<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 764, loss 4.121616363525391\n",
      "Epoch 8: |          | 765/? [17:48<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 765, loss 3.877358913421631\n",
      "Epoch 8: |          | 766/? [17:49<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 766, loss 4.278653144836426\n",
      "Epoch 8: |          | 767/? [17:50<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 767, loss 4.309478759765625\n",
      "Epoch 8: |          | 768/? [17:52<00:00,  0.72it/s, v_num=31]   TRRAINING: Batch 768, loss 3.8835837841033936\n",
      "Epoch 8: |          | 769/? [17:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 769, loss 3.1211211681365967\n",
      "Epoch 8: |          | 770/? [17:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 770, loss 3.64268159866333\n",
      "Epoch 8: |          | 771/? [18:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 771, loss 4.372257232666016\n",
      "Epoch 8: |          | 772/? [18:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 772, loss 4.075281620025635\n",
      "Epoch 8: |          | 773/? [18:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 773, loss 3.7942256927490234\n",
      "Epoch 8: |          | 774/? [18:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 774, loss 3.9034786224365234\n",
      "Epoch 8: |          | 775/? [18:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 775, loss 4.348742961883545\n",
      "Epoch 8: |          | 776/? [18:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 776, loss 3.76408052444458\n",
      "Epoch 8: |          | 777/? [18:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 777, loss 3.6684794425964355\n",
      "Epoch 8: |          | 778/? [18:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 778, loss 4.0528764724731445\n",
      "Epoch 8: |          | 779/? [18:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 779, loss 4.495860576629639\n",
      "Epoch 8: |          | 780/? [18:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 780, loss 3.5124244689941406\n",
      "Epoch 8: |          | 781/? [18:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 781, loss 3.603597640991211\n",
      "Epoch 8: |          | 782/? [18:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 782, loss 3.9777534008026123\n",
      "Epoch 8: |          | 783/? [18:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 783, loss 4.056305885314941\n",
      "Epoch 8: |          | 784/? [18:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 784, loss 3.6404292583465576\n",
      "Epoch 8: |          | 785/? [18:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 785, loss 3.413837432861328\n",
      "Epoch 8: |          | 786/? [18:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 786, loss 4.269802093505859\n",
      "Epoch 8: |          | 787/? [18:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 787, loss 4.206203460693359\n",
      "Epoch 8: |          | 788/? [18:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 788, loss 2.0567431449890137\n",
      "Epoch 8: |          | 789/? [18:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 789, loss 3.737028121948242\n",
      "Epoch 8: |          | 790/? [18:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 790, loss 4.542448043823242\n",
      "Epoch 8: |          | 791/? [18:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 791, loss 4.277379035949707\n",
      "Epoch 8: |          | 792/? [18:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 792, loss 3.428734540939331\n",
      "Epoch 8: |          | 793/? [18:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 793, loss 3.9699878692626953\n",
      "Epoch 8: |          | 794/? [18:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 794, loss 4.281917572021484\n",
      "Epoch 8: |          | 795/? [18:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 795, loss 3.7708542346954346\n",
      "Epoch 8: |          | 796/? [18:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 796, loss 4.141055583953857\n",
      "Epoch 8: |          | 797/? [18:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 797, loss 3.1637425422668457\n",
      "Epoch 8: |          | 798/? [18:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 798, loss 3.263657808303833\n",
      "Epoch 8: |          | 799/? [18:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 799, loss 4.210559844970703\n",
      "Epoch 8: |          | 800/? [18:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 800, loss 3.9928736686706543\n",
      "Epoch 8: |          | 801/? [18:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 801, loss 3.6278738975524902\n",
      "Epoch 8: |          | 802/? [18:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 802, loss 3.8993983268737793\n",
      "Epoch 8: |          | 803/? [18:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 803, loss 3.73364520072937\n",
      "Epoch 8: |          | 804/? [18:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 804, loss 3.9072628021240234\n",
      "Epoch 8: |          | 805/? [18:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 805, loss 4.0362043380737305\n",
      "Epoch 8: |          | 806/? [18:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 806, loss 4.477209568023682\n",
      "Epoch 8: |          | 807/? [18:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 807, loss 3.813159942626953\n",
      "Epoch 8: |          | 808/? [18:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 808, loss 3.4981181621551514\n",
      "Epoch 8: |          | 809/? [18:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 809, loss 4.023089408874512\n",
      "Epoch 8: |          | 810/? [18:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 810, loss 3.792255401611328\n",
      "Epoch 8: |          | 811/? [18:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 811, loss 4.075342655181885\n",
      "Epoch 8: |          | 812/? [18:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 812, loss 4.639151096343994\n",
      "Epoch 8: |          | 813/? [18:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 813, loss 4.459946632385254\n",
      "Epoch 8: |          | 814/? [19:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 814, loss 3.48136568069458\n",
      "Epoch 8: |          | 815/? [19:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 815, loss 4.179898262023926\n",
      "Epoch 8: |          | 816/? [19:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 816, loss 3.9903507232666016\n",
      "Epoch 8: |          | 817/? [19:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 817, loss 3.3432021141052246\n",
      "Epoch 8: |          | 818/? [19:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 818, loss 4.3119916915893555\n",
      "Epoch 8: |          | 819/? [19:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 819, loss 3.9847609996795654\n",
      "Epoch 8: |          | 820/? [19:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 820, loss 3.850661039352417\n",
      "Epoch 8: |          | 821/? [19:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 821, loss 3.8156871795654297\n",
      "Epoch 8: |          | 822/? [19:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 822, loss 3.494464874267578\n",
      "Epoch 8: |          | 823/? [19:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 823, loss 3.5114455223083496\n",
      "Epoch 8: |          | 824/? [19:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 824, loss 3.9747633934020996\n",
      "Epoch 8: |          | 825/? [19:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 825, loss 3.5041065216064453\n",
      "Epoch 8: |          | 826/? [19:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 826, loss 4.048392295837402\n",
      "Epoch 8: |          | 827/? [19:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 827, loss 3.651360034942627\n",
      "Epoch 8: |          | 828/? [19:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 828, loss 4.193714141845703\n",
      "Epoch 8: |          | 829/? [19:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 829, loss 3.8516628742218018\n",
      "Epoch 8: |          | 830/? [19:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 830, loss 4.389780044555664\n",
      "Epoch 8: |          | 831/? [19:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 831, loss 2.334998607635498\n",
      "Epoch 8: |          | 832/? [19:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 832, loss 3.8304131031036377\n",
      "Epoch 8: |          | 833/? [19:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 833, loss 3.710923671722412\n",
      "Epoch 8: |          | 834/? [19:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 834, loss 4.445679664611816\n",
      "Epoch 8: |          | 835/? [19:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 835, loss 3.7787113189697266\n",
      "Epoch 8: |          | 836/? [19:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 836, loss 4.472060680389404\n",
      "Epoch 8: |          | 837/? [19:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 837, loss 3.895481824874878\n",
      "Epoch 8: |          | 838/? [19:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 838, loss 3.220412492752075\n",
      "Epoch 8: |          | 839/? [19:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 839, loss 3.6004672050476074\n",
      "Epoch 8: |          | 840/? [19:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 840, loss 4.1457390785217285\n",
      "Epoch 8: |          | 841/? [19:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 841, loss 4.164220333099365\n",
      "Epoch 8: |          | 842/? [19:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 842, loss 3.8394081592559814\n",
      "Epoch 8: |          | 843/? [19:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 843, loss 4.148861885070801\n",
      "Epoch 8: |          | 844/? [19:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 844, loss 3.5248656272888184\n",
      "Epoch 8: |          | 845/? [19:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 845, loss 3.944272518157959\n",
      "Epoch 8: |          | 846/? [19:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 846, loss 4.365370750427246\n",
      "Epoch 8: |          | 847/? [19:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 847, loss 3.947092056274414\n",
      "Epoch 8: |          | 848/? [19:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 848, loss 3.5253005027770996\n",
      "Epoch 8: |          | 849/? [19:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 849, loss 3.601222515106201\n",
      "Epoch 8: |          | 850/? [19:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 850, loss 3.719740629196167\n",
      "Epoch 8: |          | 851/? [19:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 851, loss 3.997957944869995\n",
      "Epoch 8: |          | 852/? [19:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 852, loss 4.147526264190674\n",
      "Epoch 8: |          | 853/? [19:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 853, loss 3.965222120285034\n",
      "Epoch 8: |          | 854/? [19:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 854, loss 3.3023719787597656\n",
      "Epoch 8: |          | 855/? [19:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 855, loss 3.5874290466308594\n",
      "Epoch 8: |          | 856/? [20:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 856, loss 3.516319990158081\n",
      "Epoch 8: |          | 857/? [20:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 857, loss 4.038062572479248\n",
      "Epoch 8: |          | 858/? [20:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 858, loss 3.9631729125976562\n",
      "Epoch 8: |          | 859/? [20:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 859, loss 3.9370365142822266\n",
      "Epoch 8: |          | 860/? [20:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 860, loss 4.330529689788818\n",
      "Epoch 8: |          | 861/? [20:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 861, loss 3.649898052215576\n",
      "Epoch 8: |          | 862/? [20:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 862, loss 3.9518725872039795\n",
      "Epoch 8: |          | 863/? [20:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 863, loss 3.382455825805664\n",
      "Epoch 8: |          | 864/? [20:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 864, loss 3.9006340503692627\n",
      "Epoch 8: |          | 865/? [20:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 865, loss 3.8602232933044434\n",
      "Epoch 8: |          | 866/? [20:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 866, loss 2.9365077018737793\n",
      "Epoch 8: |          | 867/? [20:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 867, loss 3.1046335697174072\n",
      "Epoch 8: |          | 868/? [20:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 868, loss 3.9889755249023438\n",
      "Epoch 8: |          | 869/? [20:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 869, loss 4.044280052185059\n",
      "Epoch 8: |          | 870/? [20:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 870, loss 3.6397063732147217\n",
      "Epoch 8: |          | 871/? [20:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 871, loss 3.9602208137512207\n",
      "Epoch 8: |          | 872/? [20:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 872, loss 3.7947487831115723\n",
      "Epoch 8: |          | 873/? [20:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 873, loss 3.806699275970459\n",
      "Epoch 8: |          | 874/? [20:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 874, loss 3.296062469482422\n",
      "Epoch 8: |          | 875/? [20:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 875, loss 4.041664123535156\n",
      "Epoch 8: |          | 876/? [20:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 876, loss 3.567075252532959\n",
      "Epoch 8: |          | 877/? [20:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 877, loss 3.9598910808563232\n",
      "Epoch 8: |          | 878/? [20:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 878, loss 3.389066219329834\n",
      "Epoch 8: |          | 879/? [20:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 879, loss 3.456960678100586\n",
      "Epoch 8: |          | 880/? [20:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 880, loss 4.5201897621154785\n",
      "Epoch 8: |          | 881/? [20:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 881, loss 3.9579761028289795\n",
      "Epoch 8: |          | 882/? [20:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 882, loss 3.7433276176452637\n",
      "Epoch 8: |          | 883/? [20:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 883, loss 3.8617172241210938\n",
      "Epoch 8: |          | 884/? [20:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 884, loss 3.914019823074341\n",
      "Epoch 8: |          | 885/? [20:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 885, loss 3.698246717453003\n",
      "Epoch 8: |          | 886/? [20:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 886, loss 4.333856582641602\n",
      "Epoch 8: |          | 887/? [20:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 887, loss 4.354034900665283\n",
      "Epoch 8: |          | 888/? [20:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 888, loss 4.057305812835693\n",
      "Epoch 8: |          | 889/? [20:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 889, loss 3.5932514667510986\n",
      "Epoch 8: |          | 890/? [20:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 890, loss 3.848304271697998\n",
      "Epoch 8: |          | 891/? [20:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 891, loss 3.6753063201904297\n",
      "Epoch 8: |          | 892/? [20:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 892, loss 4.2621307373046875\n",
      "Epoch 8: |          | 893/? [20:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 893, loss 3.739532470703125\n",
      "Epoch 8: |          | 894/? [20:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 894, loss 3.2543067932128906\n",
      "Epoch 8: |          | 895/? [20:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 895, loss 4.371659278869629\n",
      "Epoch 8: |          | 896/? [20:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 896, loss 3.9684901237487793\n",
      "Epoch 8: |          | 897/? [20:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 897, loss 3.9791693687438965\n",
      "Epoch 8: |          | 898/? [20:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 898, loss 3.9482669830322266\n",
      "Epoch 8: |          | 899/? [20:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 899, loss 3.7300639152526855\n",
      "Epoch 8: |          | 900/? [21:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 900, loss 3.639836549758911\n",
      "Epoch 8: |          | 901/? [21:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 901, loss 4.054388523101807\n",
      "Epoch 8: |          | 902/? [21:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 902, loss 4.177666664123535\n",
      "Epoch 8: |          | 903/? [21:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 903, loss 3.471829891204834\n",
      "Epoch 8: |          | 904/? [21:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 904, loss 3.943232774734497\n",
      "Epoch 8: |          | 905/? [21:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 905, loss 4.14474630355835\n",
      "Epoch 8: |          | 906/? [21:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 906, loss 3.8912246227264404\n",
      "Epoch 8: |          | 907/? [21:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 907, loss 3.931096315383911\n",
      "Epoch 8: |          | 908/? [21:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 908, loss 3.996673107147217\n",
      "Epoch 8: |          | 909/? [21:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 909, loss 3.9836742877960205\n",
      "Epoch 8: |          | 910/? [21:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 910, loss 3.7491042613983154\n",
      "Epoch 8: |          | 911/? [21:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 911, loss 3.8524863719940186\n",
      "Epoch 8: |          | 912/? [21:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 912, loss 3.787114381790161\n",
      "Epoch 8: |          | 913/? [21:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 913, loss 3.795154571533203\n",
      "Epoch 8: |          | 914/? [21:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 914, loss 4.080158710479736\n",
      "Epoch 8: |          | 915/? [21:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 915, loss 3.8750922679901123\n",
      "Epoch 8: |          | 916/? [21:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 916, loss 3.8448586463928223\n",
      "Epoch 8: |          | 917/? [21:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 917, loss 3.7467193603515625\n",
      "Epoch 8: |          | 918/? [21:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 918, loss 3.7223281860351562\n",
      "Epoch 8: |          | 919/? [21:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 919, loss 3.714672803878784\n",
      "Epoch 8: |          | 920/? [21:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 920, loss 3.86161732673645\n",
      "Epoch 8: |          | 921/? [21:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 921, loss 3.6845924854278564\n",
      "Epoch 8: |          | 922/? [21:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 922, loss 3.8526368141174316\n",
      "Epoch 8: |          | 923/? [21:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 923, loss 3.7121500968933105\n",
      "Epoch 8: |          | 924/? [21:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 924, loss 3.67346453666687\n",
      "Epoch 8: |          | 925/? [21:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 925, loss 4.017277717590332\n",
      "Epoch 8: |          | 926/? [21:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 926, loss 3.7889404296875\n",
      "Epoch 8: |          | 927/? [21:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 927, loss 4.076343536376953\n",
      "Epoch 8: |          | 928/? [21:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 928, loss 3.5617148876190186\n",
      "Epoch 8: |          | 929/? [21:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 929, loss 3.676792860031128\n",
      "Epoch 8: |          | 930/? [21:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 930, loss 3.571903944015503\n",
      "Epoch 8: |          | 931/? [21:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 931, loss 3.304306745529175\n",
      "Epoch 8: |          | 932/? [21:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 932, loss 3.9110031127929688\n",
      "Epoch 8: |          | 933/? [21:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 933, loss 3.637542724609375\n",
      "Epoch 8: |          | 934/? [21:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 934, loss 4.235743522644043\n",
      "Epoch 8: |          | 935/? [21:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 935, loss 4.596458435058594\n",
      "Epoch 8: |          | 936/? [21:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 936, loss 3.77288818359375\n",
      "Epoch 8: |          | 937/? [21:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 937, loss 3.703554630279541\n",
      "Epoch 8: |          | 938/? [21:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 938, loss 3.7385497093200684\n",
      "Epoch 8: |          | 939/? [21:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 939, loss 3.9706616401672363\n",
      "Epoch 8: |          | 940/? [21:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 940, loss 4.152997016906738\n",
      "Epoch 8: |          | 941/? [21:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 941, loss 3.720012664794922\n",
      "Epoch 8: |          | 942/? [21:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 942, loss 3.2075939178466797\n",
      "Epoch 8: |          | 943/? [22:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 943, loss 4.075816631317139\n",
      "Epoch 8: |          | 944/? [22:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 944, loss 3.1264257431030273\n",
      "Epoch 8: |          | 945/? [22:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 945, loss 3.8644633293151855\n",
      "Epoch 8: |          | 946/? [22:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 946, loss 3.752490282058716\n",
      "Epoch 8: |          | 947/? [22:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 947, loss 3.684860944747925\n",
      "Epoch 8: |          | 948/? [22:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 948, loss 3.9478917121887207\n",
      "Epoch 8: |          | 949/? [22:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 949, loss 3.783950090408325\n",
      "Epoch 8: |          | 950/? [22:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 950, loss 3.5695300102233887\n",
      "Epoch 8: |          | 951/? [22:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 951, loss 4.204498291015625\n",
      "Epoch 8: |          | 952/? [22:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 952, loss 4.1771464347839355\n",
      "Epoch 8: |          | 953/? [22:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 953, loss 4.693832874298096\n",
      "Epoch 8: |          | 954/? [22:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 954, loss 3.677062511444092\n",
      "Epoch 8: |          | 955/? [22:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 955, loss 4.299006938934326\n",
      "Epoch 8: |          | 956/? [22:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 956, loss 3.7764830589294434\n",
      "Epoch 8: |          | 957/? [22:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 957, loss 3.9515063762664795\n",
      "Epoch 8: |          | 958/? [22:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 958, loss 3.984732151031494\n",
      "Epoch 8: |          | 959/? [22:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 959, loss 3.5066585540771484\n",
      "Epoch 8: |          | 960/? [22:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 960, loss 4.083081245422363\n",
      "Epoch 8: |          | 961/? [22:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 961, loss 4.349627494812012\n",
      "Epoch 8: |          | 962/? [22:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 962, loss 3.8466274738311768\n",
      "Epoch 8: |          | 963/? [22:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 963, loss 3.64323353767395\n",
      "Epoch 8: |          | 964/? [22:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 964, loss 4.101626873016357\n",
      "Epoch 8: |          | 965/? [22:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 965, loss 3.5610156059265137\n",
      "Epoch 8: |          | 966/? [22:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 966, loss 3.450904130935669\n",
      "Epoch 8: |          | 967/? [22:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 967, loss 3.6909823417663574\n",
      "Epoch 8: |          | 968/? [22:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 968, loss 3.6280388832092285\n",
      "Epoch 8: |          | 969/? [22:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 969, loss 3.5304362773895264\n",
      "Epoch 8: |          | 970/? [22:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 970, loss 3.9798896312713623\n",
      "Epoch 8: |          | 971/? [22:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 971, loss 4.182136535644531\n",
      "Epoch 8: |          | 972/? [22:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 972, loss 3.6541543006896973\n",
      "Epoch 8: |          | 973/? [22:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 973, loss 3.7914721965789795\n",
      "Epoch 8: |          | 974/? [22:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 974, loss 3.8306403160095215\n",
      "Epoch 8: |          | 975/? [22:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 975, loss 3.8652777671813965\n",
      "Epoch 8: |          | 976/? [22:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 976, loss 3.9214584827423096\n",
      "Epoch 8: |          | 977/? [22:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 977, loss 4.529869556427002\n",
      "Epoch 8: |          | 978/? [22:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 978, loss 3.9519829750061035\n",
      "Epoch 8: |          | 979/? [22:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 979, loss 4.268688201904297\n",
      "Epoch 8: |          | 980/? [22:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 980, loss 3.3893496990203857\n",
      "Epoch 8: |          | 981/? [22:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 981, loss 3.2336573600769043\n",
      "Epoch 8: |          | 982/? [22:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 982, loss 3.8881709575653076\n",
      "Epoch 8: |          | 983/? [22:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 983, loss 4.304558753967285\n",
      "Epoch 8: |          | 984/? [22:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 984, loss 3.400023937225342\n",
      "Epoch 8: |          | 985/? [22:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 985, loss 3.627492904663086\n",
      "Epoch 8: |          | 986/? [23:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 986, loss 3.689136505126953\n",
      "Epoch 8: |          | 987/? [23:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 987, loss 3.179159641265869\n",
      "Epoch 8: |          | 988/? [23:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 988, loss 4.214343547821045\n",
      "Epoch 8: |          | 989/? [23:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 989, loss 3.838710069656372\n",
      "Epoch 8: |          | 990/? [23:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 990, loss 3.2053158283233643\n",
      "Epoch 8: |          | 991/? [23:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 991, loss 3.9317848682403564\n",
      "Epoch 8: |          | 992/? [23:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 992, loss 4.618270397186279\n",
      "Epoch 8: |          | 993/? [23:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 993, loss 3.714693784713745\n",
      "Epoch 8: |          | 994/? [23:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 994, loss 3.7270889282226562\n",
      "Epoch 8: |          | 995/? [23:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 995, loss 4.095459461212158\n",
      "Epoch 8: |          | 996/? [23:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 996, loss 4.116019248962402\n",
      "Epoch 8: |          | 997/? [23:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 997, loss 3.743074417114258\n",
      "Epoch 8: |          | 998/? [23:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 998, loss 3.978391647338867\n",
      "Epoch 8: |          | 999/? [23:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 999, loss 3.9207923412323\n",
      "Epoch 8: |          | 1000/? [23:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1000, loss 3.456103563308716\n",
      "Epoch 8: |          | 1001/? [23:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1001, loss 4.126122951507568\n",
      "Epoch 8: |          | 1002/? [23:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1002, loss 4.046383857727051\n",
      "Epoch 8: |          | 1003/? [23:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1003, loss 4.2639312744140625\n",
      "Epoch 8: |          | 1004/? [23:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1004, loss 3.2904086112976074\n",
      "Epoch 8: |          | 1005/? [23:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1005, loss 3.8404159545898438\n",
      "Epoch 8: |          | 1006/? [23:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1006, loss 4.1261515617370605\n",
      "Epoch 8: |          | 1007/? [23:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1007, loss 3.73514986038208\n",
      "Epoch 8: |          | 1008/? [23:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1008, loss 3.8834385871887207\n",
      "Epoch 8: |          | 1009/? [23:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1009, loss 4.144278526306152\n",
      "Epoch 8: |          | 1010/? [23:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1010, loss 3.2829384803771973\n",
      "Epoch 8: |          | 1011/? [23:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1011, loss 3.8524887561798096\n",
      "Epoch 8: |          | 1012/? [23:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1012, loss 3.670024871826172\n",
      "Epoch 8: |          | 1013/? [23:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1013, loss 3.832505702972412\n",
      "Epoch 8: |          | 1014/? [23:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1014, loss 4.269306659698486\n",
      "Epoch 8: |          | 1015/? [23:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1015, loss 3.9405899047851562\n",
      "Epoch 8: |          | 1016/? [23:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1016, loss 3.661125659942627\n",
      "Epoch 8: |          | 1017/? [23:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1017, loss 3.0920729637145996\n",
      "Epoch 8: |          | 1018/? [23:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1018, loss 3.752833843231201\n",
      "Epoch 8: |          | 1019/? [23:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1019, loss 3.8162994384765625\n",
      "Epoch 8: |          | 1020/? [23:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1020, loss 3.4598593711853027\n",
      "Epoch 8: |          | 1021/? [23:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1021, loss 3.6976475715637207\n",
      "Epoch 8: |          | 1022/? [23:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1022, loss 3.4492030143737793\n",
      "Epoch 8: |          | 1023/? [23:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1023, loss 3.2175936698913574\n",
      "Epoch 8: |          | 1024/? [23:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1024, loss 3.6688010692596436\n",
      "Epoch 8: |          | 1025/? [23:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1025, loss 3.5731797218322754\n",
      "Epoch 8: |          | 1026/? [23:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1026, loss 2.7340645790100098\n",
      "Epoch 8: |          | 1027/? [23:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1027, loss 3.9344277381896973\n",
      "Epoch 8: |          | 1028/? [23:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1028, loss 3.7689220905303955\n",
      "Epoch 8: |          | 1029/? [24:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1029, loss 3.618820905685425\n",
      "Epoch 8: |          | 1030/? [24:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1030, loss 3.500452756881714\n",
      "Epoch 8: |          | 1031/? [24:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1031, loss 3.550856351852417\n",
      "Epoch 8: |          | 1032/? [24:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1032, loss 3.956448793411255\n",
      "Epoch 8: |          | 1033/? [24:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1033, loss 4.190659046173096\n",
      "Epoch 8: |          | 1034/? [24:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1034, loss 3.580634593963623\n",
      "Epoch 8: |          | 1035/? [24:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1035, loss 3.600595474243164\n",
      "Epoch 8: |          | 1036/? [24:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1036, loss 3.541731357574463\n",
      "Epoch 8: |          | 1037/? [24:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1037, loss 4.126209259033203\n",
      "Epoch 8: |          | 1038/? [24:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1038, loss 4.288542747497559\n",
      "Epoch 8: |          | 1039/? [24:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1039, loss 4.632001876831055\n",
      "Epoch 8: |          | 1040/? [24:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1040, loss 3.909703016281128\n",
      "Epoch 8: |          | 1041/? [24:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1041, loss 4.189517498016357\n",
      "Epoch 8: |          | 1042/? [24:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1042, loss 3.8167197704315186\n",
      "Epoch 8: |          | 1043/? [24:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1043, loss 4.195048809051514\n",
      "Epoch 8: |          | 1044/? [24:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1044, loss 3.779066801071167\n",
      "Epoch 8: |          | 1045/? [24:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1045, loss 3.3036205768585205\n",
      "Epoch 8: |          | 1046/? [24:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1046, loss 3.1566147804260254\n",
      "Epoch 8: |          | 1047/? [24:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1047, loss 4.330399036407471\n",
      "Epoch 8: |          | 1048/? [24:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1048, loss 3.775097370147705\n",
      "Epoch 8: |          | 1049/? [24:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1049, loss 3.990435838699341\n",
      "Epoch 8: |          | 1050/? [24:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1050, loss 3.5566024780273438\n",
      "Epoch 8: |          | 1051/? [24:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1051, loss 3.488259792327881\n",
      "Epoch 8: |          | 1052/? [24:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1052, loss 4.085982322692871\n",
      "Epoch 8: |          | 1053/? [24:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1053, loss 4.303503513336182\n",
      "Epoch 8: |          | 1054/? [24:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1054, loss 3.686185121536255\n",
      "Epoch 8: |          | 1055/? [24:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1055, loss 3.370084762573242\n",
      "Epoch 8: |          | 1056/? [24:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1056, loss 3.385911464691162\n",
      "Epoch 8: |          | 1057/? [24:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1057, loss 4.005882740020752\n",
      "Epoch 8: |          | 1058/? [24:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1058, loss 3.5731289386749268\n",
      "Epoch 8: |          | 1059/? [24:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1059, loss 4.15952205657959\n",
      "Epoch 8: |          | 1060/? [24:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1060, loss 4.026303768157959\n",
      "Epoch 8: |          | 1061/? [24:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1061, loss 2.7973058223724365\n",
      "Epoch 8: |          | 1062/? [24:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1062, loss 3.699277400970459\n",
      "Epoch 8: |          | 1063/? [24:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1063, loss 3.782670497894287\n",
      "Epoch 8: |          | 1064/? [24:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1064, loss 3.9528870582580566\n",
      "Epoch 8: |          | 1065/? [24:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1065, loss 2.5940818786621094\n",
      "Epoch 8: |          | 1066/? [24:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1066, loss 3.87457013130188\n",
      "Epoch 8: |          | 1067/? [24:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1067, loss 3.4286487102508545\n",
      "Epoch 8: |          | 1068/? [24:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1068, loss 3.581475019454956\n",
      "Epoch 8: |          | 1069/? [24:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1069, loss 3.910168170928955\n",
      "Epoch 8: |          | 1070/? [24:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1070, loss 3.7148730754852295\n",
      "Epoch 8: |          | 1071/? [24:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1071, loss 4.130051612854004\n",
      "Epoch 8: |          | 1072/? [25:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1072, loss 4.147960662841797\n",
      "Epoch 8: |          | 1073/? [25:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1073, loss 4.318086624145508\n",
      "Epoch 8: |          | 1074/? [25:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1074, loss 3.6375389099121094\n",
      "Epoch 8: |          | 1075/? [25:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1075, loss 3.4346702098846436\n",
      "Epoch 8: |          | 1076/? [25:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1076, loss 4.0097880363464355\n",
      "Epoch 8: |          | 1077/? [25:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1077, loss 3.5409083366394043\n",
      "Epoch 8: |          | 1078/? [25:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1078, loss 3.847135543823242\n",
      "Epoch 8: |          | 1079/? [25:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1079, loss 4.205279350280762\n",
      "Epoch 8: |          | 1080/? [25:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1080, loss 3.7452945709228516\n",
      "Epoch 8: |          | 1081/? [25:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1081, loss 4.064878463745117\n",
      "Epoch 8: |          | 1082/? [25:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1082, loss 3.6678595542907715\n",
      "Epoch 8: |          | 1083/? [25:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1083, loss 3.2273056507110596\n",
      "Epoch 8: |          | 1084/? [25:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1084, loss 3.0409939289093018\n",
      "Epoch 8: |          | 1085/? [25:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1085, loss 3.7029013633728027\n",
      "Epoch 8: |          | 1086/? [25:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1086, loss 3.975677490234375\n",
      "Epoch 8: |          | 1087/? [25:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1087, loss 4.486471176147461\n",
      "Epoch 8: |          | 1088/? [25:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1088, loss 4.0352020263671875\n",
      "Epoch 8: |          | 1089/? [25:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1089, loss 3.9392342567443848\n",
      "Epoch 8: |          | 1090/? [25:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1090, loss 3.813232898712158\n",
      "Epoch 8: |          | 1091/? [25:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1091, loss 3.591090679168701\n",
      "Epoch 8: |          | 1092/? [25:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1092, loss 3.881193161010742\n",
      "Epoch 8: |          | 1093/? [25:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1093, loss 3.357241153717041\n",
      "Epoch 8: |          | 1094/? [25:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1094, loss 3.852872848510742\n",
      "Epoch 8: |          | 1095/? [25:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1095, loss 3.9302608966827393\n",
      "Epoch 8: |          | 1096/? [25:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1096, loss 4.179056167602539\n",
      "Epoch 8: |          | 1097/? [25:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1097, loss 3.749495029449463\n",
      "Epoch 8: |          | 1098/? [25:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1098, loss 3.046370506286621\n",
      "Epoch 8: |          | 1099/? [25:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1099, loss 3.701444149017334\n",
      "Epoch 8: |          | 1100/? [25:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1100, loss 3.897581100463867\n",
      "Epoch 8: |          | 1101/? [25:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1101, loss 3.549151659011841\n",
      "Epoch 8: |          | 1102/? [25:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1102, loss 4.255403995513916\n",
      "Epoch 8: |          | 1103/? [25:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1103, loss 4.651902198791504\n",
      "Epoch 8: |          | 1104/? [25:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1104, loss 4.038128852844238\n",
      "Epoch 8: |          | 1105/? [25:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1105, loss 4.211024761199951\n",
      "Epoch 8: |          | 1106/? [25:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1106, loss 3.7185332775115967\n",
      "Epoch 8: |          | 1107/? [25:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1107, loss 3.8179123401641846\n",
      "Epoch 8: |          | 1108/? [25:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1108, loss 3.808436155319214\n",
      "Epoch 8: |          | 1109/? [25:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1109, loss 3.4410324096679688\n",
      "Epoch 8: |          | 1110/? [25:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1110, loss 4.331411838531494\n",
      "Epoch 8: |          | 1111/? [25:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1111, loss 4.005486011505127\n",
      "Epoch 8: |          | 1112/? [25:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1112, loss 3.873581647872925\n",
      "Epoch 8: |          | 1113/? [25:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1113, loss 3.7162392139434814\n",
      "Epoch 8: |          | 1114/? [25:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1114, loss 3.152679920196533\n",
      "Epoch 8: |          | 1115/? [26:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1115, loss 2.9547080993652344\n",
      "Epoch 8: |          | 1116/? [26:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1116, loss 3.325202226638794\n",
      "Epoch 8: |          | 1117/? [26:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1117, loss 3.489936351776123\n",
      "Epoch 8: |          | 1118/? [26:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1118, loss 3.6678097248077393\n",
      "Epoch 8: |          | 1119/? [26:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1119, loss 4.2601470947265625\n",
      "Epoch 8: |          | 1120/? [26:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1120, loss 3.80804705619812\n",
      "Epoch 8: |          | 1121/? [26:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1121, loss 4.061290264129639\n",
      "Epoch 8: |          | 1122/? [26:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1122, loss 3.545464038848877\n",
      "Epoch 8: |          | 1123/? [26:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1123, loss 3.8566722869873047\n",
      "Epoch 8: |          | 1124/? [26:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1124, loss 4.202398777008057\n",
      "Epoch 8: |          | 1125/? [26:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1125, loss 3.4961276054382324\n",
      "Epoch 8: |          | 1126/? [26:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1126, loss 3.4153265953063965\n",
      "Epoch 8: |          | 1127/? [26:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1127, loss 3.734236478805542\n",
      "Epoch 8: |          | 1128/? [26:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1128, loss 3.7821338176727295\n",
      "Epoch 8: |          | 1129/? [26:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1129, loss 3.9190948009490967\n",
      "Epoch 8: |          | 1130/? [26:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1130, loss 4.077574253082275\n",
      "Epoch 8: |          | 1131/? [26:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1131, loss 4.177333831787109\n",
      "Epoch 8: |          | 1132/? [26:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1132, loss 2.960033416748047\n",
      "Epoch 8: |          | 1133/? [26:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1133, loss 3.752734422683716\n",
      "Epoch 8: |          | 1134/? [26:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1134, loss 3.580613613128662\n",
      "Epoch 8: |          | 1135/? [26:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1135, loss 4.188227653503418\n",
      "Epoch 8: |          | 1136/? [26:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1136, loss 3.842569351196289\n",
      "Epoch 8: |          | 1137/? [26:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1137, loss 3.8425121307373047\n",
      "Epoch 8: |          | 1138/? [26:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1138, loss 4.326160430908203\n",
      "Epoch 8: |          | 1139/? [26:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1139, loss 4.273310661315918\n",
      "Epoch 8: |          | 1140/? [26:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1140, loss 3.632373094558716\n",
      "Epoch 8: |          | 1141/? [26:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1141, loss 4.164199352264404\n",
      "Epoch 8: |          | 1142/? [26:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1142, loss 4.2328104972839355\n",
      "Epoch 8: |          | 1143/? [26:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1143, loss 4.277584075927734\n",
      "Epoch 8: |          | 1144/? [26:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1144, loss 3.7051994800567627\n",
      "Epoch 8: |          | 1145/? [26:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1145, loss 3.8139853477478027\n",
      "Epoch 8: |          | 1146/? [26:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1146, loss 3.502748489379883\n",
      "Epoch 8: |          | 1147/? [26:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1147, loss 3.4244868755340576\n",
      "Epoch 8: |          | 1148/? [26:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1148, loss 3.5871715545654297\n",
      "Epoch 8: |          | 1149/? [26:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1149, loss 4.618411064147949\n",
      "Epoch 8: |          | 1150/? [26:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1150, loss 4.007086277008057\n",
      "Epoch 8: |          | 1151/? [26:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1151, loss 4.289607048034668\n",
      "Epoch 8: |          | 1152/? [26:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1152, loss 3.5104496479034424\n",
      "Epoch 8: |          | 1153/? [26:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1153, loss 3.929821729660034\n",
      "Epoch 8: |          | 1154/? [26:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1154, loss 3.6005241870880127\n",
      "Epoch 8: |          | 1155/? [26:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1155, loss 3.8136887550354004\n",
      "Epoch 8: |          | 1156/? [26:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1156, loss 3.793513774871826\n",
      "Epoch 8: |          | 1157/? [26:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1157, loss 4.054693222045898\n",
      "Epoch 8: |          | 1158/? [27:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1158, loss 4.2277116775512695\n",
      "Epoch 8: |          | 1159/? [27:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1159, loss 3.0713050365448\n",
      "Epoch 8: |          | 1160/? [27:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1160, loss 4.224583625793457\n",
      "Epoch 8: |          | 1161/? [27:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1161, loss 4.072271347045898\n",
      "Epoch 8: |          | 1162/? [27:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1162, loss 4.00419807434082\n",
      "Epoch 8: |          | 1163/? [27:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1163, loss 4.582655906677246\n",
      "Epoch 8: |          | 1164/? [27:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1164, loss 4.368791580200195\n",
      "Epoch 8: |          | 1165/? [27:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1165, loss 3.5348076820373535\n",
      "Epoch 8: |          | 1166/? [27:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1166, loss 4.032087326049805\n",
      "Epoch 8: |          | 1167/? [27:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1167, loss 4.052918434143066\n",
      "Epoch 8: |          | 1168/? [27:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1168, loss 4.491452217102051\n",
      "Epoch 8: |          | 1169/? [27:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1169, loss 3.58842396736145\n",
      "Epoch 8: |          | 1170/? [27:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1170, loss 4.137281894683838\n",
      "Epoch 8: |          | 1171/? [27:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1171, loss 3.5184123516082764\n",
      "Epoch 8: |          | 1172/? [27:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1172, loss 3.4272968769073486\n",
      "Epoch 8: |          | 1173/? [27:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1173, loss 3.99450421333313\n",
      "Epoch 8: |          | 1174/? [27:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1174, loss 3.4638068675994873\n",
      "Epoch 8: |          | 1175/? [27:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1175, loss 4.09353494644165\n",
      "Epoch 8: |          | 1176/? [27:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1176, loss 4.119953155517578\n",
      "Epoch 8: |          | 1177/? [27:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1177, loss 4.293015956878662\n",
      "Epoch 8: |          | 1178/? [27:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1178, loss 3.7122623920440674\n",
      "Epoch 8: |          | 1179/? [27:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1179, loss 4.255158424377441\n",
      "Epoch 8: |          | 1180/? [27:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1180, loss 4.066887855529785\n",
      "Epoch 8: |          | 1181/? [27:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1181, loss 3.994373321533203\n",
      "Epoch 8: |          | 1182/? [27:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1182, loss 3.797163486480713\n",
      "Epoch 8: |          | 1183/? [27:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1183, loss 3.500486373901367\n",
      "Epoch 8: |          | 1184/? [27:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1184, loss 3.9319519996643066\n",
      "Epoch 8: |          | 1185/? [27:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1185, loss 3.6904022693634033\n",
      "Epoch 8: |          | 1186/? [27:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1186, loss 3.8874497413635254\n",
      "Epoch 8: |          | 1187/? [27:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1187, loss 3.7717366218566895\n",
      "Epoch 8: |          | 1188/? [27:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1188, loss 4.14949893951416\n",
      "Epoch 8: |          | 1189/? [27:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1189, loss 4.251523971557617\n",
      "Epoch 8: |          | 1190/? [27:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1190, loss 3.7775051593780518\n",
      "Epoch 8: |          | 1191/? [27:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1191, loss 3.842802047729492\n",
      "Epoch 8: |          | 1192/? [27:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1192, loss 4.1523613929748535\n",
      "Epoch 8: |          | 1193/? [27:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1193, loss 3.6353039741516113\n",
      "Epoch 8: |          | 1194/? [27:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1194, loss 3.3688735961914062\n",
      "Epoch 8: |          | 1195/? [27:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1195, loss 3.876636505126953\n",
      "Epoch 8: |          | 1196/? [27:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1196, loss 4.072157859802246\n",
      "Epoch 8: |          | 1197/? [27:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1197, loss 3.8351855278015137\n",
      "Epoch 8: |          | 1198/? [27:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1198, loss 3.9578940868377686\n",
      "Epoch 8: |          | 1199/? [27:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1199, loss 4.229030609130859\n",
      "Epoch 8: |          | 1200/? [28:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1200, loss 3.442349672317505\n",
      "Epoch 8: |          | 1201/? [28:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1201, loss 4.010450839996338\n",
      "Epoch 8: |          | 1202/? [28:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1202, loss 3.7383193969726562\n",
      "Epoch 8: |          | 1203/? [28:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1203, loss 3.690596342086792\n",
      "Epoch 8: |          | 1204/? [28:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1204, loss 3.246739149093628\n",
      "Epoch 8: |          | 1205/? [28:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1205, loss 3.8044381141662598\n",
      "Epoch 8: |          | 1206/? [28:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1206, loss 3.8352456092834473\n",
      "Epoch 8: |          | 1207/? [28:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1207, loss 4.1248273849487305\n",
      "Epoch 8: |          | 1208/? [28:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1208, loss 4.319861888885498\n",
      "Epoch 8: |          | 1209/? [28:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1209, loss 3.9132869243621826\n",
      "Epoch 8: |          | 1210/? [28:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1210, loss 4.162995338439941\n",
      "Epoch 8: |          | 1211/? [28:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1211, loss 4.112878322601318\n",
      "Epoch 8: |          | 1212/? [28:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1212, loss 3.9500675201416016\n",
      "Epoch 8: |          | 1213/? [28:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1213, loss 3.6536102294921875\n",
      "Epoch 8: |          | 1214/? [28:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1214, loss 4.2421770095825195\n",
      "Epoch 8: |          | 1215/? [28:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1215, loss 3.7196202278137207\n",
      "Epoch 8: |          | 1216/? [28:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1216, loss 3.805351972579956\n",
      "Epoch 8: |          | 1217/? [28:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1217, loss 3.944852828979492\n",
      "Epoch 8: |          | 1218/? [28:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1218, loss 3.9931366443634033\n",
      "Epoch 8: |          | 1219/? [28:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1219, loss 3.697917938232422\n",
      "Epoch 8: |          | 1220/? [28:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1220, loss 4.362431049346924\n",
      "Epoch 8: |          | 1221/? [28:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1221, loss 3.944121837615967\n",
      "Epoch 8: |          | 1222/? [28:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1222, loss 3.0662319660186768\n",
      "Epoch 8: |          | 1223/? [28:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1223, loss 3.264242649078369\n",
      "Epoch 8: |          | 1224/? [28:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1224, loss 3.590827226638794\n",
      "Epoch 8: |          | 1225/? [28:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1225, loss 4.233565330505371\n",
      "Epoch 8: |          | 1226/? [28:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1226, loss 4.211705207824707\n",
      "Epoch 8: |          | 1227/? [28:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1227, loss 3.876192808151245\n",
      "Epoch 8: |          | 1228/? [28:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1228, loss 3.73201322555542\n",
      "Epoch 8: |          | 1229/? [28:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1229, loss 3.3388009071350098\n",
      "Epoch 8: |          | 1230/? [28:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1230, loss 4.017736911773682\n",
      "Epoch 8: |          | 1231/? [28:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1231, loss 4.062920570373535\n",
      "Epoch 8: |          | 1232/? [28:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1232, loss 4.221253395080566\n",
      "Epoch 8: |          | 1233/? [28:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1233, loss 3.989841938018799\n",
      "Epoch 8: |          | 1234/? [28:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1234, loss 3.0551514625549316\n",
      "Epoch 8: |          | 1235/? [28:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1235, loss 4.122403621673584\n",
      "Epoch 8: |          | 1236/? [28:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1236, loss 3.490565776824951\n",
      "Epoch 8: |          | 1237/? [28:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1237, loss 3.781672716140747\n",
      "Epoch 8: |          | 1238/? [28:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1238, loss 3.8386390209198\n",
      "Epoch 8: |          | 1239/? [28:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1239, loss 3.6826205253601074\n",
      "Epoch 8: |          | 1240/? [28:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1240, loss 4.316237449645996\n",
      "Epoch 8: |          | 1241/? [28:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1241, loss 3.853682279586792\n",
      "Epoch 8: |          | 1242/? [28:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1242, loss 3.7037055492401123\n",
      "Epoch 8: |          | 1243/? [29:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1243, loss 3.5934677124023438\n",
      "Epoch 8: |          | 1244/? [29:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1244, loss 3.740126848220825\n",
      "Epoch 8: |          | 1245/? [29:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1245, loss 3.36043119430542\n",
      "Epoch 8: |          | 1246/? [29:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1246, loss 4.016286849975586\n",
      "Epoch 8: |          | 1247/? [29:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1247, loss 4.008098125457764\n",
      "Epoch 8: |          | 1248/? [29:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1248, loss 3.5960090160369873\n",
      "Epoch 8: |          | 1249/? [29:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1249, loss 3.7423548698425293\n",
      "Epoch 8: |          | 1250/? [29:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1250, loss 3.907421588897705\n",
      "Epoch 8: |          | 1251/? [29:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1251, loss 3.625744581222534\n",
      "Epoch 8: |          | 1252/? [29:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1252, loss 4.409092903137207\n",
      "Epoch 8: |          | 1253/? [29:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1253, loss 3.7871603965759277\n",
      "Epoch 8: |          | 1254/? [29:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1254, loss 3.1287639141082764\n",
      "Epoch 8: |          | 1255/? [29:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1255, loss 4.463982582092285\n",
      "Epoch 8: |          | 1256/? [29:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1256, loss 3.477565050125122\n",
      "Epoch 8: |          | 1257/? [29:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1257, loss 3.51039457321167\n",
      "Epoch 8: |          | 1258/? [29:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1258, loss 4.201717376708984\n",
      "Epoch 8: |          | 1259/? [29:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1259, loss 3.863969326019287\n",
      "Epoch 8: |          | 1260/? [29:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1260, loss 4.336962699890137\n",
      "Epoch 8: |          | 1261/? [29:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1261, loss 3.6532421112060547\n",
      "Epoch 8: |          | 1262/? [29:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1262, loss 3.6560184955596924\n",
      "Epoch 8: |          | 1263/? [29:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1263, loss 3.973837375640869\n",
      "Epoch 8: |          | 1264/? [29:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1264, loss 4.25447416305542\n",
      "Epoch 8: |          | 1265/? [29:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1265, loss 4.13242244720459\n",
      "Epoch 8: |          | 1266/? [29:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1266, loss 3.794750213623047\n",
      "Epoch 8: |          | 1267/? [29:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1267, loss 3.886523723602295\n",
      "Epoch 8: |          | 1268/? [29:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1268, loss 3.753260374069214\n",
      "Epoch 8: |          | 1269/? [29:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1269, loss 3.339503526687622\n",
      "Epoch 8: |          | 1270/? [29:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1270, loss 3.6555800437927246\n",
      "Epoch 8: |          | 1271/? [29:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1271, loss 3.8162245750427246\n",
      "Epoch 8: |          | 1272/? [29:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1272, loss 3.430441379547119\n",
      "Epoch 8: |          | 1273/? [29:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1273, loss 4.0912394523620605\n",
      "Epoch 8: |          | 1274/? [29:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1274, loss 3.015568733215332\n",
      "Epoch 8: |          | 1275/? [29:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1275, loss 3.5149269104003906\n",
      "Epoch 8: |          | 1276/? [29:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1276, loss 3.8486526012420654\n",
      "Epoch 8: |          | 1277/? [29:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1277, loss 3.5654473304748535\n",
      "Epoch 8: |          | 1278/? [29:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1278, loss 3.2447426319122314\n",
      "Epoch 8: |          | 1279/? [29:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1279, loss 3.947542905807495\n",
      "Epoch 8: |          | 1280/? [29:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1280, loss 3.1595382690429688\n",
      "Epoch 8: |          | 1281/? [29:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1281, loss 3.687582492828369\n",
      "Epoch 8: |          | 1282/? [30:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1282, loss 3.364652156829834\n",
      "Epoch 8: |          | 1283/? [30:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1283, loss 4.113248348236084\n",
      "Epoch 8: |          | 1284/? [30:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1284, loss 3.2357373237609863\n",
      "Epoch 8: |          | 1285/? [30:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1285, loss 4.265656471252441\n",
      "Epoch 8: |          | 1286/? [30:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1286, loss 2.769953727722168\n",
      "Epoch 8: |          | 1287/? [30:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1287, loss 4.19722843170166\n",
      "Epoch 8: |          | 1288/? [30:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1288, loss 4.025223731994629\n",
      "Epoch 8: |          | 1289/? [30:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1289, loss 3.15291166305542\n",
      "Epoch 8: |          | 1290/? [30:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1290, loss 3.878368854522705\n",
      "Epoch 8: |          | 1291/? [30:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1291, loss 4.720389366149902\n",
      "Epoch 8: |          | 1292/? [30:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1292, loss 4.073704719543457\n",
      "Epoch 8: |          | 1293/? [30:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1293, loss 3.5901126861572266\n",
      "Epoch 8: |          | 1294/? [30:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1294, loss 3.8204681873321533\n",
      "Epoch 8: |          | 1295/? [30:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1295, loss 3.923656940460205\n",
      "Epoch 8: |          | 1296/? [30:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1296, loss 3.207277774810791\n",
      "Epoch 8: |          | 1297/? [30:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1297, loss 4.084271430969238\n",
      "Epoch 8: |          | 1298/? [30:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1298, loss 3.8076717853546143\n",
      "Epoch 8: |          | 1299/? [30:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1299, loss 2.9767982959747314\n",
      "Epoch 8: |          | 1300/? [30:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1300, loss 3.8746273517608643\n",
      "Epoch 8: |          | 1301/? [30:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1301, loss 3.5470595359802246\n",
      "Epoch 8: |          | 1302/? [30:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1302, loss 3.6787962913513184\n",
      "Epoch 8: |          | 1303/? [30:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1303, loss 3.6428914070129395\n",
      "Epoch 8: |          | 1304/? [30:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1304, loss 4.369627952575684\n",
      "Epoch 8: |          | 1305/? [30:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1305, loss 3.1967272758483887\n",
      "Epoch 8: |          | 1306/? [30:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1306, loss 3.8048763275146484\n",
      "Epoch 8: |          | 1307/? [30:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1307, loss 3.427891492843628\n",
      "Epoch 8: |          | 1308/? [30:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1308, loss 3.36787748336792\n",
      "Epoch 8: |          | 1309/? [30:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1309, loss 3.4355812072753906\n",
      "Epoch 8: |          | 1310/? [30:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1310, loss 3.9558353424072266\n",
      "Epoch 8: |          | 1311/? [30:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1311, loss 3.4449563026428223\n",
      "Epoch 8: |          | 1312/? [30:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1312, loss 3.1869094371795654\n",
      "Epoch 8: |          | 1313/? [30:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1313, loss 4.314990043640137\n",
      "Epoch 8: |          | 1314/? [30:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1314, loss 3.540590286254883\n",
      "Epoch 8: |          | 1315/? [30:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1315, loss 4.292096138000488\n",
      "Epoch 8: |          | 1316/? [30:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1316, loss 4.048910140991211\n",
      "Epoch 8: |          | 1317/? [30:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1317, loss 3.6957943439483643\n",
      "Epoch 8: |          | 1318/? [30:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1318, loss 3.882376194000244\n",
      "Epoch 8: |          | 1319/? [30:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1319, loss 4.029372215270996\n",
      "Epoch 8: |          | 1320/? [30:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1320, loss 3.649967670440674\n",
      "Epoch 8: |          | 1321/? [30:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1321, loss 4.074995994567871\n",
      "Epoch 8: |          | 1322/? [30:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1322, loss 3.9237148761749268\n",
      "Epoch 8: |          | 1323/? [30:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1323, loss 3.4312210083007812\n",
      "Epoch 8: |          | 1324/? [30:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1324, loss 4.3582587242126465\n",
      "Epoch 8: |          | 1325/? [31:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1325, loss 4.504238128662109\n",
      "Epoch 8: |          | 1326/? [31:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1326, loss 3.933971405029297\n",
      "Epoch 8: |          | 1327/? [31:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1327, loss 3.8199291229248047\n",
      "Epoch 8: |          | 1328/? [31:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1328, loss 3.4926693439483643\n",
      "Epoch 8: |          | 1329/? [31:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1329, loss 4.0787272453308105\n",
      "Epoch 8: |          | 1330/? [31:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1330, loss 3.8294806480407715\n",
      "Epoch 8: |          | 1331/? [31:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1331, loss 3.9241700172424316\n",
      "Epoch 8: |          | 1332/? [31:10<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1332, loss 3.65051531791687\n",
      "Epoch 8: |          | 1333/? [31:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1333, loss 3.633812427520752\n",
      "Epoch 8: |          | 1334/? [31:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1334, loss 3.7637314796447754\n",
      "Epoch 8: |          | 1335/? [31:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1335, loss 3.6890532970428467\n",
      "Epoch 8: |          | 1336/? [31:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1336, loss 3.2902941703796387\n",
      "Epoch 8: |          | 1337/? [31:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1337, loss 3.9004485607147217\n",
      "Epoch 8: |          | 1338/? [31:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1338, loss 3.17472505569458\n",
      "Epoch 8: |          | 1339/? [31:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1339, loss 3.8423924446105957\n",
      "Epoch 8: |          | 1340/? [31:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1340, loss 3.243846893310547\n",
      "Epoch 8: |          | 1341/? [31:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1341, loss 3.9516212940216064\n",
      "Epoch 8: |          | 1342/? [31:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1342, loss 4.230419635772705\n",
      "Epoch 8: |          | 1343/? [31:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1343, loss 3.7144646644592285\n",
      "Epoch 8: |          | 1344/? [31:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1344, loss 3.8119049072265625\n",
      "Epoch 8: |          | 1345/? [31:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1345, loss 3.9402740001678467\n",
      "Epoch 8: |          | 1346/? [31:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1346, loss 5.0560078620910645\n",
      "Epoch 8: |          | 1347/? [31:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1347, loss 3.9943771362304688\n",
      "Epoch 8: |          | 1348/? [31:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1348, loss 4.120072364807129\n",
      "Epoch 8: |          | 1349/? [31:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1349, loss 3.986335277557373\n",
      "Epoch 8: |          | 1350/? [31:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1350, loss 4.081673622131348\n",
      "Epoch 8: |          | 1351/? [31:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1351, loss 4.026270866394043\n",
      "Epoch 8: |          | 1352/? [31:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1352, loss 3.2711944580078125\n",
      "Epoch 8: |          | 1353/? [31:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1353, loss 3.6424338817596436\n",
      "Epoch 8: |          | 1354/? [31:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1354, loss 4.007785797119141\n",
      "Epoch 8: |          | 1355/? [31:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1355, loss 4.151798248291016\n",
      "Epoch 8: |          | 1356/? [31:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1356, loss 3.8760151863098145\n",
      "Epoch 8: |          | 1357/? [31:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1357, loss 3.657956600189209\n",
      "Epoch 8: |          | 1358/? [31:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1358, loss 3.83795428276062\n",
      "Epoch 8: |          | 1359/? [31:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1359, loss 3.7086055278778076\n",
      "Epoch 8: |          | 1360/? [31:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1360, loss 3.872631072998047\n",
      "Epoch 8: |          | 1361/? [31:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1361, loss 3.8614706993103027\n",
      "Epoch 8: |          | 1362/? [31:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1362, loss 3.7007992267608643\n",
      "Epoch 8: |          | 1363/? [31:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1363, loss 3.144688129425049\n",
      "Epoch 8: |          | 1364/? [31:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1364, loss 3.6238083839416504\n",
      "Epoch 8: |          | 1365/? [31:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1365, loss 3.3432319164276123\n",
      "Epoch 8: |          | 1366/? [31:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1366, loss 4.060459136962891\n",
      "Epoch 8: |          | 1367/? [31:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1367, loss 3.361034870147705\n",
      "Epoch 8: |          | 1368/? [32:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1368, loss 3.17661190032959\n",
      "Epoch 8: |          | 1369/? [32:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1369, loss 3.8463332653045654\n",
      "Epoch 8: |          | 1370/? [32:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1370, loss 3.393618106842041\n",
      "Epoch 8: |          | 1371/? [32:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1371, loss 4.306414604187012\n",
      "Epoch 8: |          | 1372/? [32:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1372, loss 3.7522411346435547\n",
      "Epoch 8: |          | 1373/? [32:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1373, loss 4.140927314758301\n",
      "Epoch 8: |          | 1374/? [32:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1374, loss 3.2518277168273926\n",
      "Epoch 8: |          | 1375/? [32:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1375, loss 3.873478651046753\n",
      "Epoch 8: |          | 1376/? [32:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1376, loss 3.7406227588653564\n",
      "Epoch 8: |          | 1377/? [32:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1377, loss 3.741499662399292\n",
      "Epoch 8: |          | 1378/? [32:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1378, loss 3.703026294708252\n",
      "Epoch 8: |          | 1379/? [32:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1379, loss 3.7623679637908936\n",
      "Epoch 8: |          | 1380/? [32:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1380, loss 3.86506724357605\n",
      "Epoch 8: |          | 1381/? [32:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1381, loss 3.9815735816955566\n",
      "Epoch 8: |          | 1382/? [32:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1382, loss 3.570582866668701\n",
      "Epoch 8: |          | 1383/? [32:21<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1383, loss 3.7316181659698486\n",
      "Epoch 8: |          | 1384/? [32:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1384, loss 3.7829177379608154\n",
      "Epoch 8: |          | 1385/? [32:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1385, loss 3.7629172801971436\n",
      "Epoch 8: |          | 1386/? [32:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1386, loss 3.8293728828430176\n",
      "Epoch 8: |          | 1387/? [32:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1387, loss 3.760554790496826\n",
      "Epoch 8: |          | 1388/? [32:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1388, loss 3.28760027885437\n",
      "Epoch 8: |          | 1389/? [32:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1389, loss 3.9778122901916504\n",
      "Epoch 8: |          | 1390/? [32:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1390, loss 4.313925743103027\n",
      "Epoch 8: |          | 1391/? [32:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1391, loss 3.908846378326416\n",
      "Epoch 8: |          | 1392/? [32:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1392, loss 3.4099230766296387\n",
      "Epoch 8: |          | 1393/? [32:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1393, loss 3.5771994590759277\n",
      "Epoch 8: |          | 1394/? [32:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1394, loss 3.2066731452941895\n",
      "Epoch 8: |          | 1395/? [32:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1395, loss 3.9000275135040283\n",
      "Epoch 8: |          | 1396/? [32:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1396, loss 3.7966971397399902\n",
      "Epoch 8: |          | 1397/? [32:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1397, loss 3.0328643321990967\n",
      "Epoch 8: |          | 1398/? [32:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1398, loss 4.243203163146973\n",
      "Epoch 8: |          | 1399/? [32:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1399, loss 4.2622504234313965\n",
      "Epoch 8: |          | 1400/? [32:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1400, loss 3.370082139968872\n",
      "Epoch 8: |          | 1401/? [32:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1401, loss 4.142261028289795\n",
      "Epoch 8: |          | 1402/? [32:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1402, loss 3.7612838745117188\n",
      "Epoch 8: |          | 1403/? [32:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1403, loss 3.9607994556427\n",
      "Epoch 8: |          | 1404/? [32:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1404, loss 3.9327900409698486\n",
      "Epoch 8: |          | 1405/? [32:52<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1405, loss 4.257254600524902\n",
      "Epoch 8: |          | 1406/? [32:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1406, loss 4.185067653656006\n",
      "Epoch 8: |          | 1407/? [32:55<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1407, loss 4.247353553771973\n",
      "Epoch 8: |          | 1408/? [32:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1408, loss 3.4447200298309326\n",
      "Epoch 8: |          | 1409/? [32:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1409, loss 3.558687686920166\n",
      "Epoch 8: |          | 1410/? [32:59<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1410, loss 3.5595550537109375\n",
      "Epoch 8: |          | 1411/? [33:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1411, loss 3.8925952911376953\n",
      "Epoch 8: |          | 1412/? [33:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1412, loss 3.55228853225708\n",
      "Epoch 8: |          | 1413/? [33:03<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1413, loss 3.456812620162964\n",
      "Epoch 8: |          | 1414/? [33:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1414, loss 3.582609176635742\n",
      "Epoch 8: |          | 1415/? [33:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1415, loss 3.832656145095825\n",
      "Epoch 8: |          | 1416/? [33:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1416, loss 4.1995849609375\n",
      "Epoch 8: |          | 1417/? [33:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1417, loss 3.8785758018493652\n",
      "Epoch 8: |          | 1418/? [33:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1418, loss 4.012908935546875\n",
      "Epoch 8: |          | 1419/? [33:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1419, loss 3.802595853805542\n",
      "Epoch 8: |          | 1420/? [33:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1420, loss 3.6307263374328613\n",
      "Epoch 8: |          | 1421/? [33:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1421, loss 3.4071335792541504\n",
      "Epoch 8: |          | 1422/? [33:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1422, loss 4.035330295562744\n",
      "Epoch 8: |          | 1423/? [33:17<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1423, loss 4.068165302276611\n",
      "Epoch 8: |          | 1424/? [33:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1424, loss 3.7791812419891357\n",
      "Epoch 8: |          | 1425/? [33:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1425, loss 3.970245838165283\n",
      "Epoch 8: |          | 1426/? [33:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1426, loss 3.5169243812561035\n",
      "Epoch 8: |          | 1427/? [33:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1427, loss 4.1225714683532715\n",
      "Epoch 8: |          | 1428/? [33:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1428, loss 4.09368896484375\n",
      "Epoch 8: |          | 1429/? [33:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1429, loss 3.9934611320495605\n",
      "Epoch 8: |          | 1430/? [33:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1430, loss 4.051988124847412\n",
      "Epoch 8: |          | 1431/? [33:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1431, loss 3.7973265647888184\n",
      "Epoch 8: |          | 1432/? [33:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1432, loss 3.8747050762176514\n",
      "Epoch 8: |          | 1433/? [33:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1433, loss 3.772305965423584\n",
      "Epoch 8: |          | 1434/? [33:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1434, loss 3.922557830810547\n",
      "Epoch 8: |          | 1435/? [33:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1435, loss 3.5256004333496094\n",
      "Epoch 8: |          | 1436/? [33:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1436, loss 3.8807101249694824\n",
      "Epoch 8: |          | 1437/? [33:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1437, loss 3.1172330379486084\n",
      "Epoch 8: |          | 1438/? [33:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1438, loss 4.771703243255615\n",
      "Epoch 8: |          | 1439/? [33:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1439, loss 3.8815808296203613\n",
      "Epoch 8: |          | 1440/? [33:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1440, loss 4.045392036437988\n",
      "Epoch 8: |          | 1441/? [33:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1441, loss 4.345677852630615\n",
      "Epoch 8: |          | 1442/? [33:45<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1442, loss 4.383474826812744\n",
      "Epoch 8: |          | 1443/? [33:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1443, loss 3.5307393074035645\n",
      "Epoch 8: |          | 1444/? [33:48<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1444, loss 3.5557479858398438\n",
      "Epoch 8: |          | 1445/? [33:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1445, loss 4.0352983474731445\n",
      "Epoch 8: |          | 1446/? [33:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1446, loss 3.620443820953369\n",
      "Epoch 8: |          | 1447/? [33:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1447, loss 3.7738823890686035\n",
      "Epoch 8: |          | 1448/? [33:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1448, loss 3.6069369316101074\n",
      "Epoch 8: |          | 1449/? [33:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1449, loss 3.8077430725097656\n",
      "Epoch 8: |          | 1450/? [33:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1450, loss 3.885202407836914\n",
      "Epoch 8: |          | 1451/? [33:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1451, loss 4.261226654052734\n",
      "Epoch 8: |          | 1452/? [33:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1452, loss 3.8850574493408203\n",
      "Epoch 8: |          | 1453/? [34:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1453, loss 3.238619565963745\n",
      "Epoch 8: |          | 1454/? [34:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1454, loss 3.839648723602295\n",
      "Epoch 8: |          | 1455/? [34:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1455, loss 3.9036591053009033\n",
      "Epoch 8: |          | 1456/? [34:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1456, loss 3.492321014404297\n",
      "Epoch 8: |          | 1457/? [34:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1457, loss 3.6714768409729004\n",
      "Epoch 8: |          | 1458/? [34:06<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1458, loss 3.8964149951934814\n",
      "Epoch 8: |          | 1459/? [34:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1459, loss 4.00077486038208\n",
      "Epoch 8: |          | 1460/? [34:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1460, loss 3.9574198722839355\n",
      "Epoch 8: |          | 1461/? [34:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1461, loss 3.8715591430664062\n",
      "Epoch 8: |          | 1462/? [34:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1462, loss 4.175036430358887\n",
      "Epoch 8: |          | 1463/? [34:13<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1463, loss 4.097319602966309\n",
      "Epoch 8: |          | 1464/? [34:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1464, loss 3.395324230194092\n",
      "Epoch 8: |          | 1465/? [34:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1465, loss 3.778515338897705\n",
      "Epoch 8: |          | 1466/? [34:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1466, loss 3.453686237335205\n",
      "Epoch 8: |          | 1467/? [34:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1467, loss 4.09561014175415\n",
      "Epoch 8: |          | 1468/? [34:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1468, loss 3.673659563064575\n",
      "Epoch 8: |          | 1469/? [34:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1469, loss 3.4148640632629395\n",
      "Epoch 8: |          | 1470/? [34:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1470, loss 3.938326358795166\n",
      "Epoch 8: |          | 1471/? [34:25<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1471, loss 4.218101978302002\n",
      "Epoch 8: |          | 1472/? [34:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1472, loss 3.9439539909362793\n",
      "Epoch 8: |          | 1473/? [34:28<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1473, loss 3.7593600749969482\n",
      "Epoch 8: |          | 1474/? [34:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1474, loss 3.6755123138427734\n",
      "Epoch 8: |          | 1475/? [34:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1475, loss 3.2635772228240967\n",
      "Epoch 8: |          | 1476/? [34:32<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1476, loss 3.842773914337158\n",
      "Epoch 8: |          | 1477/? [34:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1477, loss 3.857771635055542\n",
      "Epoch 8: |          | 1478/? [34:35<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1478, loss 3.740156888961792\n",
      "Epoch 8: |          | 1479/? [34:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1479, loss 4.261069297790527\n",
      "Epoch 8: |          | 1480/? [34:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1480, loss 3.980433702468872\n",
      "Epoch 8: |          | 1481/? [34:39<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1481, loss 3.736694812774658\n",
      "Epoch 8: |          | 1482/? [34:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1482, loss 3.8945541381835938\n",
      "Epoch 8: |          | 1483/? [34:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1483, loss 3.547898530960083\n",
      "Epoch 8: |          | 1484/? [34:43<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1484, loss 3.7373452186584473\n",
      "Epoch 8: |          | 1485/? [34:44<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1485, loss 3.873143434524536\n",
      "Epoch 8: |          | 1486/? [34:46<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1486, loss 3.872159481048584\n",
      "Epoch 8: |          | 1487/? [34:47<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1487, loss 3.3316092491149902\n",
      "Epoch 8: |          | 1488/? [34:49<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1488, loss 3.9759433269500732\n",
      "Epoch 8: |          | 1489/? [34:50<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1489, loss 3.945685625076294\n",
      "Epoch 8: |          | 1490/? [34:51<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1490, loss 3.802954912185669\n",
      "Epoch 8: |          | 1491/? [34:53<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1491, loss 2.844421863555908\n",
      "Epoch 8: |          | 1492/? [34:54<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1492, loss 3.3963820934295654\n",
      "Epoch 8: |          | 1493/? [34:56<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1493, loss 2.959827184677124\n",
      "Epoch 8: |          | 1494/? [34:57<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1494, loss 3.749483823776245\n",
      "Epoch 8: |          | 1495/? [34:58<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1495, loss 3.6209285259246826\n",
      "Epoch 8: |          | 1496/? [35:00<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1496, loss 3.9708621501922607\n",
      "Epoch 8: |          | 1497/? [35:01<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1497, loss 3.237974166870117\n",
      "Epoch 8: |          | 1498/? [35:02<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1498, loss 3.5023269653320312\n",
      "Epoch 8: |          | 1499/? [35:04<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1499, loss 4.066424369812012\n",
      "Epoch 8: |          | 1500/? [35:05<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1500, loss 4.001810073852539\n",
      "Epoch 8: |          | 1501/? [35:07<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1501, loss 3.8505516052246094\n",
      "Epoch 8: |          | 1502/? [35:08<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1502, loss 3.9247124195098877\n",
      "Epoch 8: |          | 1503/? [35:09<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1503, loss 3.6741955280303955\n",
      "Epoch 8: |          | 1504/? [35:11<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1504, loss 4.311420917510986\n",
      "Epoch 8: |          | 1505/? [35:12<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1505, loss 4.129515647888184\n",
      "Epoch 8: |          | 1506/? [35:14<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1506, loss 3.740463972091675\n",
      "Epoch 8: |          | 1507/? [35:15<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1507, loss 3.6253185272216797\n",
      "Epoch 8: |          | 1508/? [35:16<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1508, loss 3.746140241622925\n",
      "Epoch 8: |          | 1509/? [35:18<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1509, loss 3.763153553009033\n",
      "Epoch 8: |          | 1510/? [35:19<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1510, loss 4.049070835113525\n",
      "Epoch 8: |          | 1511/? [35:20<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1511, loss 3.63423228263855\n",
      "Epoch 8: |          | 1512/? [35:22<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1512, loss 4.195056438446045\n",
      "Epoch 8: |          | 1513/? [35:23<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1513, loss 4.442961692810059\n",
      "Epoch 8: |          | 1514/? [35:24<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1514, loss 3.5074622631073\n",
      "Epoch 8: |          | 1515/? [35:26<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1515, loss 4.369097709655762\n",
      "Epoch 8: |          | 1516/? [35:27<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1516, loss 4.265948295593262\n",
      "Epoch 8: |          | 1517/? [35:29<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1517, loss 3.7126235961914062\n",
      "Epoch 8: |          | 1518/? [35:30<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1518, loss 3.499774932861328\n",
      "Epoch 8: |          | 1519/? [35:31<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1519, loss 4.06399393081665\n",
      "Epoch 8: |          | 1520/? [35:33<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1520, loss 4.092286586761475\n",
      "Epoch 8: |          | 1521/? [35:34<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1521, loss 3.8389759063720703\n",
      "Epoch 8: |          | 1522/? [35:36<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1522, loss 3.603198289871216\n",
      "Epoch 8: |          | 1523/? [35:37<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1523, loss 3.915889024734497\n",
      "Epoch 8: |          | 1524/? [35:38<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1524, loss 3.8033394813537598\n",
      "Epoch 8: |          | 1525/? [35:40<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1525, loss 3.4634220600128174\n",
      "Epoch 8: |          | 1526/? [35:41<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1526, loss 4.04886531829834\n",
      "Epoch 8: |          | 1527/? [35:42<00:00,  0.71it/s, v_num=31]   TRRAINING: Batch 1527, loss 4.065554618835449\n",
      "Epoch 8: |          | 1528/? [35:44<00:00,  0.71it/s, v_num=31]ERROR: Input has inproper shape\n",
      "Epoch 8: |          | 1529/? [35:44<00:00,  0.71it/s, v_num=31]   VALIDATION: Batch 0, loss 4.573963165283203\n",
      "   VALIDATION: Batch 1, loss 3.5588290691375732\n",
      "   VALIDATION: Batch 2, loss 4.73590612411499\n",
      "   VALIDATION: Batch 3, loss 4.431698799133301\n",
      "   VALIDATION: Batch 4, loss 3.993523120880127\n",
      "   VALIDATION: Batch 5, loss 3.6787750720977783\n",
      "   VALIDATION: Batch 6, loss 3.9643332958221436\n",
      "   VALIDATION: Batch 7, loss 4.635593414306641\n",
      "   VALIDATION: Batch 8, loss 4.468750476837158\n",
      "   VALIDATION: Batch 9, loss 4.6470441818237305\n",
      "   VALIDATION: Batch 10, loss 4.287342548370361\n",
      "   VALIDATION: Batch 11, loss 3.9686660766601562\n",
      "   VALIDATION: Batch 12, loss 4.1283392906188965\n",
      "   VALIDATION: Batch 13, loss 4.701785087585449\n",
      "   VALIDATION: Batch 14, loss 3.9159436225891113\n",
      "   VALIDATION: Batch 15, loss 3.923844814300537\n",
      "   VALIDATION: Batch 16, loss 4.630644798278809\n",
      "   VALIDATION: Batch 17, loss 4.280159950256348\n",
      "   VALIDATION: Batch 18, loss 3.519531726837158\n",
      "   VALIDATION: Batch 19, loss 4.402314186096191\n",
      "   VALIDATION: Batch 20, loss 4.7180094718933105\n",
      "   VALIDATION: Batch 21, loss 4.927956581115723\n",
      "   VALIDATION: Batch 22, loss 4.581517219543457\n",
      "   VALIDATION: Batch 23, loss 4.120124816894531\n",
      "   VALIDATION: Batch 24, loss 3.9382922649383545\n",
      "   VALIDATION: Batch 25, loss 4.3491902351379395\n",
      "   VALIDATION: Batch 26, loss 4.565576553344727\n",
      "   VALIDATION: Batch 27, loss 4.44707727432251\n",
      "   VALIDATION: Batch 28, loss 4.219202995300293\n",
      "   VALIDATION: Batch 29, loss 4.399104595184326\n",
      "   VALIDATION: Batch 30, loss 4.049787521362305\n",
      "   VALIDATION: Batch 31, loss 4.350287437438965\n",
      "   VALIDATION: Batch 32, loss 4.902798652648926\n",
      "   VALIDATION: Batch 33, loss 3.119711399078369\n",
      "   VALIDATION: Batch 34, loss 4.314967155456543\n",
      "   VALIDATION: Batch 35, loss 4.554183006286621\n",
      "   VALIDATION: Batch 36, loss 3.811208724975586\n",
      "   VALIDATION: Batch 37, loss 3.8408024311065674\n",
      "   VALIDATION: Batch 38, loss 3.914257526397705\n",
      "   VALIDATION: Batch 39, loss 4.322869300842285\n",
      "   VALIDATION: Batch 40, loss 4.328804969787598\n",
      "   VALIDATION: Batch 41, loss 3.1982803344726562\n",
      "   VALIDATION: Batch 42, loss 4.380770206451416\n",
      "   VALIDATION: Batch 43, loss 4.525271892547607\n",
      "   VALIDATION: Batch 44, loss 4.1373796463012695\n",
      "   VALIDATION: Batch 45, loss 4.524147987365723\n",
      "   VALIDATION: Batch 46, loss 3.6676783561706543\n",
      "   VALIDATION: Batch 47, loss 4.675851345062256\n",
      "   VALIDATION: Batch 48, loss 4.7821221351623535\n",
      "   VALIDATION: Batch 49, loss 4.372885227203369\n",
      "   VALIDATION: Batch 50, loss 4.400352954864502\n",
      "   VALIDATION: Batch 51, loss 4.8475661277771\n",
      "   VALIDATION: Batch 52, loss 4.0266218185424805\n",
      "   VALIDATION: Batch 53, loss 3.8865227699279785\n",
      "   VALIDATION: Batch 54, loss 3.9941811561584473\n",
      "   VALIDATION: Batch 55, loss 4.6863298416137695\n",
      "   VALIDATION: Batch 56, loss 4.107338905334473\n",
      "   VALIDATION: Batch 57, loss 5.705550670623779\n",
      "   VALIDATION: Batch 58, loss 4.240869998931885\n",
      "   VALIDATION: Batch 59, loss 3.896496534347534\n",
      "   VALIDATION: Batch 60, loss 3.390537977218628\n",
      "   VALIDATION: Batch 61, loss 4.260460376739502\n",
      "   VALIDATION: Batch 62, loss 4.246504306793213\n",
      "   VALIDATION: Batch 63, loss 4.804184913635254\n",
      "   VALIDATION: Batch 64, loss 4.576913833618164\n",
      "   VALIDATION: Batch 65, loss 3.7258167266845703\n",
      "   VALIDATION: Batch 66, loss 4.67171573638916\n",
      "   VALIDATION: Batch 67, loss 4.071665287017822\n",
      "   VALIDATION: Batch 68, loss 4.2215895652771\n",
      "   VALIDATION: Batch 69, loss 4.470138072967529\n",
      "   VALIDATION: Batch 70, loss 4.652615547180176\n",
      "   VALIDATION: Batch 71, loss 4.1609697341918945\n",
      "   VALIDATION: Batch 72, loss 5.034101486206055\n",
      "   VALIDATION: Batch 73, loss 3.8396828174591064\n",
      "   VALIDATION: Batch 74, loss 4.434957504272461\n",
      "   VALIDATION: Batch 75, loss 4.408596038818359\n",
      "   VALIDATION: Batch 76, loss 4.258604049682617\n",
      "   VALIDATION: Batch 77, loss 4.567943572998047\n",
      "   VALIDATION: Batch 78, loss 4.3693132400512695\n",
      "   VALIDATION: Batch 79, loss 4.338578224182129\n",
      "   VALIDATION: Batch 80, loss 4.422601699829102\n",
      "   VALIDATION: Batch 81, loss 4.138182640075684\n",
      "   VALIDATION: Batch 82, loss 4.53311014175415\n",
      "   VALIDATION: Batch 83, loss 3.8113174438476562\n",
      "   VALIDATION: Batch 84, loss 4.501341342926025\n",
      "   VALIDATION: Batch 85, loss 4.1418657302856445\n",
      "   VALIDATION: Batch 86, loss 4.227651119232178\n",
      "   VALIDATION: Batch 87, loss 4.090725421905518\n",
      "   VALIDATION: Batch 88, loss 3.6819019317626953\n",
      "   VALIDATION: Batch 89, loss 4.009408473968506\n",
      "   VALIDATION: Batch 90, loss 4.256026744842529\n",
      "   VALIDATION: Batch 91, loss 4.302677631378174\n",
      "   VALIDATION: Batch 92, loss 4.141860008239746\n",
      "   VALIDATION: Batch 93, loss 4.735859394073486\n",
      "   VALIDATION: Batch 94, loss 4.203551292419434\n",
      "   VALIDATION: Batch 95, loss 3.7097887992858887\n",
      "   VALIDATION: Batch 96, loss 4.1618852615356445\n",
      "   VALIDATION: Batch 97, loss 3.889031171798706\n",
      "   VALIDATION: Batch 98, loss 4.5147385597229\n",
      "   VALIDATION: Batch 99, loss 4.63102912902832\n",
      "   VALIDATION: Batch 100, loss 4.942314147949219\n",
      "   VALIDATION: Batch 101, loss 3.554887294769287\n",
      "   VALIDATION: Batch 102, loss 4.963684558868408\n",
      "   VALIDATION: Batch 103, loss 4.902725696563721\n",
      "   VALIDATION: Batch 104, loss 3.8446602821350098\n",
      "   VALIDATION: Batch 105, loss 4.28336238861084\n",
      "   VALIDATION: Batch 106, loss 4.1818718910217285\n",
      "   VALIDATION: Batch 107, loss 4.3155412673950195\n",
      "   VALIDATION: Batch 108, loss 3.9936511516571045\n",
      "   VALIDATION: Batch 109, loss 4.606545448303223\n",
      "   VALIDATION: Batch 110, loss 4.2830328941345215\n",
      "   VALIDATION: Batch 111, loss 4.618608474731445\n",
      "   VALIDATION: Batch 112, loss 5.5032124519348145\n",
      "   VALIDATION: Batch 113, loss 4.788658618927002\n",
      "   VALIDATION: Batch 114, loss 4.5852837562561035\n",
      "   VALIDATION: Batch 115, loss 4.037039279937744\n",
      "   VALIDATION: Batch 116, loss 3.874844789505005\n",
      "   VALIDATION: Batch 117, loss 4.523616313934326\n",
      "   VALIDATION: Batch 118, loss 4.726835250854492\n",
      "   VALIDATION: Batch 119, loss 3.9086716175079346\n",
      "   VALIDATION: Batch 120, loss 3.5341312885284424\n",
      "   VALIDATION: Batch 121, loss 3.7818379402160645\n",
      "   VALIDATION: Batch 122, loss 4.159475803375244\n",
      "   VALIDATION: Batch 123, loss 4.184510707855225\n",
      "   VALIDATION: Batch 124, loss 3.59147572517395\n",
      "   VALIDATION: Batch 125, loss 4.206786632537842\n",
      "   VALIDATION: Batch 126, loss 4.424215316772461\n",
      "   VALIDATION: Batch 127, loss 4.182595252990723\n",
      "   VALIDATION: Batch 128, loss 4.345493316650391\n",
      "   VALIDATION: Batch 129, loss 3.96771240234375\n",
      "   VALIDATION: Batch 130, loss 3.6399307250976562\n",
      "   VALIDATION: Batch 131, loss 3.6483466625213623\n",
      "   VALIDATION: Batch 132, loss 4.289206504821777\n",
      "   VALIDATION: Batch 133, loss 4.454622745513916\n",
      "   VALIDATION: Batch 134, loss 4.3187055587768555\n",
      "   VALIDATION: Batch 135, loss 4.592162609100342\n",
      "   VALIDATION: Batch 136, loss 4.690117359161377\n",
      "   VALIDATION: Batch 137, loss 4.503903388977051\n",
      "   VALIDATION: Batch 138, loss 4.254653453826904\n",
      "   VALIDATION: Batch 139, loss 4.618941307067871\n",
      "   VALIDATION: Batch 140, loss 3.7507147789001465\n",
      "   VALIDATION: Batch 141, loss 4.686971187591553\n",
      "   VALIDATION: Batch 142, loss 3.429476499557495\n",
      "   VALIDATION: Batch 143, loss 4.21060848236084\n",
      "   VALIDATION: Batch 144, loss 4.490625381469727\n",
      "   VALIDATION: Batch 145, loss 4.245298862457275\n",
      "   VALIDATION: Batch 146, loss 4.08621883392334\n",
      "   VALIDATION: Batch 147, loss 4.414650917053223\n",
      "   VALIDATION: Batch 148, loss 4.49412202835083\n",
      "   VALIDATION: Batch 149, loss 5.009600639343262\n",
      "   VALIDATION: Batch 150, loss 4.558187007904053\n",
      "   VALIDATION: Batch 151, loss 4.873656272888184\n",
      "   VALIDATION: Batch 152, loss 4.222233295440674\n",
      "   VALIDATION: Batch 153, loss 4.477733612060547\n",
      "   VALIDATION: Batch 154, loss 4.29965877532959\n",
      "   VALIDATION: Batch 155, loss 4.0819010734558105\n",
      "   VALIDATION: Batch 156, loss 4.732747554779053\n",
      "   VALIDATION: Batch 157, loss 4.386096000671387\n",
      "   VALIDATION: Batch 158, loss 3.8351504802703857\n",
      "   VALIDATION: Batch 159, loss 4.288686752319336\n",
      "   VALIDATION: Batch 160, loss 4.644120693206787\n",
      "   VALIDATION: Batch 161, loss 4.75824499130249\n",
      "   VALIDATION: Batch 162, loss 4.245611190795898\n",
      "   VALIDATION: Batch 163, loss 3.697183609008789\n",
      "   VALIDATION: Batch 164, loss 4.232666969299316\n",
      "   VALIDATION: Batch 165, loss 4.668761730194092\n",
      "   VALIDATION: Batch 166, loss 4.098412036895752\n",
      "   VALIDATION: Batch 167, loss 4.548816680908203\n",
      "   VALIDATION: Batch 168, loss 3.408398151397705\n",
      "   VALIDATION: Batch 169, loss 4.108901023864746\n",
      "   VALIDATION: Batch 170, loss 4.2597150802612305\n",
      "   VALIDATION: Batch 171, loss 4.4237565994262695\n",
      "   VALIDATION: Batch 172, loss 4.265996932983398\n",
      "   VALIDATION: Batch 173, loss 4.171440124511719\n",
      "   VALIDATION: Batch 174, loss 4.586554527282715\n",
      "   VALIDATION: Batch 175, loss 4.368201732635498\n",
      "   VALIDATION: Batch 176, loss 4.1407976150512695\n",
      "   VALIDATION: Batch 177, loss 4.060406684875488\n",
      "   VALIDATION: Batch 178, loss 5.183404445648193\n",
      "   VALIDATION: Batch 179, loss 4.45664644241333\n",
      "   VALIDATION: Batch 180, loss 3.930943012237549\n",
      "   VALIDATION: Batch 181, loss 4.084717750549316\n",
      "   VALIDATION: Batch 182, loss 4.413000583648682\n",
      "   VALIDATION: Batch 183, loss 3.478095531463623\n",
      "   VALIDATION: Batch 184, loss 3.234696626663208\n",
      "   VALIDATION: Batch 185, loss 4.005923271179199\n",
      "   VALIDATION: Batch 186, loss 3.87296724319458\n",
      "   VALIDATION: Batch 187, loss 4.086021900177002\n",
      "   VALIDATION: Batch 188, loss 4.501556396484375\n",
      "   VALIDATION: Batch 189, loss 3.8465094566345215\n",
      "   VALIDATION: Batch 190, loss 3.946561098098755\n",
      "   VALIDATION: Batch 191, loss 4.425379753112793\n",
      "   VALIDATION: Batch 192, loss 4.715365409851074\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 9: |          | 0/? [00:00<?, ?it/s, v_num=31]              TRRAINING: Batch 0, loss 4.052762985229492\n",
      "Epoch 9: |          | 1/? [00:01<00:00,  0.56it/s, v_num=31]   TRRAINING: Batch 1, loss 3.6337649822235107\n",
      "Epoch 9: |          | 2/? [00:03<00:00,  0.61it/s, v_num=31]   TRRAINING: Batch 2, loss 3.70733904838562\n",
      "Epoch 9: |          | 3/? [00:04<00:00,  0.63it/s, v_num=31]   TRRAINING: Batch 3, loss 3.388883113861084\n",
      "Epoch 9: |          | 4/? [00:06<00:00,  0.64it/s, v_num=31]   TRRAINING: Batch 4, loss 3.7998859882354736\n",
      "Epoch 9: |          | 5/? [00:07<00:00,  0.67it/s, v_num=31]   TRRAINING: Batch 5, loss 4.568976879119873\n",
      "Epoch 9: |          | 6/? [00:08<00:00,  0.67it/s, v_num=31]   TRRAINING: Batch 6, loss 4.032947063446045\n",
      "Epoch 9: |          | 7/? [00:10<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 7, loss 3.3814499378204346\n",
      "Epoch 9: |          | 8/? [00:11<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 8, loss 3.4898078441619873\n",
      "Epoch 9: |          | 9/? [00:13<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 9, loss 3.7783355712890625\n",
      "Epoch 9: |          | 10/? [00:14<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 10, loss 4.024214744567871\n",
      "Epoch 9: |          | 11/? [00:16<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 11, loss 3.9441323280334473\n",
      "Epoch 9: |          | 12/? [00:17<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 12, loss 4.397500038146973\n",
      "Epoch 9: |          | 13/? [00:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 13, loss 3.89564847946167\n",
      "Epoch 9: |          | 14/? [00:20<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 14, loss 4.113540172576904\n",
      "Epoch 9: |          | 15/? [00:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 15, loss 3.3871073722839355\n",
      "Epoch 9: |          | 16/? [00:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 16, loss 3.2201874256134033\n",
      "Epoch 9: |          | 17/? [00:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 17, loss 4.353357791900635\n",
      "Epoch 9: |          | 18/? [00:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 18, loss 3.8483989238739014\n",
      "Epoch 9: |          | 19/? [00:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 19, loss 3.6472747325897217\n",
      "Epoch 9: |          | 20/? [00:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 20, loss 3.904961109161377\n",
      "Epoch 9: |          | 21/? [00:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 21, loss 4.043981552124023\n",
      "Epoch 9: |          | 22/? [00:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 22, loss 3.9528002738952637\n",
      "Epoch 9: |          | 23/? [00:33<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 23, loss 3.3315048217773438\n",
      "Epoch 9: |          | 24/? [00:35<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 24, loss 3.9563941955566406\n",
      "Epoch 9: |          | 25/? [00:36<00:00,  0.68it/s, v_num=31]   TRRAINING: Batch 25, loss 3.793020725250244\n",
      "Epoch 9: |          | 26/? [00:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 26, loss 3.547208070755005\n",
      "Epoch 9: |          | 27/? [00:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 27, loss 3.501491069793701\n",
      "Epoch 9: |          | 28/? [00:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 28, loss 4.3404951095581055\n",
      "Epoch 9: |          | 29/? [00:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 29, loss 3.910067319869995\n",
      "Epoch 9: |          | 30/? [00:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 30, loss 3.7780914306640625\n",
      "Epoch 9: |          | 31/? [00:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 31, loss 4.340973377227783\n",
      "Epoch 9: |          | 32/? [00:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 32, loss 3.898346424102783\n",
      "Epoch 9: |          | 33/? [00:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 33, loss 3.74666166305542\n",
      "Epoch 9: |          | 34/? [00:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 34, loss 3.7157859802246094\n",
      "Epoch 9: |          | 35/? [00:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 35, loss 3.18461537361145\n",
      "Epoch 9: |          | 36/? [00:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 36, loss 3.941483736038208\n",
      "Epoch 9: |          | 37/? [00:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 37, loss 4.095177173614502\n",
      "Epoch 9: |          | 38/? [00:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 38, loss 4.4098310470581055\n",
      "Epoch 9: |          | 39/? [00:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 39, loss 4.287407875061035\n",
      "Epoch 9: |          | 40/? [00:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 40, loss 3.7104995250701904\n",
      "Epoch 9: |          | 41/? [00:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 41, loss 3.7993202209472656\n",
      "Epoch 9: |          | 42/? [01:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 42, loss 3.5692901611328125\n",
      "Epoch 9: |          | 43/? [01:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 43, loss 3.746314287185669\n",
      "Epoch 9: |          | 44/? [01:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 44, loss 2.891526937484741\n",
      "Epoch 9: |          | 45/? [01:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 45, loss 2.4404685497283936\n",
      "Epoch 9: |          | 46/? [01:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 46, loss 4.295163154602051\n",
      "Epoch 9: |          | 47/? [01:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 47, loss 3.5287699699401855\n",
      "Epoch 9: |          | 48/? [01:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 48, loss 3.403768539428711\n",
      "Epoch 9: |          | 49/? [01:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 49, loss 3.8638243675231934\n",
      "Epoch 9: |          | 50/? [01:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 50, loss 3.693617343902588\n",
      "Epoch 9: |          | 51/? [01:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 51, loss 3.670912504196167\n",
      "Epoch 9: |          | 52/? [01:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 52, loss 4.374785423278809\n",
      "Epoch 9: |          | 53/? [01:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 53, loss 3.9995079040527344\n",
      "Epoch 9: |          | 54/? [01:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 54, loss 3.7805676460266113\n",
      "Epoch 9: |          | 55/? [01:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 55, loss 3.8382599353790283\n",
      "Epoch 9: |          | 56/? [01:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 56, loss 3.9748358726501465\n",
      "Epoch 9: |          | 57/? [01:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 57, loss 3.8725991249084473\n",
      "Epoch 9: |          | 58/? [01:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 58, loss 5.037964344024658\n",
      "Epoch 9: |          | 59/? [01:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 59, loss 4.01699161529541\n",
      "Epoch 9: |          | 60/? [01:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 60, loss 4.165745735168457\n",
      "Epoch 9: |          | 61/? [01:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 61, loss 4.115687370300293\n",
      "Epoch 9: |          | 62/? [01:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 62, loss 3.7508978843688965\n",
      "Epoch 9: |          | 63/? [01:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 63, loss 3.939378261566162\n",
      "Epoch 9: |          | 64/? [01:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 64, loss 3.7805259227752686\n",
      "Epoch 9: |          | 65/? [01:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 65, loss 3.8080272674560547\n",
      "Epoch 9: |          | 66/? [01:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 66, loss 3.2567265033721924\n",
      "Epoch 9: |          | 67/? [01:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 67, loss 3.9341301918029785\n",
      "Epoch 9: |          | 68/? [01:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 68, loss 3.9381814002990723\n",
      "Epoch 9: |          | 69/? [01:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 69, loss 3.8817801475524902\n",
      "Epoch 9: |          | 70/? [01:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 70, loss 3.5368893146514893\n",
      "Epoch 9: |          | 71/? [01:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 71, loss 3.505206346511841\n",
      "Epoch 9: |          | 72/? [01:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 72, loss 3.6941585540771484\n",
      "Epoch 9: |          | 73/? [01:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 73, loss 3.8804569244384766\n",
      "Epoch 9: |          | 74/? [01:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 74, loss 3.6697444915771484\n",
      "Epoch 9: |          | 75/? [01:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 75, loss 3.7806880474090576\n",
      "Epoch 9: |          | 76/? [01:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 76, loss 3.7931220531463623\n",
      "Epoch 9: |          | 77/? [01:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 77, loss 3.802316188812256\n",
      "Epoch 9: |          | 78/? [01:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 78, loss 3.656899929046631\n",
      "Epoch 9: |          | 79/? [01:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 79, loss 3.8246169090270996\n",
      "Epoch 9: |          | 80/? [01:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 80, loss 3.742158889770508\n",
      "Epoch 9: |          | 81/? [01:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 81, loss 3.2305996417999268\n",
      "Epoch 9: |          | 82/? [01:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 82, loss 4.008009433746338\n",
      "Epoch 9: |          | 83/? [01:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 83, loss 3.5030951499938965\n",
      "Epoch 9: |          | 84/? [02:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 84, loss 3.3547611236572266\n",
      "Epoch 9: |          | 85/? [02:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 85, loss 3.1972739696502686\n",
      "Epoch 9: |          | 86/? [02:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 86, loss 3.37565541267395\n",
      "Epoch 9: |          | 87/? [02:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 87, loss 3.540513277053833\n",
      "Epoch 9: |          | 88/? [02:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 88, loss 4.181723594665527\n",
      "Epoch 9: |          | 89/? [02:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 89, loss 4.155352592468262\n",
      "Epoch 9: |          | 90/? [02:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 90, loss 3.8931808471679688\n",
      "Epoch 9: |          | 91/? [02:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 91, loss 3.59991455078125\n",
      "Epoch 9: |          | 92/? [02:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 92, loss 4.0680365562438965\n",
      "Epoch 9: |          | 93/? [02:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 93, loss 4.2147674560546875\n",
      "Epoch 9: |          | 94/? [02:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 94, loss 4.067713260650635\n",
      "Epoch 9: |          | 95/? [02:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 95, loss 3.8212974071502686\n",
      "Epoch 9: |          | 96/? [02:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 96, loss 3.647303819656372\n",
      "Epoch 9: |          | 97/? [02:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 97, loss 3.4281013011932373\n",
      "Epoch 9: |          | 98/? [02:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 98, loss 3.94929575920105\n",
      "Epoch 9: |          | 99/? [02:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 99, loss 4.1346049308776855\n",
      "Epoch 9: |          | 100/? [02:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 100, loss 4.055668830871582\n",
      "Epoch 9: |          | 101/? [02:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 101, loss 3.733311891555786\n",
      "Epoch 9: |          | 102/? [02:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 102, loss 3.754021406173706\n",
      "Epoch 9: |          | 103/? [02:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 103, loss 3.5322136878967285\n",
      "Epoch 9: |          | 104/? [02:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 104, loss 3.865849018096924\n",
      "Epoch 9: |          | 105/? [02:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 105, loss 3.817920207977295\n",
      "Epoch 9: |          | 106/? [02:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 106, loss 3.9632790088653564\n",
      "Epoch 9: |          | 107/? [02:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 107, loss 3.9636433124542236\n",
      "Epoch 9: |          | 108/? [02:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 108, loss 3.9304802417755127\n",
      "Epoch 9: |          | 109/? [02:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 109, loss 3.684799909591675\n",
      "Epoch 9: |          | 110/? [02:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 110, loss 3.835692882537842\n",
      "Epoch 9: |          | 111/? [02:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 111, loss 4.4559102058410645\n",
      "Epoch 9: |          | 112/? [02:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 112, loss 3.170976400375366\n",
      "Epoch 9: |          | 113/? [02:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 113, loss 2.770731210708618\n",
      "Epoch 9: |          | 114/? [02:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 114, loss 4.040469646453857\n",
      "Epoch 9: |          | 115/? [02:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 115, loss 4.192478656768799\n",
      "Epoch 9: |          | 116/? [02:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 116, loss 3.3446030616760254\n",
      "Epoch 9: |          | 117/? [02:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 117, loss 3.305948257446289\n",
      "Epoch 9: |          | 118/? [02:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 118, loss 4.109868049621582\n",
      "Epoch 9: |          | 119/? [02:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 119, loss 4.364041328430176\n",
      "Epoch 9: |          | 120/? [02:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 120, loss 4.139781951904297\n",
      "Epoch 9: |          | 121/? [02:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 121, loss 3.8422482013702393\n",
      "Epoch 9: |          | 122/? [02:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 122, loss 3.325965166091919\n",
      "Epoch 9: |          | 123/? [02:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 123, loss 3.810598373413086\n",
      "Epoch 9: |          | 124/? [02:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 124, loss 3.9398300647735596\n",
      "Epoch 9: |          | 125/? [02:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 125, loss 3.5797247886657715\n",
      "Epoch 9: |          | 126/? [03:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 126, loss 4.035947322845459\n",
      "Epoch 9: |          | 127/? [03:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 127, loss 4.18033504486084\n",
      "Epoch 9: |          | 128/? [03:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 128, loss 3.3294291496276855\n",
      "Epoch 9: |          | 129/? [03:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 129, loss 3.9042999744415283\n",
      "Epoch 9: |          | 130/? [03:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 130, loss 3.0521507263183594\n",
      "Epoch 9: |          | 131/? [03:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 131, loss 3.837703227996826\n",
      "Epoch 9: |          | 132/? [03:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 132, loss 3.8921539783477783\n",
      "Epoch 9: |          | 133/? [03:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 133, loss 3.7652957439422607\n",
      "Epoch 9: |          | 134/? [03:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 134, loss 3.9489893913269043\n",
      "Epoch 9: |          | 135/? [03:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 135, loss 4.031594276428223\n",
      "Epoch 9: |          | 136/? [03:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 136, loss 4.096993446350098\n",
      "Epoch 9: |          | 137/? [03:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 137, loss 3.105832576751709\n",
      "Epoch 9: |          | 138/? [03:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 138, loss 3.6756210327148438\n",
      "Epoch 9: |          | 139/? [03:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 139, loss 4.115204811096191\n",
      "Epoch 9: |          | 140/? [03:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 140, loss 3.388303279876709\n",
      "Epoch 9: |          | 141/? [03:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 141, loss 3.4796504974365234\n",
      "Epoch 9: |          | 142/? [03:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 142, loss 4.893658638000488\n",
      "Epoch 9: |          | 143/? [03:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 143, loss 4.566534519195557\n",
      "Epoch 9: |          | 144/? [03:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 144, loss 3.8375887870788574\n",
      "Epoch 9: |          | 145/? [03:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 145, loss 3.3270649909973145\n",
      "Epoch 9: |          | 146/? [03:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 146, loss 3.4788761138916016\n",
      "Epoch 9: |          | 147/? [03:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 147, loss 3.7970070838928223\n",
      "Epoch 9: |          | 148/? [03:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 148, loss 3.554079532623291\n",
      "Epoch 9: |          | 149/? [03:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 149, loss 3.1050312519073486\n",
      "Epoch 9: |          | 150/? [03:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 150, loss 3.957300901412964\n",
      "Epoch 9: |          | 151/? [03:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 151, loss 4.077111721038818\n",
      "Epoch 9: |          | 152/? [03:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 152, loss 3.989180326461792\n",
      "Epoch 9: |          | 153/? [03:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 153, loss 3.12803316116333\n",
      "Epoch 9: |          | 154/? [03:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 154, loss 4.3800554275512695\n",
      "Epoch 9: |          | 155/? [03:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 155, loss 3.886931896209717\n",
      "Epoch 9: |          | 156/? [03:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 156, loss 3.3043532371520996\n",
      "Epoch 9: |          | 157/? [03:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 157, loss 3.9271349906921387\n",
      "Epoch 9: |          | 158/? [03:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 158, loss 4.044946670532227\n",
      "Epoch 9: |          | 159/? [03:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 159, loss 3.7628941535949707\n",
      "Epoch 9: |          | 160/? [03:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 160, loss 3.541355848312378\n",
      "Epoch 9: |          | 161/? [03:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 161, loss 3.9083704948425293\n",
      "Epoch 9: |          | 162/? [03:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 162, loss 3.9258837699890137\n",
      "Epoch 9: |          | 163/? [03:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 163, loss 3.097761392593384\n",
      "Epoch 9: |          | 164/? [03:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 164, loss 3.523578643798828\n",
      "Epoch 9: |          | 165/? [03:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 165, loss 4.319032192230225\n",
      "Epoch 9: |          | 166/? [03:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 166, loss 3.9584450721740723\n",
      "Epoch 9: |          | 167/? [03:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 167, loss 4.070416450500488\n",
      "Epoch 9: |          | 168/? [04:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 168, loss 3.665369749069214\n",
      "Epoch 9: |          | 169/? [04:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 169, loss 3.3349449634552\n",
      "Epoch 9: |          | 170/? [04:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 170, loss 3.52496075630188\n",
      "Epoch 9: |          | 171/? [04:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 171, loss 3.881669521331787\n",
      "Epoch 9: |          | 172/? [04:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 172, loss 3.7262985706329346\n",
      "Epoch 9: |          | 173/? [04:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 173, loss 4.293580055236816\n",
      "Epoch 9: |          | 174/? [04:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 174, loss 3.9004769325256348\n",
      "Epoch 9: |          | 175/? [04:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 175, loss 4.402783393859863\n",
      "Epoch 9: |          | 176/? [04:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 176, loss 3.635667324066162\n",
      "Epoch 9: |          | 177/? [04:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 177, loss 3.702035903930664\n",
      "Epoch 9: |          | 178/? [04:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 178, loss 3.522742509841919\n",
      "Epoch 9: |          | 179/? [04:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 179, loss 4.195480823516846\n",
      "Epoch 9: |          | 180/? [04:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 180, loss 3.741638660430908\n",
      "Epoch 9: |          | 181/? [04:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 181, loss 3.532215118408203\n",
      "Epoch 9: |          | 182/? [04:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 182, loss 3.9327304363250732\n",
      "Epoch 9: |          | 183/? [04:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 183, loss 3.426004409790039\n",
      "Epoch 9: |          | 184/? [04:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 184, loss 3.6488232612609863\n",
      "Epoch 9: |          | 185/? [04:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 185, loss 4.166428565979004\n",
      "Epoch 9: |          | 186/? [04:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 186, loss 3.6694958209991455\n",
      "Epoch 9: |          | 187/? [04:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 187, loss 4.1637115478515625\n",
      "Epoch 9: |          | 188/? [04:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 188, loss 3.5563082695007324\n",
      "Epoch 9: |          | 189/? [04:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 189, loss 4.203145980834961\n",
      "Epoch 9: |          | 190/? [04:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 190, loss 3.722407102584839\n",
      "Epoch 9: |          | 191/? [04:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 191, loss 4.480240821838379\n",
      "Epoch 9: |          | 192/? [04:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 192, loss 4.2244343757629395\n",
      "Epoch 9: |          | 193/? [04:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 193, loss 3.5977864265441895\n",
      "Epoch 9: |          | 194/? [04:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 194, loss 3.484394073486328\n",
      "Epoch 9: |          | 195/? [04:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 195, loss 4.164875030517578\n",
      "Epoch 9: |          | 196/? [04:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 196, loss 4.043249607086182\n",
      "Epoch 9: |          | 197/? [04:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 197, loss 3.910520553588867\n",
      "Epoch 9: |          | 198/? [04:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 198, loss 3.2294399738311768\n",
      "Epoch 9: |          | 199/? [04:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 199, loss 4.153012752532959\n",
      "Epoch 9: |          | 200/? [04:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 200, loss 3.6216628551483154\n",
      "Epoch 9: |          | 201/? [04:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 201, loss 3.8489508628845215\n",
      "Epoch 9: |          | 202/? [04:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 202, loss 4.010836601257324\n",
      "Epoch 9: |          | 203/? [04:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 203, loss 3.654709577560425\n",
      "Epoch 9: |          | 204/? [04:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 204, loss 3.6974244117736816\n",
      "Epoch 9: |          | 205/? [04:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 205, loss 3.5760409832000732\n",
      "Epoch 9: |          | 206/? [04:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 206, loss 3.406980514526367\n",
      "Epoch 9: |          | 207/? [04:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 207, loss 3.9524734020233154\n",
      "Epoch 9: |          | 208/? [04:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 208, loss 3.8366589546203613\n",
      "Epoch 9: |          | 209/? [04:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 209, loss 3.672560930252075\n",
      "Epoch 9: |          | 210/? [05:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 210, loss 4.3745317459106445\n",
      "Epoch 9: |          | 211/? [05:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 211, loss 3.707939624786377\n",
      "Epoch 9: |          | 212/? [05:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 212, loss 3.9777443408966064\n",
      "Epoch 9: |          | 213/? [05:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 213, loss 3.8065788745880127\n",
      "Epoch 9: |          | 214/? [05:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 214, loss 3.676074504852295\n",
      "Epoch 9: |          | 215/? [05:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 215, loss 3.347782850265503\n",
      "Epoch 9: |          | 216/? [05:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 216, loss 3.9703476428985596\n",
      "Epoch 9: |          | 217/? [05:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 217, loss 3.913548231124878\n",
      "Epoch 9: |          | 218/? [05:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 218, loss 3.9239742755889893\n",
      "Epoch 9: |          | 219/? [05:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 219, loss 3.8872578144073486\n",
      "Epoch 9: |          | 220/? [05:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 220, loss 3.945279598236084\n",
      "Epoch 9: |          | 221/? [05:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 221, loss 3.7684695720672607\n",
      "Epoch 9: |          | 222/? [05:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 222, loss 3.058955430984497\n",
      "Epoch 9: |          | 223/? [05:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 223, loss 4.010582447052002\n",
      "Epoch 9: |          | 224/? [05:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 224, loss 4.099708557128906\n",
      "Epoch 9: |          | 225/? [05:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 225, loss 3.8400397300720215\n",
      "Epoch 9: |          | 226/? [05:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 226, loss 3.774209976196289\n",
      "Epoch 9: |          | 227/? [05:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 227, loss 4.109065055847168\n",
      "Epoch 9: |          | 228/? [05:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 228, loss 3.824185848236084\n",
      "Epoch 9: |          | 229/? [05:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 229, loss 3.871065855026245\n",
      "Epoch 9: |          | 230/? [05:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 230, loss 3.8298630714416504\n",
      "Epoch 9: |          | 231/? [05:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 231, loss 3.77899432182312\n",
      "Epoch 9: |          | 232/? [05:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 232, loss 3.485966205596924\n",
      "Epoch 9: |          | 233/? [05:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 233, loss 4.130244255065918\n",
      "Epoch 9: |          | 234/? [05:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 234, loss 4.161012172698975\n",
      "Epoch 9: |          | 235/? [05:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 235, loss 4.249505043029785\n",
      "Epoch 9: |          | 236/? [05:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 236, loss 3.6820011138916016\n",
      "Epoch 9: |          | 237/? [05:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 237, loss 3.8293910026550293\n",
      "Epoch 9: |          | 238/? [05:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 238, loss 4.071683883666992\n",
      "Epoch 9: |          | 239/? [05:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 239, loss 3.655971050262451\n",
      "Epoch 9: |          | 240/? [05:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 240, loss 3.222583770751953\n",
      "Epoch 9: |          | 241/? [05:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 241, loss 3.832071304321289\n",
      "Epoch 9: |          | 242/? [05:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 242, loss 4.165364742279053\n",
      "Epoch 9: |          | 243/? [05:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 243, loss 3.075385570526123\n",
      "Epoch 9: |          | 244/? [05:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 244, loss 3.4898693561553955\n",
      "Epoch 9: |          | 245/? [05:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 245, loss 3.8167471885681152\n",
      "Epoch 9: |          | 246/? [05:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 246, loss 3.8997719287872314\n",
      "Epoch 9: |          | 247/? [05:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 247, loss 3.9344382286071777\n",
      "Epoch 9: |          | 248/? [05:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 248, loss 3.487056255340576\n",
      "Epoch 9: |          | 249/? [06:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 249, loss 3.343883514404297\n",
      "Epoch 9: |          | 250/? [06:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 250, loss 3.9405484199523926\n",
      "Epoch 9: |          | 251/? [06:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 251, loss 3.914768934249878\n",
      "Epoch 9: |          | 252/? [06:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 252, loss 3.7257251739501953\n",
      "Epoch 9: |          | 253/? [06:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 253, loss 4.500334739685059\n",
      "Epoch 9: |          | 254/? [06:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 254, loss 4.189902305603027\n",
      "Epoch 9: |          | 255/? [06:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 255, loss 3.80224871635437\n",
      "Epoch 9: |          | 256/? [06:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 256, loss 4.696990013122559\n",
      "Epoch 9: |          | 257/? [06:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 257, loss 3.715597152709961\n",
      "Epoch 9: |          | 258/? [06:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 258, loss 3.8235201835632324\n",
      "Epoch 9: |          | 259/? [06:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 259, loss 3.694096803665161\n",
      "Epoch 9: |          | 260/? [06:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 260, loss 3.6099987030029297\n",
      "Epoch 9: |          | 261/? [06:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 261, loss 3.4158873558044434\n",
      "Epoch 9: |          | 262/? [06:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 262, loss 4.102039813995361\n",
      "Epoch 9: |          | 263/? [06:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 263, loss 3.7934677600860596\n",
      "Epoch 9: |          | 264/? [06:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 264, loss 3.934070587158203\n",
      "Epoch 9: |          | 265/? [06:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 265, loss 3.3693737983703613\n",
      "Epoch 9: |          | 266/? [06:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 266, loss 3.852832078933716\n",
      "Epoch 9: |          | 267/? [06:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 267, loss 3.392462968826294\n",
      "Epoch 9: |          | 268/? [06:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 268, loss 3.7872867584228516\n",
      "Epoch 9: |          | 269/? [06:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 269, loss 4.156390190124512\n",
      "Epoch 9: |          | 270/? [06:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 270, loss 3.7450966835021973\n",
      "Epoch 9: |          | 271/? [06:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 271, loss 4.007453918457031\n",
      "Epoch 9: |          | 272/? [06:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 272, loss 4.310089111328125\n",
      "Epoch 9: |          | 273/? [06:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 273, loss 3.478041410446167\n",
      "Epoch 9: |          | 274/? [06:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 274, loss 4.3503594398498535\n",
      "Epoch 9: |          | 275/? [06:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 275, loss 3.986327648162842\n",
      "Epoch 9: |          | 276/? [06:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 276, loss 3.2620882987976074\n",
      "Epoch 9: |          | 277/? [06:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 277, loss 3.9464669227600098\n",
      "Epoch 9: |          | 278/? [06:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 278, loss 2.9830451011657715\n",
      "Epoch 9: |          | 279/? [06:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 279, loss 3.671757936477661\n",
      "Epoch 9: |          | 280/? [06:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 280, loss 3.396678924560547\n",
      "Epoch 9: |          | 281/? [06:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 281, loss 4.057692527770996\n",
      "Epoch 9: |          | 282/? [06:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 282, loss 3.5764644145965576\n",
      "Epoch 9: |          | 283/? [06:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 283, loss 3.7100167274475098\n",
      "Epoch 9: |          | 284/? [06:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 284, loss 3.6219468116760254\n",
      "Epoch 9: |          | 285/? [06:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 285, loss 3.1306004524230957\n",
      "Epoch 9: |          | 286/? [06:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 286, loss 3.70110821723938\n",
      "Epoch 9: |          | 287/? [06:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 287, loss 3.455451250076294\n",
      "Epoch 9: |          | 288/? [06:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 288, loss 3.3791210651397705\n",
      "Epoch 9: |          | 289/? [06:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 289, loss 3.265925645828247\n",
      "Epoch 9: |          | 290/? [06:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 290, loss 2.898846387863159\n",
      "Epoch 9: |          | 291/? [07:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 291, loss 3.9028213024139404\n",
      "Epoch 9: |          | 292/? [07:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 292, loss 3.563896656036377\n",
      "Epoch 9: |          | 293/? [07:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 293, loss 3.8575260639190674\n",
      "Epoch 9: |          | 294/? [07:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 294, loss 3.7239749431610107\n",
      "Epoch 9: |          | 295/? [07:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 295, loss 4.027413368225098\n",
      "Epoch 9: |          | 296/? [07:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 296, loss 3.517266035079956\n",
      "Epoch 9: |          | 297/? [07:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 297, loss 4.194178581237793\n",
      "Epoch 9: |          | 298/? [07:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 298, loss 3.9773125648498535\n",
      "Epoch 9: |          | 299/? [07:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 299, loss 4.367165565490723\n",
      "Epoch 9: |          | 300/? [07:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 300, loss 3.779134750366211\n",
      "Epoch 9: |          | 301/? [07:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 301, loss 3.6445720195770264\n",
      "Epoch 9: |          | 302/? [07:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 302, loss 4.022617816925049\n",
      "Epoch 9: |          | 303/? [07:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 303, loss 3.833998203277588\n",
      "Epoch 9: |          | 304/? [07:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 304, loss 4.009649753570557\n",
      "Epoch 9: |          | 305/? [07:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 305, loss 4.055820941925049\n",
      "Epoch 9: |          | 306/? [07:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 306, loss 3.7320868968963623\n",
      "Epoch 9: |          | 307/? [07:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 307, loss 3.9643592834472656\n",
      "Epoch 9: |          | 308/? [07:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 308, loss 4.129164695739746\n",
      "Epoch 9: |          | 309/? [07:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 309, loss 3.7406375408172607\n",
      "Epoch 9: |          | 310/? [07:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 310, loss 4.128861904144287\n",
      "Epoch 9: |          | 311/? [07:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 311, loss 3.763917922973633\n",
      "Epoch 9: |          | 312/? [07:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 312, loss 3.736759662628174\n",
      "Epoch 9: |          | 313/? [07:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 313, loss 3.603905200958252\n",
      "Epoch 9: |          | 314/? [07:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 314, loss 3.9121246337890625\n",
      "Epoch 9: |          | 315/? [07:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 315, loss 3.593404769897461\n",
      "Epoch 9: |          | 316/? [07:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 316, loss 4.089022636413574\n",
      "Epoch 9: |          | 317/? [07:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 317, loss 3.9159610271453857\n",
      "Epoch 9: |          | 318/? [07:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 318, loss 4.072635650634766\n",
      "Epoch 9: |          | 319/? [07:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 319, loss 3.3141772747039795\n",
      "Epoch 9: |          | 320/? [07:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 320, loss 3.802285671234131\n",
      "Epoch 9: |          | 321/? [07:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 321, loss 3.6168875694274902\n",
      "Epoch 9: |          | 322/? [07:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 322, loss 4.204336643218994\n",
      "Epoch 9: |          | 323/? [07:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 323, loss 4.1605448722839355\n",
      "Epoch 9: |          | 324/? [07:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 324, loss 3.90269136428833\n",
      "Epoch 9: |          | 325/? [07:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 325, loss 4.266959190368652\n",
      "Epoch 9: |          | 326/? [07:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 326, loss 3.8946681022644043\n",
      "Epoch 9: |          | 327/? [07:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 327, loss 3.579810619354248\n",
      "Epoch 9: |          | 328/? [07:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 328, loss 3.383871555328369\n",
      "Epoch 9: |          | 329/? [07:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 329, loss 3.986372709274292\n",
      "Epoch 9: |          | 330/? [07:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 330, loss 4.468433856964111\n",
      "Epoch 9: |          | 331/? [07:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 331, loss 2.7929344177246094\n",
      "Epoch 9: |          | 332/? [08:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 332, loss 3.8137736320495605\n",
      "Epoch 9: |          | 333/? [08:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 333, loss 3.7197296619415283\n",
      "Epoch 9: |          | 334/? [08:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 334, loss 4.308865070343018\n",
      "Epoch 9: |          | 335/? [08:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 335, loss 4.249879837036133\n",
      "Epoch 9: |          | 336/? [08:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 336, loss 4.199667930603027\n",
      "Epoch 9: |          | 337/? [08:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 337, loss 4.726388454437256\n",
      "Epoch 9: |          | 338/? [08:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 338, loss 4.483003616333008\n",
      "Epoch 9: |          | 339/? [08:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 339, loss 3.58510160446167\n",
      "Epoch 9: |          | 340/? [08:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 340, loss 3.3347580432891846\n",
      "Epoch 9: |          | 341/? [08:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 341, loss 3.302338123321533\n",
      "Epoch 9: |          | 342/? [08:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 342, loss 3.8986353874206543\n",
      "Epoch 9: |          | 343/? [08:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 343, loss 3.6047680377960205\n",
      "Epoch 9: |          | 344/? [08:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 344, loss 4.445111274719238\n",
      "Epoch 9: |          | 345/? [08:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 345, loss 3.6545166969299316\n",
      "Epoch 9: |          | 346/? [08:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 346, loss 3.941333770751953\n",
      "Epoch 9: |          | 347/? [08:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 347, loss 3.734006881713867\n",
      "Epoch 9: |          | 348/? [08:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 348, loss 3.1298913955688477\n",
      "Epoch 9: |          | 349/? [08:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 349, loss 2.992126941680908\n",
      "Epoch 9: |          | 350/? [08:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 350, loss 4.167094707489014\n",
      "Epoch 9: |          | 351/? [08:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 351, loss 4.183836936950684\n",
      "Epoch 9: |          | 352/? [08:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 352, loss 3.4931342601776123\n",
      "Epoch 9: |          | 353/? [08:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 353, loss 3.2654261589050293\n",
      "Epoch 9: |          | 354/? [08:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 354, loss 3.685842514038086\n",
      "Epoch 9: |          | 355/? [08:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 355, loss 3.988257646560669\n",
      "Epoch 9: |          | 356/? [08:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 356, loss 4.009866714477539\n",
      "Epoch 9: |          | 357/? [08:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 357, loss 3.4999184608459473\n",
      "Epoch 9: |          | 358/? [08:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 358, loss 3.4521918296813965\n",
      "Epoch 9: |          | 359/? [08:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 359, loss 4.008849143981934\n",
      "Epoch 9: |          | 360/? [08:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 360, loss 3.6369144916534424\n",
      "Epoch 9: |          | 361/? [08:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 361, loss 3.7547214031219482\n",
      "Epoch 9: |          | 362/? [08:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 362, loss 3.532080888748169\n",
      "Epoch 9: |          | 363/? [08:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 363, loss 3.4442660808563232\n",
      "Epoch 9: |          | 364/? [08:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 364, loss 4.0344624519348145\n",
      "Epoch 9: |          | 365/? [08:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 365, loss 4.063458442687988\n",
      "Epoch 9: |          | 366/? [08:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 366, loss 3.889678955078125\n",
      "Epoch 9: |          | 367/? [08:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 367, loss 3.799870014190674\n",
      "Epoch 9: |          | 368/? [08:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 368, loss 3.4209580421447754\n",
      "Epoch 9: |          | 369/? [08:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 369, loss 3.685495376586914\n",
      "Epoch 9: |          | 370/? [08:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 370, loss 3.395115613937378\n",
      "Epoch 9: |          | 371/? [08:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 371, loss 4.286876678466797\n",
      "Epoch 9: |          | 372/? [08:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 372, loss 3.6413979530334473\n",
      "Epoch 9: |          | 373/? [08:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 373, loss 3.9939651489257812\n",
      "Epoch 9: |          | 374/? [08:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 374, loss 3.6645381450653076\n",
      "Epoch 9: |          | 375/? [09:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 375, loss 4.298577785491943\n",
      "Epoch 9: |          | 376/? [09:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 376, loss 3.723388195037842\n",
      "Epoch 9: |          | 377/? [09:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 377, loss 3.894141674041748\n",
      "Epoch 9: |          | 378/? [09:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 378, loss 4.048062324523926\n",
      "Epoch 9: |          | 379/? [09:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 379, loss 3.84382700920105\n",
      "Epoch 9: |          | 380/? [09:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 380, loss 3.8952088356018066\n",
      "Epoch 9: |          | 381/? [09:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 381, loss 3.9829699993133545\n",
      "Epoch 9: |          | 382/? [09:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 382, loss 3.7077510356903076\n",
      "Epoch 9: |          | 383/? [09:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 383, loss 3.788116455078125\n",
      "Epoch 9: |          | 384/? [09:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 384, loss 4.184974670410156\n",
      "Epoch 9: |          | 385/? [09:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 385, loss 3.825852632522583\n",
      "Epoch 9: |          | 386/? [09:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 386, loss 2.854079246520996\n",
      "Epoch 9: |          | 387/? [09:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 387, loss 3.6353137493133545\n",
      "Epoch 9: |          | 388/? [09:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 388, loss 3.4049911499023438\n",
      "Epoch 9: |          | 389/? [09:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 389, loss 4.160099029541016\n",
      "Epoch 9: |          | 390/? [09:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 390, loss 3.6314034461975098\n",
      "Epoch 9: |          | 391/? [09:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 391, loss 3.9890975952148438\n",
      "Epoch 9: |          | 392/? [09:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 392, loss 4.134219169616699\n",
      "Epoch 9: |          | 393/? [09:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 393, loss 4.15407657623291\n",
      "Epoch 9: |          | 394/? [09:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 394, loss 3.8189735412597656\n",
      "Epoch 9: |          | 395/? [09:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 395, loss 4.010107517242432\n",
      "Epoch 9: |          | 396/? [09:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 396, loss 3.9501044750213623\n",
      "Epoch 9: |          | 397/? [09:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 397, loss 3.7118778228759766\n",
      "Epoch 9: |          | 398/? [09:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 398, loss 3.6504600048065186\n",
      "Epoch 9: |          | 399/? [09:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 399, loss 3.7088890075683594\n",
      "Epoch 9: |          | 400/? [09:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 400, loss 3.6834475994110107\n",
      "Epoch 9: |          | 401/? [09:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 401, loss 3.6556382179260254\n",
      "Epoch 9: |          | 402/? [09:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 402, loss 4.018825531005859\n",
      "Epoch 9: |          | 403/? [09:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 403, loss 3.953721284866333\n",
      "Epoch 9: |          | 404/? [09:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 404, loss 3.4630439281463623\n",
      "Epoch 9: |          | 405/? [09:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 405, loss 3.548758029937744\n",
      "Epoch 9: |          | 406/? [09:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 406, loss 3.8392181396484375\n",
      "Epoch 9: |          | 407/? [09:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 407, loss 3.738767623901367\n",
      "Epoch 9: |          | 408/? [09:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 408, loss 4.175644874572754\n",
      "Epoch 9: |          | 409/? [09:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 409, loss 4.092719554901123\n",
      "Epoch 9: |          | 410/? [09:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 410, loss 3.776533603668213\n",
      "Epoch 9: |          | 411/? [09:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 411, loss 3.6788525581359863\n",
      "Epoch 9: |          | 412/? [09:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 412, loss 3.2512917518615723\n",
      "Epoch 9: |          | 413/? [09:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 413, loss 3.9417221546173096\n",
      "Epoch 9: |          | 414/? [09:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 414, loss 3.5639915466308594\n",
      "Epoch 9: |          | 415/? [09:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 415, loss 3.9865059852600098\n",
      "Epoch 9: |          | 416/? [09:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 416, loss 4.386378288269043\n",
      "Epoch 9: |          | 417/? [10:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 417, loss 4.3651628494262695\n",
      "Epoch 9: |          | 418/? [10:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 418, loss 3.9608662128448486\n",
      "Epoch 9: |          | 419/? [10:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 419, loss 3.7419590950012207\n",
      "Epoch 9: |          | 420/? [10:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 420, loss 3.9164974689483643\n",
      "Epoch 9: |          | 421/? [10:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 421, loss 4.400323867797852\n",
      "Epoch 9: |          | 422/? [10:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 422, loss 3.940819263458252\n",
      "Epoch 9: |          | 423/? [10:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 423, loss 3.5775063037872314\n",
      "Epoch 9: |          | 424/? [10:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 424, loss 4.092170238494873\n",
      "Epoch 9: |          | 425/? [10:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 425, loss 3.8932528495788574\n",
      "Epoch 9: |          | 426/? [10:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 426, loss 3.5537688732147217\n",
      "Epoch 9: |          | 427/? [10:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 427, loss 3.641674518585205\n",
      "Epoch 9: |          | 428/? [10:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 428, loss 4.346420764923096\n",
      "Epoch 9: |          | 429/? [10:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 429, loss 3.282766342163086\n",
      "Epoch 9: |          | 430/? [10:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 430, loss 3.9108188152313232\n",
      "Epoch 9: |          | 431/? [10:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 431, loss 3.8047385215759277\n",
      "Epoch 9: |          | 432/? [10:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 432, loss 3.9403579235076904\n",
      "Epoch 9: |          | 433/? [10:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 433, loss 3.9513187408447266\n",
      "Epoch 9: |          | 434/? [10:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 434, loss 3.789942502975464\n",
      "Epoch 9: |          | 435/? [10:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 435, loss 3.5141079425811768\n",
      "Epoch 9: |          | 436/? [10:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 436, loss 3.8799405097961426\n",
      "Epoch 9: |          | 437/? [10:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 437, loss 4.074349403381348\n",
      "Epoch 9: |          | 438/? [10:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 438, loss 3.7008919715881348\n",
      "Epoch 9: |          | 439/? [10:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 439, loss 3.569404125213623\n",
      "Epoch 9: |          | 440/? [10:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 440, loss 3.482745409011841\n",
      "Epoch 9: |          | 441/? [10:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 441, loss 3.879042148590088\n",
      "Epoch 9: |          | 442/? [10:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 442, loss 3.693039655685425\n",
      "Epoch 9: |          | 443/? [10:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 443, loss 3.825230836868286\n",
      "Epoch 9: |          | 444/? [10:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 444, loss 3.8547768592834473\n",
      "Epoch 9: |          | 445/? [10:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 445, loss 4.7802019119262695\n",
      "Epoch 9: |          | 446/? [10:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 446, loss 3.8200881481170654\n",
      "Epoch 9: |          | 447/? [10:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 447, loss 4.327498912811279\n",
      "Epoch 9: |          | 448/? [10:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 448, loss 3.386254072189331\n",
      "Epoch 9: |          | 449/? [10:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 449, loss 3.8197646141052246\n",
      "Epoch 9: |          | 450/? [10:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 450, loss 4.136513710021973\n",
      "Epoch 9: |          | 451/? [10:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 451, loss 3.7632603645324707\n",
      "Epoch 9: |          | 452/? [10:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 452, loss 3.5368659496307373\n",
      "Epoch 9: |          | 453/? [10:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 453, loss 4.250003814697266\n",
      "Epoch 9: |          | 454/? [10:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 454, loss 3.6434855461120605\n",
      "Epoch 9: |          | 455/? [10:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 455, loss 3.993809223175049\n",
      "Epoch 9: |          | 456/? [10:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 456, loss 3.3049705028533936\n",
      "Epoch 9: |          | 457/? [10:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 457, loss 3.7562358379364014\n",
      "Epoch 9: |          | 458/? [10:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 458, loss 4.225828647613525\n",
      "Epoch 9: |          | 459/? [11:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 459, loss 4.179657459259033\n",
      "Epoch 9: |          | 460/? [11:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 460, loss 3.9416465759277344\n",
      "Epoch 9: |          | 461/? [11:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 461, loss 3.861934185028076\n",
      "Epoch 9: |          | 462/? [11:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 462, loss 3.9539310932159424\n",
      "Epoch 9: |          | 463/? [11:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 463, loss 3.7934882640838623\n",
      "Epoch 9: |          | 464/? [11:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 464, loss 3.3115363121032715\n",
      "Epoch 9: |          | 465/? [11:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 465, loss 3.587714433670044\n",
      "Epoch 9: |          | 466/? [11:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 466, loss 4.063753604888916\n",
      "Epoch 9: |          | 467/? [11:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 467, loss 3.8688769340515137\n",
      "Epoch 9: |          | 468/? [11:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 468, loss 3.7943623065948486\n",
      "Epoch 9: |          | 469/? [11:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 469, loss 3.90116810798645\n",
      "Epoch 9: |          | 470/? [11:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 470, loss 3.3511669635772705\n",
      "Epoch 9: |          | 471/? [11:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 471, loss 4.132444381713867\n",
      "Epoch 9: |          | 472/? [11:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 472, loss 3.5891366004943848\n",
      "Epoch 9: |          | 473/? [11:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 473, loss 3.6049182415008545\n",
      "Epoch 9: |          | 474/? [11:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 474, loss 3.2846152782440186\n",
      "Epoch 9: |          | 475/? [11:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 475, loss 4.509432792663574\n",
      "Epoch 9: |          | 476/? [11:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 476, loss 3.5500214099884033\n",
      "Epoch 9: |          | 477/? [11:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 477, loss 3.1111159324645996\n",
      "Epoch 9: |          | 478/? [11:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 478, loss 3.3317272663116455\n",
      "Epoch 9: |          | 479/? [11:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 479, loss 3.771519184112549\n",
      "Epoch 9: |          | 480/? [11:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 480, loss 3.638674259185791\n",
      "Epoch 9: |          | 481/? [11:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 481, loss 3.279825210571289\n",
      "Epoch 9: |          | 482/? [11:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 482, loss 3.5712623596191406\n",
      "Epoch 9: |          | 483/? [11:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 483, loss 3.2397964000701904\n",
      "Epoch 9: |          | 484/? [11:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 484, loss 4.1604228019714355\n",
      "Epoch 9: |          | 485/? [11:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 485, loss 4.070448398590088\n",
      "Epoch 9: |          | 486/? [11:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 486, loss 3.6386966705322266\n",
      "Epoch 9: |          | 487/? [11:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 487, loss 3.967825412750244\n",
      "Epoch 9: |          | 488/? [11:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 488, loss 3.6901772022247314\n",
      "Epoch 9: |          | 489/? [11:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 489, loss 3.283365249633789\n",
      "Epoch 9: |          | 490/? [11:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 490, loss 3.8617146015167236\n",
      "Epoch 9: |          | 491/? [11:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 491, loss 3.7385220527648926\n",
      "Epoch 9: |          | 492/? [11:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 492, loss 3.0911500453948975\n",
      "Epoch 9: |          | 493/? [11:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 493, loss 4.064997673034668\n",
      "Epoch 9: |          | 494/? [11:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 494, loss 3.8391659259796143\n",
      "Epoch 9: |          | 495/? [11:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 495, loss 3.9818828105926514\n",
      "Epoch 9: |          | 496/? [11:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 496, loss 3.5737030506134033\n",
      "Epoch 9: |          | 497/? [11:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 497, loss 4.148754119873047\n",
      "Epoch 9: |          | 498/? [11:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 498, loss 3.750718355178833\n",
      "Epoch 9: |          | 499/? [11:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 499, loss 3.8827667236328125\n",
      "Epoch 9: |          | 500/? [11:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 500, loss 3.660003662109375\n",
      "Epoch 9: |          | 501/? [12:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 501, loss 3.4222381114959717\n",
      "Epoch 9: |          | 502/? [12:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 502, loss 3.8741812705993652\n",
      "Epoch 9: |          | 503/? [12:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 503, loss 3.8119139671325684\n",
      "Epoch 9: |          | 504/? [12:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 504, loss 3.7073028087615967\n",
      "Epoch 9: |          | 505/? [12:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 505, loss 3.199897289276123\n",
      "Epoch 9: |          | 506/? [12:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 506, loss 3.771496534347534\n",
      "Epoch 9: |          | 507/? [12:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 507, loss 3.784106492996216\n",
      "Epoch 9: |          | 508/? [12:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 508, loss 4.1690592765808105\n",
      "Epoch 9: |          | 509/? [12:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 509, loss 3.538059949874878\n",
      "Epoch 9: |          | 510/? [12:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 510, loss 4.010313987731934\n",
      "Epoch 9: |          | 511/? [12:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 511, loss 3.8380820751190186\n",
      "Epoch 9: |          | 512/? [12:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 512, loss 3.257228136062622\n",
      "Epoch 9: |          | 513/? [12:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 513, loss 3.5192012786865234\n",
      "Epoch 9: |          | 514/? [12:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 514, loss 3.700094699859619\n",
      "Epoch 9: |          | 515/? [12:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 515, loss 3.3328235149383545\n",
      "Epoch 9: |          | 516/? [12:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 516, loss 3.618748188018799\n",
      "Epoch 9: |          | 517/? [12:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 517, loss 3.833470106124878\n",
      "Epoch 9: |          | 518/? [12:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 518, loss 3.409637928009033\n",
      "Epoch 9: |          | 519/? [12:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 519, loss 3.867292881011963\n",
      "Epoch 9: |          | 520/? [12:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 520, loss 3.72039532661438\n",
      "Epoch 9: |          | 521/? [12:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 521, loss 3.7064735889434814\n",
      "Epoch 9: |          | 522/? [12:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 522, loss 4.231515407562256\n",
      "Epoch 9: |          | 523/? [12:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 523, loss 4.328273773193359\n",
      "Epoch 9: |          | 524/? [12:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 524, loss 4.096478462219238\n",
      "Epoch 9: |          | 525/? [12:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 525, loss 3.7158939838409424\n",
      "Epoch 9: |          | 526/? [12:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 526, loss 3.51667857170105\n",
      "Epoch 9: |          | 527/? [12:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 527, loss 4.179782390594482\n",
      "Epoch 9: |          | 528/? [12:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 528, loss 3.9003448486328125\n",
      "Epoch 9: |          | 529/? [12:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 529, loss 3.546067714691162\n",
      "Epoch 9: |          | 530/? [12:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 530, loss 4.0771965980529785\n",
      "Epoch 9: |          | 531/? [12:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 531, loss 3.595813751220703\n",
      "Epoch 9: |          | 532/? [12:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 532, loss 3.9071316719055176\n",
      "Epoch 9: |          | 533/? [12:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 533, loss 3.5274994373321533\n",
      "Epoch 9: |          | 534/? [12:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 534, loss 3.211150646209717\n",
      "Epoch 9: |          | 535/? [12:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 535, loss 3.502629518508911\n",
      "Epoch 9: |          | 536/? [12:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 536, loss 4.1451849937438965\n",
      "Epoch 9: |          | 537/? [12:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 537, loss 3.9173550605773926\n",
      "Epoch 9: |          | 538/? [12:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 538, loss 3.5681700706481934\n",
      "Epoch 9: |          | 539/? [12:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 539, loss 3.6697890758514404\n",
      "Epoch 9: |          | 540/? [12:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 540, loss 4.0521063804626465\n",
      "Epoch 9: |          | 541/? [12:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 541, loss 3.8224937915802\n",
      "Epoch 9: |          | 542/? [12:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 542, loss 3.599087953567505\n",
      "Epoch 9: |          | 543/? [13:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 543, loss 3.9628899097442627\n",
      "Epoch 9: |          | 544/? [13:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 544, loss 3.9048995971679688\n",
      "Epoch 9: |          | 545/? [13:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 545, loss 3.1814889907836914\n",
      "Epoch 9: |          | 546/? [13:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 546, loss 3.9251480102539062\n",
      "Epoch 9: |          | 547/? [13:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 547, loss 4.430885314941406\n",
      "Epoch 9: |          | 548/? [13:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 548, loss 4.0133137702941895\n",
      "Epoch 9: |          | 549/? [13:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 549, loss 3.9193921089172363\n",
      "Epoch 9: |          | 550/? [13:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 550, loss 4.229971885681152\n",
      "Epoch 9: |          | 551/? [13:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 551, loss 3.897416591644287\n",
      "Epoch 9: |          | 552/? [13:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 552, loss 3.8475379943847656\n",
      "Epoch 9: |          | 553/? [13:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 553, loss 3.373619556427002\n",
      "Epoch 9: |          | 554/? [13:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 554, loss 3.901397228240967\n",
      "Epoch 9: |          | 555/? [13:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 555, loss 4.217515468597412\n",
      "Epoch 9: |          | 556/? [13:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 556, loss 3.978085994720459\n",
      "Epoch 9: |          | 557/? [13:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 557, loss 3.461571216583252\n",
      "Epoch 9: |          | 558/? [13:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 558, loss 3.7212517261505127\n",
      "Epoch 9: |          | 559/? [13:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 559, loss 3.7044589519500732\n",
      "Epoch 9: |          | 560/? [13:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 560, loss 3.194709062576294\n",
      "Epoch 9: |          | 561/? [13:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 561, loss 3.095383405685425\n",
      "Epoch 9: |          | 562/? [13:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 562, loss 4.106473445892334\n",
      "Epoch 9: |          | 563/? [13:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 563, loss 3.200054883956909\n",
      "Epoch 9: |          | 564/? [13:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 564, loss 3.624840497970581\n",
      "Epoch 9: |          | 565/? [13:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 565, loss 4.024133682250977\n",
      "Epoch 9: |          | 566/? [13:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 566, loss 4.078248023986816\n",
      "Epoch 9: |          | 567/? [13:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 567, loss 4.1673712730407715\n",
      "Epoch 9: |          | 568/? [13:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 568, loss 3.279264450073242\n",
      "Epoch 9: |          | 569/? [13:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 569, loss 3.9026832580566406\n",
      "Epoch 9: |          | 570/? [13:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 570, loss 4.002192974090576\n",
      "Epoch 9: |          | 571/? [13:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 571, loss 3.5707309246063232\n",
      "Epoch 9: |          | 572/? [13:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 572, loss 4.4920334815979\n",
      "Epoch 9: |          | 573/? [13:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 573, loss 2.973644971847534\n",
      "Epoch 9: |          | 574/? [13:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 574, loss 4.072808265686035\n",
      "Epoch 9: |          | 575/? [13:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 575, loss 3.430405855178833\n",
      "Epoch 9: |          | 576/? [13:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 576, loss 3.6672492027282715\n",
      "Epoch 9: |          | 577/? [13:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 577, loss 3.8521289825439453\n",
      "Epoch 9: |          | 578/? [13:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 578, loss 4.078944206237793\n",
      "Epoch 9: |          | 579/? [13:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 579, loss 3.268167018890381\n",
      "Epoch 9: |          | 580/? [13:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 580, loss 3.8946070671081543\n",
      "Epoch 9: |          | 581/? [13:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 581, loss 3.910337448120117\n",
      "Epoch 9: |          | 582/? [13:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 582, loss 3.9981956481933594\n",
      "Epoch 9: |          | 583/? [13:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 583, loss 3.750807523727417\n",
      "Epoch 9: |          | 584/? [13:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 584, loss 3.972787380218506\n",
      "Epoch 9: |          | 585/? [13:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 585, loss 3.9317150115966797\n",
      "Epoch 9: |          | 586/? [14:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 586, loss 3.982348918914795\n",
      "Epoch 9: |          | 587/? [14:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 587, loss 4.016324520111084\n",
      "Epoch 9: |          | 588/? [14:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 588, loss 3.9462084770202637\n",
      "Epoch 9: |          | 589/? [14:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 589, loss 3.413468837738037\n",
      "Epoch 9: |          | 590/? [14:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 590, loss 3.973475217819214\n",
      "Epoch 9: |          | 591/? [14:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 591, loss 3.868201494216919\n",
      "Epoch 9: |          | 592/? [14:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 592, loss 3.554253101348877\n",
      "Epoch 9: |          | 593/? [14:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 593, loss 3.848140239715576\n",
      "Epoch 9: |          | 594/? [14:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 594, loss 4.6153693199157715\n",
      "Epoch 9: |          | 595/? [14:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 595, loss 3.3957648277282715\n",
      "Epoch 9: |          | 596/? [14:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 596, loss 3.4612464904785156\n",
      "Epoch 9: |          | 597/? [14:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 597, loss 3.6631362438201904\n",
      "Epoch 9: |          | 598/? [14:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 598, loss 4.163900375366211\n",
      "Epoch 9: |          | 599/? [14:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 599, loss 3.776721954345703\n",
      "Epoch 9: |          | 600/? [14:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 600, loss 3.5875885486602783\n",
      "Epoch 9: |          | 601/? [14:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 601, loss 3.856144666671753\n",
      "Epoch 9: |          | 602/? [14:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 602, loss 3.4507319927215576\n",
      "Epoch 9: |          | 603/? [14:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 603, loss 3.5459201335906982\n",
      "Epoch 9: |          | 604/? [14:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 604, loss 4.759732246398926\n",
      "Epoch 9: |          | 605/? [14:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 605, loss 3.3191094398498535\n",
      "Epoch 9: |          | 606/? [14:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 606, loss 3.5787911415100098\n",
      "Epoch 9: |          | 607/? [14:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 607, loss 3.889331102371216\n",
      "Epoch 9: |          | 608/? [14:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 608, loss 3.6934216022491455\n",
      "Epoch 9: |          | 609/? [14:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 609, loss 3.5936713218688965\n",
      "Epoch 9: |          | 610/? [14:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 610, loss 3.675741195678711\n",
      "Epoch 9: |          | 611/? [14:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 611, loss 3.8378632068634033\n",
      "Epoch 9: |          | 612/? [14:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 612, loss 3.5777390003204346\n",
      "Epoch 9: |          | 613/? [14:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 613, loss 3.8740291595458984\n",
      "Epoch 9: |          | 614/? [14:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 614, loss 3.70646333694458\n",
      "Epoch 9: |          | 615/? [14:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 615, loss 4.214343547821045\n",
      "Epoch 9: |          | 616/? [14:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 616, loss 4.383392333984375\n",
      "Epoch 9: |          | 617/? [14:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 617, loss 2.925034284591675\n",
      "Epoch 9: |          | 618/? [14:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 618, loss 3.912825107574463\n",
      "Epoch 9: |          | 619/? [14:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 619, loss 3.4554176330566406\n",
      "Epoch 9: |          | 620/? [14:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 620, loss 4.02233362197876\n",
      "Epoch 9: |          | 621/? [14:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 621, loss 3.4964759349823\n",
      "Epoch 9: |          | 622/? [14:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 622, loss 3.279844284057617\n",
      "Epoch 9: |          | 623/? [14:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 623, loss 3.0756938457489014\n",
      "Epoch 9: |          | 624/? [14:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 624, loss 2.8123388290405273\n",
      "Epoch 9: |          | 625/? [14:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 625, loss 4.265276908874512\n",
      "Epoch 9: |          | 626/? [14:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 626, loss 3.738985776901245\n",
      "Epoch 9: |          | 627/? [14:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 627, loss 3.6591506004333496\n",
      "Epoch 9: |          | 628/? [15:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 628, loss 3.667161464691162\n",
      "Epoch 9: |          | 629/? [15:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 629, loss 4.017584800720215\n",
      "Epoch 9: |          | 630/? [15:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 630, loss 3.790062665939331\n",
      "Epoch 9: |          | 631/? [15:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 631, loss 3.8912746906280518\n",
      "Epoch 9: |          | 632/? [15:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 632, loss 3.286149263381958\n",
      "Epoch 9: |          | 633/? [15:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 633, loss 3.996741533279419\n",
      "Epoch 9: |          | 634/? [15:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 634, loss 3.5681710243225098\n",
      "Epoch 9: |          | 635/? [15:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 635, loss 3.3883261680603027\n",
      "Epoch 9: |          | 636/? [15:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 636, loss 3.797576904296875\n",
      "Epoch 9: |          | 637/? [15:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 637, loss 3.583583116531372\n",
      "Epoch 9: |          | 638/? [15:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 638, loss 3.8488316535949707\n",
      "Epoch 9: |          | 639/? [15:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 639, loss 3.6109204292297363\n",
      "Epoch 9: |          | 640/? [15:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 640, loss 4.155889987945557\n",
      "Epoch 9: |          | 641/? [15:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 641, loss 3.203855037689209\n",
      "Epoch 9: |          | 642/? [15:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 642, loss 3.9500255584716797\n",
      "Epoch 9: |          | 643/? [15:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 643, loss 3.8432705402374268\n",
      "Epoch 9: |          | 644/? [15:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 644, loss 3.821220874786377\n",
      "Epoch 9: |          | 645/? [15:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 645, loss 3.551166534423828\n",
      "Epoch 9: |          | 646/? [15:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 646, loss 3.5901951789855957\n",
      "Epoch 9: |          | 647/? [15:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 647, loss 4.11782169342041\n",
      "Epoch 9: |          | 648/? [15:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 648, loss 3.54620361328125\n",
      "Epoch 9: |          | 649/? [15:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 649, loss 2.995022773742676\n",
      "Epoch 9: |          | 650/? [15:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 650, loss 4.063839912414551\n",
      "Epoch 9: |          | 651/? [15:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 651, loss 4.175808906555176\n",
      "Epoch 9: |          | 652/? [15:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 652, loss 3.649508237838745\n",
      "Epoch 9: |          | 653/? [15:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 653, loss 3.7883973121643066\n",
      "Epoch 9: |          | 654/? [15:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 654, loss 3.852450132369995\n",
      "Epoch 9: |          | 655/? [15:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 655, loss 3.725079298019409\n",
      "Epoch 9: |          | 656/? [15:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 656, loss 3.3477377891540527\n",
      "Epoch 9: |          | 657/? [15:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 657, loss 5.6133222579956055\n",
      "Epoch 9: |          | 658/? [15:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 658, loss 3.2484564781188965\n",
      "Epoch 9: |          | 659/? [15:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 659, loss 3.800405979156494\n",
      "Epoch 9: |          | 660/? [15:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 660, loss 4.11091423034668\n",
      "Epoch 9: |          | 661/? [15:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 661, loss 4.031584739685059\n",
      "Epoch 9: |          | 662/? [15:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 662, loss 3.921862840652466\n",
      "Epoch 9: |          | 663/? [15:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 663, loss 3.68354868888855\n",
      "Epoch 9: |          | 664/? [15:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 664, loss 3.6114063262939453\n",
      "Epoch 9: |          | 665/? [15:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 665, loss 3.8977763652801514\n",
      "Epoch 9: |          | 666/? [15:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 666, loss 3.7055695056915283\n",
      "Epoch 9: |          | 667/? [15:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 667, loss 4.516366958618164\n",
      "Epoch 9: |          | 668/? [15:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 668, loss 3.3171095848083496\n",
      "Epoch 9: |          | 669/? [15:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 669, loss 3.5377540588378906\n",
      "Epoch 9: |          | 670/? [16:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 670, loss 4.179612159729004\n",
      "Epoch 9: |          | 671/? [16:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 671, loss 3.944077253341675\n",
      "Epoch 9: |          | 672/? [16:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 672, loss 3.9313418865203857\n",
      "Epoch 9: |          | 673/? [16:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 673, loss 3.787522077560425\n",
      "Epoch 9: |          | 674/? [16:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 674, loss 2.2559561729431152\n",
      "Epoch 9: |          | 675/? [16:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 675, loss 0.8422110676765442\n",
      "Epoch 9: |          | 676/? [16:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 676, loss 0.7000942230224609\n",
      "Epoch 9: |          | 677/? [16:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 677, loss 0.5257052779197693\n",
      "Epoch 9: |          | 678/? [16:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 678, loss 1.7052125930786133\n",
      "Epoch 9: |          | 679/? [16:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 679, loss 3.2318496704101562\n",
      "Epoch 9: |          | 680/? [16:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 680, loss 3.687182903289795\n",
      "Epoch 9: |          | 681/? [16:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 681, loss 3.2857723236083984\n",
      "Epoch 9: |          | 682/? [16:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 682, loss 3.5440986156463623\n",
      "Epoch 9: |          | 683/? [16:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 683, loss 3.2902462482452393\n",
      "Epoch 9: |          | 684/? [16:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 684, loss 4.293160915374756\n",
      "Epoch 9: |          | 685/? [16:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 685, loss 3.9088573455810547\n",
      "Epoch 9: |          | 686/? [16:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 686, loss 3.537522554397583\n",
      "Epoch 9: |          | 687/? [16:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 687, loss 3.996511459350586\n",
      "Epoch 9: |          | 688/? [16:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 688, loss 3.552889585494995\n",
      "Epoch 9: |          | 689/? [16:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 689, loss 3.6147618293762207\n",
      "Epoch 9: |          | 690/? [16:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 690, loss 4.278618335723877\n",
      "Epoch 9: |          | 691/? [16:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 691, loss 3.760585308074951\n",
      "Epoch 9: |          | 692/? [16:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 692, loss 3.774921417236328\n",
      "Epoch 9: |          | 693/? [16:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 693, loss 4.337513446807861\n",
      "Epoch 9: |          | 694/? [16:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 694, loss 3.6575939655303955\n",
      "Epoch 9: |          | 695/? [16:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 695, loss 4.200689315795898\n",
      "Epoch 9: |          | 696/? [16:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 696, loss 3.522839307785034\n",
      "Epoch 9: |          | 697/? [16:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 697, loss 3.6907966136932373\n",
      "Epoch 9: |          | 698/? [16:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 698, loss 3.169325828552246\n",
      "Epoch 9: |          | 699/? [16:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 699, loss 3.8839969635009766\n",
      "Epoch 9: |          | 700/? [16:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 700, loss 3.9453659057617188\n",
      "Epoch 9: |          | 701/? [16:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 701, loss 3.594388484954834\n",
      "Epoch 9: |          | 702/? [16:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 702, loss 3.85137939453125\n",
      "Epoch 9: |          | 703/? [16:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 703, loss 3.9903221130371094\n",
      "Epoch 9: |          | 704/? [16:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 704, loss 3.847815752029419\n",
      "Epoch 9: |          | 705/? [16:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 705, loss 3.4800925254821777\n",
      "Epoch 9: |          | 706/? [16:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 706, loss 3.5306782722473145\n",
      "Epoch 9: |          | 707/? [16:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 707, loss 4.010168552398682\n",
      "Epoch 9: |          | 708/? [16:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 708, loss 3.7450766563415527\n",
      "Epoch 9: |          | 709/? [16:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 709, loss 3.6673989295959473\n",
      "Epoch 9: |          | 710/? [16:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 710, loss 4.20856237411499\n",
      "Epoch 9: |          | 711/? [16:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 711, loss 4.281002044677734\n",
      "Epoch 9: |          | 712/? [17:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 712, loss 3.9792723655700684\n",
      "Epoch 9: |          | 713/? [17:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 713, loss 4.057757377624512\n",
      "Epoch 9: |          | 714/? [17:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 714, loss 4.120128631591797\n",
      "Epoch 9: |          | 715/? [17:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 715, loss 3.116177558898926\n",
      "Epoch 9: |          | 716/? [17:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 716, loss 3.8001022338867188\n",
      "Epoch 9: |          | 717/? [17:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 717, loss 3.6853556632995605\n",
      "Epoch 9: |          | 718/? [17:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 718, loss 3.2089149951934814\n",
      "Epoch 9: |          | 719/? [17:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 719, loss 3.7372634410858154\n",
      "Epoch 9: |          | 720/? [17:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 720, loss 3.404177188873291\n",
      "Epoch 9: |          | 721/? [17:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 721, loss 4.080474853515625\n",
      "Epoch 9: |          | 722/? [17:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 722, loss 3.4118735790252686\n",
      "Epoch 9: |          | 723/? [17:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 723, loss 3.963568925857544\n",
      "Epoch 9: |          | 724/? [17:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 724, loss 3.525270462036133\n",
      "Epoch 9: |          | 725/? [17:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 725, loss 3.461188554763794\n",
      "Epoch 9: |          | 726/? [17:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 726, loss 3.638697385787964\n",
      "Epoch 9: |          | 727/? [17:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 727, loss 3.442490816116333\n",
      "Epoch 9: |          | 728/? [17:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 728, loss 3.244290828704834\n",
      "Epoch 9: |          | 729/? [17:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 729, loss 3.748081922531128\n",
      "Epoch 9: |          | 730/? [17:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 730, loss 3.7069759368896484\n",
      "Epoch 9: |          | 731/? [17:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 731, loss 3.819819211959839\n",
      "Epoch 9: |          | 732/? [17:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 732, loss 4.061957359313965\n",
      "Epoch 9: |          | 733/? [17:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 733, loss 3.796602725982666\n",
      "Epoch 9: |          | 734/? [17:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 734, loss 3.964303493499756\n",
      "Epoch 9: |          | 735/? [17:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 735, loss 3.8789966106414795\n",
      "Epoch 9: |          | 736/? [17:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 736, loss 3.4181132316589355\n",
      "Epoch 9: |          | 737/? [17:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 737, loss 4.197863578796387\n",
      "Epoch 9: |          | 738/? [17:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 738, loss 3.3792431354522705\n",
      "Epoch 9: |          | 739/? [17:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 739, loss 3.84605073928833\n",
      "Epoch 9: |          | 740/? [17:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 740, loss 3.579019546508789\n",
      "Epoch 9: |          | 741/? [17:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 741, loss 3.7221519947052\n",
      "Epoch 9: |          | 742/? [17:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 742, loss 4.121657371520996\n",
      "Epoch 9: |          | 743/? [17:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 743, loss 3.9682464599609375\n",
      "Epoch 9: |          | 744/? [17:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 744, loss 3.9279677867889404\n",
      "Epoch 9: |          | 745/? [17:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 745, loss 3.5578110218048096\n",
      "Epoch 9: |          | 746/? [17:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 746, loss 3.795048236846924\n",
      "Epoch 9: |          | 747/? [17:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 747, loss 3.5168254375457764\n",
      "Epoch 9: |          | 748/? [17:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 748, loss 2.5743966102600098\n",
      "Epoch 9: |          | 749/? [17:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 749, loss 3.7141640186309814\n",
      "Epoch 9: |          | 750/? [17:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 750, loss 3.9672515392303467\n",
      "Epoch 9: |          | 751/? [18:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 751, loss 2.3272528648376465\n",
      "Epoch 9: |          | 752/? [18:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 752, loss 3.8912899494171143\n",
      "Epoch 9: |          | 753/? [18:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 753, loss 3.0577776432037354\n",
      "Epoch 9: |          | 754/? [18:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 754, loss 3.5296249389648438\n",
      "Epoch 9: |          | 755/? [18:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 755, loss 3.3397216796875\n",
      "Epoch 9: |          | 756/? [18:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 756, loss 3.729137420654297\n",
      "Epoch 9: |          | 757/? [18:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 757, loss 3.7890961170196533\n",
      "Epoch 9: |          | 758/? [18:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 758, loss 3.5548148155212402\n",
      "Epoch 9: |          | 759/? [18:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 759, loss 3.478832244873047\n",
      "Epoch 9: |          | 760/? [18:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 760, loss 3.9963538646698\n",
      "Epoch 9: |          | 761/? [18:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 761, loss 4.012362003326416\n",
      "Epoch 9: |          | 762/? [18:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 762, loss 3.6656875610351562\n",
      "Epoch 9: |          | 763/? [18:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 763, loss 3.7875239849090576\n",
      "Epoch 9: |          | 764/? [18:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 764, loss 4.058361530303955\n",
      "Epoch 9: |          | 765/? [18:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 765, loss 3.8434462547302246\n",
      "Epoch 9: |          | 766/? [18:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 766, loss 4.228548049926758\n",
      "Epoch 9: |          | 767/? [18:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 767, loss 4.2698750495910645\n",
      "Epoch 9: |          | 768/? [18:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 768, loss 3.8540561199188232\n",
      "Epoch 9: |          | 769/? [18:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 769, loss 3.072873115539551\n",
      "Epoch 9: |          | 770/? [18:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 770, loss 3.5911784172058105\n",
      "Epoch 9: |          | 771/? [18:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 771, loss 4.340950012207031\n",
      "Epoch 9: |          | 772/? [18:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 772, loss 4.029261112213135\n",
      "Epoch 9: |          | 773/? [18:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 773, loss 3.7472214698791504\n",
      "Epoch 9: |          | 774/? [18:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 774, loss 3.8679862022399902\n",
      "Epoch 9: |          | 775/? [18:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 775, loss 4.310976982116699\n",
      "Epoch 9: |          | 776/? [18:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 776, loss 3.71881103515625\n",
      "Epoch 9: |          | 777/? [18:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 777, loss 3.6179041862487793\n",
      "Epoch 9: |          | 778/? [18:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 778, loss 4.00280237197876\n",
      "Epoch 9: |          | 779/? [18:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 779, loss 4.459216117858887\n",
      "Epoch 9: |          | 780/? [18:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 780, loss 3.480815887451172\n",
      "Epoch 9: |          | 781/? [18:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 781, loss 3.553046464920044\n",
      "Epoch 9: |          | 782/? [18:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 782, loss 3.9196085929870605\n",
      "Epoch 9: |          | 783/? [18:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 783, loss 4.012803077697754\n",
      "Epoch 9: |          | 784/? [18:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 784, loss 3.5921530723571777\n",
      "Epoch 9: |          | 785/? [18:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 785, loss 3.3662147521972656\n",
      "Epoch 9: |          | 786/? [18:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 786, loss 4.214041233062744\n",
      "Epoch 9: |          | 787/? [18:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 787, loss 4.143772602081299\n",
      "Epoch 9: |          | 788/? [18:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 788, loss 2.044203996658325\n",
      "Epoch 9: |          | 789/? [18:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 789, loss 3.6912803649902344\n",
      "Epoch 9: |          | 790/? [18:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 790, loss 4.487519264221191\n",
      "Epoch 9: |          | 791/? [18:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 791, loss 4.227396488189697\n",
      "Epoch 9: |          | 792/? [18:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 792, loss 3.3871445655822754\n",
      "Epoch 9: |          | 793/? [19:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 793, loss 3.950122117996216\n",
      "Epoch 9: |          | 794/? [19:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 794, loss 4.222715377807617\n",
      "Epoch 9: |          | 795/? [19:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 795, loss 3.7346904277801514\n",
      "Epoch 9: |          | 796/? [19:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 796, loss 4.08986234664917\n",
      "Epoch 9: |          | 797/? [19:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 797, loss 3.1178879737854004\n",
      "Epoch 9: |          | 798/? [19:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 798, loss 3.227933168411255\n",
      "Epoch 9: |          | 799/? [19:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 799, loss 4.1632208824157715\n",
      "Epoch 9: |          | 800/? [19:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 800, loss 3.956636905670166\n",
      "Epoch 9: |          | 801/? [19:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 801, loss 3.5930819511413574\n",
      "Epoch 9: |          | 802/? [19:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 802, loss 3.845078229904175\n",
      "Epoch 9: |          | 803/? [19:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 803, loss 3.693885326385498\n",
      "Epoch 9: |          | 804/? [19:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 804, loss 3.8538641929626465\n",
      "Epoch 9: |          | 805/? [19:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 805, loss 4.015505313873291\n",
      "Epoch 9: |          | 806/? [19:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 806, loss 4.4415106773376465\n",
      "Epoch 9: |          | 807/? [19:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 807, loss 3.7733654975891113\n",
      "Epoch 9: |          | 808/? [19:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 808, loss 3.473670482635498\n",
      "Epoch 9: |          | 809/? [19:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 809, loss 3.9856209754943848\n",
      "Epoch 9: |          | 810/? [19:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 810, loss 3.7426059246063232\n",
      "Epoch 9: |          | 811/? [19:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 811, loss 4.02617883682251\n",
      "Epoch 9: |          | 812/? [19:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 812, loss 4.600921154022217\n",
      "Epoch 9: |          | 813/? [19:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 813, loss 4.400184631347656\n",
      "Epoch 9: |          | 814/? [19:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 814, loss 3.4488041400909424\n",
      "Epoch 9: |          | 815/? [19:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 815, loss 4.137369632720947\n",
      "Epoch 9: |          | 816/? [19:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 816, loss 3.937448024749756\n",
      "Epoch 9: |          | 817/? [19:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 817, loss 3.319852828979492\n",
      "Epoch 9: |          | 818/? [19:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 818, loss 4.269657611846924\n",
      "Epoch 9: |          | 819/? [19:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 819, loss 3.940898895263672\n",
      "Epoch 9: |          | 820/? [19:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 820, loss 3.8121395111083984\n",
      "Epoch 9: |          | 821/? [19:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 821, loss 3.762770891189575\n",
      "Epoch 9: |          | 822/? [19:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 822, loss 3.4595699310302734\n",
      "Epoch 9: |          | 823/? [19:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 823, loss 3.4845917224884033\n",
      "Epoch 9: |          | 824/? [19:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 824, loss 3.9227752685546875\n",
      "Epoch 9: |          | 825/? [19:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 825, loss 3.4709994792938232\n",
      "Epoch 9: |          | 826/? [19:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 826, loss 4.001115798950195\n",
      "Epoch 9: |          | 827/? [19:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 827, loss 3.6113579273223877\n",
      "Epoch 9: |          | 828/? [19:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 828, loss 4.1500043869018555\n",
      "Epoch 9: |          | 829/? [19:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 829, loss 3.8179726600646973\n",
      "Epoch 9: |          | 830/? [19:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 830, loss 4.3414154052734375\n",
      "Epoch 9: |          | 831/? [19:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 831, loss 2.3032848834991455\n",
      "Epoch 9: |          | 832/? [19:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 832, loss 3.7723324298858643\n",
      "Epoch 9: |          | 833/? [19:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 833, loss 3.665121555328369\n",
      "Epoch 9: |          | 834/? [19:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 834, loss 4.393292427062988\n",
      "Epoch 9: |          | 835/? [20:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 835, loss 3.724365234375\n",
      "Epoch 9: |          | 836/? [20:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 836, loss 4.423200607299805\n",
      "Epoch 9: |          | 837/? [20:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 837, loss 3.835859775543213\n",
      "Epoch 9: |          | 838/? [20:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 838, loss 3.219956874847412\n",
      "Epoch 9: |          | 839/? [20:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 839, loss 3.5529003143310547\n",
      "Epoch 9: |          | 840/? [20:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 840, loss 4.090385437011719\n",
      "Epoch 9: |          | 841/? [20:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 841, loss 4.130922317504883\n",
      "Epoch 9: |          | 842/? [20:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 842, loss 3.797478437423706\n",
      "Epoch 9: |          | 843/? [20:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 843, loss 4.105075359344482\n",
      "Epoch 9: |          | 844/? [20:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 844, loss 3.4915518760681152\n",
      "Epoch 9: |          | 845/? [20:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 845, loss 3.9050567150115967\n",
      "Epoch 9: |          | 846/? [20:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 846, loss 4.304576873779297\n",
      "Epoch 9: |          | 847/? [20:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 847, loss 3.898197889328003\n",
      "Epoch 9: |          | 848/? [20:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 848, loss 3.4802467823028564\n",
      "Epoch 9: |          | 849/? [20:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 849, loss 3.5471444129943848\n",
      "Epoch 9: |          | 850/? [20:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 850, loss 3.664557695388794\n",
      "Epoch 9: |          | 851/? [20:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 851, loss 3.9412121772766113\n",
      "Epoch 9: |          | 852/? [20:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 852, loss 4.1009440422058105\n",
      "Epoch 9: |          | 853/? [20:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 853, loss 3.9229023456573486\n",
      "Epoch 9: |          | 854/? [20:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 854, loss 3.2583000659942627\n",
      "Epoch 9: |          | 855/? [20:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 855, loss 3.5324370861053467\n",
      "Epoch 9: |          | 856/? [20:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 856, loss 3.4731178283691406\n",
      "Epoch 9: |          | 857/? [20:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 857, loss 3.981031894683838\n",
      "Epoch 9: |          | 858/? [20:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 858, loss 3.9083263874053955\n",
      "Epoch 9: |          | 859/? [20:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 859, loss 3.9003119468688965\n",
      "Epoch 9: |          | 860/? [20:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 860, loss 4.29161262512207\n",
      "Epoch 9: |          | 861/? [20:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 861, loss 3.6050095558166504\n",
      "Epoch 9: |          | 862/? [20:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 862, loss 3.917748212814331\n",
      "Epoch 9: |          | 863/? [20:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 863, loss 3.3384742736816406\n",
      "Epoch 9: |          | 864/? [20:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 864, loss 3.846295118331909\n",
      "Epoch 9: |          | 865/? [20:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 865, loss 3.818798780441284\n",
      "Epoch 9: |          | 866/? [20:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 866, loss 2.9104461669921875\n",
      "Epoch 9: |          | 867/? [20:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 867, loss 3.062777042388916\n",
      "Epoch 9: |          | 868/? [20:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 868, loss 3.935204267501831\n",
      "Epoch 9: |          | 869/? [20:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 869, loss 3.987048625946045\n",
      "Epoch 9: |          | 870/? [20:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 870, loss 3.5995476245880127\n",
      "Epoch 9: |          | 871/? [20:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 871, loss 3.9071648120880127\n",
      "Epoch 9: |          | 872/? [20:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 872, loss 3.750105619430542\n",
      "Epoch 9: |          | 873/? [20:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 873, loss 3.769218921661377\n",
      "Epoch 9: |          | 874/? [20:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 874, loss 3.2551186084747314\n",
      "Epoch 9: |          | 875/? [20:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 875, loss 4.001854419708252\n",
      "Epoch 9: |          | 876/? [21:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 876, loss 3.5313591957092285\n",
      "Epoch 9: |          | 877/? [21:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 877, loss 3.916614532470703\n",
      "Epoch 9: |          | 878/? [21:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 878, loss 3.3329994678497314\n",
      "Epoch 9: |          | 879/? [21:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 879, loss 3.4197640419006348\n",
      "Epoch 9: |          | 880/? [21:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 880, loss 4.482585430145264\n",
      "Epoch 9: |          | 881/? [21:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 881, loss 3.9073843955993652\n",
      "Epoch 9: |          | 882/? [21:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 882, loss 3.685539960861206\n",
      "Epoch 9: |          | 883/? [21:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 883, loss 3.8248202800750732\n",
      "Epoch 9: |          | 884/? [21:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 884, loss 3.8714985847473145\n",
      "Epoch 9: |          | 885/? [21:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 885, loss 3.661897659301758\n",
      "Epoch 9: |          | 886/? [21:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 886, loss 4.295965671539307\n",
      "Epoch 9: |          | 887/? [21:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 887, loss 4.307976722717285\n",
      "Epoch 9: |          | 888/? [21:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 888, loss 3.99299955368042\n",
      "Epoch 9: |          | 889/? [21:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 889, loss 3.544404983520508\n",
      "Epoch 9: |          | 890/? [21:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 890, loss 3.807706356048584\n",
      "Epoch 9: |          | 891/? [21:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 891, loss 3.6317038536071777\n",
      "Epoch 9: |          | 892/? [21:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 892, loss 4.225939750671387\n",
      "Epoch 9: |          | 893/? [21:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 893, loss 3.6937389373779297\n",
      "Epoch 9: |          | 894/? [21:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 894, loss 3.2244555950164795\n",
      "Epoch 9: |          | 895/? [21:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 895, loss 4.304251194000244\n",
      "Epoch 9: |          | 896/? [21:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 896, loss 3.9137165546417236\n",
      "Epoch 9: |          | 897/? [21:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 897, loss 3.9211151599884033\n",
      "Epoch 9: |          | 898/? [21:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 898, loss 3.908538818359375\n",
      "Epoch 9: |          | 899/? [21:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 899, loss 3.6887993812561035\n",
      "Epoch 9: |          | 900/? [21:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 900, loss 3.6032118797302246\n",
      "Epoch 9: |          | 901/? [21:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 901, loss 3.999631404876709\n",
      "Epoch 9: |          | 902/? [21:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 902, loss 4.12732458114624\n",
      "Epoch 9: |          | 903/? [21:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 903, loss 3.428684711456299\n",
      "Epoch 9: |          | 904/? [21:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 904, loss 3.906485080718994\n",
      "Epoch 9: |          | 905/? [21:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 905, loss 4.095093250274658\n",
      "Epoch 9: |          | 906/? [21:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 906, loss 3.8578743934631348\n",
      "Epoch 9: |          | 907/? [21:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 907, loss 3.8805809020996094\n",
      "Epoch 9: |          | 908/? [21:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 908, loss 3.9534478187561035\n",
      "Epoch 9: |          | 909/? [21:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 909, loss 3.9415900707244873\n",
      "Epoch 9: |          | 910/? [21:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 910, loss 3.7035720348358154\n",
      "Epoch 9: |          | 911/? [21:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 911, loss 3.8117001056671143\n",
      "Epoch 9: |          | 912/? [21:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 912, loss 3.736560106277466\n",
      "Epoch 9: |          | 913/? [21:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 913, loss 3.7483391761779785\n",
      "Epoch 9: |          | 914/? [21:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 914, loss 4.031068801879883\n",
      "Epoch 9: |          | 915/? [21:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 915, loss 3.8271255493164062\n",
      "Epoch 9: |          | 916/? [21:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 916, loss 3.789830446243286\n",
      "Epoch 9: |          | 917/? [21:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 917, loss 3.698718547821045\n",
      "Epoch 9: |          | 918/? [22:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 918, loss 3.6707558631896973\n",
      "Epoch 9: |          | 919/? [22:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 919, loss 3.6745471954345703\n",
      "Epoch 9: |          | 920/? [22:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 920, loss 3.8262600898742676\n",
      "Epoch 9: |          | 921/? [22:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 921, loss 3.659447431564331\n",
      "Epoch 9: |          | 922/? [22:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 922, loss 3.8069992065429688\n",
      "Epoch 9: |          | 923/? [22:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 923, loss 3.661221981048584\n",
      "Epoch 9: |          | 924/? [22:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 924, loss 3.6399478912353516\n",
      "Epoch 9: |          | 925/? [22:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 925, loss 3.978163957595825\n",
      "Epoch 9: |          | 926/? [22:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 926, loss 3.7341551780700684\n",
      "Epoch 9: |          | 927/? [22:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 927, loss 4.023405075073242\n",
      "Epoch 9: |          | 928/? [22:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 928, loss 3.533698320388794\n",
      "Epoch 9: |          | 929/? [22:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 929, loss 3.6366374492645264\n",
      "Epoch 9: |          | 930/? [22:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 930, loss 3.5377655029296875\n",
      "Epoch 9: |          | 931/? [22:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 931, loss 3.2726142406463623\n",
      "Epoch 9: |          | 932/? [22:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 932, loss 3.878462553024292\n",
      "Epoch 9: |          | 933/? [22:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 933, loss 3.5926876068115234\n",
      "Epoch 9: |          | 934/? [22:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 934, loss 4.180017948150635\n",
      "Epoch 9: |          | 935/? [22:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 935, loss 4.5547194480896\n",
      "Epoch 9: |          | 936/? [22:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 936, loss 3.734830856323242\n",
      "Epoch 9: |          | 937/? [22:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 937, loss 3.6361641883850098\n",
      "Epoch 9: |          | 938/? [22:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 938, loss 3.684770107269287\n",
      "Epoch 9: |          | 939/? [22:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 939, loss 3.920058488845825\n",
      "Epoch 9: |          | 940/? [22:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 940, loss 4.1171159744262695\n",
      "Epoch 9: |          | 941/? [22:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 941, loss 3.6869964599609375\n",
      "Epoch 9: |          | 942/? [22:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 942, loss 3.1730470657348633\n",
      "Epoch 9: |          | 943/? [22:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 943, loss 4.016913414001465\n",
      "Epoch 9: |          | 944/? [22:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 944, loss 3.0750558376312256\n",
      "Epoch 9: |          | 945/? [22:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 945, loss 3.8128554821014404\n",
      "Epoch 9: |          | 946/? [22:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 946, loss 3.7038776874542236\n",
      "Epoch 9: |          | 947/? [22:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 947, loss 3.636589765548706\n",
      "Epoch 9: |          | 948/? [22:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 948, loss 3.909822463989258\n",
      "Epoch 9: |          | 949/? [22:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 949, loss 3.7349700927734375\n",
      "Epoch 9: |          | 950/? [22:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 950, loss 3.5281670093536377\n",
      "Epoch 9: |          | 951/? [22:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 951, loss 4.1617841720581055\n",
      "Epoch 9: |          | 952/? [22:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 952, loss 4.127254962921143\n",
      "Epoch 9: |          | 953/? [22:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 953, loss 4.646371364593506\n",
      "Epoch 9: |          | 954/? [22:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 954, loss 3.6258506774902344\n",
      "Epoch 9: |          | 955/? [22:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 955, loss 4.238492488861084\n",
      "Epoch 9: |          | 956/? [22:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 956, loss 3.7306666374206543\n",
      "Epoch 9: |          | 957/? [22:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 957, loss 3.913468837738037\n",
      "Epoch 9: |          | 958/? [22:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 958, loss 3.940507173538208\n",
      "Epoch 9: |          | 959/? [22:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 959, loss 3.4204208850860596\n",
      "Epoch 9: |          | 960/? [23:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 960, loss 4.027273178100586\n",
      "Epoch 9: |          | 961/? [23:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 961, loss 4.306525707244873\n",
      "Epoch 9: |          | 962/? [23:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 962, loss 3.789477586746216\n",
      "Epoch 9: |          | 963/? [23:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 963, loss 3.603921413421631\n",
      "Epoch 9: |          | 964/? [23:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 964, loss 4.054129600524902\n",
      "Epoch 9: |          | 965/? [23:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 965, loss 3.5318267345428467\n",
      "Epoch 9: |          | 966/? [23:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 966, loss 3.4068284034729004\n",
      "Epoch 9: |          | 967/? [23:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 967, loss 3.6560873985290527\n",
      "Epoch 9: |          | 968/? [23:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 968, loss 3.5858683586120605\n",
      "Epoch 9: |          | 969/? [23:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 969, loss 3.49596905708313\n",
      "Epoch 9: |          | 970/? [23:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 970, loss 3.9407219886779785\n",
      "Epoch 9: |          | 971/? [23:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 971, loss 4.109206676483154\n",
      "Epoch 9: |          | 972/? [23:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 972, loss 3.6099162101745605\n",
      "Epoch 9: |          | 973/? [23:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 973, loss 3.756049633026123\n",
      "Epoch 9: |          | 974/? [23:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 974, loss 3.794837474822998\n",
      "Epoch 9: |          | 975/? [23:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 975, loss 3.8204338550567627\n",
      "Epoch 9: |          | 976/? [23:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 976, loss 3.881007671356201\n",
      "Epoch 9: |          | 977/? [23:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 977, loss 4.472460746765137\n",
      "Epoch 9: |          | 978/? [23:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 978, loss 3.880755662918091\n",
      "Epoch 9: |          | 979/? [23:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 979, loss 4.240540027618408\n",
      "Epoch 9: |          | 980/? [23:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 980, loss 3.3486990928649902\n",
      "Epoch 9: |          | 981/? [23:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 981, loss 3.195417881011963\n",
      "Epoch 9: |          | 982/? [23:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 982, loss 3.850334882736206\n",
      "Epoch 9: |          | 983/? [23:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 983, loss 4.286293983459473\n",
      "Epoch 9: |          | 984/? [23:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 984, loss 3.3638644218444824\n",
      "Epoch 9: |          | 985/? [23:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 985, loss 3.582170009613037\n",
      "Epoch 9: |          | 986/? [23:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 986, loss 3.651998519897461\n",
      "Epoch 9: |          | 987/? [23:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 987, loss 3.1780648231506348\n",
      "Epoch 9: |          | 988/? [23:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 988, loss 4.167678356170654\n",
      "Epoch 9: |          | 989/? [23:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 989, loss 3.816831588745117\n",
      "Epoch 9: |          | 990/? [23:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 990, loss 3.172179698944092\n",
      "Epoch 9: |          | 991/? [23:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 991, loss 3.8893184661865234\n",
      "Epoch 9: |          | 992/? [23:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 992, loss 4.574225425720215\n",
      "Epoch 9: |          | 993/? [23:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 993, loss 3.6768341064453125\n",
      "Epoch 9: |          | 994/? [23:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 994, loss 3.700368881225586\n",
      "Epoch 9: |          | 995/? [23:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 995, loss 4.038997650146484\n",
      "Epoch 9: |          | 996/? [23:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 996, loss 4.068722724914551\n",
      "Epoch 9: |          | 997/? [23:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 997, loss 3.694348096847534\n",
      "Epoch 9: |          | 998/? [23:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 998, loss 3.9386203289031982\n",
      "Epoch 9: |          | 999/? [23:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 999, loss 3.8527424335479736\n",
      "Epoch 9: |          | 1000/? [23:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1000, loss 3.3950278759002686\n",
      "Epoch 9: |          | 1001/? [23:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1001, loss 4.082156181335449\n",
      "Epoch 9: |          | 1002/? [24:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1002, loss 3.992457151412964\n",
      "Epoch 9: |          | 1003/? [24:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1003, loss 4.220907211303711\n",
      "Epoch 9: |          | 1004/? [24:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1004, loss 3.2205729484558105\n",
      "Epoch 9: |          | 1005/? [24:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1005, loss 3.7890801429748535\n",
      "Epoch 9: |          | 1006/? [24:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1006, loss 4.080794334411621\n",
      "Epoch 9: |          | 1007/? [24:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1007, loss 3.67258882522583\n",
      "Epoch 9: |          | 1008/? [24:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1008, loss 3.833975315093994\n",
      "Epoch 9: |          | 1009/? [24:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1009, loss 4.100266456604004\n",
      "Epoch 9: |          | 1010/? [24:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1010, loss 3.2441413402557373\n",
      "Epoch 9: |          | 1011/? [24:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1011, loss 3.8110008239746094\n",
      "Epoch 9: |          | 1012/? [24:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1012, loss 3.620659351348877\n",
      "Epoch 9: |          | 1013/? [24:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1013, loss 3.771528720855713\n",
      "Epoch 9: |          | 1014/? [24:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1014, loss 4.214071750640869\n",
      "Epoch 9: |          | 1015/? [24:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1015, loss 3.867205858230591\n",
      "Epoch 9: |          | 1016/? [24:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1016, loss 3.6018879413604736\n",
      "Epoch 9: |          | 1017/? [24:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1017, loss 2.9943130016326904\n",
      "Epoch 9: |          | 1018/? [24:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1018, loss 3.705260753631592\n",
      "Epoch 9: |          | 1019/? [24:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1019, loss 3.772075653076172\n",
      "Epoch 9: |          | 1020/? [24:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1020, loss 3.4056293964385986\n",
      "Epoch 9: |          | 1021/? [24:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1021, loss 3.648366928100586\n",
      "Epoch 9: |          | 1022/? [24:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1022, loss 3.4065277576446533\n",
      "Epoch 9: |          | 1023/? [24:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1023, loss 3.180589199066162\n",
      "Epoch 9: |          | 1024/? [24:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1024, loss 3.6297028064727783\n",
      "Epoch 9: |          | 1025/? [24:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1025, loss 3.5090012550354004\n",
      "Epoch 9: |          | 1026/? [24:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1026, loss 2.701791763305664\n",
      "Epoch 9: |          | 1027/? [24:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1027, loss 3.8954970836639404\n",
      "Epoch 9: |          | 1028/? [24:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1028, loss 3.7322661876678467\n",
      "Epoch 9: |          | 1029/? [24:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1029, loss 3.5767807960510254\n",
      "Epoch 9: |          | 1030/? [24:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1030, loss 3.4543590545654297\n",
      "Epoch 9: |          | 1031/? [24:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1031, loss 3.526735305786133\n",
      "Epoch 9: |          | 1032/? [24:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1032, loss 3.898275375366211\n",
      "Epoch 9: |          | 1033/? [24:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1033, loss 4.133737564086914\n",
      "Epoch 9: |          | 1034/? [24:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1034, loss 3.5217909812927246\n",
      "Epoch 9: |          | 1035/? [24:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1035, loss 3.5574562549591064\n",
      "Epoch 9: |          | 1036/? [24:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1036, loss 3.499514102935791\n",
      "Epoch 9: |          | 1037/? [24:51<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1037, loss 4.077520847320557\n",
      "Epoch 9: |          | 1038/? [24:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1038, loss 4.2481865882873535\n",
      "Epoch 9: |          | 1039/? [24:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1039, loss 4.583247661590576\n",
      "Epoch 9: |          | 1040/? [24:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1040, loss 3.8592631816864014\n",
      "Epoch 9: |          | 1041/? [24:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1041, loss 4.148462772369385\n",
      "Epoch 9: |          | 1042/? [24:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1042, loss 3.773502826690674\n",
      "Epoch 9: |          | 1043/? [25:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1043, loss 4.141210556030273\n",
      "Epoch 9: |          | 1044/? [25:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1044, loss 3.7289772033691406\n",
      "Epoch 9: |          | 1045/? [25:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1045, loss 3.2440617084503174\n",
      "Epoch 9: |          | 1046/? [25:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1046, loss 3.1210248470306396\n",
      "Epoch 9: |          | 1047/? [25:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1047, loss 4.269108772277832\n",
      "Epoch 9: |          | 1048/? [25:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1048, loss 3.733318328857422\n",
      "Epoch 9: |          | 1049/? [25:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1049, loss 3.9369564056396484\n",
      "Epoch 9: |          | 1050/? [25:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1050, loss 3.5220165252685547\n",
      "Epoch 9: |          | 1051/? [25:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1051, loss 3.430039644241333\n",
      "Epoch 9: |          | 1052/? [25:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1052, loss 4.0221076011657715\n",
      "Epoch 9: |          | 1053/? [25:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1053, loss 4.25931453704834\n",
      "Epoch 9: |          | 1054/? [25:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1054, loss 3.6403629779815674\n",
      "Epoch 9: |          | 1055/? [25:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1055, loss 3.3307902812957764\n",
      "Epoch 9: |          | 1056/? [25:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1056, loss 3.3511714935302734\n",
      "Epoch 9: |          | 1057/? [25:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1057, loss 3.948340654373169\n",
      "Epoch 9: |          | 1058/? [25:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1058, loss 3.528045654296875\n",
      "Epoch 9: |          | 1059/? [25:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1059, loss 4.1091132164001465\n",
      "Epoch 9: |          | 1060/? [25:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1060, loss 3.973400831222534\n",
      "Epoch 9: |          | 1061/? [25:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1061, loss 2.796581745147705\n",
      "Epoch 9: |          | 1062/? [25:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1062, loss 3.6400554180145264\n",
      "Epoch 9: |          | 1063/? [25:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1063, loss 3.727160692214966\n",
      "Epoch 9: |          | 1064/? [25:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1064, loss 3.9093470573425293\n",
      "Epoch 9: |          | 1065/? [25:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1065, loss 2.5466976165771484\n",
      "Epoch 9: |          | 1066/? [25:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1066, loss 3.8230719566345215\n",
      "Epoch 9: |          | 1067/? [25:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1067, loss 3.3868911266326904\n",
      "Epoch 9: |          | 1068/? [25:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1068, loss 3.5358474254608154\n",
      "Epoch 9: |          | 1069/? [25:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1069, loss 3.877836227416992\n",
      "Epoch 9: |          | 1070/? [25:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1070, loss 3.646811008453369\n",
      "Epoch 9: |          | 1071/? [25:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1071, loss 4.085341453552246\n",
      "Epoch 9: |          | 1072/? [25:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1072, loss 4.090272426605225\n",
      "Epoch 9: |          | 1073/? [25:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1073, loss 4.260372161865234\n",
      "Epoch 9: |          | 1074/? [25:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1074, loss 3.5910210609436035\n",
      "Epoch 9: |          | 1075/? [25:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1075, loss 3.3994956016540527\n",
      "Epoch 9: |          | 1076/? [25:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1076, loss 3.9608874320983887\n",
      "Epoch 9: |          | 1077/? [25:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1077, loss 3.498120069503784\n",
      "Epoch 9: |          | 1078/? [25:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1078, loss 3.800596237182617\n",
      "Epoch 9: |          | 1079/? [25:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1079, loss 4.200235843658447\n",
      "Epoch 9: |          | 1080/? [25:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1080, loss 3.7169976234436035\n",
      "Epoch 9: |          | 1081/? [25:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1081, loss 4.025498867034912\n",
      "Epoch 9: |          | 1082/? [25:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1082, loss 3.612335205078125\n",
      "Epoch 9: |          | 1083/? [25:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1083, loss 3.173069715499878\n",
      "Epoch 9: |          | 1084/? [25:58<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1084, loss 3.00106143951416\n",
      "Epoch 9: |          | 1085/? [26:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1085, loss 3.6625747680664062\n",
      "Epoch 9: |          | 1086/? [26:01<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1086, loss 3.932586669921875\n",
      "Epoch 9: |          | 1087/? [26:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1087, loss 4.44307804107666\n",
      "Epoch 9: |          | 1088/? [26:04<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1088, loss 3.979971408843994\n",
      "Epoch 9: |          | 1089/? [26:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1089, loss 3.8906848430633545\n",
      "Epoch 9: |          | 1090/? [26:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1090, loss 3.7702622413635254\n",
      "Epoch 9: |          | 1091/? [26:08<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1091, loss 3.5397167205810547\n",
      "Epoch 9: |          | 1092/? [26:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1092, loss 3.842210054397583\n",
      "Epoch 9: |          | 1093/? [26:11<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1093, loss 3.31469464302063\n",
      "Epoch 9: |          | 1094/? [26:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1094, loss 3.807138442993164\n",
      "Epoch 9: |          | 1095/? [26:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1095, loss 3.882070541381836\n",
      "Epoch 9: |          | 1096/? [26:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1096, loss 4.1349711418151855\n",
      "Epoch 9: |          | 1097/? [26:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1097, loss 3.7018611431121826\n",
      "Epoch 9: |          | 1098/? [26:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1098, loss 3.01735520362854\n",
      "Epoch 9: |          | 1099/? [26:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1099, loss 3.6601874828338623\n",
      "Epoch 9: |          | 1100/? [26:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1100, loss 3.8454031944274902\n",
      "Epoch 9: |          | 1101/? [26:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1101, loss 3.5159339904785156\n",
      "Epoch 9: |          | 1102/? [26:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1102, loss 4.199110507965088\n",
      "Epoch 9: |          | 1103/? [26:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1103, loss 4.584339618682861\n",
      "Epoch 9: |          | 1104/? [26:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1104, loss 4.002920150756836\n",
      "Epoch 9: |          | 1105/? [26:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1105, loss 4.166961669921875\n",
      "Epoch 9: |          | 1106/? [26:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1106, loss 3.661022186279297\n",
      "Epoch 9: |          | 1107/? [26:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1107, loss 3.7875053882598877\n",
      "Epoch 9: |          | 1108/? [26:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1108, loss 3.769162654876709\n",
      "Epoch 9: |          | 1109/? [26:35<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1109, loss 3.400148868560791\n",
      "Epoch 9: |          | 1110/? [26:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1110, loss 4.280889987945557\n",
      "Epoch 9: |          | 1111/? [26:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1111, loss 3.964935779571533\n",
      "Epoch 9: |          | 1112/? [26:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1112, loss 3.841963291168213\n",
      "Epoch 9: |          | 1113/? [26:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1113, loss 3.659759998321533\n",
      "Epoch 9: |          | 1114/? [26:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1114, loss 3.121412515640259\n",
      "Epoch 9: |          | 1115/? [26:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1115, loss 2.918807029724121\n",
      "Epoch 9: |          | 1116/? [26:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1116, loss 3.301362991333008\n",
      "Epoch 9: |          | 1117/? [26:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1117, loss 3.4223086833953857\n",
      "Epoch 9: |          | 1118/? [26:48<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1118, loss 3.6298065185546875\n",
      "Epoch 9: |          | 1119/? [26:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1119, loss 4.21342658996582\n",
      "Epoch 9: |          | 1120/? [26:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1120, loss 3.7625579833984375\n",
      "Epoch 9: |          | 1121/? [26:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1121, loss 4.019782066345215\n",
      "Epoch 9: |          | 1122/? [26:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1122, loss 3.5068111419677734\n",
      "Epoch 9: |          | 1123/? [26:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1123, loss 3.803102493286133\n",
      "Epoch 9: |          | 1124/? [26:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1124, loss 4.150172233581543\n",
      "Epoch 9: |          | 1125/? [26:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1125, loss 3.4616096019744873\n",
      "Epoch 9: |          | 1126/? [26:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1126, loss 3.375591278076172\n",
      "Epoch 9: |          | 1127/? [27:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1127, loss 3.643977403640747\n",
      "Epoch 9: |          | 1128/? [27:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1128, loss 3.744906187057495\n",
      "Epoch 9: |          | 1129/? [27:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1129, loss 3.8656699657440186\n",
      "Epoch 9: |          | 1130/? [27:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1130, loss 4.031793117523193\n",
      "Epoch 9: |          | 1131/? [27:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1131, loss 4.129042625427246\n",
      "Epoch 9: |          | 1132/? [27:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1132, loss 2.9379680156707764\n",
      "Epoch 9: |          | 1133/? [27:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1133, loss 3.716564893722534\n",
      "Epoch 9: |          | 1134/? [27:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1134, loss 3.5491161346435547\n",
      "Epoch 9: |          | 1135/? [27:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1135, loss 4.163328647613525\n",
      "Epoch 9: |          | 1136/? [27:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1136, loss 3.7850418090820312\n",
      "Epoch 9: |          | 1137/? [27:14<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1137, loss 3.7947545051574707\n",
      "Epoch 9: |          | 1138/? [27:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1138, loss 4.281155586242676\n",
      "Epoch 9: |          | 1139/? [27:17<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1139, loss 3.994490385055542\n",
      "Epoch 9: |          | 1140/? [27:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1140, loss 3.6912734508514404\n",
      "Epoch 9: |          | 1141/? [27:20<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1141, loss 4.149049758911133\n",
      "Epoch 9: |          | 1142/? [27:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1142, loss 4.176180362701416\n",
      "Epoch 9: |          | 1143/? [27:23<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1143, loss 4.232357978820801\n",
      "Epoch 9: |          | 1144/? [27:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1144, loss 3.6440818309783936\n",
      "Epoch 9: |          | 1145/? [27:26<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1145, loss 3.7696871757507324\n",
      "Epoch 9: |          | 1146/? [27:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1146, loss 3.3968424797058105\n",
      "Epoch 9: |          | 1147/? [27:29<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1147, loss 3.3854737281799316\n",
      "Epoch 9: |          | 1148/? [27:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1148, loss 3.5405845642089844\n",
      "Epoch 9: |          | 1149/? [27:32<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1149, loss 4.577853202819824\n",
      "Epoch 9: |          | 1150/? [27:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1150, loss 3.965007781982422\n",
      "Epoch 9: |          | 1151/? [27:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1151, loss 4.228593349456787\n",
      "Epoch 9: |          | 1152/? [27:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1152, loss 3.475512981414795\n",
      "Epoch 9: |          | 1153/? [27:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1153, loss 3.885572910308838\n",
      "Epoch 9: |          | 1154/? [27:39<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1154, loss 3.5657639503479004\n",
      "Epoch 9: |          | 1155/? [27:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1155, loss 3.769214153289795\n",
      "Epoch 9: |          | 1156/? [27:42<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1156, loss 3.7488045692443848\n",
      "Epoch 9: |          | 1157/? [27:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1157, loss 3.9840195178985596\n",
      "Epoch 9: |          | 1158/? [27:45<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1158, loss 4.184616565704346\n",
      "Epoch 9: |          | 1159/? [27:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1159, loss 3.043705463409424\n",
      "Epoch 9: |          | 1160/? [27:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1160, loss 4.182507514953613\n",
      "Epoch 9: |          | 1161/? [27:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1161, loss 4.008635520935059\n",
      "Epoch 9: |          | 1162/? [27:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1162, loss 3.968230724334717\n",
      "Epoch 9: |          | 1163/? [27:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1163, loss 4.5347490310668945\n",
      "Epoch 9: |          | 1164/? [27:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1164, loss 4.323126792907715\n",
      "Epoch 9: |          | 1165/? [27:54<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1165, loss 3.5074551105499268\n",
      "Epoch 9: |          | 1166/? [27:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1166, loss 3.992642641067505\n",
      "Epoch 9: |          | 1167/? [27:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1167, loss 4.0044732093811035\n",
      "Epoch 9: |          | 1168/? [27:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1168, loss 4.451509952545166\n",
      "Epoch 9: |          | 1169/? [28:00<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1169, loss 3.5412216186523438\n",
      "Epoch 9: |          | 1170/? [28:02<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1170, loss 4.085780143737793\n",
      "Epoch 9: |          | 1171/? [28:03<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1171, loss 3.4453940391540527\n",
      "Epoch 9: |          | 1172/? [28:05<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1172, loss 3.3797607421875\n",
      "Epoch 9: |          | 1173/? [28:06<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1173, loss 3.958397626876831\n",
      "Epoch 9: |          | 1174/? [28:07<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1174, loss 3.4062469005584717\n",
      "Epoch 9: |          | 1175/? [28:09<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1175, loss 4.04741096496582\n",
      "Epoch 9: |          | 1176/? [28:10<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1176, loss 4.0684309005737305\n",
      "Epoch 9: |          | 1177/? [28:12<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1177, loss 4.226273536682129\n",
      "Epoch 9: |          | 1178/? [28:13<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1178, loss 3.6461691856384277\n",
      "Epoch 9: |          | 1179/? [28:15<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1179, loss 4.1933794021606445\n",
      "Epoch 9: |          | 1180/? [28:16<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1180, loss 4.022337913513184\n",
      "Epoch 9: |          | 1181/? [28:18<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1181, loss 3.9478487968444824\n",
      "Epoch 9: |          | 1182/? [28:19<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1182, loss 3.7522125244140625\n",
      "Epoch 9: |          | 1183/? [28:21<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1183, loss 3.465967893600464\n",
      "Epoch 9: |          | 1184/? [28:22<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1184, loss 3.881932020187378\n",
      "Epoch 9: |          | 1185/? [28:24<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1185, loss 3.641310930252075\n",
      "Epoch 9: |          | 1186/? [28:25<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1186, loss 3.84318470954895\n",
      "Epoch 9: |          | 1187/? [28:27<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1187, loss 3.7230937480926514\n",
      "Epoch 9: |          | 1188/? [28:28<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1188, loss 4.091789722442627\n",
      "Epoch 9: |          | 1189/? [28:30<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1189, loss 4.21060848236084\n",
      "Epoch 9: |          | 1190/? [28:31<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1190, loss 3.7431445121765137\n",
      "Epoch 9: |          | 1191/? [28:33<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1191, loss 3.785971164703369\n",
      "Epoch 9: |          | 1192/? [28:34<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1192, loss 4.1036810874938965\n",
      "Epoch 9: |          | 1193/? [28:36<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1193, loss 3.5848896503448486\n",
      "Epoch 9: |          | 1194/? [28:37<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1194, loss 3.333591938018799\n",
      "Epoch 9: |          | 1195/? [28:38<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1195, loss 3.833068370819092\n",
      "Epoch 9: |          | 1196/? [28:40<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1196, loss 4.023730278015137\n",
      "Epoch 9: |          | 1197/? [28:41<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1197, loss 3.787717819213867\n",
      "Epoch 9: |          | 1198/? [28:43<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1198, loss 3.8991780281066895\n",
      "Epoch 9: |          | 1199/? [28:44<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1199, loss 4.184962272644043\n",
      "Epoch 9: |          | 1200/? [28:46<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1200, loss 3.4053409099578857\n",
      "Epoch 9: |          | 1201/? [28:47<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1201, loss 3.95758318901062\n",
      "Epoch 9: |          | 1202/? [28:49<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1202, loss 3.6821913719177246\n",
      "Epoch 9: |          | 1203/? [28:50<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1203, loss 3.6393959522247314\n",
      "Epoch 9: |          | 1204/? [28:52<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1204, loss 3.1993329524993896\n",
      "Epoch 9: |          | 1205/? [28:53<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1205, loss 3.7620151042938232\n",
      "Epoch 9: |          | 1206/? [28:55<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1206, loss 3.79875111579895\n",
      "Epoch 9: |          | 1207/? [28:56<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1207, loss 4.061593532562256\n",
      "Epoch 9: |          | 1208/? [28:57<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1208, loss 4.276525974273682\n",
      "Epoch 9: |          | 1209/? [28:59<00:00,  0.70it/s, v_num=31]   TRRAINING: Batch 1209, loss 3.85748028755188\n",
      "Epoch 9: |          | 1210/? [29:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1210, loss 4.116436958312988\n",
      "Epoch 9: |          | 1211/? [29:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1211, loss 4.067084789276123\n",
      "Epoch 9: |          | 1212/? [29:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1212, loss 3.9108340740203857\n",
      "Epoch 9: |          | 1213/? [29:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1213, loss 3.6109328269958496\n",
      "Epoch 9: |          | 1214/? [29:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1214, loss 4.186041831970215\n",
      "Epoch 9: |          | 1215/? [29:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1215, loss 3.6777586936950684\n",
      "Epoch 9: |          | 1216/? [29:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1216, loss 3.7473349571228027\n",
      "Epoch 9: |          | 1217/? [29:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1217, loss 3.8980727195739746\n",
      "Epoch 9: |          | 1218/? [29:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1218, loss 3.94873309135437\n",
      "Epoch 9: |          | 1219/? [29:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1219, loss 3.63873028755188\n",
      "Epoch 9: |          | 1220/? [29:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1220, loss 4.2821760177612305\n",
      "Epoch 9: |          | 1221/? [29:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1221, loss 3.8707149028778076\n",
      "Epoch 9: |          | 1222/? [29:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1222, loss 3.019469976425171\n",
      "Epoch 9: |          | 1223/? [29:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1223, loss 3.227639675140381\n",
      "Epoch 9: |          | 1224/? [29:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1224, loss 3.5616798400878906\n",
      "Epoch 9: |          | 1225/? [29:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1225, loss 4.1894731521606445\n",
      "Epoch 9: |          | 1226/? [29:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1226, loss 4.152344703674316\n",
      "Epoch 9: |          | 1227/? [29:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1227, loss 3.835752487182617\n",
      "Epoch 9: |          | 1228/? [29:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1228, loss 3.6873421669006348\n",
      "Epoch 9: |          | 1229/? [29:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1229, loss 3.3058571815490723\n",
      "Epoch 9: |          | 1230/? [29:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1230, loss 3.968032121658325\n",
      "Epoch 9: |          | 1231/? [29:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1231, loss 4.021708011627197\n",
      "Epoch 9: |          | 1232/? [29:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1232, loss 4.167788505554199\n",
      "Epoch 9: |          | 1233/? [29:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1233, loss 3.9493813514709473\n",
      "Epoch 9: |          | 1234/? [29:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1234, loss 3.0189778804779053\n",
      "Epoch 9: |          | 1235/? [29:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1235, loss 4.07920503616333\n",
      "Epoch 9: |          | 1236/? [29:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1236, loss 3.463766098022461\n",
      "Epoch 9: |          | 1237/? [29:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1237, loss 3.733675479888916\n",
      "Epoch 9: |          | 1238/? [29:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1238, loss 3.8003220558166504\n",
      "Epoch 9: |          | 1239/? [29:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1239, loss 3.6547393798828125\n",
      "Epoch 9: |          | 1240/? [29:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1240, loss 4.265370845794678\n",
      "Epoch 9: |          | 1241/? [29:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1241, loss 3.803525447845459\n",
      "Epoch 9: |          | 1242/? [29:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1242, loss 3.6392219066619873\n",
      "Epoch 9: |          | 1243/? [29:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1243, loss 3.5580058097839355\n",
      "Epoch 9: |          | 1244/? [29:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1244, loss 3.6992053985595703\n",
      "Epoch 9: |          | 1245/? [29:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1245, loss 3.315852642059326\n",
      "Epoch 9: |          | 1246/? [29:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1246, loss 3.960742473602295\n",
      "Epoch 9: |          | 1247/? [29:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1247, loss 3.9719696044921875\n",
      "Epoch 9: |          | 1248/? [29:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1248, loss 3.558866500854492\n",
      "Epoch 9: |          | 1249/? [30:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1249, loss 3.712036609649658\n",
      "Epoch 9: |          | 1250/? [30:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1250, loss 3.865696430206299\n",
      "Epoch 9: |          | 1251/? [30:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1251, loss 3.5788967609405518\n",
      "Epoch 9: |          | 1252/? [30:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1252, loss 4.36671257019043\n",
      "Epoch 9: |          | 1253/? [30:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1253, loss 3.7336769104003906\n",
      "Epoch 9: |          | 1254/? [30:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1254, loss 3.084855556488037\n",
      "Epoch 9: |          | 1255/? [30:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1255, loss 4.434798717498779\n",
      "Epoch 9: |          | 1256/? [30:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1256, loss 3.4310545921325684\n",
      "Epoch 9: |          | 1257/? [30:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1257, loss 3.465092420578003\n",
      "Epoch 9: |          | 1258/? [30:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1258, loss 4.157567024230957\n",
      "Epoch 9: |          | 1259/? [30:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1259, loss 3.80473256111145\n",
      "Epoch 9: |          | 1260/? [30:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1260, loss 4.2814836502075195\n",
      "Epoch 9: |          | 1261/? [30:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1261, loss 3.615142345428467\n",
      "Epoch 9: |          | 1262/? [30:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1262, loss 3.61553955078125\n",
      "Epoch 9: |          | 1263/? [30:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1263, loss 3.91355562210083\n",
      "Epoch 9: |          | 1264/? [30:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1264, loss 4.214539527893066\n",
      "Epoch 9: |          | 1265/? [30:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1265, loss 4.076294898986816\n",
      "Epoch 9: |          | 1266/? [30:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1266, loss 3.7493624687194824\n",
      "Epoch 9: |          | 1267/? [30:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1267, loss 3.8525142669677734\n",
      "Epoch 9: |          | 1268/? [30:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1268, loss 3.6945419311523438\n",
      "Epoch 9: |          | 1269/? [30:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1269, loss 3.291447401046753\n",
      "Epoch 9: |          | 1270/? [30:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1270, loss 3.618752956390381\n",
      "Epoch 9: |          | 1271/? [30:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1271, loss 3.7650344371795654\n",
      "Epoch 9: |          | 1272/? [30:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1272, loss 3.3891711235046387\n",
      "Epoch 9: |          | 1273/? [30:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1273, loss 4.036772727966309\n",
      "Epoch 9: |          | 1274/? [30:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1274, loss 2.983551025390625\n",
      "Epoch 9: |          | 1275/? [30:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1275, loss 3.482840061187744\n",
      "Epoch 9: |          | 1276/? [30:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1276, loss 3.791922092437744\n",
      "Epoch 9: |          | 1277/? [30:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1277, loss 3.517436981201172\n",
      "Epoch 9: |          | 1278/? [30:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1278, loss 3.190882682800293\n",
      "Epoch 9: |          | 1279/? [30:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1279, loss 3.8985378742218018\n",
      "Epoch 9: |          | 1280/? [30:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1280, loss 3.1051182746887207\n",
      "Epoch 9: |          | 1281/? [30:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1281, loss 3.64379620552063\n",
      "Epoch 9: |          | 1282/? [30:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1282, loss 3.3104846477508545\n",
      "Epoch 9: |          | 1283/? [30:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1283, loss 4.059809684753418\n",
      "Epoch 9: |          | 1284/? [30:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1284, loss 3.2052903175354004\n",
      "Epoch 9: |          | 1285/? [30:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1285, loss 4.216605186462402\n",
      "Epoch 9: |          | 1286/? [30:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1286, loss 2.7393558025360107\n",
      "Epoch 9: |          | 1287/? [30:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1287, loss 4.164417266845703\n",
      "Epoch 9: |          | 1288/? [30:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1288, loss 3.9767203330993652\n",
      "Epoch 9: |          | 1289/? [30:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1289, loss 3.1135880947113037\n",
      "Epoch 9: |          | 1290/? [31:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1290, loss 3.8218154907226562\n",
      "Epoch 9: |          | 1291/? [31:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1291, loss 4.6666579246521\n",
      "Epoch 9: |          | 1292/? [31:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1292, loss 4.035693168640137\n",
      "Epoch 9: |          | 1293/? [31:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1293, loss 3.5520858764648438\n",
      "Epoch 9: |          | 1294/? [31:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1294, loss 3.770033359527588\n",
      "Epoch 9: |          | 1295/? [31:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1295, loss 3.8653690814971924\n",
      "Epoch 9: |          | 1296/? [31:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1296, loss 3.178762912750244\n",
      "Epoch 9: |          | 1297/? [31:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1297, loss 4.038483142852783\n",
      "Epoch 9: |          | 1298/? [31:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1298, loss 3.7497055530548096\n",
      "Epoch 9: |          | 1299/? [31:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1299, loss 2.946512460708618\n",
      "Epoch 9: |          | 1300/? [31:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1300, loss 3.7922511100769043\n",
      "Epoch 9: |          | 1301/? [31:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1301, loss 3.50494122505188\n",
      "Epoch 9: |          | 1302/? [31:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1302, loss 3.6306064128875732\n",
      "Epoch 9: |          | 1303/? [31:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1303, loss 3.5993800163269043\n",
      "Epoch 9: |          | 1304/? [31:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1304, loss 4.320092678070068\n",
      "Epoch 9: |          | 1305/? [31:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1305, loss 3.152836322784424\n",
      "Epoch 9: |          | 1306/? [31:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1306, loss 3.76139497756958\n",
      "Epoch 9: |          | 1307/? [31:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1307, loss 3.394087553024292\n",
      "Epoch 9: |          | 1308/? [31:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1308, loss 3.3327269554138184\n",
      "Epoch 9: |          | 1309/? [31:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1309, loss 3.390817165374756\n",
      "Epoch 9: |          | 1310/? [31:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1310, loss 3.9081382751464844\n",
      "Epoch 9: |          | 1311/? [31:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1311, loss 3.4026637077331543\n",
      "Epoch 9: |          | 1312/? [31:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1312, loss 3.142099618911743\n",
      "Epoch 9: |          | 1313/? [31:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1313, loss 4.257651329040527\n",
      "Epoch 9: |          | 1314/? [31:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1314, loss 3.5040366649627686\n",
      "Epoch 9: |          | 1315/? [31:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1315, loss 4.249209403991699\n",
      "Epoch 9: |          | 1316/? [31:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1316, loss 4.006384372711182\n",
      "Epoch 9: |          | 1317/? [31:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1317, loss 3.657245635986328\n",
      "Epoch 9: |          | 1318/? [31:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1318, loss 3.8375866413116455\n",
      "Epoch 9: |          | 1319/? [31:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1319, loss 3.997128963470459\n",
      "Epoch 9: |          | 1320/? [31:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1320, loss 3.6095263957977295\n",
      "Epoch 9: |          | 1321/? [31:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1321, loss 4.054203510284424\n",
      "Epoch 9: |          | 1322/? [31:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1322, loss 3.8753135204315186\n",
      "Epoch 9: |          | 1323/? [31:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1323, loss 3.393332004547119\n",
      "Epoch 9: |          | 1324/? [31:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1324, loss 4.3220534324646\n",
      "Epoch 9: |          | 1325/? [31:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1325, loss 4.451061725616455\n",
      "Epoch 9: |          | 1326/? [31:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1326, loss 3.8750452995300293\n",
      "Epoch 9: |          | 1327/? [31:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1327, loss 3.7619919776916504\n",
      "Epoch 9: |          | 1328/? [31:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1328, loss 3.4540324211120605\n",
      "Epoch 9: |          | 1329/? [31:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1329, loss 4.023544788360596\n",
      "Epoch 9: |          | 1330/? [31:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1330, loss 3.785968780517578\n",
      "Epoch 9: |          | 1331/? [31:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1331, loss 3.885672092437744\n",
      "Epoch 9: |          | 1332/? [32:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1332, loss 3.5866668224334717\n",
      "Epoch 9: |          | 1333/? [32:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1333, loss 3.588251829147339\n",
      "Epoch 9: |          | 1334/? [32:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1334, loss 3.7266011238098145\n",
      "Epoch 9: |          | 1335/? [32:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1335, loss 3.6456634998321533\n",
      "Epoch 9: |          | 1336/? [32:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1336, loss 3.2486374378204346\n",
      "Epoch 9: |          | 1337/? [32:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1337, loss 3.8825924396514893\n",
      "Epoch 9: |          | 1338/? [32:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1338, loss 3.140094757080078\n",
      "Epoch 9: |          | 1339/? [32:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1339, loss 3.79455828666687\n",
      "Epoch 9: |          | 1340/? [32:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1340, loss 3.208495616912842\n",
      "Epoch 9: |          | 1341/? [32:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1341, loss 3.9042961597442627\n",
      "Epoch 9: |          | 1342/? [32:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1342, loss 4.1814351081848145\n",
      "Epoch 9: |          | 1343/? [32:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1343, loss 3.6645355224609375\n",
      "Epoch 9: |          | 1344/? [32:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1344, loss 3.7620232105255127\n",
      "Epoch 9: |          | 1345/? [32:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1345, loss 3.885692596435547\n",
      "Epoch 9: |          | 1346/? [32:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1346, loss 5.012316703796387\n",
      "Epoch 9: |          | 1347/? [32:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1347, loss 3.9429728984832764\n",
      "Epoch 9: |          | 1348/? [32:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1348, loss 4.0551581382751465\n",
      "Epoch 9: |          | 1349/? [32:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1349, loss 3.9427425861358643\n",
      "Epoch 9: |          | 1350/? [32:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1350, loss 4.011631011962891\n",
      "Epoch 9: |          | 1351/? [32:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1351, loss 3.9736580848693848\n",
      "Epoch 9: |          | 1352/? [32:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1352, loss 3.2314581871032715\n",
      "Epoch 9: |          | 1353/? [32:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1353, loss 3.595864772796631\n",
      "Epoch 9: |          | 1354/? [32:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1354, loss 3.9443142414093018\n",
      "Epoch 9: |          | 1355/? [32:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1355, loss 4.096050262451172\n",
      "Epoch 9: |          | 1356/? [32:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1356, loss 3.838921070098877\n",
      "Epoch 9: |          | 1357/? [32:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1357, loss 3.6270179748535156\n",
      "Epoch 9: |          | 1358/? [32:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1358, loss 3.810539960861206\n",
      "Epoch 9: |          | 1359/? [32:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1359, loss 3.6701278686523438\n",
      "Epoch 9: |          | 1360/? [32:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1360, loss 3.8311588764190674\n",
      "Epoch 9: |          | 1361/? [32:42<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1361, loss 3.8203883171081543\n",
      "Epoch 9: |          | 1362/? [32:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1362, loss 3.651102066040039\n",
      "Epoch 9: |          | 1363/? [32:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1363, loss 3.115832805633545\n",
      "Epoch 9: |          | 1364/? [32:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1364, loss 3.5934860706329346\n",
      "Epoch 9: |          | 1365/? [32:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1365, loss 3.3067994117736816\n",
      "Epoch 9: |          | 1366/? [32:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1366, loss 4.0161943435668945\n",
      "Epoch 9: |          | 1367/? [32:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1367, loss 3.3202903270721436\n",
      "Epoch 9: |          | 1368/? [32:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1368, loss 3.1396632194519043\n",
      "Epoch 9: |          | 1369/? [32:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1369, loss 3.8271985054016113\n",
      "Epoch 9: |          | 1370/? [32:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1370, loss 3.3600430488586426\n",
      "Epoch 9: |          | 1371/? [32:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1371, loss 4.269495964050293\n",
      "Epoch 9: |          | 1372/? [32:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1372, loss 3.713594436645508\n",
      "Epoch 9: |          | 1373/? [32:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1373, loss 4.1049041748046875\n",
      "Epoch 9: |          | 1374/? [33:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1374, loss 3.2081775665283203\n",
      "Epoch 9: |          | 1375/? [33:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1375, loss 3.819377899169922\n",
      "Epoch 9: |          | 1376/? [33:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1376, loss 3.6880745887756348\n",
      "Epoch 9: |          | 1377/? [33:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1377, loss 3.713806629180908\n",
      "Epoch 9: |          | 1378/? [33:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1378, loss 3.6603426933288574\n",
      "Epoch 9: |          | 1379/? [33:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1379, loss 3.726292371749878\n",
      "Epoch 9: |          | 1380/? [33:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1380, loss 3.8253371715545654\n",
      "Epoch 9: |          | 1381/? [33:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1381, loss 3.938129425048828\n",
      "Epoch 9: |          | 1382/? [33:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1382, loss 3.536991834640503\n",
      "Epoch 9: |          | 1383/? [33:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1383, loss 3.680676221847534\n",
      "Epoch 9: |          | 1384/? [33:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1384, loss 3.7501449584960938\n",
      "Epoch 9: |          | 1385/? [33:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1385, loss 3.723179578781128\n",
      "Epoch 9: |          | 1386/? [33:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1386, loss 3.7928378582000732\n",
      "Epoch 9: |          | 1387/? [33:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1387, loss 3.7195403575897217\n",
      "Epoch 9: |          | 1388/? [33:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1388, loss 3.256678819656372\n",
      "Epoch 9: |          | 1389/? [33:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1389, loss 3.936063051223755\n",
      "Epoch 9: |          | 1390/? [33:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1390, loss 4.271315574645996\n",
      "Epoch 9: |          | 1391/? [33:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1391, loss 3.8597607612609863\n",
      "Epoch 9: |          | 1392/? [33:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1392, loss 3.374915599822998\n",
      "Epoch 9: |          | 1393/? [33:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1393, loss 3.535439968109131\n",
      "Epoch 9: |          | 1394/? [33:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1394, loss 3.1767101287841797\n",
      "Epoch 9: |          | 1395/? [33:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1395, loss 3.8640029430389404\n",
      "Epoch 9: |          | 1396/? [33:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1396, loss 3.7480320930480957\n",
      "Epoch 9: |          | 1397/? [33:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1397, loss 3.0291056632995605\n",
      "Epoch 9: |          | 1398/? [33:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1398, loss 4.1966986656188965\n",
      "Epoch 9: |          | 1399/? [33:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1399, loss 4.193343162536621\n",
      "Epoch 9: |          | 1400/? [33:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1400, loss 3.3397903442382812\n",
      "Epoch 9: |          | 1401/? [33:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1401, loss 4.100505352020264\n",
      "Epoch 9: |          | 1402/? [33:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1402, loss 3.7137656211853027\n",
      "Epoch 9: |          | 1403/? [33:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1403, loss 3.928117275238037\n",
      "Epoch 9: |          | 1404/? [33:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1404, loss 3.891693592071533\n",
      "Epoch 9: |          | 1405/? [33:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1405, loss 4.223526477813721\n",
      "Epoch 9: |          | 1406/? [33:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1406, loss 4.137356758117676\n",
      "Epoch 9: |          | 1407/? [33:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1407, loss 4.192697048187256\n",
      "Epoch 9: |          | 1408/? [33:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1408, loss 3.4090163707733154\n",
      "Epoch 9: |          | 1409/? [33:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1409, loss 3.5188732147216797\n",
      "Epoch 9: |          | 1410/? [33:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1410, loss 3.5304439067840576\n",
      "Epoch 9: |          | 1411/? [33:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1411, loss 3.845731019973755\n",
      "Epoch 9: |          | 1412/? [33:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1412, loss 3.5233588218688965\n",
      "Epoch 9: |          | 1413/? [33:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1413, loss 3.4159717559814453\n",
      "Epoch 9: |          | 1414/? [33:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1414, loss 3.5481674671173096\n",
      "Epoch 9: |          | 1415/? [34:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1415, loss 3.7931885719299316\n",
      "Epoch 9: |          | 1416/? [34:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1416, loss 4.154192924499512\n",
      "Epoch 9: |          | 1417/? [34:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1417, loss 3.8388123512268066\n",
      "Epoch 9: |          | 1418/? [34:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1418, loss 3.952993869781494\n",
      "Epoch 9: |          | 1419/? [34:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1419, loss 3.7597720623016357\n",
      "Epoch 9: |          | 1420/? [34:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1420, loss 3.5908656120300293\n",
      "Epoch 9: |          | 1421/? [34:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1421, loss 3.370953321456909\n",
      "Epoch 9: |          | 1422/? [34:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1422, loss 3.9829063415527344\n",
      "Epoch 9: |          | 1423/? [34:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1423, loss 4.029747486114502\n",
      "Epoch 9: |          | 1424/? [34:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1424, loss 3.7335269451141357\n",
      "Epoch 9: |          | 1425/? [34:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1425, loss 3.918259382247925\n",
      "Epoch 9: |          | 1426/? [34:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1426, loss 3.4688847064971924\n",
      "Epoch 9: |          | 1427/? [34:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1427, loss 4.068049430847168\n",
      "Epoch 9: |          | 1428/? [34:19<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1428, loss 4.048277378082275\n",
      "Epoch 9: |          | 1429/? [34:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1429, loss 3.9535751342773438\n",
      "Epoch 9: |          | 1430/? [34:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1430, loss 4.004896640777588\n",
      "Epoch 9: |          | 1431/? [34:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1431, loss 3.7644832134246826\n",
      "Epoch 9: |          | 1432/? [34:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1432, loss 3.8214097023010254\n",
      "Epoch 9: |          | 1433/? [34:26<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1433, loss 3.7237753868103027\n",
      "Epoch 9: |          | 1434/? [34:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1434, loss 3.8646087646484375\n",
      "Epoch 9: |          | 1435/? [34:29<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1435, loss 3.484745502471924\n",
      "Epoch 9: |          | 1436/? [34:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1436, loss 3.839995861053467\n",
      "Epoch 9: |          | 1437/? [34:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1437, loss 3.0873963832855225\n",
      "Epoch 9: |          | 1438/? [34:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1438, loss 4.725069522857666\n",
      "Epoch 9: |          | 1439/? [34:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1439, loss 3.8320040702819824\n",
      "Epoch 9: |          | 1440/? [34:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1440, loss 3.9954609870910645\n",
      "Epoch 9: |          | 1441/? [34:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1441, loss 4.308668613433838\n",
      "Epoch 9: |          | 1442/? [34:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1442, loss 4.332024097442627\n",
      "Epoch 9: |          | 1443/? [34:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1443, loss 3.500011444091797\n",
      "Epoch 9: |          | 1444/? [34:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1444, loss 3.521611452102661\n",
      "Epoch 9: |          | 1445/? [34:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1445, loss 3.9734158515930176\n",
      "Epoch 9: |          | 1446/? [34:45<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1446, loss 3.5671794414520264\n",
      "Epoch 9: |          | 1447/? [34:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1447, loss 3.7393240928649902\n",
      "Epoch 9: |          | 1448/? [34:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1448, loss 3.562232255935669\n",
      "Epoch 9: |          | 1449/? [34:49<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1449, loss 3.768077850341797\n",
      "Epoch 9: |          | 1450/? [34:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1450, loss 3.8202946186065674\n",
      "Epoch 9: |          | 1451/? [34:52<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1451, loss 4.234614372253418\n",
      "Epoch 9: |          | 1452/? [34:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1452, loss 3.8465709686279297\n",
      "Epoch 9: |          | 1453/? [34:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1453, loss 3.2092978954315186\n",
      "Epoch 9: |          | 1454/? [34:56<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1454, loss 3.788395643234253\n",
      "Epoch 9: |          | 1455/? [34:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1455, loss 3.859593152999878\n",
      "Epoch 9: |          | 1456/? [34:59<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1456, loss 3.4465231895446777\n",
      "Epoch 9: |          | 1457/? [35:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1457, loss 3.6263439655303955\n",
      "Epoch 9: |          | 1458/? [35:02<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1458, loss 3.858058214187622\n",
      "Epoch 9: |          | 1459/? [35:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1459, loss 3.9431710243225098\n",
      "Epoch 9: |          | 1460/? [35:05<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1460, loss 3.916936159133911\n",
      "Epoch 9: |          | 1461/? [35:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1461, loss 3.814540386199951\n",
      "Epoch 9: |          | 1462/? [35:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1462, loss 4.118036270141602\n",
      "Epoch 9: |          | 1463/? [35:09<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1463, loss 4.03536319732666\n",
      "Epoch 9: |          | 1464/? [35:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1464, loss 3.345510959625244\n",
      "Epoch 9: |          | 1465/? [35:12<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1465, loss 3.7243728637695312\n",
      "Epoch 9: |          | 1466/? [35:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1466, loss 3.4257049560546875\n",
      "Epoch 9: |          | 1467/? [35:15<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1467, loss 4.062585353851318\n",
      "Epoch 9: |          | 1468/? [35:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1468, loss 3.623699188232422\n",
      "Epoch 9: |          | 1469/? [35:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1469, loss 3.377150774002075\n",
      "Epoch 9: |          | 1470/? [35:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1470, loss 3.8902270793914795\n",
      "Epoch 9: |          | 1471/? [35:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1471, loss 4.176024913787842\n",
      "Epoch 9: |          | 1472/? [35:23<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1472, loss 3.904695987701416\n",
      "Epoch 9: |          | 1473/? [35:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1473, loss 3.70324969291687\n",
      "Epoch 9: |          | 1474/? [35:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1474, loss 3.6424057483673096\n",
      "Epoch 9: |          | 1475/? [35:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1475, loss 3.224884510040283\n",
      "Epoch 9: |          | 1476/? [35:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1476, loss 3.7945454120635986\n",
      "Epoch 9: |          | 1477/? [35:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1477, loss 3.8065688610076904\n",
      "Epoch 9: |          | 1478/? [35:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1478, loss 3.691702365875244\n",
      "Epoch 9: |          | 1479/? [35:33<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1479, loss 4.219189167022705\n",
      "Epoch 9: |          | 1480/? [35:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1480, loss 3.936908721923828\n",
      "Epoch 9: |          | 1481/? [35:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1481, loss 3.687117338180542\n",
      "Epoch 9: |          | 1482/? [35:37<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1482, loss 3.857919692993164\n",
      "Epoch 9: |          | 1483/? [35:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1483, loss 3.5177206993103027\n",
      "Epoch 9: |          | 1484/? [35:40<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1484, loss 3.703291654586792\n",
      "Epoch 9: |          | 1485/? [35:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1485, loss 3.8235905170440674\n",
      "Epoch 9: |          | 1486/? [35:43<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1486, loss 3.825319290161133\n",
      "Epoch 9: |          | 1487/? [35:44<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1487, loss 3.2934861183166504\n",
      "Epoch 9: |          | 1488/? [35:46<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1488, loss 3.9390740394592285\n",
      "Epoch 9: |          | 1489/? [35:47<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1489, loss 3.8904964923858643\n",
      "Epoch 9: |          | 1490/? [35:48<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1490, loss 3.751547336578369\n",
      "Epoch 9: |          | 1491/? [35:50<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1491, loss 2.8145933151245117\n",
      "Epoch 9: |          | 1492/? [35:51<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1492, loss 3.362058162689209\n",
      "Epoch 9: |          | 1493/? [35:53<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1493, loss 2.91784405708313\n",
      "Epoch 9: |          | 1494/? [35:54<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1494, loss 3.701288938522339\n",
      "Epoch 9: |          | 1495/? [35:55<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1495, loss 3.572416305541992\n",
      "Epoch 9: |          | 1496/? [35:57<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1496, loss 3.9583141803741455\n",
      "Epoch 9: |          | 1497/? [35:58<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1497, loss 3.190793514251709\n",
      "Epoch 9: |          | 1498/? [36:00<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1498, loss 3.4436802864074707\n",
      "Epoch 9: |          | 1499/? [36:01<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1499, loss 4.020808696746826\n",
      "Epoch 9: |          | 1500/? [36:03<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1500, loss 3.9558894634246826\n",
      "Epoch 9: |          | 1501/? [36:04<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1501, loss 3.808488368988037\n",
      "Epoch 9: |          | 1502/? [36:06<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1502, loss 3.873553514480591\n",
      "Epoch 9: |          | 1503/? [36:07<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1503, loss 3.6345620155334473\n",
      "Epoch 9: |          | 1504/? [36:08<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1504, loss 4.267378807067871\n",
      "Epoch 9: |          | 1505/? [36:10<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1505, loss 4.074429988861084\n",
      "Epoch 9: |          | 1506/? [36:11<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1506, loss 3.712369918823242\n",
      "Epoch 9: |          | 1507/? [36:13<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1507, loss 3.586395740509033\n",
      "Epoch 9: |          | 1508/? [36:14<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1508, loss 3.7002480030059814\n",
      "Epoch 9: |          | 1509/? [36:16<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1509, loss 3.7199058532714844\n",
      "Epoch 9: |          | 1510/? [36:17<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1510, loss 4.005105972290039\n",
      "Epoch 9: |          | 1511/? [36:18<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1511, loss 3.5975136756896973\n",
      "Epoch 9: |          | 1512/? [36:20<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1512, loss 4.151284694671631\n",
      "Epoch 9: |          | 1513/? [36:21<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1513, loss 4.397542476654053\n",
      "Epoch 9: |          | 1514/? [36:22<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1514, loss 3.4699673652648926\n",
      "Epoch 9: |          | 1515/? [36:24<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1515, loss 4.315952777862549\n",
      "Epoch 9: |          | 1516/? [36:25<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1516, loss 4.221352577209473\n",
      "Epoch 9: |          | 1517/? [36:27<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1517, loss 3.691016435623169\n",
      "Epoch 9: |          | 1518/? [36:28<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1518, loss 3.4565036296844482\n",
      "Epoch 9: |          | 1519/? [36:30<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1519, loss 4.009697914123535\n",
      "Epoch 9: |          | 1520/? [36:31<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1520, loss 4.049851894378662\n",
      "Epoch 9: |          | 1521/? [36:32<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1521, loss 3.802534580230713\n",
      "Epoch 9: |          | 1522/? [36:34<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1522, loss 3.569483518600464\n",
      "Epoch 9: |          | 1523/? [36:35<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1523, loss 3.8730874061584473\n",
      "Epoch 9: |          | 1524/? [36:36<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1524, loss 3.7616610527038574\n",
      "Epoch 9: |          | 1525/? [36:38<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1525, loss 3.4214038848876953\n",
      "Epoch 9: |          | 1526/? [36:39<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1526, loss 4.007258892059326\n",
      "Epoch 9: |          | 1527/? [36:41<00:00,  0.69it/s, v_num=31]   TRRAINING: Batch 1527, loss 4.016476631164551\n",
      "Epoch 9: |          | 1528/? [36:42<00:00,  0.69it/s, v_num=31]ERROR: Input has inproper shape\n",
      "Epoch 9: |          | 1529/? [36:42<00:00,  0.69it/s, v_num=31]   VALIDATION: Batch 0, loss 4.560156345367432\n",
      "   VALIDATION: Batch 1, loss 3.56654691696167\n",
      "   VALIDATION: Batch 2, loss 4.724747180938721\n",
      "   VALIDATION: Batch 3, loss 4.4192585945129395\n",
      "   VALIDATION: Batch 4, loss 3.9820303916931152\n",
      "   VALIDATION: Batch 5, loss 3.6775708198547363\n",
      "   VALIDATION: Batch 6, loss 3.9631924629211426\n",
      "   VALIDATION: Batch 7, loss 4.633586406707764\n",
      "   VALIDATION: Batch 8, loss 4.459654331207275\n",
      "   VALIDATION: Batch 9, loss 4.622735500335693\n",
      "   VALIDATION: Batch 10, loss 4.277223587036133\n",
      "   VALIDATION: Batch 11, loss 3.9545788764953613\n",
      "   VALIDATION: Batch 12, loss 4.1216254234313965\n",
      "   VALIDATION: Batch 13, loss 4.701296806335449\n",
      "   VALIDATION: Batch 14, loss 3.919792890548706\n",
      "   VALIDATION: Batch 15, loss 3.8982975482940674\n",
      "   VALIDATION: Batch 16, loss 4.598074436187744\n",
      "   VALIDATION: Batch 17, loss 4.25850772857666\n",
      "   VALIDATION: Batch 18, loss 3.5154030323028564\n",
      "   VALIDATION: Batch 19, loss 4.392787456512451\n",
      "   VALIDATION: Batch 20, loss 4.706362724304199\n",
      "   VALIDATION: Batch 21, loss 4.9222517013549805\n",
      "   VALIDATION: Batch 22, loss 4.57614803314209\n",
      "   VALIDATION: Batch 23, loss 4.107302665710449\n",
      "   VALIDATION: Batch 24, loss 3.9316375255584717\n",
      "   VALIDATION: Batch 25, loss 4.340515613555908\n",
      "   VALIDATION: Batch 26, loss 4.55312442779541\n",
      "   VALIDATION: Batch 27, loss 4.426487922668457\n",
      "   VALIDATION: Batch 28, loss 4.2120866775512695\n",
      "   VALIDATION: Batch 29, loss 4.392986297607422\n",
      "   VALIDATION: Batch 30, loss 4.040156364440918\n",
      "   VALIDATION: Batch 31, loss 4.33152437210083\n",
      "   VALIDATION: Batch 32, loss 4.856106758117676\n",
      "   VALIDATION: Batch 33, loss 3.1083741188049316\n",
      "   VALIDATION: Batch 34, loss 4.291240692138672\n",
      "   VALIDATION: Batch 35, loss 4.540325164794922\n",
      "   VALIDATION: Batch 36, loss 3.7904486656188965\n",
      "   VALIDATION: Batch 37, loss 3.8346779346466064\n",
      "   VALIDATION: Batch 38, loss 3.906919002532959\n",
      "   VALIDATION: Batch 39, loss 4.314210414886475\n",
      "   VALIDATION: Batch 40, loss 4.3159894943237305\n",
      "   VALIDATION: Batch 41, loss 3.15259051322937\n",
      "   VALIDATION: Batch 42, loss 4.332393646240234\n",
      "   VALIDATION: Batch 43, loss 4.528188228607178\n",
      "   VALIDATION: Batch 44, loss 4.130617141723633\n",
      "   VALIDATION: Batch 45, loss 4.519808769226074\n",
      "   VALIDATION: Batch 46, loss 3.6583728790283203\n",
      "   VALIDATION: Batch 47, loss 4.654278755187988\n",
      "   VALIDATION: Batch 48, loss 4.7758917808532715\n",
      "   VALIDATION: Batch 49, loss 4.380183219909668\n",
      "   VALIDATION: Batch 50, loss 4.354994297027588\n",
      "   VALIDATION: Batch 51, loss 4.841220378875732\n",
      "   VALIDATION: Batch 52, loss 4.011932373046875\n",
      "   VALIDATION: Batch 53, loss 3.8811252117156982\n",
      "   VALIDATION: Batch 54, loss 3.99576997756958\n",
      "   VALIDATION: Batch 55, loss 4.679406642913818\n",
      "   VALIDATION: Batch 56, loss 4.106429576873779\n",
      "   VALIDATION: Batch 57, loss 5.670012950897217\n",
      "   VALIDATION: Batch 58, loss 4.222416400909424\n",
      "   VALIDATION: Batch 59, loss 3.8816895484924316\n",
      "   VALIDATION: Batch 60, loss 3.3831043243408203\n",
      "   VALIDATION: Batch 61, loss 4.257176399230957\n",
      "   VALIDATION: Batch 62, loss 4.246237754821777\n",
      "   VALIDATION: Batch 63, loss 4.79160737991333\n",
      "   VALIDATION: Batch 64, loss 4.5685954093933105\n",
      "   VALIDATION: Batch 65, loss 3.715686321258545\n",
      "   VALIDATION: Batch 66, loss 4.667932987213135\n",
      "   VALIDATION: Batch 67, loss 4.071496486663818\n",
      "   VALIDATION: Batch 68, loss 4.221735000610352\n",
      "   VALIDATION: Batch 69, loss 4.4573822021484375\n",
      "   VALIDATION: Batch 70, loss 4.631264686584473\n",
      "   VALIDATION: Batch 71, loss 4.152949333190918\n",
      "   VALIDATION: Batch 72, loss 5.030712604522705\n",
      "   VALIDATION: Batch 73, loss 3.8302314281463623\n",
      "   VALIDATION: Batch 74, loss 4.430749893188477\n",
      "   VALIDATION: Batch 75, loss 4.384148597717285\n",
      "   VALIDATION: Batch 76, loss 4.22836971282959\n",
      "   VALIDATION: Batch 77, loss 4.562222480773926\n",
      "   VALIDATION: Batch 78, loss 4.378029823303223\n",
      "   VALIDATION: Batch 79, loss 4.323854923248291\n",
      "   VALIDATION: Batch 80, loss 4.412933349609375\n",
      "   VALIDATION: Batch 81, loss 4.138669013977051\n",
      "   VALIDATION: Batch 82, loss 4.514361381530762\n",
      "   VALIDATION: Batch 83, loss 3.798943042755127\n",
      "   VALIDATION: Batch 84, loss 4.481993675231934\n",
      "   VALIDATION: Batch 85, loss 4.115292072296143\n",
      "   VALIDATION: Batch 86, loss 4.221741676330566\n",
      "   VALIDATION: Batch 87, loss 4.094822883605957\n",
      "   VALIDATION: Batch 88, loss 3.6753249168395996\n",
      "   VALIDATION: Batch 89, loss 3.9919419288635254\n",
      "   VALIDATION: Batch 90, loss 4.252007484436035\n",
      "   VALIDATION: Batch 91, loss 4.200877666473389\n",
      "   VALIDATION: Batch 92, loss 3.9631431102752686\n",
      "   VALIDATION: Batch 93, loss 4.727688312530518\n",
      "   VALIDATION: Batch 94, loss 4.185838222503662\n",
      "   VALIDATION: Batch 95, loss 3.706857204437256\n",
      "   VALIDATION: Batch 96, loss 4.151669025421143\n",
      "   VALIDATION: Batch 97, loss 3.8755576610565186\n",
      "   VALIDATION: Batch 98, loss 4.507391929626465\n",
      "   VALIDATION: Batch 99, loss 4.615006446838379\n",
      "   VALIDATION: Batch 100, loss 4.932997703552246\n",
      "   VALIDATION: Batch 101, loss 3.5413413047790527\n",
      "   VALIDATION: Batch 102, loss 4.9912309646606445\n",
      "   VALIDATION: Batch 103, loss 4.904054641723633\n",
      "   VALIDATION: Batch 104, loss 3.829932689666748\n",
      "   VALIDATION: Batch 105, loss 4.286465644836426\n",
      "   VALIDATION: Batch 106, loss 4.217896938323975\n",
      "   VALIDATION: Batch 107, loss 4.308815956115723\n",
      "   VALIDATION: Batch 108, loss 3.98791241645813\n",
      "   VALIDATION: Batch 109, loss 4.593378067016602\n",
      "   VALIDATION: Batch 110, loss 4.285216331481934\n",
      "   VALIDATION: Batch 111, loss 4.616700649261475\n",
      "   VALIDATION: Batch 112, loss 5.501315116882324\n",
      "   VALIDATION: Batch 113, loss 4.788510322570801\n",
      "   VALIDATION: Batch 114, loss 4.575277805328369\n",
      "   VALIDATION: Batch 115, loss 4.024298667907715\n",
      "   VALIDATION: Batch 116, loss 3.8539319038391113\n",
      "   VALIDATION: Batch 117, loss 4.520196437835693\n",
      "   VALIDATION: Batch 118, loss 4.726334095001221\n",
      "   VALIDATION: Batch 119, loss 3.9050889015197754\n",
      "   VALIDATION: Batch 120, loss 3.543254852294922\n",
      "   VALIDATION: Batch 121, loss 3.7735581398010254\n",
      "   VALIDATION: Batch 122, loss 4.151362895965576\n",
      "   VALIDATION: Batch 123, loss 4.17971658706665\n",
      "   VALIDATION: Batch 124, loss 3.58868408203125\n",
      "   VALIDATION: Batch 125, loss 4.194118022918701\n",
      "   VALIDATION: Batch 126, loss 4.4114203453063965\n",
      "   VALIDATION: Batch 127, loss 4.177013874053955\n",
      "   VALIDATION: Batch 128, loss 4.346105098724365\n",
      "   VALIDATION: Batch 129, loss 3.9582107067108154\n",
      "   VALIDATION: Batch 130, loss 3.6277709007263184\n",
      "   VALIDATION: Batch 131, loss 3.647902727127075\n",
      "   VALIDATION: Batch 132, loss 4.2844133377075195\n",
      "   VALIDATION: Batch 133, loss 4.440784454345703\n",
      "   VALIDATION: Batch 134, loss 4.3025312423706055\n",
      "   VALIDATION: Batch 135, loss 4.578286170959473\n",
      "   VALIDATION: Batch 136, loss 4.69237756729126\n",
      "   VALIDATION: Batch 137, loss 4.50520658493042\n",
      "   VALIDATION: Batch 138, loss 4.242585182189941\n",
      "   VALIDATION: Batch 139, loss 4.6157402992248535\n",
      "   VALIDATION: Batch 140, loss 3.7441630363464355\n",
      "   VALIDATION: Batch 141, loss 4.6772356033325195\n",
      "   VALIDATION: Batch 142, loss 3.422471523284912\n",
      "   VALIDATION: Batch 143, loss 4.194090843200684\n",
      "   VALIDATION: Batch 144, loss 4.465465545654297\n",
      "   VALIDATION: Batch 145, loss 4.228706359863281\n",
      "   VALIDATION: Batch 146, loss 4.076786041259766\n",
      "   VALIDATION: Batch 147, loss 4.390089988708496\n",
      "   VALIDATION: Batch 148, loss 4.482083320617676\n",
      "   VALIDATION: Batch 149, loss 4.999016284942627\n",
      "   VALIDATION: Batch 150, loss 4.550928115844727\n",
      "   VALIDATION: Batch 151, loss 4.863330841064453\n",
      "   VALIDATION: Batch 152, loss 4.215516090393066\n",
      "   VALIDATION: Batch 153, loss 4.467240333557129\n",
      "   VALIDATION: Batch 154, loss 4.298105716705322\n",
      "   VALIDATION: Batch 155, loss 4.085472583770752\n",
      "   VALIDATION: Batch 156, loss 4.723144054412842\n",
      "   VALIDATION: Batch 157, loss 4.3627095222473145\n",
      "   VALIDATION: Batch 158, loss 3.8275694847106934\n",
      "   VALIDATION: Batch 159, loss 4.279777526855469\n",
      "   VALIDATION: Batch 160, loss 4.632780075073242\n",
      "   VALIDATION: Batch 161, loss 4.7551116943359375\n",
      "   VALIDATION: Batch 162, loss 4.251031398773193\n",
      "   VALIDATION: Batch 163, loss 3.696399211883545\n",
      "   VALIDATION: Batch 164, loss 4.23724365234375\n",
      "   VALIDATION: Batch 165, loss 4.652951717376709\n",
      "   VALIDATION: Batch 166, loss 4.1077470779418945\n",
      "   VALIDATION: Batch 167, loss 4.534592628479004\n",
      "   VALIDATION: Batch 168, loss 3.4027819633483887\n",
      "   VALIDATION: Batch 169, loss 4.099398612976074\n",
      "   VALIDATION: Batch 170, loss 4.257242679595947\n",
      "   VALIDATION: Batch 171, loss 4.440490245819092\n",
      "   VALIDATION: Batch 172, loss 4.254630088806152\n",
      "   VALIDATION: Batch 173, loss 4.161900520324707\n",
      "   VALIDATION: Batch 174, loss 4.583796501159668\n",
      "   VALIDATION: Batch 175, loss 4.370207786560059\n",
      "   VALIDATION: Batch 176, loss 4.128566265106201\n",
      "   VALIDATION: Batch 177, loss 4.037179946899414\n",
      "   VALIDATION: Batch 178, loss 5.173797130584717\n",
      "   VALIDATION: Batch 179, loss 4.440576553344727\n",
      "   VALIDATION: Batch 180, loss 3.9041361808776855\n",
      "   VALIDATION: Batch 181, loss 4.105227470397949\n",
      "   VALIDATION: Batch 182, loss 4.391221523284912\n",
      "   VALIDATION: Batch 183, loss 3.476666212081909\n",
      "   VALIDATION: Batch 184, loss 3.238030195236206\n",
      "   VALIDATION: Batch 185, loss 4.007686138153076\n",
      "   VALIDATION: Batch 186, loss 3.8467628955841064\n",
      "   VALIDATION: Batch 187, loss 4.07326602935791\n",
      "   VALIDATION: Batch 188, loss 4.500702857971191\n",
      "   VALIDATION: Batch 189, loss 3.832399368286133\n",
      "   VALIDATION: Batch 190, loss 3.9257240295410156\n",
      "   VALIDATION: Batch 191, loss 4.40859317779541\n",
      "   VALIDATION: Batch 192, loss 4.708729267120361\n",
      "ERROR: Input has inproper shape\n",
      "Epoch 9: |          | 1529/? [38:45<00:00,  0.66it/s, v_num=31]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: |          | 1529/? [38:45<00:00,  0.66it/s, v_num=31]\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "# Define the checkpoint callback to save the model every 1000 batches\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=model_saving_path,  # Directory to save the checkpoints\n",
    "    filename=saving_filename,  # Filename pattern\n",
    "    save_top_k=-1,  # Save all models\n",
    "    save_weights_only=False,  # Save only the weights (or set to False to save the full model)\n",
    "    every_n_train_steps=save_every_n_baches  # Save the model every 1000 batches\n",
    ")\n",
    "#new tensorboard for displaying logs\n",
    "\n",
    "# Define the logger\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"moe_plain_model\") ### CHANGE NAME FOR DIFFERENT RUN (different model)\n",
    "\n",
    "# Initialize the trainer with the checkpoint callback\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=epochs, # Set the number of epochs\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "trainer.fit(model = model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      batch_idx   val_loss\n",
      "0             0  10.493760\n",
      "1             1  10.454000\n",
      "2             0   5.458968\n",
      "3             1   4.351250\n",
      "4             2   5.631401\n",
      "...         ...        ...\n",
      "1927        188   4.500703\n",
      "1928        189   3.832399\n",
      "1929        190   3.925724\n",
      "1930        191   4.408593\n",
      "1931        192   4.708729\n",
      "\n",
      "[1932 rows x 2 columns]\n",
      "       batch_idx  train_loss\n",
      "0              0   10.485570\n",
      "1              1   10.003714\n",
      "2              2    8.945668\n",
      "3              3    9.716655\n",
      "4              4    8.552960\n",
      "...          ...         ...\n",
      "15275       1523    3.873087\n",
      "15276       1524    3.761661\n",
      "15277       1525    3.421404\n",
      "15278       1526    4.007259\n",
      "15279       1527    4.016477\n",
      "\n",
      "[15280 rows x 2 columns]\n",
      "DataFrame saved to D:/Projekt_NLP/Saved_stuff/logs/mh_moe\\logs_train_Normal_moe_model.csv\n",
      "DataFrame saved to D:/Projekt_NLP/Saved_stuff/logs/mh_moe\\logs_val_Normal_moe_model.csv\n"
     ]
    }
   ],
   "source": [
    "### Saving logs to csv\n",
    "val_data=model.val_losses_list\n",
    "train_data=model.train_losses_list\n",
    "# Convert list of dictionaries to DataFrame\n",
    "log_val_df = pd.DataFrame(val_data)\n",
    "log_train_df = pd.DataFrame(train_data)\n",
    "\n",
    "print(log_val_df)\n",
    "print(log_train_df)\n",
    "\n",
    "# Directory to save the CSV file\n",
    "save_dir = log_saving_path #'/path/to/your/directory'  # Replace with your desired directory path\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "# save_dir = 'D:/Projekt_NLP/Saved_stuff/logs/vectorized_moe'#'/path/to/your/directory'  # Replace with your desired directory path\n",
    "# os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "model_name='Normal_moe_model'\n",
    "# Define the filename for your CSV file\n",
    "csv_filename_train = f'logs_train_{model_name}.csv'\n",
    "csv_filename_val = f'logs_val_{model_name}.csv'\n",
    "\n",
    "# Construct the full file path\n",
    "csv_file_path_train = os.path.join(save_dir, csv_filename_train)\n",
    "csv_file_path_val = os.path.join(save_dir, csv_filename_val)\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "log_train_df.to_csv(csv_file_path_train, index=False)\n",
    "log_val_df.to_csv(csv_file_path_val, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {csv_file_path_train}\")\n",
    "print(f\"DataFrame saved to {csv_file_path_val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
