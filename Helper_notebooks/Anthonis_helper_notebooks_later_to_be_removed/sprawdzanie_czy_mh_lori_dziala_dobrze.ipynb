{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Komputer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "main_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(main_dir)\n",
    "\n",
    "import model_classes\n",
    "from model_classes import *\n",
    "from MH_Lori_poprawianie import *\n",
    "from dataloader import *\n",
    "import dataloader\n",
    "from helper_functions import *\n",
    "import torch\n",
    "from transformers import PretrainedConfig\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy\n",
    "import lightning.pytorch as pl\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kod miriam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
    "# Output: [batch_size, seq_len, num_experts] - expert routing weights\n",
    "class Router_miriam(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_experts_per_token = config.num_experts_per_token\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_experts = config.num_experts\n",
    "\n",
    "        self.expert_embeddings = nn.Parameter(torch.randn(self.num_experts, self.hidden_size)).to(config.device)\n",
    "        torch.nn.init.kaiming_uniform_(self.expert_embeddings, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        dot = torch.einsum(\"bsh,eh->bse\", x, self.expert_embeddings)\n",
    "        # top_k_out = torch.topk(dot, k=self.num_experts_per_token)\n",
    "        # top_k = (float(\"-inf\") * torch.ones_like(dot)).scatter_(dim=-1, index=top_k_out.indices, src=top_k_out.values)\n",
    "        # res = torch.nn.functional.softmax(top_k, dim=-1)\n",
    "        res = torch.nn.functional.softmax(dot, dim=-1)\n",
    "        return res\n",
    "    \n",
    "import math\n",
    "\n",
    "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
    "# Output: [batch_size, seq_len, hidden_size] - output embeddings\n",
    "class MoE_Lory(nn.Module):\n",
    "    \"\"\"version which takes first not random tokens up to expert_capacity\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.num_experts = config.num_experts\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_experts_per_token = config.num_experts_per_token\n",
    "        self.capacity_factor = config.capacity_factor\n",
    "        self.T_segments=config.T_segments\n",
    "        # You can change experts representation if you want\n",
    "        # self.experts = nn.ModuleList([MLP(config) for _ in range(self.num_experts)])\n",
    "        #not as above but as below instead so as to compare more easily with the vectorized version\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.first_linear = nn.Parameter(torch.randn(self.num_experts, self.intermediate_size, self.hidden_size))\n",
    "        torch.nn.init.kaiming_uniform_(self.first_linear, nonlinearity='linear')\n",
    "        self.second_linear = nn.Parameter(torch.randn(self.num_experts, self.hidden_size, self.intermediate_size))\n",
    "        torch.nn.init.kaiming_uniform_(self.second_linear, nonlinearity='linear')\n",
    "\n",
    "        self.router = Router_miriam(config)\n",
    "\n",
    "    def compute_out(self, data,linear1,linear2):\n",
    "        return linear2 @ torch.nn.functional.relu(linear1 @ data)\n",
    "\n",
    "    def merge_expert(self, weights):\n",
    "        num_exp,_,_=weights.shape\n",
    "        # expanded_weights1 = torch.ones((num_exp, self.intermediate_size, self.hidden_size)) * weights\n",
    "        # expanded_weights2 = torch.ones((num_exp, self.hidden_size, self.intermediate_size)) * weights\n",
    "        # linear1 = expanded_weights1 @ self.first_linear\n",
    "        # linear2 = expanded_weights1 @ self.second_linear\n",
    "        weighted_first_linear = torch.sum(weights * self.first_linear, dim=0)\n",
    "        weighted_second_linear = torch.sum(weights * self.second_linear, dim=0)\n",
    "        return weighted_first_linear,weighted_second_linear\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_size = x.shape\n",
    "        #assert hidden_size == self.hidden_size\n",
    "        # expert_capacity = math.ceil(batch_size * seq_len / self.num_experts * self.capacity_factor)\n",
    "        result = torch.zeros_like(x)\n",
    "        segment_size=seq_len//self.T_segments\n",
    "        for i in range(batch_size):\n",
    "            for t in range(self.T_segments):\n",
    "                segment=x[i, t*segment_size:(t+1)*segment_size]\n",
    "                #print(\"segment shape check:\",segment.shape,\"seq_len/T, hidden_dim\")\n",
    "\n",
    "                if t==0:\n",
    "                  with torch.no_grad():\n",
    "                    h_x=segment.sum(axis=0)/segment_size\n",
    "                    h_start = h_x.unsqueeze(dim  = 0)\n",
    "                    h_x=h_x.unsqueeze(0)\n",
    "                    h_x=h_x.unsqueeze(0)\n",
    "                    #print(\"h_x shapecheck:\",h_x.shape)\n",
    "                    print(f'h_x shape: {h_x.shape}, input routera')\n",
    "                    old_weights = self.router(h_x)\n",
    "                    print(f'output routera: {old_weights.shape}')\n",
    "                    log_w = old_weights\n",
    "                    # print(old_weights.shape)\n",
    "                    old_weights=old_weights.permute(2,0,1)\n",
    "                    #print(\"old_weights shapecheck:\",old_weights.shape)\n",
    "                    merged_linear1,merged_linear2=self.merge_expert(old_weights)\n",
    "                  for j in range(segment_size):\n",
    "                    result[i, t*segment_size+j] = self.compute_out(x[i, t*segment_size+j],merged_linear1,merged_linear2)\n",
    "                else:\n",
    "                  h_x=segment.sum(axis=0)/segment_size\n",
    "                  \n",
    "                  h_x=h_x.unsqueeze(0)\n",
    "                  h_start = torch.cat((h_start, h_x), dim = 0)\n",
    "\n",
    "                  h_x=h_x.unsqueeze(0)\n",
    "                  weights=self.router(h_x)\n",
    "                  log_w = torch.cat((log_w, weights), dim = 0)\n",
    "                  weights=weights.permute(2,0,1)\n",
    "                  #print(\"weights shapecheck:\",weights.shape)\n",
    "                  merged_linear1,merged_linear2=self.merge_expert(old_weights)\n",
    "                  old_weights=weights\n",
    "                  for j in range(segment_size):\n",
    "                    result[i, t*segment_size+j] = self.compute_out(x[i, t*segment_size+j],merged_linear1,merged_linear2)\n",
    "\n",
    "        return result, h_start, log_w ################################### TEN KOD JEST ZLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_config = dict(\n",
    "    vocab_size=5000,\n",
    "    max_position_embeddings=None,#256\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=None,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=512,\n",
    "    num_labels=2,#2 co to robi?\n",
    "    device = DEVICE #I added this one\n",
    ")\n",
    "moe_config = PretrainedConfig(\n",
    "    **base_config,\n",
    "    T_segments=5,\n",
    "    num_experts=2,\n",
    "    capacity_factor=None, #2.0\n",
    "    num_experts_per_token=None,#1\n",
    "    ff_cls=MoE_Lory\n",
    ")\n",
    "\n",
    "config_small= PretrainedConfig(\n",
    "    num_experts_per_token=2,\n",
    "    hidden_size=128,\n",
    "    num_attention_heads = 8,\n",
    "    num_MH_MOE_heads = 1,\n",
    "    num_experts=2,\n",
    "    batch_size = 1,\n",
    "    seq_len = 20,\n",
    "    capacity_factor = 3,\n",
    "    device = device,\n",
    "    intermediate_size = 512,\n",
    "    forward_layer_class = MH_Lori,\n",
    "    vocab_size = 5000,\n",
    "    n_layers = 8,\n",
    "    no_lori_segments = 5,\n",
    "    py_lightning_loging = False,\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(),\n",
    "    lr = 0.0006, #SET TO 0.0002\n",
    "    betas = (0.9, 0.95),\n",
    "    treat_mh_lori_as_regular_lori = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_x shape: torch.Size([1, 1, 128]), input routera\n",
      "output routera: torch.Size([1, 1, 2])\n",
      "torch.Size([1, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "lory = MoE_Lory(moe_config).to(DEVICE)\n",
    "lory.eval()\n",
    "batch_size, seq_len, hidden_size=1,20,128\n",
    "\n",
    "input = torch.randn((batch_size, seq_len, hidden_size)).to(DEVICE) * 10\n",
    "v, h_start, w_m = lory(input)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh_lori = MH_Lori(config_small).to(config_small.device)\n",
    "mh_lori.router.expert_embeddings.data = torch.transpose(lory.router.expert_embeddings.data, 0, 1)\n",
    "mh_lori.first_linear.data = lory.first_linear.data\n",
    "mh_lori.second_linear.data = lory.second_linear.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avarage segment embeding shape = torch.Size([1, 5, 1, 128]) [batch size, no segments, num_heads, head_dim] (to jest input routera)\n",
      "expert_weights shape = torch.Size([1, 5, 1, 2]) [bs, no seq, num heads, num experts]\n",
      "torch.Size([1, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "mh_lori.eval()\n",
    "o, h_antoni, w_a = mh_lori(input)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 0.14374220371246338\n"
     ]
    }
   ],
   "source": [
    "print(torch.equal(o, v), torch.max(abs(o - v)).detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2]) torch.Size([20, 2])\n",
      "True\n",
      "torch.Size([20, 2]) torch.Size([20, 2])\n",
      "True\n",
      "torch.Size([20, 2]) torch.Size([20, 2])\n",
      "True\n",
      "torch.Size([20, 2]) torch.Size([20, 2])\n",
      "True\n",
      "torch.Size([20, 2]) torch.Size([20, 2])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Router test\n",
    "for i in range(5):\n",
    "    router_a = Router_mh_lori(config_small)\n",
    "    router_m = Router_miriam(moe_config)\n",
    "    router_a.expert_embeddings.data = torch.transpose(router_m.expert_embeddings.data, 0, 1)\n",
    "    input = torch.randn((batch_size, seq_len, hidden_size)).to(DEVICE)\n",
    "    o_m = router_m(input).squeeze()\n",
    "    o_a = router_a(input.unsqueeze(dim = 2)).squeeze()\n",
    "    print(o_m.shape, o_a.shape)\n",
    "    print(torch.equal(o_m, o_a))\n",
    "\n",
    "#ROUTERY ROBIĄ TO SAMO, również na batch size 1 po malym mieszaniu wymiarow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_start.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 128])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_antoni[0].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(h_start, h_antoni[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5]) torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "print(torch.transpose(w_m.squeeze(), 1, 0).shape, w_a.squeeze()[:, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(torch.transpose(w_m.squeeze(), 1, 0), w_a.squeeze()[:, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
