% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage[
backend=biber,
style=apa,
sorting=ynt
]{biblatex} %Imports biblatex package
\addbibresource{citations.bib} %Import the bibliography file
 

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Multi-Headed Lory: Fine-Grained Differentiable Mixture-of-Experts}%replace X with the appropriate number
\author{MichaÅ‚ Maszkowski, Antoni Janowski, Miriam Lipniacka\\ %replace with your name
{m.maszkowsk2, m.lipniacka}@student.uw.edu.pl, aj421072@students.mimuw.edu.pl\\
Supervisor: Jan Ludziejewski}
\maketitle


\begin{center}
\begin{minipage}{0.8\textwidth}
\begin{abstract}
% Transformers have become a cornerstone in natural language processing, driving advances in tasks such as machine translation and text generation. The Transformer architecture revolutionized language modeling by enabling parallel information integration across sequences. Improving the attention mechanism is challenging due to its complexity, so researchers often focus on modifying the MLP layer to optimize performance. One of the most crucial advancements was Mixture-of-Experts (MoE) which allows models to scale up significantly in parameter size without proportionate increases in training or inference costs. Despite these advancements, Mixture-of-Experts (MoE) models continue to encounter challenges related to their non-differentiable discrete routing mechanisms and the under-utilization of certain experts. Recent research has concentrated on resolving these issues. This study aims to integrate the strengths of two of these recent innovations in the MoE layer, Multi-Head Mixture-of-Experts (MH-MoE) and Lory, into a unified framework to harness the benefits of both approaches.
%wersja Michala:
Recently 2 different MoE architecture improvements were introduced: (1) Lory (\cite{zhong2024lory}) that efficiently scales fully differentiable MoE architecture to autoregressive language modelling pretraining and (2) MH-MoE (\cite{wu2024multihead}) that employs multi-head mechanism to split tokens into subtokens that are processed in parallel before beinng reintegrated together allowing for more fine-grained analytical capabilities. In this paper, we present MH-Lory - the first architecture combining both of these advancements. 
Experimental results suggest/demonstrate ... Our work highlights the potential of merging Lory and MH-MoE architectures and encourages investigating this issue further with more resouces. 
Our code is available at \url{https://github.com/MichalMaszkowski/Multi-Headed_Lory}.
\end{abstract}
\end{minipage}
\end{center}

\section{Introduction}
Transformer architecture (\cite{vaswani2017attention}) introduced parallelizable way to combine information from distant parts of sequences allowing effective language modeling. It was improved by Mixture-of-Experts [MoE] (\cite{shazeer2017outrageously}) which allowed to train models with significantly more parameters without much increase in training or inference costs. But due to non-differentiable discrete routing, such as top-k expert-activation routing, MoE has a problem with learning effective routing strategies (\cite{muqeeth2023soft}). It also suffers from low expert activation and lack of fine-grained capabilities to analyze multiple semantic concepts within individual token (\cite{wu2024multihead}).

Recently multiple works have tried to address non-differentiability of expert choice in the MoE architecture (\cite{Puigcerver2023FromST}, \cite{muqeeth2023soft}). In (\cite{muqeeth2023soft}) each token is processed  by a "merged expert" which has parameters that are a sum of experts' parameters weighted uniquely for every token according to the router. This operation is differentiable, but increases computational complexity and memory requirements, scaling linearly with the number of experts, which makes it impractical for auto-regressive tasks. This problem is answered by Lory (\cite{zhong2024lory}) which recomputes the "merged expert" only once per segment (comprising about few hundred tokens) instead of doing so for each token, while avoiding information leakage.

A related problem of low expert activation is addressed by Multi-Head Mixture-of-Experts (MH-MoE)(\cite{wu2024multihead}). At the same time it claims to answer the lack of fine-grained capabilities to analyze multiple semantic concepts within individual token. 
It does so by splitting tokens into sub-tokens (as many as there are "heads") and then assigning and processing those in parallel with experts before reintegrating them. The naming convention resembles that of multi-head attention, though it is worth to note that the number of heads in these two parts doesn't need to be related.

In this work we propose to apply both techniques in a single architecture: introduced by Lory (\cite{zhong2024lory}) and by Multi-Head Mixture-of-Experts (MH-MoE)(\cite{wu2024multihead}). We introduce novel MoE architecture, MH-Lory, that is fully-differentiable and enables fine-grained capabilities to analyze different semantic meanings of every token. MH-Lory is a Lory baseline with addition of heads, that is precisely described in section 3 of this paper.



\section{Related work}

\subsection{MH-MoE} 
In (Sparse) MoE (\cite{shazeer2017outrageously}) that is a baseline for authors of MH-MoE each MoE block consists of $E$ independent feed forward networks [FFNs] called Experts, $e_i$, and a gating function (router), $g$, used to calculate weights over these experts' outputs, with k of them (activated according to top-k gated values) processing any token. That is for any input token's hidden representation $h \in \mathbb{R}^d$ output of the MoE block is:

\[ output(h)=h+\sum_{i=1}^{i=k} e_i (h)  \cdot g(h, e_i)\]

MH-MoE uses a multi-head mechanism to split each input token into sub-tokens, then processes them in parallel by diverse experts, and then reintegrates them into the original token form. Let us denote number of heads as $H$ and token's hidden representation $h \in \mathbb{R}^d $ split into $H$ sub-tokens as $h_1,h_2,...,h_H \in \mathbb{R}^{\frac{d}{H}} $. Then given $p$-th sub-token output of the MH-MoE block would be computed as:

\[ output(h_p)= h_p + \sum_{i=1}^{i=k} e_i (h_p)  \cdot g(h_p,e_i)\]

Additionally before splitting tokens they are multiplied by "multi-head projection" matrix, similarly they are multiplied by "merge" matrix after reintegration.

Thanks to this technique the average volume of data routed to a specific expert is increased by a factor of $H$.
Moreover, while current tokenization patterns limit a model's ability to capture multiple semantic interpretations of tokens, MH-MoE enables capturing information from diverse feature spaces across these experts (\cite{wu2024multihead}).


\subsection{Lory}
Assume that we have $E$ feed-forward expert networks, each parameterised by weights $\theta_i$. To process a sequence of $L$ tokens, with hidden representations $h_1, h_2, \dots h_L$ we divide it into $N$ segments of length $T = \frac{L}{N}$. Each segment will be processed by a single "merged expert" that is created based on the information from the previous segment. Let's suppose that we currently want to start to process segment $k>1$ (so we want to process tokens $h_k, h_{k+1}\dots ,h_{k+T -1}$, and previous segment consisted of tokens $h_{k - T}, h_{k-T+1}\dots ,h_{k-1}$). We process it like this:
\[\text{output}(h_{k + j}) = (h_{k + j} + \text{FFN}(h_{k+j}, \sum_{i=1}^E \theta_i w_i) \text{,   where  } w_i = \text{Softmax}(g(\frac{1}{T}\sum_{q = 0}^{T-1}h_{k-T+q}), e_i)\] where $\text{FFN}(a, b)$ means an expert (feed-forward networks) with weights $b$ on input $a$ and $R$ is our router network.

This approach allows to compute weighted sum of experts only once per segment making it suitable for auto-regressive tasks. It prevents information leakage and keeps the causal nature of the model.

At inference the prompt is used to calculate one routing used throughout generation. 
For processing the first segment, the authors propose to create an expert matrix by using all tokens from the first segment, and adding a stop gradient operation on top of the router.

The authors also used similarity-based data batching technique introduced by (\cite{shi2023context}). 
It ensures that batches contain similar documents, so even when one document ends and other start inside one segment, routing decision made based on the previous document can still be relevant to the next document, which enhances expert specialization in later layers.



\section{Our architecture: MH-Lory}
Both Lory and MH-MoE report promising gains in performance, while using techniques that we believe can be beneficially combined into MH-Lorry. 

Combining Lory and MH-MoE architectures admittedly poses a challenge as their strategies aren't orthogonal but diverge slightly. Lory achieves full activation of experts by merging them in pursue of full differentiability. Keeping this feature in the MH-Lory makes one of two advantages of MH-MoE irrelevant as introducing heads can't increase expert activation further. What's more, in Lory whole segments are processed by the same "merged expert" while MH-MoE goes in different direction, routing mere parts of tokens to different experts. Thus Lory decreases the variability of assigned experts between tokens compared to traditional MoE while MH-MoE does the opposite.

The most natural ways to combine Lory and MH-MoE is to use the Lory model as a baseline and modify it by adding heads. More precisely MH-Lory will calculate the distribution over experts used for making "merged expert" once per segment, separately for each head. We hope that we can still make an improvement on Lory by reaping the rewards of allowing more fine-grained understanding of multiple semantic concepts within a token as happens in MH-MoE. As an additional benefit by introducing expert heads to Lory architecture we decrease the number of parameters per expert allowing to increase the number of experts.

\section{Experiments}
Our main goal was to compare 2 versions of our model to Lory, MH-MoE and baseline MoE in terms of perplexity on a language modelling task with the equal number of active parameters. We will also compare the overall number of parameters that each has, and estimate inference and training efficiency.
Constrained by limited computational resources we plan to train and validate the aforementioned models on the Wikipedia dataset (\cite{wikidump}). We leave it for a future research to test how these models scale. We hope to pursue this question in case of promising experiment results.

We will experiment with and compare two versions of MH-Lory depending on whether all heads share the set of experts from which a "merged expert" is computed (v1) or whether each head has a separate set of experts (v2). Intuitively MH-Lory-v1 has advantage of increasing the number of examples (sub-tokens) per expert and having less parameters. But on the other hand MH-Lory-v2 allows for greater specialization of heads, potentially leading each head to focus on a particular aspect of token meaning. There are $N$ experts in MH-Lory-v1 and $N \cdot H$ experts in MH-Lory-v2.

%opis MH-Lory po opisie bardziej dokladnym jak dzialaja oba

\section{Models}

%We implemented and compared four models: Baseline MoE Transformer, Multi Head MoE, Lory for Expert Merging and the Combination of MH-MoE and Lory which we will call MH-Lory. We compared them based on model perplexity, numbers of parameters needed to achieve similar pre-training loss, computational and memory cost, inference and training time.
We implemented and evaluated four distinct models: the Baseline MoE Transformer, the Multi-Head MoE (MH-MoE), Lory for Expert Merging, and a hybrid model combining MH-MoE and Lory, referred to as MH-Lory. The models were compared based on several metrics, including model perplexity, the number of parameters required to achieve comparable pre-training loss, computational and memory costs, as well as inference and training times.
%As our computational resources were quite limited, we did not evaluate our models on any downstream tasks, rather checked their ability to predict next tokens from the dataset. As our baseline we chose simple MoE transformer.
Due to limitations in computational resources, we did not evaluate our models on any downstream tasks. Instead, we assessed their ability to predict the next tokens from the dataset. The simple MoE Transformer was selected as our baseline model for comparison.

\subsection{Hyperparameters}

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|r|r|}
\hline
Parameter & \textbf{MoE} & \textbf{MH-MoE} & \textbf{Lory} & \textbf{MH-Lory} \\
\hline
FFNs within layer & Data 2 & Data 3 & Data 4 & Data 5 \\
Transformer blocks & Data 7 & Data 8 & Data 9 & Data 10 \\
Hidden size  & Data 12 & Data 13 & Data 14 & Data 15 \\
Number of experts& Data 12 & Data 13 & Data 14 & Data 15 \\
Expert embedding dimension& Data 12 & Data 13 & Data 14 & Data 15 \\
\hline
Number of heads & - & Data 13 & - & Data 15 \\
Number of segments $T$ & - & - & d & d \\
Load balancing coefficient $\alpha$ & d & d & - & d \\
Capacity factor $c$ & 3 & 3 & - & - \\
\hline
\end{tabular}
\caption{Models' hyperparameters}
\label{tab:hyperparams_table}
\end{table}
The number of heads is specific to the MH-MoE and MH-Lory models, affecting their ability to attend to multiple positions simultaneously. The number of segments $T$ is a unique parameter for the Lory and MH-Lory models, determining the sequence's segmentation granularity. 
The load balancing coefficient $\alpha$ and capacity factor $c$ play crucial roles in distributing computational load among experts. 
For MoE and MH-MoE models, the loss function consists of a primary loss related to the target task and an auxiliary load balancing loss (\cite{wu2024multihead}). The load balancing loss is computed as follows:
\[ L_{balance} = \frac{N}{|\ddot{X}|} \sum_{p=1}^{N} \sum_{x_i^j \in \ddot{X}} t_p \cdot g(f_p^{FFN}) \]
where $N$ denotes the number of experts, $|\ddot{X}|$ is the number of sub-tokens contained in $\ddot{X}$. $g(f_p^{FFN})$ is the value of routing a certain sub-token $x_{i,j}$ into the $p^{th}$ expert.

 
\subsection{Training} 
The dataset was partitioned using an 80-10-10 train-validation-test split. Each model was trained for 10 epochs using the AdamW optimizer with a learning rate of $0.006$, betas set to $(0.9, 0.95)$, and a batch size of 20. For comparison purposes, the number of parameters in each model was standardized to 0.1 billion. The number of active parameters naturally differs. In the Lory model, each output is computed using weighted experts, meaning that all experts' embeddings are utilized during both the forward and backward passes. This resulted in prolonged training times for both the Lory and MH-Lory models. 
We tested our models using Cross-Entropy loss. The loss on both the training and validation sets was highly similar for each model, indicating that none of the models overfitted to the training data. 


\section{Results}

\subsection{Model comparison}
\subsubsection{Loss}
 PLOTS probne

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{val_val.png}
    \caption{Validation Loss Comparison for Models}
    \label{fig:validation_loss}
\end{figure}

The training and validation loss for are depicted in Figure \ref{fig:train_val_loss}, demonstrating the performance over epochs.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{train_val.png}
    \caption{Training and Validation Loss Comparison}
    \label{fig:train_val_loss}
\end{figure}

\subsubsection{Performance metrics}

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|r|r|}
\hline
Metric & \textbf{MoE} & \textbf{MH-MoE} & \textbf{Lory} & \textbf{MH-Lory} \\
\hline
Model Size & Data 2 & Data 3 & Data 4 & Data 5 \\
Training Time & Data 7 & Data 8 & Data 9 & Data 10 \\
Inference Time  & Data 12 & Data 13 & Data 14 & Data 15 \\
FLOPs & Data 12 & Data 13 & Data 14 & Data 15 \\
Memory Usage & Data 12 & Data 13 & Data 14 & Data 15 \\

\hline
\end{tabular}
\caption{Models' performance}
\label{tab:performance_table}
\end{table}

\section{Conclusions}



%


\printbibliography 

\appendix


\section{Additional Details}
The code for data processing, models and their training can be viewed on our github repository under https://github.com/
MichalMaszkowski/Multi-Headed Lory. We included not-vectorized version of MH-MoE, Lory and MH-Lory which we implemented to test our vectorized versions. 


% 
--------------------------------------------------------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------After MH-MoE layer comes merge layer in which outputs $O(h_p^j)$ are rearranged in the original order of sub-tokens. 

\end{document}