@misc{zhong2024lory,
      title={Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training}, 
      author={Zexuan Zhong and Mengzhou Xia and Danqi Chen and Mike Lewis},
      year={2024},
      eprint={2405.03133},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2024multihead,
      title={Multi-Head Mixture-of-Experts}, 
      author={Xun Wu and Shaohan Huang and Wenhui Wang and Furu Wei},
      year={2024},
      eprint={2404.15045},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{johnson2019billion,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and Jégou, Hervé},
  journal={Computer Vision and Pattern Recognition},
  year={2017},
  publisher={IEEE}
}

@article{izacard2022unsupervised,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={Transactions on Machine Learning Research (TMLR)},
  year={2022}
}


@misc{shazeer2017outrageously,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shi2024incontext,
      title={In-Context Pretraining: Language Modeling Beyond Document Boundaries}, 
      author={Weijia Shi and Sewon Min and Maria Lomeli and Chunting Zhou and Margaret Li and Gergely Szilvasy and Rich James and Xi Victoria Lin and Noah A. Smith and Luke Zettlemoyer and Scott Yih and Mike Lewis},
      year={2024},
      eprint={2310.10638},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{shi2023context,
  title={In-Context Pretraining: Language Modeling Beyond Document Boundaries},
  author={Shi, Weijia and Min, Sewon and Lomeli, Maria and Zhou, Chunting and Li, Margaret and Lin, Victoria and Smith, Noah A and Zettlemoyer, Luke and Yih, Scott and Lewis, Mike},
  journal={arXiv preprint arXiv:2310.10638},
  year={2023}
}

@article{muqeeth2023soft,
  title={Soft merging of experts with adaptive routing},
  author={Muqeeth, Mohammed and Liu, Haokun and Raffel, Colin},
  journal={arXiv preprint arXiv:2306.03745},
  year={2023}
}

@article{Puigcerver2023FromST,
  title={From Sparse to Soft Mixtures of Experts},
  author={Joan Puigcerver and Carlos Riquelme and Basil Mustafa and Neil Houlsby},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.00951},
  url={https://api.semanticscholar.org/CorpusID:260378993}
}

@ONLINE{wikidump,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org"
}