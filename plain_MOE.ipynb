{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Router, and a simple test of it (test is below configs)\n",
        "- MoE (naive version) - I also changed here how the experts look to simplify testing\n",
        "- VectorizedMoE\n",
        "- test of whether naive and vectorized version return the same outputs - Here I forcibly set the weights of vectorized one to be the same as in the naive moe\n",
        "- comparison of execution time of both versions of moe"
      ],
      "metadata": {
        "id": "WXTBinAuZ887"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to MoE"
      ],
      "metadata": {
        "id": "m_w7JfijN495"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/2101.03961.pdf\n",
        "\n",
        "https://arxiv.org/pdf/1701.06538.pdf"
      ],
      "metadata": {
        "id": "bF66o0v2Pjww"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_VRo3GfIFp"
      },
      "source": [
        "From Switch Transformer paper:\n",
        "\n",
        ">In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost.\n",
        "\n",
        "A vanilla Transformer block looks like this:\n",
        "\n",
        "```python\n",
        "class ModernTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, n_heads, up):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, n_heads)\n",
        "        self.mlp = nn.Sequential(\n",
        "            SwishGLU(embed_dim, embed_dim * up),\n",
        "            nn.Linear(embed_dim * up, embed_dim),\n",
        "        )\n",
        "        self.pre_attn_norm = RMSNorm(embed_dim)\n",
        "        self.pre_mlp_norm = RMSNorm(embed_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.pre_attn_norm(x))\n",
        "        x = x + self.mlp(self.pre_mlp_norm(x))\n",
        "        return x\n",
        "```\n",
        "\n",
        "The Mixture-of-Experts layer replaces the MLP layer. Instead of having one MLP layer, we have `num_experts` different MLP layers called *experts*.\n",
        "\n",
        "The idea is to process a contextualized token, by sending it to a subset of experts. In this way we could efficiently increase the number of parameters of the model without affecting computational cost too much.\n",
        "\n",
        "First, the token is fed into *router*, which determines to which experts a token should go to be processed. For computational reasons, there is a fixed limit on:\n",
        "* how many tokens an expert can process, and\n",
        "* by how many experts a token is processed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh3a3zPwfIFt"
      },
      "outputs": [],
      "source": [
        "%pip install torch_tb_profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrwQghs3fIFt"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from transformers import PretrainedConfig\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.intermediate_size, config.hidden_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMG2OUC1fIFu"
      },
      "source": [
        "# Router\n",
        "The router is a module which assigns tokens to experts. It answers two questions:\n",
        "1. Which tokens should be assigned to which expert.\n",
        "2. How much weight should be assigned to each expert. The weight is determined by similarity between the token embedding and the expert embedding\n",
        "\n",
        "The following conditions must be satisfied:\n",
        "1. The routing weights must sum to 1 for each token and be non-negative\n",
        "2. A token should have exactly `num_experts_per_token` non-zero weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyie1JaVfIFu"
      },
      "outputs": [],
      "source": [
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, num_experts] - expert routing weights\n",
        "class Router(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts = config.num_experts\n",
        "\n",
        "        self.expert_embeddings = nn.Parameter(torch.randn(self.num_experts, self.hidden_size)).to(config.device)\n",
        "        torch.nn.init.kaiming_uniform_(self.expert_embeddings, nonlinearity='linear')\n",
        "\n",
        "    def forward(self, x):\n",
        "        #my code{\n",
        "        dot = torch.einsum(\"bsh,eh->bse\", x, self.expert_embeddings)\n",
        "        top_k_out = torch.topk(dot, k=self.num_experts_per_token)\n",
        "        top_k = (float(\"-inf\") * torch.ones_like(dot)).scatter_(dim=-1, index=top_k_out.indices, src=top_k_out.values)\n",
        "        res = torch.nn.functional.softmax(top_k, dim=-1)\n",
        "        return res\n",
        "        #}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MoE"
      ],
      "metadata": {
        "id": "I2f_JMjMX5od"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJph0HYCfIFv"
      },
      "source": [
        "The MoE module is a module which wraps around a set of expert modules and a router module.\n",
        "\n",
        "It takes input embeddings and routes them to the experts.\n",
        "\n",
        "Each token is processed individually by a subset of experts.\n",
        "\n",
        "The output token embedding is a weighted sum of the expert outputs.\n",
        "\n",
        "The weights are determined by the router module.\n",
        "\n",
        "The subset of experts is determined by non-zero weights in the routing output.\n",
        "\n",
        "Additionally each expert might process at most `expert_capacity = ceil((batch_size * seq_len) / num_experts * capacity_factor)` tokens\n",
        "\n",
        "Superfluous tokens to be discarded by a particular expert should be selected uniformly at random.\n",
        "\n",
        "Discarding should be equivalent to setting the appropriate routing weight to 0, while other weights remain the same.\n",
        "\n",
        "This means that a token is processed by at most num_experts_per_token experts with a sum of weights of at most 1.\n",
        "\n",
        "Specifically, this could mean that a token is processed by 0 experts - in this case the resulting embedding should be a zero tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYWT1tWwfIFv"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, hidden_size] - output embeddings\n",
        "class MoE(nn.Module):\n",
        "    \"\"\"version which takes first not random tokens up to expert_capacity\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts = config.num_experts\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.capacity_factor = config.capacity_factor\n",
        "\n",
        "        # You can change experts representation if you want\n",
        "        #self.experts = nn.ModuleList([MLP(config) for _ in range(self.num_experts)])\n",
        "        #not as above but as below instead so as to compare more easily with the vectorized version\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "        self.first_linear = nn.Parameter(torch.randn(self.num_experts, self.intermediate_size, self.hidden_size)).to(config.device)\n",
        "        torch.nn.init.kaiming_uniform_(self.first_linear, nonlinearity='linear')\n",
        "        self.second_linear = nn.Parameter(torch.randn(self.num_experts, self.hidden_size, self.intermediate_size)).to(config.device)\n",
        "        torch.nn.init.kaiming_uniform_(self.second_linear, nonlinearity='linear')\n",
        "\n",
        "        self.router = Router(config)\n",
        "\n",
        "    def experts(self, nr, data):\n",
        "        return self.second_linear[nr] @ torch.nn.functional.relu(self.first_linear[nr] @ data)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        #assert hidden_size == self.hidden_size\n",
        "        expert_capacity = math.ceil(batch_size * seq_len / self.num_experts * self.capacity_factor)\n",
        "\n",
        "        result = torch.zeros_like(x)\n",
        "        weights = self.router(x)\n",
        "\n",
        "        for e in range(self.num_experts):\n",
        "            used = 0\n",
        "            for i in range(batch_size):\n",
        "                for j in range(seq_len):\n",
        "                    if (used < expert_capacity):\n",
        "                        w = weights[i, j, e]\n",
        "                        if(w > 0):\n",
        "                            used += 1\n",
        "                            result[i, j] += (w * self.experts(e, x[i, j]))\n",
        "\n",
        "\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Input: [batch_size, seq_len, hidden_size] - input embeddings\n",
        "# Output: [batch_size, seq_len, hidden_size] - output embeddings\n",
        "class VectorizedMoE(nn.Module):\n",
        "    \"\"\"version which takes first not random tokens up to expert_capacity\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_experts = config.num_experts\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_experts_per_token = config.num_experts_per_token\n",
        "        self.capacity_factor = config.capacity_factor\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "\n",
        "        # You can change experts representation if you want\n",
        "        self.first_linear = nn.Parameter(torch.randn(self.num_experts, self.intermediate_size, self.hidden_size)).to(config.device)\n",
        "        torch.nn.init.kaiming_uniform_(self.first_linear, nonlinearity='linear')\n",
        "        self.second_linear = nn.Parameter(torch.randn(self.num_experts, self.hidden_size, self.intermediate_size)).to(config.device)\n",
        "        torch.nn.init.kaiming_uniform_(self.second_linear, nonlinearity='linear')\n",
        "\n",
        "        self.router = Router(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        #assert hidden_size == self.hidden_size\n",
        "        expert_capacity = math.ceil(batch_size * seq_len / self.num_experts * self.capacity_factor)\n",
        "\n",
        "        weights = self.router(x) #[batch_size, seq_len, num_experts]\n",
        "\n",
        "        experts_where_ones = torch.where((weights <= 0), 0, 1) #ceiling of weights\n",
        "        experts_where_ones = torch.reshape(experts_where_ones, shape=(-1, self.num_experts)) #[num_of_tokens, num_experts]\n",
        "        capacity_aware_ones = torch.where((torch.cumsum(experts_where_ones, dim= 0) <= expert_capacity), input = experts_where_ones, other = 0)\n",
        "\n",
        "        # dec_seq = experts_where_ones.shape[0] - torch.arange(experts_where_ones.shape[0]).unsqueeze(dim = 1)\n",
        "        # numbered = (experts_where_ones * dec_seq)\n",
        "        # which = torch.topk(numbered, k=expert_capacity, dim = 0)\n",
        "        capacity_aware_weights = weights.reshape(shape=(-1, self.num_experts)) * capacity_aware_ones\n",
        "        which = torch.topk(capacity_aware_weights, k=expert_capacity, dim = 0)\n",
        "        indices = which.indices.transpose(1,0)\n",
        "        index = indices.reshape((-1))\n",
        "\n",
        "        tokens_for_experts = torch.index_select(input=x.reshape((-1, hidden_size)), dim=0, index=index) #[capacity*num_experts, hidden_size]\n",
        "        tokens_for_experts  = tokens_for_experts.reshape((self.num_experts, expert_capacity, hidden_size))\n",
        "        #now I have the proper input to the \"experts\", which I should process by first layer parameters\n",
        "\n",
        "        intermediate_result = torch.einsum(\"ech,eih->eci\", tokens_for_experts, self.first_linear)\n",
        "        intermediate_result = torch.nn.functional.relu(intermediate_result)\n",
        "        result = torch.einsum(\"eci,ehi->ech\", intermediate_result, self.second_linear)\n",
        "        #now tokens are processed by the \"experts\", I need to multiply by the weights and add them up\n",
        "\n",
        "        w = which.values.transpose(1,0).unsqueeze(-1)\n",
        "\n",
        "        result = result * w\n",
        "\n",
        "        final_result = torch.zeros_like(x).reshape((-1, hidden_size)).index_add_(dim = 0, index=index, source = result.reshape((-1, hidden_size)))\n",
        "\n",
        "        return final_result.reshape(x.shape)"
      ],
      "metadata": {
        "id": "oijb9KVoXRCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDRxR0HafIFw"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3qW0kGyOTRw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_0dW5DsfIFw"
      },
      "outputs": [],
      "source": [
        "base_config = dict(\n",
        "    vocab_size=5000,\n",
        "    max_position_embeddings=256,\n",
        "    num_attention_heads=8,\n",
        "    num_hidden_layers=4,\n",
        "    hidden_dropout_prob=0.1,\n",
        "    hidden_size=128,\n",
        "    intermediate_size=512,\n",
        "    num_labels=2,\n",
        "    device = DEVICE #I added this one\n",
        ")\n",
        "\n",
        "standard_config = PretrainedConfig(\n",
        "    **base_config,\n",
        "    ff_cls=MLP\n",
        ")\n",
        "\n",
        "moe_config = PretrainedConfig(\n",
        "    **base_config,\n",
        "    num_experts=4,\n",
        "    capacity_factor=2.0,\n",
        "    num_experts_per_token=1,\n",
        "    ff_cls=MoE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simple test of router:"
      ],
      "metadata": {
        "id": "HqvKdbAKQdSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#simple_test - there should be 2 nonzero entries in each row and they should add up to one\n",
        "(batch_size, seq_len, hidden_size) = (3, 5, moe_config.hidden_size)\n",
        "\n",
        "input = torch.randn((batch_size, seq_len, hidden_size))\n",
        "moe_config.num_experts_per_token = 2\n",
        "r = Router(moe_config)\n",
        "out = r(input)\n",
        "\n",
        "#do they add up to one?\n",
        "s = torch.sum(out, dim = -1)\n",
        "assert torch.all(torch.isclose(torch.ones_like(s), s)) #yes\n",
        "\n",
        "#visual inspection:\n",
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlC1wHcyq27H",
        "outputId": "aa003b00-9bb1-4b8c-f645-69e9162ea5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.1953, 0.8047, 0.0000],\n",
              "        [0.8655, 0.0000, 0.0000, 0.1345],\n",
              "        [0.5185, 0.0000, 0.4815, 0.0000],\n",
              "        [0.0000, 0.0613, 0.0000, 0.9387],\n",
              "        [0.0000, 0.5570, 0.0000, 0.4430]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing naive and vectorized versions:\n",
        "\"settings can be changed by passing a different config\""
      ],
      "metadata": {
        "id": "_qbnzyTuMqEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_result(config=moe_config, num=1, batch_size=20, seq_len=50):\n",
        "    vectorized_moe = VectorizedMoE(config)\n",
        "    naive_moe = MoE(config)\n",
        "    #setting weights of vectorized moe to the naive one's\n",
        "    vectorized_moe.first_linear = naive_moe.first_linear\n",
        "    vectorized_moe.second_linear = naive_moe.second_linear\n",
        "    vectorized_moe.router = naive_moe.router\n",
        "\n",
        "    hidden_size = config.hidden_size\n",
        "    input = torch.randn((batch_size, seq_len, hidden_size))\n",
        "\n",
        "    v = vectorized_moe(input)\n",
        "    n = naive_moe(input)\n",
        "\n",
        "    for i in range(num-1):\n",
        "        v = vectorized_moe(v)\n",
        "        n = naive_moe(n)\n",
        "    return torch.all(torch.isclose(n, v))\n",
        "\n",
        "compare_result(num=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jELCAXPwNNNH",
        "outputId": "fdd03d03-8563-4beb-8568-a6ec88db2188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def compare_time(config=moe_config, num=5, batch_size=20, seq_len=50):\n",
        "    vectorized_moe = VectorizedMoE(config)\n",
        "    naive_moe = MoE(config)\n",
        "\n",
        "    hidden_size = config.hidden_size\n",
        "    input = torch.randn((batch_size, seq_len, hidden_size))\n",
        "\n",
        "    durationV = 0\n",
        "    for i in range(num):\n",
        "        start_time = time.time()\n",
        "        vectorized_moe(input)\n",
        "        end_time = time.time()\n",
        "        durationV += end_time-start_time\n",
        "\n",
        "    durationN = 0\n",
        "    for i in range(num):\n",
        "        start_time = time.time()\n",
        "        naive_moe(input)\n",
        "        end_time = time.time()\n",
        "        durationN += end_time-start_time\n",
        "\n",
        "    return durationN/num, durationV/num\n",
        "compare_time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtIyuQsyVHPN",
        "outputId": "fad5b329-c6d5-4565-9854-b1674f02fc5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3744971752166748, 0.012471437454223633)"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "or using timeit:"
      ],
      "metadata": {
        "id": "_M9GXlQ4ZIoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "vectorized_moe = VectorizedMoE(config)\n",
        "naive_moe = MoE(config)\n",
        "\n",
        "(batch_size, seq_len, hidden_size) = (20, 50, config.hidden_size)\n",
        "input = torch.randn((batch_size, seq_len, hidden_size))"
      ],
      "metadata": {
        "id": "Y_TZFjVEW2Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit vectorized_moe(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTN_Y8k3W6vC",
        "outputId": "34a3895f-103f-46c2-e883-05a1fb283aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.07 ms ± 186 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit naive_moe(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gkn21_8XF_m",
        "outputId": "7e600b11-111d-405d-b7c6-6cc5fc405e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.81 ms ± 132 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-training",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m_w7JfijN495",
        "H2wknptcN1L1",
        "xiPtbDzMNv1y",
        "UMG2OUC1fIFu",
        "abAV-rA3fIFy",
        "bg4Jm-uFfIFy"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}